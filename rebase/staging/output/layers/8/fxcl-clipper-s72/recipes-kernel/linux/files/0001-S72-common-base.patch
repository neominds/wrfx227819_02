From ee4d51d83e44dc13da2fd38b298bfcd44688dd2a Mon Sep 17 00:00:00 2001
From: jthomas <jacob.thomas@windriver.com>
Date: Thu, 16 Feb 2017 14:03:18 +0000
Subject: [PATCH 01/34] 0001-S72-common-base


diff --git a/arch/arm/Kconfig b/arch/arm/Kconfig
index 7d6c4e3..b8d67c5 100644
--- a/arch/arm/Kconfig
+++ b/arch/arm/Kconfig
@@ -272,6 +272,9 @@ config GENERIC_BUG
 	def_bool y
 	depends on BUG
 
+config ARCH_HIBERNATION_POSSIBLE
+	def_bool y
+
 source "init/Kconfig"
 
 source "kernel/Kconfig.freezer"
@@ -1038,6 +1041,10 @@ source "arch/arm/mach-picoxcell/Kconfig"
 source "arch/arm/mach-pxa/Kconfig"
 source "arch/arm/plat-pxa/Kconfig"
 
+source "arch/arm/mach-mb8ac0300/Kconfig"
+
+source "arch/arm/mach-mb86s70/Kconfig"
+
 source "arch/arm/mach-mmp/Kconfig"
 
 source "arch/arm/mach-realview/Kconfig"
@@ -1495,6 +1502,7 @@ config PCI_HOST_ITE8152
 	select DMABOUNCE
 
 source "drivers/pci/Kconfig"
+source "drivers/pci/pcie/Kconfig"
 
 source "drivers/pcmcia/Kconfig"
 
@@ -1572,6 +1580,109 @@ config SCHED_SMT
 	  MultiThreading at a cost of slightly increased overhead in some
 	  places. If unsure say N here.
 
+config DISABLE_CPU_SCHED_DOMAIN_BALANCE
+	bool "(EXPERIMENTAL) Disable CPU level scheduler load-balancing"
+	help
+	  Disables scheduler load-balancing at CPU sched domain level.
+
+config SCHED_HMP
+	bool "(EXPERIMENTAL) Heterogenous multiprocessor scheduling"
+	depends on DISABLE_CPU_SCHED_DOMAIN_BALANCE && SCHED_MC && FAIR_GROUP_SCHED && !SCHED_AUTOGROUP
+	help
+	  Experimental scheduler optimizations for heterogeneous platforms.
+	  Attempts to introspectively select task affinity to optimize power
+	  and performance. Basic support for multiple (>2) cpu types is in place,
+	  but it has only been tested with two types of cpus.
+	  There is currently no support for migration of task groups, hence
+	  !SCHED_AUTOGROUP. Furthermore, normal load-balancing must be disabled
+	  between cpus of different type (DISABLE_CPU_SCHED_DOMAIN_BALANCE).
+	  When turned on, this option adds sys/kernel/hmp directory which
+	  contains the following files:
+	  up_threshold - the load average threshold used for up migration
+	                 (0 - 1023)
+	  down_threshold - the load average threshold used for down migration
+	                 (0 - 1023)
+	  hmp_domains - a list of cpumasks for the present HMP domains,
+	                starting with the 'biggest' and ending with the
+	                'smallest'.
+	  Note that both the threshold files can be written at runtime to
+	  control scheduler behaviour.
+
+config SCHED_HMP_PRIO_FILTER
+	bool "(EXPERIMENTAL) Filter HMP migrations by task priority"
+	depends on SCHED_HMP
+	help
+	  Enables task priority based HMP migration filter. Any task with
+	  a NICE value above the threshold will always be on low-power cpus
+	  with less compute capacity.
+
+config SCHED_HMP_PRIO_FILTER_VAL
+	int "NICE priority threshold"
+	default 5
+	depends on SCHED_HMP_PRIO_FILTER
+
+config HMP_FAST_CPU_MASK
+	string "HMP scheduler fast CPU mask"
+	depends on SCHED_HMP
+	help
+          Leave empty to use device tree information.
+	  Specify the cpuids of the fast CPUs in the system as a list string,
+	  e.g. cpuid 0+1 should be specified as 0-1.
+
+config HMP_SLOW_CPU_MASK
+	string "HMP scheduler slow CPU mask"
+	depends on SCHED_HMP
+	help
+	  Leave empty to use device tree information.
+	  Specify the cpuids of the slow CPUs in the system as a list string,
+	  e.g. cpuid 0+1 should be specified as 0-1.
+
+config HMP_VARIABLE_SCALE
+	bool "Allows changing the load tracking scale through sysfs"
+	depends on SCHED_HMP
+	help
+	  When turned on, this option exports the load average period value
+	  for the load tracking patches through sysfs.
+	  The values can be modified to change the rate of load accumulation
+	  used for HMP migration. 'load_avg_period_ms' is the time in ms to
+	  reach a load average of 0.5 for an idle task of 0 load average
+	  ratio which becomes 100% busy.
+	  For example, with load_avg_period_ms = 128 and up_threshold = 512,
+	  a running task with a load of 0 will be migrated to a bigger CPU after
+	  128ms, because after 128ms its load_avg_ratio is 0.5 and the real
+	  up_threshold is 0.5.
+	  This patch has the same behavior as changing the Y of the load
+	  average computation to
+	        (1002/1024)^(LOAD_AVG_PERIOD/load_avg_period_ms)
+	  but removes intermediate overflows in computation.
+
+config HMP_FREQUENCY_INVARIANT_SCALE
+	bool "(EXPERIMENTAL) Frequency-Invariant Tracked Load for HMP"
+	depends on SCHED_HMP && CPU_FREQ
+	help
+	  Scales the current load contribution in line with the frequency
+	  of the CPU that the task was executed on.
+	  In this version, we use a simple linear scale derived from the
+	  maximum frequency reported by CPUFreq.
+	  Restricting tracked load to be scaled by the CPU's frequency
+	  represents the consumption of possible compute capacity
+	  (rather than consumption of actual instantaneous capacity as
+	  normal) and allows the HMP migration's simple threshold
+	  migration strategy to interact more predictably with CPUFreq's
+	  asynchronous compute capacity changes.
+
+config SCHED_HMP_LITTLE_PACKING
+	bool "Small task packing for HMP"
+	depends on SCHED_HMP
+	default n
+	help
+	  Allows the HMP Scheduler to pack small tasks into CPUs in the
+	  smallest HMP domain.
+	  Controlled by two sysfs files in sys/kernel/hmp.
+	  packing_enable: 1 to enable, 0 to disable packing. Default 1.
+	  packing_limit: runqueue load ratio where a RQ is considered
+	    to be full. Default is NICE_0_LOAD * 9/8.
+
 config HAVE_ARM_SCU
 	bool
 	help
@@ -1599,6 +1710,31 @@ config MCPM
 	  for (multi-)cluster based systems, such as big.LITTLE based
 	  systems.
 
+config BIG_LITTLE
+	bool "big.LITTLE support (Experimental)"
+	depends on CPU_V7 && SMP
+	select MCPM
+	help
+	  This option enables support for the big.LITTLE architecture.
+
+config BL_SWITCHER
+	bool "big.LITTLE switcher support"
+	depends on BIG_LITTLE && MCPM && HOTPLUG_CPU
+	select CPU_PM
+	select ARM_CPU_SUSPEND
+	help
+	  The big.LITTLE "switcher" provides the core functionality to
+	  transparently handle transition between a cluster of A15's
+	  and a cluster of A7's in a big.LITTLE system.
+
+config BL_SWITCHER_DUMMY_IF
+	tristate "Simple big.LITTLE switcher user interface"
+	depends on BL_SWITCHER && DEBUG_KERNEL
+	help
+	  This is a simple and dummy char dev interface to control
+	  the big.LITTLE switcher core code.  It is meant for
+	  debugging purposes only.
+
 choice
 	prompt "Memory split"
 	default VMSPLIT_3G
@@ -1667,6 +1803,7 @@ config ARCH_NR_GPIO
 	default 288 if ARCH_SUNXI
 	default 264 if MACH_H4700
 	default 192 if SOC_AM43XX
+	default 96 if ARCH_MB8AC0300 || ARCH_MB86S70
 	default 0
 	help
 	  Maximum number of GPIOs in the system.
diff --git a/arch/arm/Kconfig.orig b/arch/arm/Kconfig.orig
new file mode 100644
index 0000000..7d6c4e3
--- /dev/null
+++ b/arch/arm/Kconfig.orig
@@ -0,0 +1,2334 @@
+config ARM
+	bool
+	default y
+	select ARCH_BINFMT_ELF_RANDOMIZE_PIE
+	select ARCH_HAS_ATOMIC64_DEC_IF_POSITIVE
+	select ARCH_HAVE_CUSTOM_GPIO_H if (!ARCH_ZYNQ)
+	select ARCH_SUPPORTS_ATOMIC_RMW
+	select ARCH_HAS_TICK_BROADCAST if GENERIC_CLOCKEVENTS_BROADCAST
+	select ARCH_WANT_IPC_PARSE_VERSION
+	select BUILDTIME_EXTABLE_SORT if MMU
+	select CPU_PM if (SUSPEND || CPU_IDLE)
+	select DCACHE_WORD_ACCESS if (CPU_V6 || CPU_V6K || CPU_V7) && !CPU_BIG_ENDIAN && MMU
+	select GENERIC_ATOMIC64 if (CPU_V6 || !CPU_32v6K || !AEABI)
+	select GENERIC_CLOCKEVENTS_BROADCAST if SMP
+	select GENERIC_IRQ_PROBE
+	select GENERIC_IRQ_SHOW
+	select GENERIC_PCI_IOMAP
+	select GENERIC_SMP_IDLE_THREAD
+	select GENERIC_IDLE_POLL_SETUP
+	select GENERIC_STRNCPY_FROM_USER
+	select GENERIC_STRNLEN_USER
+	select HARDIRQS_SW_RESEND
+	select IRQ_FORCED_THREADING
+	select HAVE_ARCH_JUMP_LABEL if !XIP_KERNEL
+	select HAVE_ARCH_KGDB
+	select HAVE_ARCH_SECCOMP_FILTER
+	select HAVE_ARCH_TRACEHOOK
+	select HAVE_BPF_JIT
+	select HAVE_C_RECORDMCOUNT
+	select HAVE_DEBUG_KMEMLEAK
+	select HAVE_DMA_API_DEBUG
+	select HAVE_DMA_ATTRS
+	select HAVE_DMA_CONTIGUOUS if MMU
+	select HAVE_DYNAMIC_FTRACE if (!XIP_KERNEL)
+	select HAVE_FTRACE_MCOUNT_RECORD if (!XIP_KERNEL)
+	select HAVE_FUNCTION_GRAPH_TRACER if (!THUMB2_KERNEL)
+	select HAVE_FUNCTION_TRACER if (!XIP_KERNEL)
+	select HAVE_GENERIC_DMA_COHERENT
+	select HAVE_GENERIC_HARDIRQS
+	select HAVE_HW_BREAKPOINT if (PERF_EVENTS && (CPU_V6 || CPU_V6K || CPU_V7))
+	select HAVE_IDE if PCI || ISA || PCMCIA
+	select HAVE_IRQ_TIME_ACCOUNTING
+	select HAVE_KERNEL_GZIP
+	select HAVE_KERNEL_LZMA
+	select HAVE_KERNEL_LZO
+	select HAVE_KERNEL_XZ
+	select HAVE_KPROBES if !XIP_KERNEL
+	select HAVE_KRETPROBES if (HAVE_KPROBES)
+	select HAVE_MEMBLOCK
+	select HAVE_OPROFILE if (HAVE_PERF_EVENTS)
+	select HAVE_PERF_EVENTS
+	select HAVE_PREEMPT_LAZY
+	select HAVE_REGS_AND_STACK_ACCESS_API
+	select HAVE_SYSCALL_TRACEPOINTS
+	select HAVE_UID16
+	select KTIME_SCALAR
+	select PERF_USE_VMALLOC
+	select RTC_LIB
+	select SYS_SUPPORTS_APM_EMULATION
+	select HAVE_MOD_ARCH_SPECIFIC if ARM_UNWIND
+	select MODULES_USE_ELF_REL
+	select CLONE_BACKWARDS
+	select OLD_SIGSUSPEND3
+	select OLD_SIGACTION
+	select HAVE_CONTEXT_TRACKING
+	help
+	  The ARM series is a line of low-power-consumption RISC chip designs
+	  licensed by ARM Ltd and targeted at embedded applications and
+	  handhelds such as the Compaq IPAQ.  ARM-based PCs are no longer
+	  manufactured, but legacy ARM-based PC hardware remains popular in
+	  Europe.  There is an ARM Linux project with a web page at
+	  <http://www.arm.linux.org.uk/>.
+
+config ARM_HAS_SG_CHAIN
+	bool
+
+config NEED_SG_DMA_LENGTH
+	bool
+
+config ARM_DMA_USE_IOMMU
+	bool
+	select ARM_HAS_SG_CHAIN
+	select NEED_SG_DMA_LENGTH
+
+if ARM_DMA_USE_IOMMU
+
+config ARM_DMA_IOMMU_ALIGNMENT
+	int "Maximum PAGE_SIZE order of alignment for DMA IOMMU buffers"
+	range 4 9
+	default 8
+	help
+	  DMA mapping framework by default aligns all buffers to the smallest
+	  PAGE_SIZE order which is greater than or equal to the requested buffer
+	  size. This works well for buffers up to a few hundreds kilobytes, but
+	  for larger buffers it just a waste of address space. Drivers which has
+	  relatively small addressing window (like 64Mib) might run out of
+	  virtual space with just a few allocations.
+
+	  With this parameter you can specify the maximum PAGE_SIZE order for
+	  DMA IOMMU buffers. Larger buffers will be aligned only to this
+	  specified order. The order is expressed as a power of two multiplied
+	  by the PAGE_SIZE.
+
+endif
+
+config HAVE_PWM
+	bool
+
+config MIGHT_HAVE_PCI
+	bool
+
+config SYS_SUPPORTS_APM_EMULATION
+	bool
+
+config HAVE_TCM
+	bool
+	select GENERIC_ALLOCATOR
+
+config HAVE_PROC_CPU
+	bool
+
+config NO_IOPORT
+	bool
+
+config EISA
+	bool
+	---help---
+	  The Extended Industry Standard Architecture (EISA) bus was
+	  developed as an open alternative to the IBM MicroChannel bus.
+
+	  The EISA bus provided some of the features of the IBM MicroChannel
+	  bus while maintaining backward compatibility with cards made for
+	  the older ISA bus.  The EISA bus saw limited use between 1988 and
+	  1995 when it was made obsolete by the PCI bus.
+
+	  Say Y here if you are building a kernel for an EISA-based machine.
+
+	  Otherwise, say N.
+
+config SBUS
+	bool
+
+config STACKTRACE_SUPPORT
+	bool
+	default y
+
+config HAVE_LATENCYTOP_SUPPORT
+	bool
+	depends on !SMP
+	default y
+
+config LOCKDEP_SUPPORT
+	bool
+	default y
+
+config TRACE_IRQFLAGS_SUPPORT
+	bool
+	default y
+
+config RWSEM_GENERIC_SPINLOCK
+	bool
+	default y
+
+config RWSEM_XCHGADD_ALGORITHM
+	bool
+
+config ARCH_HAS_ILOG2_U32
+	bool
+
+config ARCH_HAS_ILOG2_U64
+	bool
+
+config ARCH_HAS_CPUFREQ
+	bool
+	help
+	  Internal node to signify that the ARCH has CPUFREQ support
+	  and that the relevant menu configurations are displayed for
+	  it.
+
+config GENERIC_HWEIGHT
+	bool
+	default y
+
+config GENERIC_CALIBRATE_DELAY
+	bool
+	default y
+
+config ARCH_MAY_HAVE_PC_FDC
+	bool
+
+config ZONE_DMA
+	bool
+
+config DMA_NONCOHERENT
+	bool
+	select NEED_DMA_MAP_STATE
+
+config NEED_DMA_MAP_STATE
+       def_bool y
+
+config ARCH_HAS_DMA_SET_COHERENT_MASK
+	bool
+
+config GENERIC_ISA_DMA
+	bool
+
+config FIQ
+	bool
+
+config NEED_RET_TO_USER
+	bool
+
+config ARCH_MTD_XIP
+	bool
+
+config VECTORS_BASE
+	hex
+	default 0xffff0000 if MMU || CPU_HIGH_VECTOR
+	default DRAM_BASE if REMAP_VECTORS_TO_RAM
+	default 0x00000000
+	help
+	  The base address of exception vectors.  This must be two pages
+	  in size.
+
+config ARM_PATCH_PHYS_VIRT
+	bool "Patch physical to virtual translations at runtime" if EMBEDDED
+	default y
+	depends on !XIP_KERNEL && MMU
+	depends on !ARCH_REALVIEW || !SPARSEMEM
+	help
+	  Patch phys-to-virt and virt-to-phys translation functions at
+	  boot and module load time according to the position of the
+	  kernel in system memory.
+
+	  This can only be used with non-XIP MMU kernels where the base
+	  of physical memory is at a 16MB boundary.
+
+	  Only disable this option if you know that you do not require
+	  this feature (eg, building a kernel for a single machine) and
+	  you need to shrink the kernel to the minimal size.
+
+config NEED_MACH_GPIO_H
+	bool
+	help
+	  Select this when mach/gpio.h is required to provide special
+	  definitions for this platform. The need for mach/gpio.h should
+	  be avoided when possible.
+
+config NEED_MACH_IO_H
+	bool
+	help
+	  Select this when mach/io.h is required to provide special
+	  definitions for this platform.  The need for mach/io.h should
+	  be avoided when possible.
+
+config NEED_MACH_MEMORY_H
+	bool
+	help
+	  Select this when mach/memory.h is required to provide special
+	  definitions for this platform.  The need for mach/memory.h should
+	  be avoided when possible.
+
+config PHYS_OFFSET
+	hex "Physical address of main memory" if MMU
+	depends on !ARM_PATCH_PHYS_VIRT && !NEED_MACH_MEMORY_H
+	default DRAM_BASE if !MMU
+	help
+	  Please provide the physical address corresponding to the
+	  location of main memory in your system.
+
+config GENERIC_BUG
+	def_bool y
+	depends on BUG
+
+source "init/Kconfig"
+
+source "kernel/Kconfig.freezer"
+
+menu "System Type"
+
+config MMU
+	bool "MMU-based Paged Memory Management Support"
+	default y
+	help
+	  Select if you want MMU-based virtualised addressing space
+	  support by paged memory management. If unsure, say 'Y'.
+
+#
+# The "ARM system type" choice list is ordered alphabetically by option
+# text.  Please add new entries in the option alphabetic order.
+#
+choice
+	prompt "ARM system type"
+	default ARCH_VERSATILE if !MMU
+	default ARCH_MULTIPLATFORM if MMU
+
+config ARCH_MULTIPLATFORM
+	bool "Allow multiple platforms to be selected"
+	depends on MMU
+	select ARM_PATCH_PHYS_VIRT
+	select AUTO_ZRELADDR
+	select COMMON_CLK
+	select MULTI_IRQ_HANDLER
+	select SPARSE_IRQ
+	select USE_OF
+
+config ARCH_INTEGRATOR
+	bool "ARM Ltd. Integrator family"
+	select ARCH_HAS_CPUFREQ
+	select ARM_AMBA
+	select COMMON_CLK
+	select COMMON_CLK_VERSATILE
+	select GENERIC_CLOCKEVENTS
+	select HAVE_TCM
+	select ICST
+	select MULTI_IRQ_HANDLER
+	select NEED_MACH_MEMORY_H
+	select PLAT_VERSATILE
+	select SPARSE_IRQ
+	select VERSATILE_FPGA_IRQ
+	help
+	  Support for ARM's Integrator platform.
+
+config ARCH_REALVIEW
+	bool "ARM Ltd. RealView family"
+	select ARCH_WANT_OPTIONAL_GPIOLIB
+	select ARM_AMBA
+	select ARM_TIMER_SP804
+	select COMMON_CLK
+	select COMMON_CLK_VERSATILE
+	select GENERIC_CLOCKEVENTS
+	select GPIO_PL061 if GPIOLIB
+	select ICST
+	select NEED_MACH_MEMORY_H
+	select PLAT_VERSATILE
+	select PLAT_VERSATILE_CLCD
+	help
+	  This enables support for ARM Ltd RealView boards.
+
+config ARCH_VERSATILE
+	bool "ARM Ltd. Versatile family"
+	select ARCH_WANT_OPTIONAL_GPIOLIB
+	select ARM_AMBA
+	select ARM_TIMER_SP804
+	select ARM_VIC
+	select CLKDEV_LOOKUP
+	select GENERIC_CLOCKEVENTS
+	select HAVE_MACH_CLKDEV
+	select ICST
+	select PLAT_VERSATILE
+	select PLAT_VERSATILE_CLCD
+	select PLAT_VERSATILE_CLOCK
+	select VERSATILE_FPGA_IRQ
+	help
+	  This enables support for ARM Ltd Versatile board.
+
+config ARCH_AT91
+	bool "Atmel AT91"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select HAVE_CLK
+	select IRQ_DOMAIN
+	select NEED_MACH_GPIO_H
+	select NEED_MACH_IO_H if PCCARD
+	select PINCTRL
+	select PINCTRL_AT91 if USE_OF
+	help
+	  This enables support for systems based on Atmel
+	  AT91RM9200 and AT91SAM9* processors.
+
+config ARCH_KEYSTONE
+	bool "Texas Instruments Keystone Devices"
+	select ARCH_REQUIRE_GPIOLIB
+	select ARM_GIC
+	select MULTI_IRQ_HANDLER
+	select CLKDEV_LOOKUP
+	select COMMON_CLK
+	select CLK_KEYSTONE_PLL
+	select DAVINCI_CLKS
+	select CLKSRC_MMIO
+	select CPU_V7
+	select GENERIC_CLOCKEVENTS
+	select USE_OF
+	select SPARSE_IRQ
+	select NEED_MACH_MEMORY_H
+	select HAVE_SCHED_CLOCK
+	select HAVE_SMP
+	select ZONE_DMA if ARM_LPAE
+	select TI_KEYSTONE
+	help
+	  Support for boards based on the Texas Instruments Keystone family of
+	  SoCs.
+
+config ARCH_CLPS711X
+	bool "Cirrus Logic CLPS711x/EP721x/EP731x-based"
+	select ARCH_REQUIRE_GPIOLIB
+	select AUTO_ZRELADDR
+	select CLKDEV_LOOKUP
+	select COMMON_CLK
+	select CPU_ARM720T
+	select GENERIC_CLOCKEVENTS
+	select MULTI_IRQ_HANDLER
+	select NEED_MACH_MEMORY_H
+	select SPARSE_IRQ
+	help
+	  Support for Cirrus Logic 711x/721x/731x based boards.
+
+config ARCH_GEMINI
+	bool "Cortina Systems Gemini"
+	select ARCH_REQUIRE_GPIOLIB
+	select ARCH_USES_GETTIMEOFFSET
+	select NEED_MACH_GPIO_H
+	select CPU_FA526
+	help
+	  Support for the Cortina Systems Gemini family SoCs
+
+config ARCH_EBSA110
+	bool "EBSA-110"
+	select ARCH_USES_GETTIMEOFFSET
+	select CPU_SA110
+	select ISA
+	select NEED_MACH_IO_H
+	select NEED_MACH_MEMORY_H
+	select NO_IOPORT
+	help
+	  This is an evaluation board for the StrongARM processor available
+	  from Digital. It has limited hardware on-board, including an
+	  Ethernet interface, two PCMCIA sockets, two serial ports and a
+	  parallel port.
+
+config ARCH_EP93XX
+	bool "EP93xx-based"
+	select ARCH_HAS_HOLES_MEMORYMODEL
+	select ARCH_REQUIRE_GPIOLIB
+	select ARCH_USES_GETTIMEOFFSET
+	select ARM_AMBA
+	select ARM_VIC
+	select CLKDEV_LOOKUP
+	select CPU_ARM920T
+	select NEED_MACH_MEMORY_H
+	help
+	  This enables support for the Cirrus EP93xx series of CPUs.
+
+config ARCH_FOOTBRIDGE
+	bool "FootBridge"
+	select CPU_SA110
+	select FOOTBRIDGE
+	select GENERIC_CLOCKEVENTS
+	select HAVE_IDE
+	select NEED_MACH_IO_H if !MMU
+	select NEED_MACH_MEMORY_H
+	help
+	  Support for systems based on the DC21285 companion chip
+	  ("FootBridge"), such as the Simtec CATS and the Rebel NetWinder.
+
+config ARCH_NETX
+	bool "Hilscher NetX based"
+	select ARM_VIC
+	select CLKSRC_MMIO
+	select CPU_ARM926T
+	select GENERIC_CLOCKEVENTS
+	help
+	  This enables support for systems based on the Hilscher NetX Soc
+
+config ARCH_IOP13XX
+	bool "IOP13xx-based"
+	depends on MMU
+	select ARCH_SUPPORTS_MSI
+	select CPU_XSC3
+	select NEED_MACH_MEMORY_H
+	select NEED_RET_TO_USER
+	select PCI
+	select PLAT_IOP
+	select VMSPLIT_1G
+	help
+	  Support for Intel's IOP13XX (XScale) family of processors.
+
+config ARCH_IOP32X
+	bool "IOP32x-based"
+	depends on MMU
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_XSCALE
+	select NEED_MACH_GPIO_H
+	select NEED_RET_TO_USER
+	select PCI
+	select PLAT_IOP
+	help
+	  Support for Intel's 80219 and IOP32X (XScale) family of
+	  processors.
+
+config ARCH_IOP33X
+	bool "IOP33x-based"
+	depends on MMU
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_XSCALE
+	select NEED_MACH_GPIO_H
+	select NEED_RET_TO_USER
+	select PCI
+	select PLAT_IOP
+	help
+	  Support for Intel's IOP33X (XScale) family of processors.
+
+config ARCH_IXP4XX
+	bool "IXP4xx-based"
+	depends on MMU
+	select ARCH_HAS_DMA_SET_COHERENT_MASK
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKSRC_MMIO
+	select CPU_XSCALE
+	select DMABOUNCE if PCI
+	select GENERIC_CLOCKEVENTS
+	select MIGHT_HAVE_PCI
+	select NEED_MACH_IO_H
+	select USB_EHCI_BIG_ENDIAN_MMIO
+	select USB_EHCI_BIG_ENDIAN_DESC
+	help
+	  Support for Intel's IXP4XX (XScale) family of processors.
+
+config ARCH_DOVE
+	bool "Marvell Dove"
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_PJ4
+	select GENERIC_CLOCKEVENTS
+	select MIGHT_HAVE_PCI
+	select PINCTRL
+	select PINCTRL_DOVE
+	select PLAT_ORION_LEGACY
+	select USB_ARCH_HAS_EHCI
+	select MVEBU_MBUS
+	help
+	  Support for the Marvell Dove SoC 88AP510
+
+config ARCH_KIRKWOOD
+	bool "Marvell Kirkwood"
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_FEROCEON
+	select GENERIC_CLOCKEVENTS
+	select PCI
+	select PCI_QUIRKS
+	select PINCTRL
+	select PINCTRL_KIRKWOOD
+	select PLAT_ORION_LEGACY
+	select MVEBU_MBUS
+	help
+	  Support for the following Marvell Kirkwood series SoCs:
+	  88F6180, 88F6192 and 88F6281.
+
+config ARCH_MV78XX0
+	bool "Marvell MV78xx0"
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_FEROCEON
+	select GENERIC_CLOCKEVENTS
+	select PCI
+	select PLAT_ORION_LEGACY
+	select MVEBU_MBUS
+	help
+	  Support for the following Marvell MV78xx0 series SoCs:
+	  MV781x0, MV782x0.
+
+config ARCH_ORION5X
+	bool "Marvell Orion"
+	depends on MMU
+	select ARCH_REQUIRE_GPIOLIB
+	select CPU_FEROCEON
+	select GENERIC_CLOCKEVENTS
+	select PCI
+	select PLAT_ORION_LEGACY
+	select MVEBU_MBUS
+	help
+	  Support for the following Marvell Orion 5x series SoCs:
+	  Orion-1 (5181), Orion-VoIP (5181L), Orion-NAS (5182),
+	  Orion-2 (5281), Orion-1-90 (6183).
+
+config ARCH_MMP
+	bool "Marvell PXA168/910/MMP2"
+	depends on MMU
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select GENERIC_ALLOCATOR
+	select GENERIC_CLOCKEVENTS
+	select GPIO_PXA
+	select IRQ_DOMAIN
+	select NEED_MACH_GPIO_H
+	select PINCTRL
+	select PLAT_PXA
+	select SPARSE_IRQ
+	help
+	  Support for Marvell's PXA168/PXA910(MMP) and MMP2 processor line.
+
+config ARCH_KS8695
+	bool "Micrel/Kendin KS8695"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKSRC_MMIO
+	select CPU_ARM922T
+	select GENERIC_CLOCKEVENTS
+	select NEED_MACH_MEMORY_H
+	help
+	  Support for Micrel/Kendin KS8695 "Centaur" (ARM922T) based
+	  System-on-Chip devices.
+
+config ARCH_W90X900
+	bool "Nuvoton W90X900 CPU"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_ARM926T
+	select GENERIC_CLOCKEVENTS
+	help
+	  Support for Nuvoton (Winbond logic dept.) ARM9 processor,
+	  At present, the w90x900 has been renamed nuc900, regarding
+	  the ARM series product line, you can login the following
+	  link address to know more.
+
+	  <http://www.nuvoton.com/hq/enu/ProductAndSales/ProductLines/
+		ConsumerElectronicsIC/ARMMicrocontroller/ARMMicrocontroller>
+
+config ARCH_LPC32XX
+	bool "NXP LPC32XX"
+	select ARCH_REQUIRE_GPIOLIB
+	select ARM_AMBA
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_ARM926T
+	select GENERIC_CLOCKEVENTS
+	select HAVE_IDE
+	select HAVE_PWM
+	select USB_ARCH_HAS_OHCI
+	select USE_OF
+	help
+	  Support for the NXP LPC32XX family of processors
+
+config ARCH_PXA
+	bool "PXA2xx/PXA3xx-based"
+	depends on MMU
+	select ARCH_HAS_CPUFREQ
+	select ARCH_MTD_XIP
+	select ARCH_REQUIRE_GPIOLIB
+	select ARM_CPU_SUSPEND if PM
+	select AUTO_ZRELADDR
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select GENERIC_CLOCKEVENTS
+	select GPIO_PXA
+	select HAVE_IDE
+	select MULTI_IRQ_HANDLER
+	select NEED_MACH_GPIO_H
+	select PLAT_PXA
+	select SPARSE_IRQ
+	help
+	  Support for Intel/Marvell's PXA2xx/PXA3xx processor line.
+
+config ARCH_MSM
+	bool "Qualcomm MSM"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	help
+	  Support for Qualcomm MSM/QSD based systems.  This runs on the
+	  apps processor of the MSM/QSD and depends on a shared memory
+	  interface to the modem processor which runs the baseband
+	  stack and controls some vital subsystems
+	  (clock and power control, etc).
+
+config ARCH_SHMOBILE_LEGACY
+	bool "Renesas ARM SoCs (non-multiplatform)"
+	select ARCH_SHMOBILE
+	select ARM_PATCH_PHYS_VIRT
+	select CLKDEV_LOOKUP
+	select GENERIC_CLOCKEVENTS
+	select HAVE_ARM_SCU if SMP
+	select HAVE_ARM_TWD if LOCAL_TIMERS
+	select HAVE_CLK
+	select HAVE_MACH_CLKDEV
+	select HAVE_SMP
+	select MIGHT_HAVE_CACHE_L2X0
+	select MULTI_IRQ_HANDLER
+	select NO_IOPORT
+	select PINCTRL if ARCH_WANT_OPTIONAL_GPIOLIB
+	select PM_GENERIC_DOMAINS if PM
+	select SPARSE_IRQ
+	help
+	  Support for Renesas ARM SoC platforms using a non-multiplatform
+	  kernel. This includes the SH-Mobile, R-Mobile, EMMA-Mobile, R-Car
+	  and RZ families.
+
+config ARCH_RPC
+	bool "RiscPC"
+	select ARCH_ACORN
+	select ARCH_MAY_HAVE_PC_FDC
+	select ARCH_SPARSEMEM_ENABLE
+	select ARCH_USES_GETTIMEOFFSET
+	select FIQ
+	select HAVE_IDE
+	select HAVE_PATA_PLATFORM
+	select ISA_DMA_API
+	select NEED_MACH_IO_H
+	select NEED_MACH_MEMORY_H
+	select NO_IOPORT
+	select VIRT_TO_BUS
+	help
+	  On the Acorn Risc-PC, Linux can support the internal IDE disk and
+	  CD-ROM interface, serial and parallel port, and the floppy drive.
+
+config ARCH_SA1100
+	bool "SA1100-based"
+	select ARCH_HAS_CPUFREQ
+	select ARCH_MTD_XIP
+	select ARCH_REQUIRE_GPIOLIB
+	select ARCH_SPARSEMEM_ENABLE
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_FREQ
+	select CPU_SA1100
+	select GENERIC_CLOCKEVENTS
+	select HAVE_IDE
+	select ISA
+	select NEED_MACH_GPIO_H
+	select NEED_MACH_MEMORY_H
+	select SPARSE_IRQ
+	help
+	  Support for StrongARM 11x0 based boards.
+
+config ARCH_S3C24XX
+	bool "Samsung S3C24XX SoCs"
+	select ARCH_HAS_CPUFREQ
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_S3C_RTC if RTC_CLASS
+	select MULTI_IRQ_HANDLER
+	select NEED_MACH_GPIO_H
+	select NEED_MACH_IO_H
+	help
+	  Samsung S3C2410, S3C2412, S3C2413, S3C2416, S3C2440, S3C2442, S3C2443
+	  and S3C2450 SoCs based systems, such as the Simtec Electronics BAST
+	  (<http://www.simtec.co.uk/products/EB110ITX/>), the IPAQ 1940 or the
+	  Samsung SMDK2410 development board (and derivatives).
+
+config ARCH_S3C64XX
+	bool "Samsung S3C64XX"
+	select ARCH_HAS_CPUFREQ
+	select ARCH_REQUIRE_GPIOLIB
+	select ARM_VIC
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_V6
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_TCM
+	select NEED_MACH_GPIO_H
+	select NO_IOPORT
+	select PLAT_SAMSUNG
+	select S3C_DEV_NAND
+	select S3C_GPIO_TRACK
+	select SAMSUNG_CLKSRC
+	select SAMSUNG_GPIOLIB_4BIT
+	select SAMSUNG_IRQ_VIC_TIMER
+	select USB_ARCH_HAS_OHCI
+	help
+	  Samsung S3C64XX series based systems
+
+config ARCH_S5P64X0
+	bool "Samsung S5P6440 S5P6450"
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_V6
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_S3C_RTC if RTC_CLASS
+	select NEED_MACH_GPIO_H
+	help
+	  Samsung S5P64X0 CPU based systems, such as the Samsung SMDK6440,
+	  SMDK6450.
+
+config ARCH_S5PC100
+	bool "Samsung S5PC100"
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_V7
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_S3C_RTC if RTC_CLASS
+	select NEED_MACH_GPIO_H
+	help
+	  Samsung S5PC100 series based systems
+
+config ARCH_S5PV210
+	bool "Samsung S5PV210/S5PC110"
+	select ARCH_HAS_CPUFREQ
+	select ARCH_HAS_HOLES_MEMORYMODEL
+	select ARCH_SPARSEMEM_ENABLE
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select CPU_V7
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_S3C_RTC if RTC_CLASS
+	select NEED_MACH_GPIO_H
+	select NEED_MACH_MEMORY_H
+	help
+	  Samsung S5PV210/S5PC110 series based systems
+
+config ARCH_EXYNOS
+	bool "Samsung EXYNOS"
+	select ARCH_HAS_CPUFREQ
+	select ARCH_HAS_HOLES_MEMORYMODEL
+	select ARCH_SPARSEMEM_ENABLE
+	select CLKDEV_LOOKUP
+	select COMMON_CLK
+	select CPU_V7
+	select GENERIC_CLOCKEVENTS
+	select HAVE_CLK
+	select HAVE_S3C2410_I2C if I2C
+	select HAVE_S3C2410_WATCHDOG if WATCHDOG
+	select HAVE_S3C_RTC if RTC_CLASS
+	select NEED_MACH_GPIO_H
+	select NEED_MACH_MEMORY_H
+	help
+	  Support for SAMSUNG's EXYNOS SoCs (EXYNOS4/5)
+
+config ARCH_SHARK
+	bool "Shark"
+	select ARCH_USES_GETTIMEOFFSET
+	select CPU_SA110
+	select ISA
+	select ISA_DMA
+	select NEED_MACH_MEMORY_H
+	select PCI
+	select VIRT_TO_BUS
+	select ZONE_DMA
+	help
+	  Support for the StrongARM based Digital DNARD machine, also known
+	  as "Shark" (<http://www.shark-linux.de/shark.html>).
+
+config ARCH_U300
+	bool "ST-Ericsson U300 Series"
+	depends on MMU
+	select ARCH_REQUIRE_GPIOLIB
+	select ARM_AMBA
+	select ARM_PATCH_PHYS_VIRT
+	select ARM_VIC
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select COMMON_CLK
+	select CPU_ARM926T
+	select GENERIC_CLOCKEVENTS
+	select HAVE_TCM
+	select SPARSE_IRQ
+	help
+	  Support for ST-Ericsson U300 series mobile platforms.
+
+config ARCH_DAVINCI
+	bool "TI DaVinci"
+	select ARCH_HAS_HOLES_MEMORYMODEL
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select GENERIC_ALLOCATOR
+	select GENERIC_CLOCKEVENTS
+	select GENERIC_IRQ_CHIP
+	select HAVE_IDE
+	select NEED_MACH_GPIO_H
+	select USE_OF
+	select ZONE_DMA
+	help
+	  Support for TI's DaVinci platform.
+
+config ARCH_OMAP1
+	bool "TI OMAP1"
+	depends on MMU
+	select ARCH_HAS_CPUFREQ
+	select ARCH_HAS_HOLES_MEMORYMODEL
+	select ARCH_OMAP
+	select ARCH_REQUIRE_GPIOLIB
+	select CLKDEV_LOOKUP
+	select CLKSRC_MMIO
+	select GENERIC_CLOCKEVENTS
+	select GENERIC_IRQ_CHIP
+	select HAVE_CLK
+	select HAVE_IDE
+	select IRQ_DOMAIN
+	select NEED_MACH_IO_H if PCCARD
+	select NEED_MACH_MEMORY_H
+	help
+	  Support for older TI OMAP1 (omap7xx, omap15xx or omap16xx)
+
+config ARCH_IPROC
+	bool "Broadcom ARMv7 iProc boards"
+	depends on MMU
+	select CPU_V7
+	select HAVE_CLK
+	select HAVE_SMP
+	select HAVE_MACH_CLKDEV
+	select COMMON_CLKDEV
+	select CLKDEV_LOOKUP
+	select ARM_GIC
+	select HAVE_ARM_TWD
+	select HAVE_ARM_SCU
+	select GENERIC_CLOCKEVENTS_BUILD
+	select GENERIC_CLOCKEVENTS
+	select LOCAL_TIMERS
+	select PCI
+	select GENERIC_GPIO
+	select ARCH_REQUIRE_GPIOLIB
+	select CACHE_L2X0
+	select ARM_AMBA
+	help
+	 This is a common family of Broadcom Cortex A9 based boards
+endchoice
+
+menu "Multiple platform selection"
+	depends on ARCH_MULTIPLATFORM
+
+comment "CPU Core family selection"
+
+config ARCH_MULTI_V4
+	bool "ARMv4 based platforms (FA526, StrongARM)"
+	depends on !ARCH_MULTI_V6_V7
+	select ARCH_MULTI_V4_V5
+
+config ARCH_MULTI_V4T
+	bool "ARMv4T based platforms (ARM720T, ARM920T, ...)"
+	depends on !ARCH_MULTI_V6_V7
+	select ARCH_MULTI_V4_V5
+
+config ARCH_MULTI_V5
+	bool "ARMv5 based platforms (ARM926T, XSCALE, PJ1, ...)"
+	depends on !ARCH_MULTI_V6_V7
+	select ARCH_MULTI_V4_V5
+
+config ARCH_MULTI_V4_V5
+	bool
+
+config ARCH_MULTI_V6
+	bool "ARMv6 based platforms (ARM11)"
+	select ARCH_MULTI_V6_V7
+	select CPU_V6
+
+config ARCH_MULTI_V7
+	bool "ARMv7 based platforms (Cortex-A, PJ4, Scorpion, Krait)"
+	default y
+	select ARCH_MULTI_V6_V7
+	select CPU_V7
+
+config ARCH_MULTI_V6_V7
+	bool
+
+config ARCH_MULTI_CPU_AUTO
+	def_bool !(ARCH_MULTI_V4 || ARCH_MULTI_V4T || ARCH_MULTI_V6_V7)
+	select ARCH_MULTI_V5
+
+endmenu
+
+#
+# This is sorted alphabetically by mach-* pathname.  However, plat-*
+# Kconfigs may be included either alphabetically (according to the
+# plat- suffix) or along side the corresponding mach-* source.
+#
+source "arch/arm/mach-mvebu/Kconfig"
+
+source "arch/arm/mach-at91/Kconfig"
+
+source "arch/arm/mach-bcm/Kconfig"
+
+source "arch/arm/mach-bcm2835/Kconfig"
+
+source "arch/arm/mach-clps711x/Kconfig"
+
+source "arch/arm/mach-cns3xxx/Kconfig"
+
+source "arch/arm/mach-davinci/Kconfig"
+
+source "arch/arm/mach-dove/Kconfig"
+
+source "arch/arm/mach-ep93xx/Kconfig"
+
+source "arch/arm/mach-footbridge/Kconfig"
+
+source "arch/arm/mach-gemini/Kconfig"
+
+source "arch/arm/mach-highbank/Kconfig"
+
+source "arch/arm/mach-integrator/Kconfig"
+
+source "arch/arm/mach-iop32x/Kconfig"
+
+source "arch/arm/mach-iop33x/Kconfig"
+
+source "arch/arm/mach-iop13xx/Kconfig"
+
+source "arch/arm/plat-iproc/Kconfig"
+
+source "arch/arm/mach-iproc/Kconfig"
+
+source "arch/arm/mach-ixp4xx/Kconfig"
+
+source "arch/arm/mach-keystone/Kconfig"
+
+source "arch/arm/mach-kirkwood/Kconfig"
+
+source "arch/arm/mach-ks8695/Kconfig"
+
+source "arch/arm/mach-msm/Kconfig"
+
+source "arch/arm/mach-mv78xx0/Kconfig"
+
+source "arch/arm/mach-imx/Kconfig"
+
+source "arch/arm/mach-mxs/Kconfig"
+
+source "arch/arm/mach-netx/Kconfig"
+
+source "arch/arm/mach-nomadik/Kconfig"
+
+source "arch/arm/plat-omap/Kconfig"
+
+source "arch/arm/mach-omap1/Kconfig"
+
+source "arch/arm/mach-omap2/Kconfig"
+
+source "arch/arm/mach-orion5x/Kconfig"
+
+source "arch/arm/mach-picoxcell/Kconfig"
+
+source "arch/arm/mach-pxa/Kconfig"
+source "arch/arm/plat-pxa/Kconfig"
+
+source "arch/arm/mach-mmp/Kconfig"
+
+source "arch/arm/mach-realview/Kconfig"
+
+source "arch/arm/mach-sa1100/Kconfig"
+
+source "arch/arm/plat-samsung/Kconfig"
+
+source "arch/arm/mach-socfpga/Kconfig"
+
+source "arch/arm/mach-spear/Kconfig"
+
+source "arch/arm/mach-s3c24xx/Kconfig"
+
+if ARCH_S3C64XX
+source "arch/arm/mach-s3c64xx/Kconfig"
+endif
+
+source "arch/arm/mach-s5p64x0/Kconfig"
+
+source "arch/arm/mach-s5pc100/Kconfig"
+
+source "arch/arm/mach-s5pv210/Kconfig"
+
+source "arch/arm/mach-exynos/Kconfig"
+
+source "arch/arm/mach-shmobile/Kconfig"
+
+source "arch/arm/mach-sunxi/Kconfig"
+
+source "arch/arm/mach-prima2/Kconfig"
+
+source "arch/arm/mach-tegra/Kconfig"
+
+source "arch/arm/mach-u300/Kconfig"
+
+source "arch/arm/mach-ux500/Kconfig"
+
+source "arch/arm/mach-versatile/Kconfig"
+
+source "arch/arm/mach-vexpress/Kconfig"
+source "arch/arm/plat-versatile/Kconfig"
+
+source "arch/arm/mach-virt/Kconfig"
+
+source "arch/arm/mach-vt8500/Kconfig"
+
+source "arch/arm/mach-w90x900/Kconfig"
+
+source "arch/arm/mach-zynq/Kconfig"
+
+# Definitions to make life easier
+config ARCH_ACORN
+	bool
+
+config PLAT_IOP
+	bool
+	select GENERIC_CLOCKEVENTS
+
+config PLAT_ORION
+	bool
+	select CLKSRC_MMIO
+	select COMMON_CLK
+	select GENERIC_IRQ_CHIP
+	select IRQ_DOMAIN
+
+config PLAT_ORION_LEGACY
+	bool
+	select PLAT_ORION
+
+config PLAT_PXA
+	bool
+
+config PLAT_VERSATILE
+	bool
+
+config ARM_TIMER_SP804
+	bool
+	select CLKSRC_MMIO
+	select CLKSRC_OF if OF
+
+source arch/arm/mm/Kconfig
+
+config ARM_NR_BANKS
+	int
+	default 16 if ARCH_EP93XX
+	default 8
+
+config IWMMXT
+	bool "Enable iWMMXt support" if !CPU_PJ4
+	depends on CPU_XSCALE || CPU_XSC3 || CPU_MOHAWK || CPU_PJ4
+	default y if PXA27x || PXA3xx || ARCH_MMP || CPU_PJ4
+	help
+	  Enable support for iWMMXt context switching at run time if
+	  running on a CPU that supports it.
+
+config XSCALE_PMU
+	bool
+	depends on CPU_XSCALE
+	default y
+
+config MULTI_IRQ_HANDLER
+	bool
+	help
+	  Allow each machine to specify it's own IRQ handler at run time.
+
+if !MMU
+source "arch/arm/Kconfig-nommu"
+endif
+
+config PJ4B_ERRATA_4742
+	bool "PJ4B Errata 4742: IDLE Wake Up Commands can Cause the CPU Core to Cease Operation"
+	depends on CPU_PJ4B && MACH_ARMADA_370
+	default y
+	help
+	  When coming out of either a Wait for Interrupt (WFI) or a Wait for
+	  Event (WFE) IDLE states, a specific timing sensitivity exists between
+	  the retiring WFI/WFE instructions and the newly issued subsequent
+	  instructions.  This sensitivity can result in a CPU hang scenario.
+	  Workaround:
+	  The software must insert either a Data Synchronization Barrier (DSB)
+	  or Data Memory Barrier (DMB) command immediately after the WFI/WFE
+	  instruction
+
+config ARM_ERRATA_326103
+	bool "ARM errata: FSR write bit incorrect on a SWP to read-only memory"
+	depends on CPU_V6
+	help
+	  Executing a SWP instruction to read-only memory does not set bit 11
+	  of the FSR on the ARM 1136 prior to r1p0. This causes the kernel to
+	  treat the access as a read, preventing a COW from occurring and
+	  causing the faulting task to livelock.
+
+config ARM_ERRATA_411920
+	bool "ARM errata: Invalidation of the Instruction Cache operation can fail"
+	depends on CPU_V6 || CPU_V6K
+	help
+	  Invalidation of the Instruction Cache operation can
+	  fail. This erratum is present in 1136 (before r1p4), 1156 and 1176.
+	  It does not affect the MPCore. This option enables the ARM Ltd.
+	  recommended workaround.
+
+config ARM_ERRATA_430973
+	bool "ARM errata: Stale prediction on replaced interworking branch"
+	depends on CPU_V7
+	help
+	  This option enables the workaround for the 430973 Cortex-A8
+	  (r1p0..r1p2) erratum. If a code sequence containing an ARM/Thumb
+	  interworking branch is replaced with another code sequence at the
+	  same virtual address, whether due to self-modifying code or virtual
+	  to physical address re-mapping, Cortex-A8 does not recover from the
+	  stale interworking branch prediction. This results in Cortex-A8
+	  executing the new code sequence in the incorrect ARM or Thumb state.
+	  The workaround enables the BTB/BTAC operations by setting ACTLR.IBE
+	  and also flushes the branch target cache at every context switch.
+	  Note that setting specific bits in the ACTLR register may not be
+	  available in non-secure mode.
+
+config ARM_ERRATA_458693
+	bool "ARM errata: Processor deadlock when a false hazard is created"
+	depends on CPU_V7
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 458693 Cortex-A8 (r2p0)
+	  erratum. For very specific sequences of memory operations, it is
+	  possible for a hazard condition intended for a cache line to instead
+	  be incorrectly associated with a different cache line. This false
+	  hazard might then cause a processor deadlock. The workaround enables
+	  the L1 caching of the NEON accesses and disables the PLD instruction
+	  in the ACTLR register. Note that setting specific bits in the ACTLR
+	  register may not be available in non-secure mode.
+
+config ARM_ERRATA_460075
+	bool "ARM errata: Data written to the L2 cache can be overwritten with stale data"
+	depends on CPU_V7
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 460075 Cortex-A8 (r2p0)
+	  erratum. Any asynchronous access to the L2 cache may encounter a
+	  situation in which recent store transactions to the L2 cache are lost
+	  and overwritten with stale memory contents from external memory. The
+	  workaround disables the write-allocate mode for the L2 cache via the
+	  ACTLR register. Note that setting specific bits in the ACTLR register
+	  may not be available in non-secure mode.
+
+config ARM_ERRATA_742230
+	bool "ARM errata: DMB operation may be faulty"
+	depends on CPU_V7 && SMP
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 742230 Cortex-A9
+	  (r1p0..r2p2) erratum. Under rare circumstances, a DMB instruction
+	  between two write operations may not ensure the correct visibility
+	  ordering of the two writes. This workaround sets a specific bit in
+	  the diagnostic register of the Cortex-A9 which causes the DMB
+	  instruction to behave as a DSB, ensuring the correct behaviour of
+	  the two writes.
+
+config ARM_ERRATA_742231
+	bool "ARM errata: Incorrect hazard handling in the SCU may lead to data corruption"
+	depends on CPU_V7 && SMP
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 742231 Cortex-A9
+	  (r2p0..r2p2) erratum. Under certain conditions, specific to the
+	  Cortex-A9 MPCore micro-architecture, two CPUs working in SMP mode,
+	  accessing some data located in the same cache line, may get corrupted
+	  data due to bad handling of the address hazard when the line gets
+	  replaced from one of the CPUs at the same time as another CPU is
+	  accessing it. This workaround sets specific bits in the diagnostic
+	  register of the Cortex-A9 which reduces the linefill issuing
+	  capabilities of the processor.
+
+config PL310_ERRATA_588369
+	bool "PL310 errata: Clean & Invalidate maintenance operations do not invalidate clean lines"
+	depends on CACHE_L2X0
+	help
+	   The PL310 L2 cache controller implements three types of Clean &
+	   Invalidate maintenance operations: by Physical Address
+	   (offset 0x7F0), by Index/Way (0x7F8) and by Way (0x7FC).
+	   They are architecturally defined to behave as the execution of a
+	   clean operation followed immediately by an invalidate operation,
+	   both performing to the same memory location. This functionality
+	   is not correctly implemented in PL310 as clean lines are not
+	   invalidated as a result of these operations.
+
+config ARM_ERRATA_643719
+	bool "ARM errata: LoUIS bit field in CLIDR register is incorrect"
+	depends on CPU_V7 && SMP
+	help
+	  This option enables the workaround for the 643719 Cortex-A9 (prior to
+	  r1p0) erratum. On affected cores the LoUIS bit field of the CLIDR
+	  register returns zero when it should return one. The workaround
+	  corrects this value, ensuring cache maintenance operations which use
+	  it behave as intended and avoiding data corruption.
+
+config ARM_ERRATA_720789
+	bool "ARM errata: TLBIASIDIS and TLBIMVAIS operations can broadcast a faulty ASID"
+	depends on CPU_V7
+	help
+	  This option enables the workaround for the 720789 Cortex-A9 (prior to
+	  r2p0) erratum. A faulty ASID can be sent to the other CPUs for the
+	  broadcasted CP15 TLB maintenance operations TLBIASIDIS and TLBIMVAIS.
+	  As a consequence of this erratum, some TLB entries which should be
+	  invalidated are not, resulting in an incoherency in the system page
+	  tables. The workaround changes the TLB flushing routines to invalidate
+	  entries regardless of the ASID.
+
+config PL310_ERRATA_727915
+	bool "PL310 errata: Background Clean & Invalidate by Way operation can cause data corruption"
+	depends on CACHE_L2X0
+	help
+	  PL310 implements the Clean & Invalidate by Way L2 cache maintenance
+	  operation (offset 0x7FC). This operation runs in background so that
+	  PL310 can handle normal accesses while it is in progress. Under very
+	  rare circumstances, due to this erratum, write data can be lost when
+	  PL310 treats a cacheable write transaction during a Clean &
+	  Invalidate by Way operation.
+
+config ARM_ERRATA_743622
+	bool "ARM errata: Faulty hazard checking in the Store Buffer may lead to data corruption"
+	depends on CPU_V7
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 743622 Cortex-A9
+	  (r2p*) erratum. Under very rare conditions, a faulty
+	  optimisation in the Cortex-A9 Store Buffer may lead to data
+	  corruption. This workaround sets a specific bit in the diagnostic
+	  register of the Cortex-A9 which disables the Store Buffer
+	  optimisation, preventing the defect from occurring. This has no
+	  visible impact on the overall performance or power consumption of the
+	  processor.
+
+config ARM_ERRATA_751472
+	bool "ARM errata: Interrupted ICIALLUIS may prevent completion of broadcasted operation"
+	depends on CPU_V7
+	depends on !ARCH_MULTIPLATFORM
+	help
+	  This option enables the workaround for the 751472 Cortex-A9 (prior
+	  to r3p0) erratum. An interrupted ICIALLUIS operation may prevent the
+	  completion of a following broadcasted operation if the second
+	  operation is received by a CPU before the ICIALLUIS has completed,
+	  potentially leading to corrupted entries in the cache or TLB.
+
+config PL310_ERRATA_753970
+	bool "PL310 errata: cache sync operation may be faulty"
+	depends on CACHE_PL310
+	help
+	  This option enables the workaround for the 753970 PL310 (r3p0) erratum.
+
+	  Under some condition the effect of cache sync operation on
+	  the store buffer still remains when the operation completes.
+	  This means that the store buffer is always asked to drain and
+	  this prevents it from merging any further writes. The workaround
+	  is to replace the normal offset of cache sync operation (0x730)
+	  by another offset targeting an unmapped PL310 register 0x740.
+	  This has the same effect as the cache sync operation: store buffer
+	  drain and waiting for all buffers empty.
+
+config ARM_ERRATA_754322
+	bool "ARM errata: possible faulty MMU translations following an ASID switch"
+	depends on CPU_V7
+	help
+	  This option enables the workaround for the 754322 Cortex-A9 (r2p*,
+	  r3p*) erratum. A speculative memory access may cause a page table walk
+	  which starts prior to an ASID switch but completes afterwards. This
+	  can populate the micro-TLB with a stale entry which may be hit with
+	  the new ASID. This workaround places two dsb instructions in the mm
+	  switching code so that no page table walks can cross the ASID switch.
+
+config ARM_ERRATA_754327
+	bool "ARM errata: no automatic Store Buffer drain"
+	depends on CPU_V7 && SMP
+	help
+	  This option enables the workaround for the 754327 Cortex-A9 (prior to
+	  r2p0) erratum. The Store Buffer does not have any automatic draining
+	  mechanism and therefore a livelock may occur if an external agent
+	  continuously polls a memory location waiting to observe an update.
+	  This workaround defines cpu_relax() as smp_mb(), preventing correctly
+	  written polling loops from denying visibility of updates to memory.
+
+config ARM_ERRATA_364296
+	bool "ARM errata: Possible cache data corruption with hit-under-miss enabled"
+	depends on CPU_V6 && !SMP
+	help
+	  This options enables the workaround for the 364296 ARM1136
+	  r0p2 erratum (possible cache data corruption with
+	  hit-under-miss enabled). It sets the undocumented bit 31 in
+	  the auxiliary control register and the FI bit in the control
+	  register, thus disabling hit-under-miss without putting the
+	  processor into full low interrupt latency mode. ARM11MPCore
+	  is not affected.
+
+config ARM_ERRATA_764369
+	bool "ARM errata: Data cache line maintenance operation by MVA may not succeed"
+	depends on CPU_V7 && SMP
+	help
+	  This option enables the workaround for erratum 764369
+	  affecting Cortex-A9 MPCore with two or more processors (all
+	  current revisions). Under certain timing circumstances, a data
+	  cache line maintenance operation by MVA targeting an Inner
+	  Shareable memory region may fail to proceed up to either the
+	  Point of Coherency or to the Point of Unification of the
+	  system. This workaround adds a DSB instruction before the
+	  relevant cache maintenance functions and sets a specific bit
+	  in the diagnostic control register of the SCU.
+
+config PL310_ERRATA_769419
+	bool "PL310 errata: no automatic Store Buffer drain"
+	depends on CACHE_L2X0
+	help
+	  On revisions of the PL310 prior to r3p2, the Store Buffer does
+	  not automatically drain. This can cause normal, non-cacheable
+	  writes to be retained when the memory system is idle, leading
+	  to suboptimal I/O performance for drivers using coherent DMA.
+	  This option adds a write barrier to the cpu_idle loop so that,
+	  on systems with an outer cache, the store buffer is drained
+	  explicitly.
+
+config ARM_ERRATA_775420
+       bool "ARM errata: A data cache maintenance operation which aborts, might lead to deadlock"
+       depends on CPU_V7
+       help
+	 This option enables the workaround for the 775420 Cortex-A9 (r2p2,
+	 r2p6,r2p8,r2p10,r3p0) erratum. In case a date cache maintenance
+	 operation aborts with MMU exception, it might cause the processor
+	 to deadlock. This workaround puts DSB before executing ISB if
+	 an abort may occur on cache maintenance.
+
+config ARM_ERRATA_798181
+	bool "ARM errata: TLBI/DSB failure on Cortex-A15"
+	depends on CPU_V7 && SMP
+	help
+	  On Cortex-A15 (r0p0..r3p2) the TLBI*IS/DSB operations are not
+	  adequately shooting down all use of the old entries. This
+	  option enables the Linux kernel workaround for this erratum
+	  which sends an IPI to the CPUs that are running the same ASID
+	  as the one being invalidated.
+
+config ARM_ERRATA_773022
+	bool "ARM errata: incorrect instructions may be executed from loop buffer"
+	depends on CPU_V7
+	help
+	  This option enables the workaround for the 773022 Cortex-A15
+	  (up to r0p4) erratum. In certain rare sequences of code, the
+	  loop buffer may deliver incorrect instructions. This
+	  workaround disables the loop buffer to avoid the erratum.
+
+config ARM_ERRATA_799270
+	bool "ARM errata: writing ACTLR.SMP when the L2 cache has been idle for an extended period may not work correctly"
+	depends on CPU_V7 && SMP
+	help
+	 This option enables the workaround for the 799270 Cortex-A15
+	 (up to r2p4) erratum. If the L2 cache has been idle for 256 or more
+	 cycles and when the CPU executes the ACTLR to modify the SMP bit, it may
+	 not work correctly
+endmenu
+
+source "arch/arm/common/Kconfig"
+
+menu "Bus support"
+
+config ARM_AMBA
+	bool
+
+config ISA
+	bool
+	help
+	  Find out whether you have ISA slots on your motherboard.  ISA is the
+	  name of a bus system, i.e. the way the CPU talks to the other stuff
+	  inside your box.  Other bus systems are PCI, EISA, MicroChannel
+	  (MCA) or VESA.  ISA is an older system, now being displaced by PCI;
+	  newer boards don't support it.  If you have ISA, say Y, otherwise N.
+
+# Select ISA DMA controller support
+config ISA_DMA
+	bool
+	select ISA_DMA_API
+
+# Select ISA DMA interface
+config ISA_DMA_API
+	bool
+
+config PCI
+	bool "PCI support" if MIGHT_HAVE_PCI
+	help
+	  Find out whether you have a PCI motherboard. PCI is the name of a
+	  bus system, i.e. the way the CPU talks to the other stuff inside
+	  your box. Other bus systems are ISA, EISA, MicroChannel (MCA) or
+	  VESA. If you have PCI, say Y, otherwise N.
+
+config PCI_DOMAINS
+	bool
+	depends on PCI
+
+config PCI_NANOENGINE
+	bool "BSE nanoEngine PCI support"
+	depends on SA1100_NANOENGINE
+	help
+	  Enable PCI on the BSE nanoEngine board.
+
+config PCI_SYSCALL
+	def_bool PCI
+
+# Select the host bridge type
+config PCI_HOST_VIA82C505
+	bool
+	depends on PCI && ARCH_SHARK
+	default y
+
+config PCI_HOST_ITE8152
+	bool
+	depends on PCI && MACH_ARMCORE
+	default y
+	select DMABOUNCE
+
+source "drivers/pci/Kconfig"
+
+source "drivers/pcmcia/Kconfig"
+
+endmenu
+
+menu "Kernel Features"
+
+config HAVE_SMP
+	bool
+	help
+	  This option should be selected by machines which have an SMP-
+	  capable CPU.
+
+	  The only effect of this option is to make the SMP-related
+	  options available to the user for configuration.
+
+config SMP
+	bool "Symmetric Multi-Processing"
+	depends on CPU_V6K || CPU_V7
+	depends on GENERIC_CLOCKEVENTS
+	depends on HAVE_SMP
+	depends on MMU
+	select USE_GENERIC_SMP_HELPERS
+	help
+	  This enables support for systems with more than one CPU. If you have
+	  a system with only one CPU, like most personal computers, say N. If
+	  you have a system with more than one CPU, say Y.
+
+	  If you say N here, the kernel will run on single and multiprocessor
+	  machines, but will use only one CPU of a multiprocessor machine. If
+	  you say Y here, the kernel will run on many, but not all, single
+	  processor machines. On a single processor machine, the kernel will
+	  run faster if you say N here.
+
+	  See also <file:Documentation/x86/i386/IO-APIC.txt>,
+	  <file:Documentation/nmi_watchdog.txt> and the SMP-HOWTO available at
+	  <http://tldp.org/HOWTO/SMP-HOWTO.html>.
+
+	  If you don't know what to do here, say N.
+
+config SMP_ON_UP
+	bool "Allow booting SMP kernel on uniprocessor systems (EXPERIMENTAL)"
+	depends on SMP && !XIP_KERNEL
+	default y
+	help
+	  SMP kernels contain instructions which fail on non-SMP processors.
+	  Enabling this option allows the kernel to modify itself to make
+	  these instructions safe.  Disabling it allows about 1K of space
+	  savings.
+
+	  If you don't know what to do here, say Y.
+
+config ARM_CPU_TOPOLOGY
+	bool "Support cpu topology definition"
+	depends on SMP && CPU_V7
+	default y
+	help
+	  Support ARM cpu topology definition. The MPIDR register defines
+	  affinity between processors which is then used to describe the cpu
+	  topology of an ARM System.
+
+config SCHED_MC
+	bool "Multi-core scheduler support"
+	depends on ARM_CPU_TOPOLOGY
+	help
+	  Multi-core scheduler support improves the CPU scheduler's decision
+	  making when dealing with multi-core CPU chips at a cost of slightly
+	  increased overhead in some places. If unsure say N here.
+
+config SCHED_SMT
+	bool "SMT scheduler support"
+	depends on ARM_CPU_TOPOLOGY
+	help
+	  Improves the CPU scheduler's decision making when dealing with
+	  MultiThreading at a cost of slightly increased overhead in some
+	  places. If unsure say N here.
+
+config HAVE_ARM_SCU
+	bool
+	help
+	  This option enables support for the ARM system coherency unit
+
+config HAVE_ARM_ARCH_TIMER
+	bool "Architected timer support"
+	depends on CPU_V7
+	select ARM_ARCH_TIMER
+	help
+	  This option enables support for the ARM architected timer
+
+config HAVE_ARM_TWD
+	bool
+	depends on SMP
+	select CLKSRC_OF if OF
+	help
+	  This options enables support for the ARM timer and watchdog unit
+
+config MCPM
+	bool "Multi-Cluster Power Management"
+	depends on CPU_V7 && SMP
+	help
+	  This option provides the common power management infrastructure
+	  for (multi-)cluster based systems, such as big.LITTLE based
+	  systems.
+
+choice
+	prompt "Memory split"
+	default VMSPLIT_3G
+	help
+	  Select the desired split between kernel and user memory.
+
+	  If you are not absolutely sure what you are doing, leave this
+	  option alone!
+
+	config VMSPLIT_3G
+		bool "3G/1G user/kernel split"
+	config VMSPLIT_2G
+		bool "2G/2G user/kernel split"
+	config VMSPLIT_1G
+		bool "1G/3G user/kernel split"
+endchoice
+
+config PAGE_OFFSET
+	hex
+	default 0x40000000 if VMSPLIT_1G
+	default 0x80000000 if VMSPLIT_2G
+	default 0xC0000000
+
+config NR_CPUS
+	int "Maximum number of CPUs (2-32)"
+	range 2 32
+	depends on SMP
+	default "4"
+
+config HOTPLUG_CPU
+	bool "Support for hot-pluggable CPUs"
+	depends on SMP && HOTPLUG
+	help
+	  Say Y here to experiment with turning CPUs off and on.  CPUs
+	  can be controlled through /sys/devices/system/cpu.
+
+config ARM_PSCI
+	bool "Support for the ARM Power State Coordination Interface (PSCI)"
+	depends on CPU_V7
+	help
+	  Say Y here if you want Linux to communicate with system firmware
+	  implementing the PSCI specification for CPU-centric power
+	  management operations described in ARM document number ARM DEN
+	  0022A ("Power State Coordination Interface System Software on
+	  ARM processors").
+
+config LOCAL_TIMERS
+	bool "Use local timer interrupts"
+	depends on SMP
+	default y
+	help
+	  Enable support for local timers on SMP platforms, rather then the
+	  legacy IPI broadcast method.  Local timers allows the system
+	  accounting to be spread across the timer interval, preventing a
+	  "thundering herd" at every timer tick.
+
+# The GPIO number here must be sorted by descending number. In case of
+# a multiplatform kernel, we just want the highest value required by the
+# selected platforms.
+config ARCH_NR_GPIO
+	int
+	default 1024 if ARCH_SHMOBILE || ARCH_TEGRA || ARCH_ZYNQ
+	default 512 if SOC_OMAP5 || SOC_DRA7XX
+	default 392 if ARCH_U8500
+	default 352 if ARCH_VT8500
+	default 288 if ARCH_SUNXI
+	default 264 if MACH_H4700
+	default 192 if SOC_AM43XX
+	default 0
+	help
+	  Maximum number of GPIOs in the system.
+
+	  If unsure, leave the default value.
+
+source kernel/Kconfig.preempt
+
+config HZ
+	int
+	default 200 if ARCH_EBSA110 || ARCH_S3C24XX || ARCH_S5P64X0 || \
+		ARCH_S5PV210 || ARCH_EXYNOS4
+	default AT91_TIMER_HZ if ARCH_AT91
+	default SHMOBILE_TIMER_HZ if ARCH_SHMOBILE_LEGACY
+	default 100
+
+config SCHED_HRTICK
+	def_bool HIGH_RES_TIMERS
+
+config THUMB2_KERNEL
+	bool "Compile the kernel in Thumb-2 mode" if !CPU_THUMBONLY
+	depends on CPU_V7 && !CPU_V6 && !CPU_V6K
+	default y if CPU_THUMBONLY
+	select AEABI
+	select ARM_ASM_UNIFIED
+	select ARM_UNWIND
+	help
+	  By enabling this option, the kernel will be compiled in
+	  Thumb-2 mode. A compiler/assembler that understand the unified
+	  ARM-Thumb syntax is needed.
+
+	  If unsure, say N.
+
+config THUMB2_AVOID_R_ARM_THM_JUMP11
+	bool "Work around buggy Thumb-2 short branch relocations in gas"
+	depends on THUMB2_KERNEL && MODULES
+	default y
+	help
+	  Various binutils versions can resolve Thumb-2 branches to
+	  locally-defined, preemptible global symbols as short-range "b.n"
+	  branch instructions.
+
+	  This is a problem, because there's no guarantee the final
+	  destination of the symbol, or any candidate locations for a
+	  trampoline, are within range of the branch.  For this reason, the
+	  kernel does not support fixing up the R_ARM_THM_JUMP11 (102)
+	  relocation in modules at all, and it makes little sense to add
+	  support.
+
+	  The symptom is that the kernel fails with an "unsupported
+	  relocation" error when loading some modules.
+
+	  Until fixed tools are available, passing
+	  -fno-optimize-sibling-calls to gcc should prevent gcc generating
+	  code which hits this problem, at the cost of a bit of extra runtime
+	  stack usage in some cases.
+
+	  The problem is described in more detail at:
+	      https://bugs.launchpad.net/binutils-linaro/+bug/725126
+
+	  Only Thumb-2 kernels are affected.
+
+	  Unless you are sure your tools don't have this problem, say Y.
+
+config ARM_ASM_UNIFIED
+	bool
+
+config AEABI
+	bool "Use the ARM EABI to compile the kernel"
+	help
+	  This option allows for the kernel to be compiled using the latest
+	  ARM ABI (aka EABI).  This is only useful if you are using a user
+	  space environment that is also compiled with EABI.
+
+	  Since there are major incompatibilities between the legacy ABI and
+	  EABI, especially with regard to structure member alignment, this
+	  option also changes the kernel syscall calling convention to
+	  disambiguate both ABIs and allow for backward compatibility support
+	  (selected with CONFIG_OABI_COMPAT).
+
+	  To use this you need GCC version 4.0.0 or later.
+
+config OABI_COMPAT
+	bool "Allow old ABI binaries to run with this kernel (EXPERIMENTAL)"
+	depends on AEABI && !THUMB2_KERNEL
+	default y
+	help
+	  This option preserves the old syscall interface along with the
+	  new (ARM EABI) one. It also provides a compatibility layer to
+	  intercept syscalls that have structure arguments which layout
+	  in memory differs between the legacy ABI and the new ARM EABI
+	  (only for non "thumb" binaries). This option adds a tiny
+	  overhead to all syscalls and produces a slightly larger kernel.
+	  If you know you'll be using only pure EABI user space then you
+	  can say N here. If this option is not selected and you attempt
+	  to execute a legacy ABI binary then the result will be
+	  UNPREDICTABLE (in fact it can be predicted that it won't work
+	  at all). If in doubt say Y.
+
+config ARCH_HAS_HOLES_MEMORYMODEL
+	bool
+
+config ARCH_SPARSEMEM_ENABLE
+	bool
+
+config ARCH_SPARSEMEM_DEFAULT
+	def_bool ARCH_SPARSEMEM_ENABLE
+
+config ARCH_SELECT_MEMORY_MODEL
+	def_bool ARCH_SPARSEMEM_ENABLE
+
+config HAVE_ARCH_PFN_VALID
+	def_bool ARCH_HAS_HOLES_MEMORYMODEL || !SPARSEMEM
+
+config HIGHMEM
+	bool "High Memory Support"
+	depends on MMU
+	help
+	  The address space of ARM processors is only 4 Gigabytes large
+	  and it has to accommodate user address space, kernel address
+	  space as well as some memory mapped IO. That means that, if you
+	  have a large amount of physical memory and/or IO, not all of the
+	  memory can be "permanently mapped" by the kernel. The physical
+	  memory that is not permanently mapped is called "high memory".
+
+	  Depending on the selected kernel/user memory split, minimum
+	  vmalloc space and actual amount of RAM, you may not need this
+	  option which should result in a slightly faster kernel.
+
+	  If unsure, say n.
+
+config HIGHPTE
+	bool "Allocate 2nd-level pagetables from highmem"
+	depends on HIGHMEM
+
+config HW_PERF_EVENTS
+	bool "Enable hardware performance counter support for perf events"
+	depends on PERF_EVENTS
+	default y
+	help
+	  Enable hardware performance counter support for perf events. If
+	  disabled, perf events will use software events only.
+
+config SYS_SUPPORTS_HUGETLBFS
+       def_bool y
+       depends on ARM_LPAE
+
+config HAVE_ARCH_TRANSPARENT_HUGEPAGE
+       def_bool y
+       depends on ARM_LPAE
+
+source "mm/Kconfig"
+
+config FORCE_MAX_ZONEORDER
+	int "Maximum zone order" if ARCH_SHMOBILE_LEGACY
+	range 11 64 if ARCH_SHMOBILE_LEGACY
+	default "12" if SOC_AM33XX
+	default "9" if SA1111
+	default "11"
+	help
+	  The kernel memory allocator divides physically contiguous memory
+	  blocks into "zones", where each zone is a power of two number of
+	  pages.  This option selects the largest power of two that the kernel
+	  keeps in the memory allocator.  If you need to allocate very large
+	  blocks of physically contiguous memory, then you may need to
+	  increase this value.
+
+	  This config option is actually maximum order plus one. For example,
+	  a value of 11 means that the largest free memory block is 2^10 pages.
+
+config ALIGNMENT_TRAP
+	bool
+	depends on CPU_CP15_MMU
+	default y if !ARCH_EBSA110
+	select HAVE_PROC_CPU if PROC_FS
+	help
+	  ARM processors cannot fetch/store information which is not
+	  naturally aligned on the bus, i.e., a 4 byte fetch must start at an
+	  address divisible by 4. On 32-bit ARM processors, these non-aligned
+	  fetch/store instructions will be emulated in software if you say
+	  here, which has a severe performance impact. This is necessary for
+	  correct operation of some network protocols. With an IP-only
+	  configuration it is safe to say N, otherwise say Y.
+
+config UACCESS_WITH_MEMCPY
+	bool "Use kernel mem{cpy,set}() for {copy_to,clear}_user()"
+	depends on MMU
+	default y if CPU_FEROCEON
+	help
+	  Implement faster copy_to_user and clear_user methods for CPU
+	  cores where a 8-word STM instruction give significantly higher
+	  memory write throughput than a sequence of individual 32bit stores.
+
+	  A possible side effect is a slight increase in scheduling latency
+	  between threads sharing the same address space if they invoke
+	  such copy operations with large buffers.
+
+	  However, if the CPU data cache is using a write-allocate mode,
+	  this option is unlikely to provide any performance gain.
+
+config SECCOMP
+	bool
+	prompt "Enable seccomp to safely compute untrusted bytecode"
+	---help---
+	  This kernel feature is useful for number crunching applications
+	  that may need to compute untrusted bytecode during their
+	  execution. By using pipes or other transports made available to
+	  the process as file descriptors supporting the read/write
+	  syscalls, it's possible to isolate those applications in
+	  their own address space using seccomp. Once seccomp is
+	  enabled via prctl(PR_SET_SECCOMP), it cannot be disabled
+	  and the task is only allowed to execute a few safe syscalls
+	  defined by each seccomp mode.
+
+config CC_STACKPROTECTOR
+	bool "Enable -fstack-protector buffer overflow detection (EXPERIMENTAL)"
+	help
+	  This option turns on the -fstack-protector GCC feature. This
+	  feature puts, at the beginning of functions, a canary value on
+	  the stack just before the return address, and validates
+	  the value just before actually returning.  Stack based buffer
+	  overflows (that need to overwrite this return address) now also
+	  overwrite the canary, which gets detected and the attack is then
+	  neutralized via a kernel panic.
+	  This feature requires gcc version 4.2 or above.
+
+config XEN_DOM0
+	def_bool y
+	depends on XEN
+
+config XEN
+	bool "Xen guest support on ARM (EXPERIMENTAL)"
+	depends on ARM && AEABI && OF
+	depends on CPU_V7 && !CPU_V6
+	depends on !GENERIC_ATOMIC64
+	select ARM_PSCI
+	help
+	  Say Y if you want to run Linux in a Virtual Machine on Xen on ARM.
+
+endmenu
+
+menu "Boot options"
+
+config USE_OF
+	bool "Flattened Device Tree support"
+	select IRQ_DOMAIN
+	select OF
+	select OF_EARLY_FLATTREE
+	help
+	  Include support for flattened device tree machine descriptions.
+
+config ATAGS
+	bool "Support for the traditional ATAGS boot data passing" if USE_OF
+	default y
+	help
+	  This is the traditional way of passing data to the kernel at boot
+	  time. If you are solely relying on the flattened device tree (or
+	  the ARM_ATAG_DTB_COMPAT option) then you may unselect this option
+	  to remove ATAGS support from your kernel binary.  If unsure,
+	  leave this to y.
+
+config DEPRECATED_PARAM_STRUCT
+	bool "Provide old way to pass kernel parameters"
+	depends on ATAGS
+	help
+	  This was deprecated in 2001 and announced to live on for 5 years.
+	  Some old boot loaders still use this way.
+
+# Compressed boot loader in ROM.  Yes, we really want to ask about
+# TEXT and BSS so we preserve their values in the config files.
+config ZBOOT_ROM_TEXT
+	hex "Compressed ROM boot loader base address"
+	default "0"
+	help
+	  The physical address at which the ROM-able zImage is to be
+	  placed in the target.  Platforms which normally make use of
+	  ROM-able zImage formats normally set this to a suitable
+	  value in their defconfig file.
+
+	  If ZBOOT_ROM is not enabled, this has no effect.
+
+config ZBOOT_ROM_BSS
+	hex "Compressed ROM boot loader BSS address"
+	default "0"
+	help
+	  The base address of an area of read/write memory in the target
+	  for the ROM-able zImage which must be available while the
+	  decompressor is running. It must be large enough to hold the
+	  entire decompressed kernel plus an additional 128 KiB.
+	  Platforms which normally make use of ROM-able zImage formats
+	  normally set this to a suitable value in their defconfig file.
+
+	  If ZBOOT_ROM is not enabled, this has no effect.
+
+config ZBOOT_ROM
+	bool "Compressed boot loader in ROM/flash"
+	depends on ZBOOT_ROM_TEXT != ZBOOT_ROM_BSS
+	help
+	  Say Y here if you intend to execute your compressed kernel image
+	  (zImage) directly from ROM or flash.  If unsure, say N.
+
+choice
+	prompt "Include SD/MMC loader in zImage (EXPERIMENTAL)"
+	depends on ZBOOT_ROM && ARCH_SH7372
+	default ZBOOT_ROM_NONE
+	help
+	  Include experimental SD/MMC loading code in the ROM-able zImage.
+	  With this enabled it is possible to write the ROM-able zImage
+	  kernel image to an MMC or SD card and boot the kernel straight
+	  from the reset vector. At reset the processor Mask ROM will load
+	  the first part of the ROM-able zImage which in turn loads the
+	  rest the kernel image to RAM.
+
+config ZBOOT_ROM_NONE
+	bool "No SD/MMC loader in zImage (EXPERIMENTAL)"
+	help
+	  Do not load image from SD or MMC
+
+config ZBOOT_ROM_MMCIF
+	bool "Include MMCIF loader in zImage (EXPERIMENTAL)"
+	help
+	  Load image from MMCIF hardware block.
+
+config ZBOOT_ROM_SH_MOBILE_SDHI
+	bool "Include SuperH Mobile SDHI loader in zImage (EXPERIMENTAL)"
+	help
+	  Load image from SDHI hardware block
+
+endchoice
+
+config ARM_APPENDED_DTB
+	bool "Use appended device tree blob to zImage (EXPERIMENTAL)"
+	depends on OF && !ZBOOT_ROM
+	help
+	  With this option, the boot code will look for a device tree binary
+	  (DTB) appended to zImage
+	  (e.g. cat zImage <filename>.dtb > zImage_w_dtb).
+
+	  This is meant as a backward compatibility convenience for those
+	  systems with a bootloader that can't be upgraded to accommodate
+	  the documented boot protocol using a device tree.
+
+	  Beware that there is very little in terms of protection against
+	  this option being confused by leftover garbage in memory that might
+	  look like a DTB header after a reboot if no actual DTB is appended
+	  to zImage.  Do not leave this option active in a production kernel
+	  if you don't intend to always append a DTB.  Proper passing of the
+	  location into r2 of a bootloader provided DTB is always preferable
+	  to this option.
+
+config ARM_ATAG_DTB_COMPAT
+	bool "Supplement the appended DTB with traditional ATAG information"
+	depends on ARM_APPENDED_DTB
+	help
+	  Some old bootloaders can't be updated to a DTB capable one, yet
+	  they provide ATAGs with memory configuration, the ramdisk address,
+	  the kernel cmdline string, etc.  Such information is dynamically
+	  provided by the bootloader and can't always be stored in a static
+	  DTB.  To allow a device tree enabled kernel to be used with such
+	  bootloaders, this option allows zImage to extract the information
+	  from the ATAG list and store it at run time into the appended DTB.
+
+choice
+	prompt "Kernel command line type" if ARM_ATAG_DTB_COMPAT
+	default ARM_ATAG_DTB_COMPAT_CMDLINE_FROM_BOOTLOADER
+
+config ARM_ATAG_DTB_COMPAT_CMDLINE_FROM_BOOTLOADER
+	bool "Use bootloader kernel arguments if available"
+	help
+	  Uses the command-line options passed by the boot loader instead of
+	  the device tree bootargs property. If the boot loader doesn't provide
+	  any, the device tree bootargs property will be used.
+
+config ARM_ATAG_DTB_COMPAT_CMDLINE_EXTEND
+	bool "Extend with bootloader kernel arguments"
+	help
+	  The command-line arguments provided by the boot loader will be
+	  appended to the the device tree bootargs property.
+
+endchoice
+
+config CMDLINE
+	string "Default kernel command string"
+	default ""
+	help
+	  On some architectures (EBSA110 and CATS), there is currently no way
+	  for the boot loader to pass arguments to the kernel. For these
+	  architectures, you should supply some command-line options at build
+	  time by entering them here. As a minimum, you should specify the
+	  memory size and the root device (e.g., mem=64M root=/dev/nfs).
+
+choice
+	prompt "Kernel command line type" if CMDLINE != ""
+	default CMDLINE_FROM_BOOTLOADER
+	depends on ATAGS
+
+config CMDLINE_FROM_BOOTLOADER
+	bool "Use bootloader kernel arguments if available"
+	help
+	  Uses the command-line options passed by the boot loader. If
+	  the boot loader doesn't provide any, the default kernel command
+	  string provided in CMDLINE will be used.
+
+config CMDLINE_EXTEND
+	bool "Extend bootloader kernel arguments"
+	help
+	  The command-line arguments provided by the boot loader will be
+	  appended to the default kernel command string.
+
+config CMDLINE_FORCE
+	bool "Always use the default kernel command string"
+	help
+	  Always use the default kernel command string, even if the boot
+	  loader passes other arguments to the kernel.
+	  This is useful if you cannot or don't want to change the
+	  command-line options your boot loader passes to the kernel.
+endchoice
+
+config XIP_KERNEL
+	bool "Kernel Execute-In-Place from ROM"
+	depends on !ZBOOT_ROM && !ARM_LPAE && !ARCH_MULTIPLATFORM
+	help
+	  Execute-In-Place allows the kernel to run from non-volatile storage
+	  directly addressable by the CPU, such as NOR flash. This saves RAM
+	  space since the text section of the kernel is not loaded from flash
+	  to RAM.  Read-write sections, such as the data section and stack,
+	  are still copied to RAM.  The XIP kernel is not compressed since
+	  it has to run directly from flash, so it will take more space to
+	  store it.  The flash address used to link the kernel object files,
+	  and for storing it, is configuration dependent. Therefore, if you
+	  say Y here, you must know the proper physical address where to
+	  store the kernel image depending on your own flash memory usage.
+
+	  Also note that the make target becomes "make xipImage" rather than
+	  "make zImage" or "make Image".  The final kernel binary to put in
+	  ROM memory will be arch/arm/boot/xipImage.
+
+	  If unsure, say N.
+
+config XIP_PHYS_ADDR
+	hex "XIP Kernel Physical Location"
+	depends on XIP_KERNEL
+	default "0x00080000"
+	help
+	  This is the physical address in your flash memory the kernel will
+	  be linked for and stored to.  This address is dependent on your
+	  own flash usage.
+
+config KEXEC
+	bool "Kexec system call (EXPERIMENTAL)"
+	depends on (!SMP || PM_SLEEP_SMP)
+	help
+	  kexec is a system call that implements the ability to shutdown your
+	  current kernel, and to start another kernel.  It is like a reboot
+	  but it is independent of the system firmware.   And like a reboot
+	  you can start any kernel with it, not just Linux.
+
+	  It is an ongoing process to be certain the hardware in a machine
+	  is properly shutdown, so do not be surprised if this code does not
+	  initially work for you.  It may help to enable device hotplugging
+	  support.
+
+config ATAGS_PROC
+	bool "Export atags in procfs"
+	depends on ATAGS && KEXEC
+	default y
+	help
+	  Should the atags used to boot the kernel be exported in an "atags"
+	  file in procfs. Useful with kexec.
+
+config CRASH_DUMP
+	bool "Build kdump crash kernel (EXPERIMENTAL)"
+	help
+	  Generate crash dump after being started by kexec. This should
+	  be normally only set in special crash dump kernels which are
+	  loaded in the main kernel with kexec-tools into a specially
+	  reserved region and then later executed after a crash by
+	  kdump/kexec. The crash dump kernel must be compiled to a
+	  memory address not used by the main kernel
+
+	  For more details see Documentation/kdump/kdump.txt
+
+config AUTO_ZRELADDR
+	bool "Auto calculation of the decompressed kernel image address"
+	depends on !ZBOOT_ROM && !ARCH_U300
+	help
+	  ZRELADDR is the physical address where the decompressed kernel
+	  image will be placed. If AUTO_ZRELADDR is selected, the address
+	  will be determined at run-time by masking the current IP with
+	  0xf8000000. This assumes the zImage being placed in the first 128MB
+	  from start of memory.
+
+endmenu
+
+menu "CPU Power Management"
+
+if ARCH_HAS_CPUFREQ
+source "drivers/cpufreq/Kconfig"
+
+config CPU_FREQ_S3C
+	bool
+	help
+	  Internal configuration node for common cpufreq on Samsung SoC
+
+config CPU_FREQ_S3C24XX
+	bool "CPUfreq driver for Samsung S3C24XX series CPUs (EXPERIMENTAL)"
+	depends on ARCH_S3C24XX && CPU_FREQ
+	select CPU_FREQ_S3C
+	help
+	  This enables the CPUfreq driver for the Samsung S3C24XX family
+	  of CPUs.
+
+	  For details, take a look at <file:Documentation/cpu-freq>.
+
+	  If in doubt, say N.
+
+config CPU_FREQ_S3C24XX_PLL
+	bool "Support CPUfreq changing of PLL frequency (EXPERIMENTAL)"
+	depends on CPU_FREQ_S3C24XX
+	help
+	  Compile in support for changing the PLL frequency from the
+	  S3C24XX series CPUfreq driver. The PLL takes time to settle
+	  after a frequency change, so by default it is not enabled.
+
+	  This also means that the PLL tables for the selected CPU(s) will
+	  be built which may increase the size of the kernel image.
+
+config CPU_FREQ_S3C24XX_DEBUG
+	bool "Debug CPUfreq Samsung driver core"
+	depends on CPU_FREQ_S3C24XX
+	help
+	  Enable s3c_freq_dbg for the Samsung S3C CPUfreq core
+
+config CPU_FREQ_S3C24XX_IODEBUG
+	bool "Debug CPUfreq Samsung driver IO timing"
+	depends on CPU_FREQ_S3C24XX
+	help
+	  Enable s3c_freq_iodbg for the Samsung S3C CPUfreq core
+
+config CPU_FREQ_S3C24XX_DEBUGFS
+	bool "Export debugfs for CPUFreq"
+	depends on CPU_FREQ_S3C24XX && DEBUG_FS
+	help
+	  Export status information via debugfs.
+
+endif
+
+source "drivers/cpuidle/Kconfig"
+
+endmenu
+
+menu "Floating point emulation"
+
+comment "At least one emulation must be selected"
+
+config FPE_NWFPE
+	bool "NWFPE math emulation"
+	depends on (!AEABI || OABI_COMPAT) && !THUMB2_KERNEL
+	---help---
+	  Say Y to include the NWFPE floating point emulator in the kernel.
+	  This is necessary to run most binaries. Linux does not currently
+	  support floating point hardware so you need to say Y here even if
+	  your machine has an FPA or floating point co-processor podule.
+
+	  You may say N here if you are going to load the Acorn FPEmulator
+	  early in the bootup.
+
+config FPE_NWFPE_XP
+	bool "Support extended precision"
+	depends on FPE_NWFPE
+	help
+	  Say Y to include 80-bit support in the kernel floating-point
+	  emulator.  Otherwise, only 32 and 64-bit support is compiled in.
+	  Note that gcc does not generate 80-bit operations by default,
+	  so in most cases this option only enlarges the size of the
+	  floating point emulator without any good reason.
+
+	  You almost surely want to say N here.
+
+config FPE_FASTFPE
+	bool "FastFPE math emulation (EXPERIMENTAL)"
+	depends on (!AEABI || OABI_COMPAT) && !CPU_32v3
+	---help---
+	  Say Y here to include the FAST floating point emulator in the kernel.
+	  This is an experimental much faster emulator which now also has full
+	  precision for the mantissa.  It does not support any exceptions.
+	  It is very simple, and approximately 3-6 times faster than NWFPE.
+
+	  It should be sufficient for most programs.  It may be not suitable
+	  for scientific calculations, but you have to check this for yourself.
+	  If you do not feel you need a faster FP emulation you should better
+	  choose NWFPE.
+
+config VFP
+	bool "VFP-format floating point maths"
+	depends on CPU_V6 || CPU_V6K || CPU_ARM926T || CPU_V7 || CPU_FEROCEON
+	help
+	  Say Y to include VFP support code in the kernel. This is needed
+	  if your hardware includes a VFP unit.
+
+	  Please see <file:Documentation/arm/VFP/release-notes.txt> for
+	  release notes and additional status information.
+
+	  Say N if your target does not have VFP hardware.
+
+config VFPv3
+	bool
+	depends on VFP
+	default y if CPU_V7
+
+config NEON
+	bool "Advanced SIMD (NEON) Extension support"
+	depends on VFPv3 && CPU_V7
+	help
+	  Say Y to include support code for NEON, the ARMv7 Advanced SIMD
+	  Extension.
+
+endmenu
+
+menu "Userspace binary formats"
+
+source "fs/Kconfig.binfmt"
+
+config ARTHUR
+	tristate "RISC OS personality"
+	depends on !AEABI
+	help
+	  Say Y here to include the kernel code necessary if you want to run
+	  Acorn RISC OS/Arthur binaries under Linux. This code is still very
+	  experimental; if this sounds frightening, say N and sleep in peace.
+	  You can also say M here to compile this support as a module (which
+	  will be called arthur).
+
+endmenu
+
+menu "Power management options"
+
+source "kernel/power/Kconfig"
+
+config ARCH_SUSPEND_POSSIBLE
+	depends on !ARCH_S5PC100
+	depends on CPU_ARM920T || CPU_ARM926T || CPU_SA1100 || \
+		CPU_V6 || CPU_V6K || CPU_V7 || CPU_XSC3 || CPU_XSCALE || CPU_MOHAWK
+	def_bool y
+
+config ARM_CPU_SUSPEND
+	def_bool PM_SLEEP
+
+endmenu
+
+source "net/Kconfig"
+
+source "drivers/Kconfig"
+
+source "fs/Kconfig"
+
+source "arch/arm/Kconfig.debug"
+
+source "security/Kconfig"
+
+source "crypto/Kconfig"
+
+source "lib/Kconfig"
+
+source "arch/arm/kvm/Kconfig"
diff --git a/arch/arm/Makefile b/arch/arm/Makefile
index 270b619..705a52b 100644
--- a/arch/arm/Makefile
+++ b/arch/arm/Makefile
@@ -16,6 +16,7 @@ LDFLAGS		:=
 LDFLAGS_vmlinux	:=-p --no-undefined -X
 ifeq ($(CONFIG_CPU_ENDIAN_BE8),y)
 LDFLAGS_vmlinux	+= --be8
+LDFLAGS_MODULE	+= --be8
 endif
 
 OBJCOPYFLAGS	:=-O binary -R .comment -S
@@ -157,6 +158,8 @@ machine-$(CONFIG_ARCH_IXP4XX)		+= ixp4xx
 machine-$(CONFIG_ARCH_KIRKWOOD)		+= kirkwood
 machine-$(CONFIG_ARCH_KS8695)		+= ks8695
 machine-$(CONFIG_ARCH_LPC32XX)		+= lpc32xx
+machine-$(CONFIG_ARCH_MB8AC0300)	+= mb8ac0300
+machine-$(CONFIG_ARCH_MB86S70)		+= mb86s70
 machine-$(CONFIG_ARCH_MMP)		+= mmp
 machine-$(CONFIG_ARCH_MSM)		+= msm
 machine-$(CONFIG_ARCH_MV78XX0)		+= mv78xx0
@@ -265,10 +268,16 @@ libs-y				:= arch/arm/lib/ $(libs-y)
 # Default target when executing plain make
 ifeq ($(CONFIG_XIP_KERNEL),y)
 KBUILD_IMAGE := xipImage
+else ifeq ($(CONFIG_BUILD_ARM_APPENDED_DTB_IMAGE),y)
+KBUILD_IMAGE := zImage-dtb.$(CONFIG_BUILD_ARM_APPENDED_DTB_IMAGE_NAME)
 else
 KBUILD_IMAGE := zImage
 endif
 
+ifeq ($(UNCOMPRESSED_IMAGE),y)
+KBUILD_IMAGE := Image
+endif
+
 # Build the DT binary blobs if we have OF configured
 ifeq ($(CONFIG_USE_OF),y)
 KBUILD_DTBS := dtbs
diff --git a/arch/arm/Makefile.orig b/arch/arm/Makefile.orig
new file mode 100644
index 0000000..270b619
--- /dev/null
+++ b/arch/arm/Makefile.orig
@@ -0,0 +1,325 @@
+#
+# arch/arm/Makefile
+#
+# This file is included by the global makefile so that you can add your own
+# architecture-specific flags and dependencies.
+#
+# This file is subject to the terms and conditions of the GNU General Public
+# License.  See the file "COPYING" in the main directory of this archive
+# for more details.
+#
+# Copyright (C) 1995-2001 by Russell King
+
+# Ensure linker flags are correct
+LDFLAGS		:=
+
+LDFLAGS_vmlinux	:=-p --no-undefined -X
+ifeq ($(CONFIG_CPU_ENDIAN_BE8),y)
+LDFLAGS_vmlinux	+= --be8
+endif
+
+OBJCOPYFLAGS	:=-O binary -R .comment -S
+GZFLAGS		:=-9
+#KBUILD_CFLAGS	+=-pipe
+
+# Never generate .eh_frame
+KBUILD_CFLAGS	+= $(call cc-option,-fno-dwarf2-cfi-asm)
+
+# Do not use arch/arm/defconfig - it's always outdated.
+# Select a platform tht is kept up-to-date
+KBUILD_DEFCONFIG := versatile_defconfig
+
+# defines filename extension depending memory management type.
+ifeq ($(CONFIG_MMU),)
+MMUEXT		:= -nommu
+KBUILD_CFLAGS	+= $(call cc-option,-mno-unaligned-access)
+endif
+
+ifeq ($(CONFIG_FRAME_POINTER),y)
+KBUILD_CFLAGS	+=-fno-omit-frame-pointer -mapcs -mno-sched-prolog
+endif
+
+ifeq ($(CONFIG_CC_STACKPROTECTOR),y)
+KBUILD_CFLAGS	+=-fstack-protector
+endif
+
+ifeq ($(CONFIG_CPU_BIG_ENDIAN),y)
+KBUILD_CPPFLAGS	+= -mbig-endian
+AS		+= -EB
+LD		+= -EB
+else
+KBUILD_CPPFLAGS	+= -mlittle-endian
+AS		+= -EL
+LD		+= -EL
+endif
+
+comma = ,
+
+# This selects which instruction set is used.
+# Note that GCC does not numerically define an architecture version
+# macro, but instead defines a whole series of macros which makes
+# testing for a specific architecture or later rather impossible.
+arch-$(CONFIG_CPU_32v7)		:=-D__LINUX_ARM_ARCH__=7 $(call cc-option,-march=armv7-a,-march=armv5t -Wa$(comma)-march=armv7-a)
+arch-$(CONFIG_CPU_32v6)		:=-D__LINUX_ARM_ARCH__=6 $(call cc-option,-march=armv6,-march=armv5t -Wa$(comma)-march=armv6)
+# Only override the compiler option if ARMv6. The ARMv6K extensions are
+# always available in ARMv7
+ifeq ($(CONFIG_CPU_32v6),y)
+arch-$(CONFIG_CPU_32v6K)	:=-D__LINUX_ARM_ARCH__=6 $(call cc-option,-march=armv6k,-march=armv5t -Wa$(comma)-march=armv6k)
+endif
+arch-$(CONFIG_CPU_32v5)		:=-D__LINUX_ARM_ARCH__=5 $(call cc-option,-march=armv5te,-march=armv4t)
+arch-$(CONFIG_CPU_32v4T)	:=-D__LINUX_ARM_ARCH__=4 -march=armv4t
+arch-$(CONFIG_CPU_32v4)		:=-D__LINUX_ARM_ARCH__=4 -march=armv4
+arch-$(CONFIG_CPU_32v3)		:=-D__LINUX_ARM_ARCH__=3 -march=armv3
+
+# This selects how we optimise for the processor.
+tune-$(CONFIG_CPU_ARM7TDMI)	:=-mtune=arm7tdmi
+tune-$(CONFIG_CPU_ARM720T)	:=-mtune=arm7tdmi
+tune-$(CONFIG_CPU_ARM740T)	:=-mtune=arm7tdmi
+tune-$(CONFIG_CPU_ARM9TDMI)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_ARM940T)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_ARM946E)	:=$(call cc-option,-mtune=arm9e,-mtune=arm9tdmi)
+tune-$(CONFIG_CPU_ARM920T)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_ARM922T)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_ARM925T)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_ARM926T)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_FA526)	:=-mtune=arm9tdmi
+tune-$(CONFIG_CPU_SA110)	:=-mtune=strongarm110
+tune-$(CONFIG_CPU_SA1100)	:=-mtune=strongarm1100
+tune-$(CONFIG_CPU_XSCALE)	:=$(call cc-option,-mtune=xscale,-mtune=strongarm110) -Wa,-mcpu=xscale
+tune-$(CONFIG_CPU_XSC3)		:=$(call cc-option,-mtune=xscale,-mtune=strongarm110) -Wa,-mcpu=xscale
+tune-$(CONFIG_CPU_FEROCEON)	:=$(call cc-option,-mtune=marvell-f,-mtune=xscale)
+tune-$(CONFIG_CPU_V6)		:=$(call cc-option,-mtune=arm1136j-s,-mtune=strongarm)
+tune-$(CONFIG_CPU_V6K)		:=$(call cc-option,-mtune=arm1136j-s,-mtune=strongarm)
+
+ifeq ($(CONFIG_AEABI),y)
+CFLAGS_ABI	:=-mabi=aapcs-linux -mno-thumb-interwork
+else
+CFLAGS_ABI	:=$(call cc-option,-mapcs-32,-mabi=apcs-gnu) $(call cc-option,-mno-thumb-interwork,)
+endif
+
+ifeq ($(CONFIG_ARM_UNWIND),y)
+CFLAGS_ABI	+=-funwind-tables
+endif
+
+ifeq ($(CONFIG_THUMB2_KERNEL),y)
+AFLAGS_AUTOIT	:=$(call as-option,-Wa$(comma)-mimplicit-it=always,-Wa$(comma)-mauto-it)
+AFLAGS_NOWARN	:=$(call as-option,-Wa$(comma)-mno-warn-deprecated,-Wa$(comma)-W)
+CFLAGS_ISA	:=-mthumb $(AFLAGS_AUTOIT) $(AFLAGS_NOWARN)
+AFLAGS_ISA	:=$(CFLAGS_ISA) -Wa$(comma)-mthumb
+# Work around buggy relocation from gas if requested:
+ifeq ($(CONFIG_THUMB2_AVOID_R_ARM_THM_JUMP11),y)
+CFLAGS_MODULE	+=-fno-optimize-sibling-calls
+endif
+else
+CFLAGS_ISA	:=$(call cc-option,-marm,)
+AFLAGS_ISA	:=$(CFLAGS_ISA)
+endif
+
+# Need -Uarm for gcc < 3.x
+KBUILD_CFLAGS	+=$(CFLAGS_ABI) $(CFLAGS_ISA) $(arch-y) $(tune-y) $(call cc-option,-mshort-load-bytes,$(call cc-option,-malignment-traps,)) -msoft-float -Uarm
+KBUILD_AFLAGS	+=$(CFLAGS_ABI) $(AFLAGS_ISA) $(arch-y) $(tune-y) -include asm/unified.h -msoft-float
+
+CHECKFLAGS	+= -D__arm__
+
+#Default value
+head-y		:= arch/arm/kernel/head$(MMUEXT).o
+textofs-y	:= 0x00008000
+textofs-$(CONFIG_ARCH_CLPS711X) := 0x00028000
+# We don't want the htc bootloader to corrupt kernel during resume
+textofs-$(CONFIG_PM_H1940)      := 0x00108000
+# SA1111 DMA bug: we don't want the kernel to live in precious DMA-able memory
+ifeq ($(CONFIG_ARCH_SA1100),y)
+textofs-$(CONFIG_SA1111) := 0x00208000
+endif
+textofs-$(CONFIG_ARCH_MSM7X30) := 0x00208000
+textofs-$(CONFIG_ARCH_MSM8X60) := 0x00208000
+textofs-$(CONFIG_ARCH_MSM8960) := 0x00208000
+
+# Machine directory name.  This list is sorted alphanumerically
+# by CONFIG_* macro name.
+machine-$(CONFIG_ARCH_AT91)		+= at91
+machine-$(CONFIG_ARCH_BCM)		+= bcm
+machine-$(CONFIG_ARCH_BCM2835)		+= bcm2835
+machine-$(CONFIG_ARCH_CLPS711X)		+= clps711x
+machine-$(CONFIG_ARCH_CNS3XXX)		+= cns3xxx
+machine-$(CONFIG_ARCH_DAVINCI)		+= davinci
+machine-$(CONFIG_ARCH_DOVE)		+= dove
+machine-$(CONFIG_ARCH_EBSA110)		+= ebsa110
+machine-$(CONFIG_ARCH_EP93XX)		+= ep93xx
+machine-$(CONFIG_ARCH_GEMINI)		+= gemini
+machine-$(CONFIG_ARCH_HIGHBANK)		+= highbank
+machine-$(CONFIG_ARCH_KEYSTONE)		:= keystone
+machine-$(CONFIG_ARCH_INTEGRATOR)	+= integrator
+machine-$(CONFIG_ARCH_IOP13XX)		+= iop13xx
+machine-$(CONFIG_ARCH_IOP32X)		+= iop32x
+machine-$(CONFIG_ARCH_IOP33X)		+= iop33x
+machine-$(CONFIG_ARCH_IXP4XX)		+= ixp4xx
+machine-$(CONFIG_ARCH_KIRKWOOD)		+= kirkwood
+machine-$(CONFIG_ARCH_KS8695)		+= ks8695
+machine-$(CONFIG_ARCH_LPC32XX)		+= lpc32xx
+machine-$(CONFIG_ARCH_MMP)		+= mmp
+machine-$(CONFIG_ARCH_MSM)		+= msm
+machine-$(CONFIG_ARCH_MV78XX0)		+= mv78xx0
+machine-$(CONFIG_ARCH_MXC)		+= imx
+machine-$(CONFIG_ARCH_MXS)		+= mxs
+machine-$(CONFIG_ARCH_MVEBU)		+= mvebu
+machine-$(CONFIG_ARCH_NETX)		+= netx
+machine-$(CONFIG_ARCH_NOMADIK)		+= nomadik
+machine-$(CONFIG_ARCH_OMAP1)		+= omap1
+machine-$(CONFIG_ARCH_OMAP2PLUS)	+= omap2
+machine-$(CONFIG_ARCH_ORION5X)		+= orion5x
+machine-$(CONFIG_ARCH_PICOXCELL)	+= picoxcell
+machine-$(CONFIG_ARCH_PRIMA2)		+= prima2
+machine-$(CONFIG_ARCH_PXA)		+= pxa
+machine-$(CONFIG_ARCH_REALVIEW)		+= realview
+machine-$(CONFIG_ARCH_RPC)		+= rpc
+machine-$(CONFIG_ARCH_S3C24XX)		+= s3c24xx
+machine-$(CONFIG_ARCH_S3C64XX)		+= s3c64xx
+machine-$(CONFIG_ARCH_S5P64X0)		+= s5p64x0
+machine-$(CONFIG_ARCH_S5PC100)		+= s5pc100
+machine-$(CONFIG_ARCH_S5PV210)		+= s5pv210
+machine-$(CONFIG_ARCH_EXYNOS)		+= exynos
+machine-$(CONFIG_ARCH_SA1100)		+= sa1100
+machine-$(CONFIG_ARCH_SHARK)		+= shark
+machine-$(CONFIG_ARCH_SHMOBILE) 	+= shmobile
+machine-$(CONFIG_ARCH_TEGRA)		+= tegra
+machine-$(CONFIG_ARCH_U300)		+= u300
+machine-$(CONFIG_ARCH_U8500)		+= ux500
+machine-$(CONFIG_ARCH_VERSATILE)	+= versatile
+machine-$(CONFIG_ARCH_VEXPRESS)		+= vexpress
+machine-$(CONFIG_ARCH_VT8500)		+= vt8500
+machine-$(CONFIG_ARCH_W90X900)		+= w90x900
+machine-$(CONFIG_FOOTBRIDGE)		+= footbridge
+machine-$(CONFIG_ARCH_SOCFPGA)		+= socfpga
+machine-$(CONFIG_PLAT_SPEAR)		+= spear
+machine-$(CONFIG_ARCH_VIRT)		+= virt
+machine-$(CONFIG_ARCH_ZYNQ)		+= zynq
+machine-$(CONFIG_ARCH_SUNXI)		+= sunxi
+
+# Platform directory name.  This list is sorted alphanumerically
+# by CONFIG_* macro name.
+plat-$(CONFIG_ARCH_OMAP)	+= omap
+plat-$(CONFIG_ARCH_S3C64XX)	+= samsung
+plat-$(CONFIG_PLAT_IOP)		+= iop
+plat-$(CONFIG_PLAT_ORION)	+= orion
+plat-$(CONFIG_PLAT_PXA)		+= pxa
+plat-$(CONFIG_PLAT_S3C24XX)	+= samsung
+plat-$(CONFIG_PLAT_S5P)		+= samsung
+plat-$(CONFIG_PLAT_VERSATILE)	+= versatile
+
+ifeq ($(CONFIG_ARCH_EBSA110),y)
+# This is what happens if you forget the IOCS16 line.
+# PCMCIA cards stop working.
+CFLAGS_3c589_cs.o :=-DISA_SIXTEEN_BIT_PERIPHERAL
+export CFLAGS_3c589_cs.o
+endif
+
+# The byte offset of the kernel image in RAM from the start of RAM.
+TEXT_OFFSET := $(textofs-y)
+
+# The first directory contains additional information for the boot setup code
+ifneq ($(machine-y),)
+MACHINE  := arch/arm/mach-$(word 1,$(machine-y))/
+else
+MACHINE  :=
+endif
+ifeq ($(CONFIG_ARCH_MULTIPLATFORM),y)
+MACHINE  :=
+endif
+
+machdirs := $(patsubst %,arch/arm/mach-%/,$(machine-y))
+platdirs := $(patsubst %,arch/arm/plat-%/,$(plat-y))
+
+ifneq ($(CONFIG_ARCH_MULTIPLATFORM),y)
+ifeq ($(KBUILD_SRC),)
+KBUILD_CPPFLAGS += $(patsubst %,-I%include,$(machdirs) $(platdirs))
+else
+KBUILD_CPPFLAGS += $(patsubst %,-I$(srctree)/%include,$(machdirs) $(platdirs))
+endif
+endif
+
+export	TEXT_OFFSET GZFLAGS MMUEXT
+
+# Do we have FASTFPE?
+FASTFPE		:=arch/arm/fastfpe
+ifeq ($(FASTFPE),$(wildcard $(FASTFPE)))
+FASTFPE_OBJ	:=$(FASTFPE)/
+endif
+
+core-$(CONFIG_FPE_NWFPE)	+= arch/arm/nwfpe/
+core-$(CONFIG_FPE_FASTFPE)	+= $(FASTFPE_OBJ)
+core-$(CONFIG_VFP)		+= arch/arm/vfp/
+core-$(CONFIG_XEN)		+= arch/arm/xen/
+core-$(CONFIG_KVM_ARM_HOST) 	+= arch/arm/kvm/
+
+# If we have a machine-specific directory, then include it in the build.
+core-y				+= arch/arm/kernel/ arch/arm/mm/ arch/arm/common/
+core-y				+= arch/arm/net/
+core-y				+= arch/arm/crypto/
+core-y				+= $(machdirs) $(platdirs)
+
+drivers-$(CONFIG_OPROFILE)      += arch/arm/oprofile/
+
+libs-y				:= arch/arm/lib/ $(libs-y)
+
+# Default target when executing plain make
+ifeq ($(CONFIG_XIP_KERNEL),y)
+KBUILD_IMAGE := xipImage
+else
+KBUILD_IMAGE := zImage
+endif
+
+# Build the DT binary blobs if we have OF configured
+ifeq ($(CONFIG_USE_OF),y)
+KBUILD_DTBS := dtbs
+endif
+
+all:	$(KBUILD_IMAGE) $(KBUILD_DTBS)
+
+boot := arch/arm/boot
+
+archprepare:
+	$(Q)$(MAKE) $(build)=arch/arm/tools include/generated/mach-types.h
+
+# Convert bzImage to zImage
+bzImage: zImage
+
+# These targets cannot be built in parallel
+.NOTPARALLEL: zImage Image xipImage bootpImage uImage
+
+zImage Image xipImage bootpImage uImage: vmlinux
+	$(Q)$(MAKE) $(build)=$(boot) MACHINE=$(MACHINE) $(boot)/$@
+
+zinstall uinstall install: vmlinux
+	$(Q)$(MAKE) $(build)=$(boot) MACHINE=$(MACHINE) $@
+
+%.dtb: scripts
+	$(Q)$(MAKE) $(build)=$(boot)/dts MACHINE=$(MACHINE) $(boot)/dts/$@
+
+dtbs: scripts
+	$(Q)$(MAKE) $(build)=$(boot)/dts MACHINE=$(MACHINE) dtbs
+
+# We use MRPROPER_FILES and CLEAN_FILES now
+archclean:
+	$(Q)$(MAKE) $(clean)=$(boot)
+
+# My testing targets (bypasses dependencies)
+bp:;	$(Q)$(MAKE) $(build)=$(boot) MACHINE=$(MACHINE) $(boot)/bootpImage
+i zi:;	$(Q)$(MAKE) $(build)=$(boot) MACHINE=$(MACHINE) $@
+
+
+define archhelp
+  echo  '* zImage        - Compressed kernel image (arch/$(ARCH)/boot/zImage)'
+  echo  '  Image         - Uncompressed kernel image (arch/$(ARCH)/boot/Image)'
+  echo  '* xipImage      - XIP kernel image, if configured (arch/$(ARCH)/boot/xipImage)'
+  echo  '  uImage        - U-Boot wrapped zImage'
+  echo  '  bootpImage    - Combined zImage and initial RAM disk'
+  echo  '                  (supply initrd image via make variable INITRD=<path>)'
+  echo  '* dtbs          - Build device tree blobs for enabled boards'
+  echo  '  install       - Install uncompressed kernel'
+  echo  '  zinstall      - Install compressed kernel'
+  echo  '  uinstall      - Install U-Boot wrapped compressed kernel'
+  echo  '                  Install using (your) ~/bin/$(INSTALLKERNEL) or'
+  echo  '                  (distribution) /sbin/$(INSTALLKERNEL) or'
+  echo  '                  install to $$(INSTALL_PATH) and run lilo'
+endef
diff --git a/arch/arm/boot/Makefile b/arch/arm/boot/Makefile
index 99dbe66..e56f0e0 100644
--- a/arch/arm/boot/Makefile
+++ b/arch/arm/boot/Makefile
@@ -14,6 +14,7 @@
 ifneq ($(MACHINE),)
 include $(srctree)/$(MACHINE)/Makefile.boot
 endif
+include $(srctree)/arch/arm/boot/dts/Makefile
 
 # Note: the following conditions must always be true:
 #   ZRELADDR == virt_to_phys(PAGE_OFFSET + TEXT_OFFSET)
diff --git a/arch/arm/boot/dts/Makefile b/arch/arm/boot/dts/Makefile
index dbd0158..18e4ee3 100644
--- a/arch/arm/boot/dts/Makefile
+++ b/arch/arm/boot/dts/Makefile
@@ -89,6 +89,8 @@ dtb-$(CONFIG_ARCH_KIRKWOOD) += kirkwood-cloudbox.dtb \
 	kirkwood-ts219-6282.dtb \
 	kirkwood-openblocks_a6.dtb
 dtb-$(CONFIG_ARCH_MARCO) += marco-evb.dtb
+dtb-$(CONFIG_ARCH_MB8AC0300) += 
+dtb-$(CONFIG_ARCH_MB86S70) += mb86s72eb.dtb
 dtb-$(CONFIG_ARCH_MSM) += msm8660-surf.dtb \
 	msm8960-cdp.dtb
 dtb-$(CONFIG_ARCH_MVEBU) += armada-370-db.dtb \
@@ -223,7 +225,7 @@ dtb-$(CONFIG_ARCH_VT8500) += vt8500-bv07.dtb \
 dtb-$(CONFIG_ARCH_ZYNQ) += zynq-zc702.dtb \
 	zynq-zc706.dtb
 
-targets += dtbs
+targets += dtbs dtbs_install
 targets += $(dtb-y)
 endif
 
@@ -233,3 +235,5 @@ dtbs: $(addprefix $(obj)/, $(dtb-y))
 	$(Q)rm -f $(obj)/../*.dtb
 
 clean-files := *.dtb
+
+dtbs_install: $(addsuffix _dtbinst_, $(dtb-y))
diff --git a/arch/arm/boot/dts/mb86s72.dtsi b/arch/arm/boot/dts/mb86s72.dtsi
new file mode 100644
index 0000000..332f7d2
--- /dev/null
+++ b/arch/arm/boot/dts/mb86s72.dtsi
@@ -0,0 +1,1575 @@
+/*
+ * Copyright (C) 2014 Linaro, LTD
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/ {
+	model = "Fujitsu mb86s72";
+	compatible = "fujitsu,mb86s72";
+	interrupt-parent = <&gic>;
+	#address-cells = <2>;
+	#size-cells = <1>;
+
+	aliases {
+		serial0 = &uart0;
+		serial1 = &uart1;
+		serial2 = &uart2;
+	};
+
+	clusters {
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		cluster0: cluster@0 { /* A15s */
+			reg = <0>;
+			freqs = <1200000000>;
+			cores {
+				#address-cells = <1>;
+				#size-cells = <0>;
+
+				core0: core@0 {
+					reg = <0>;
+				};
+
+				core1: core@1 {
+					reg = <1>;
+				};
+			};
+		};
+
+		cluster1: cluster@1 {
+			reg = <1>;
+			freqs = <800000000>;
+			cores {
+				#address-cells = <1>;
+				#size-cells = <0>;
+				core2: core@0 {
+					reg = <0>;
+				};
+
+				core3: core@1 {
+					reg = <1>;
+				};
+			};
+		};
+	};
+
+	cpus {
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		cpu0: cpu@0 {
+			device_type = "cpu";
+			compatible = "arm,cortex-a15";
+			reg = <0>;
+			cluster = <&cluster0>;
+			core = <&core0>;
+			cci-control-port = <&cci_control4>;
+			clock-frequency = <1600000000>;
+			operating-points = <
+				1600000	1020000
+			>;
+			clock-latency = <1000000>;
+		};
+
+		cpu1: cpu@1 {
+			device_type = "cpu";
+			compatible = "arm,cortex-a15";
+			reg = <1>;
+			cluster = <&cluster0>;
+			core = <&core1>;
+			cci-control-port = <&cci_control4>;
+			clock-frequency = <1600000000>;
+			operating-points = <
+				1600000	1020000
+			>;
+			clock-latency = <1000000>;
+		};
+
+		cpu2: cpu@2 {
+			device_type = "cpu";
+			compatible = "arm,cortex-a7";
+			reg = <0x100>;
+			cluster = <&cluster1>;
+			core = <&core2>;
+			cci-control-port = <&cci_control3>;
+			clock-frequency = <800000000>;
+			operating-points = <
+				800000 1020000
+			>;
+			clock-latency = <1000000>;
+		};
+
+		cpu3: cpu@3 {
+			device_type = "cpu";
+			compatible = "arm,cortex-a7";
+			reg = <0x101>;
+			cluster = <&cluster1>;
+			core = <&core3>;
+			cci-control-port = <&cci_control3>;
+			clock-frequency = <800000000>;
+			operating-points = <
+				800000 1020000
+			>;
+			clock-latency = <1000000>;
+		};
+	};
+
+	scb_sensor: scb_sensor {
+		compatible = "fujitsu,scb-thermal-s70";
+		#thermal-sensor-cells = <1>;
+		ca15-cpu0-id = <0>;
+		ca15-cpu1-id = <1>;
+		ca7-cpu0-id = <2>;
+		ca7-cpu1-id = <3>;
+	};
+
+	thermal-zones {
+		ca15-cpu0-thermal {
+			polling-delay-passive = <500>;
+			polling-delay = <1000>;
+
+			thermal-sensors = <&scb_sensor 0x0>;
+
+		};
+
+		ca15-cpu1-thermal {
+			polling-delay-passive = <500>;
+			polling-delay = <1000>;
+
+			thermal-sensors = <&scb_sensor 0x1>;
+
+		};
+
+		ca7-cpu0-thermal {
+			polling-delay-passive = <500>;
+			polling-delay = <1000>;
+
+			thermal-sensors = <&scb_sensor 0x100>;
+
+		};
+
+		ca7-cpu1-thermal {
+			polling-delay-passive = <500>;
+			polling-delay = <1000>;
+
+			thermal-sensors = <&scb_sensor 0x101>;
+
+		};
+	};
+
+	cci@2c090000 {
+		compatible = "arm,cci-400", "arm,cci";
+		#address-cells = <1>;
+		#size-cells = <1>;
+		reg = <0 0x2c090000 0x1000>;
+		ranges = <0x0 0x0 0x2c090000 0x10000>;
+
+		/* enable retention suspend semantics */
+		retention-suspend;
+
+		cci_control0: slave-if@1000 {
+			compatible = "arm,cci-400-ctrl-if";
+			interface-type = "ace-lite";
+			reg = <0x1000 0x1000>;
+		};
+
+		cci_control1: slave-if@2000 {
+			compatible = "arm,cci-400-ctrl-if";
+			interface-type = "ace-lite";
+			reg = <0x2000 0x1000>;
+		};
+
+		cci_control2: slave-if@3000 {
+			compatible = "arm,cci-400-ctrl-if";
+			interface-type = "ace-lite";
+			reg = <0x3000 0x1000>;
+		};
+
+		cci_control3: slave-if@4000 {
+			compatible = "arm,cci-400-ctrl-if";
+			interface-type = "ace";
+			reg = <0x4000 0x1000>;
+		};
+
+		cci_control4: slave-if@5000 {
+			compatible = "arm,cci-400-ctrl-if";
+			interface-type = "ace";
+			reg = <0x5000 0x1000>;
+		};
+
+	};
+
+	cci-pmu@2c099000 {
+		compatible = "arm,cci-400-pmu";
+		reg = <0 0x2c099000 0x5000>;
+		interrupts = <0 45 4>,
+				<0 45 4>,
+				<0 45 4>,
+				<0 45 4>,
+				<0 45 4>;
+	};
+
+	/**
+	 * cntrlr : 0->ALW, 1->DDR3, 2->MAIN, 3->CA7, 4->USB, 5->FPDLINK
+	 * port : [0,7] -> Gateable Clock Ports.  [8]->UngatedCLK
+	 */
+	clocks {
+		clk_alw_0_0: clk_alw_0_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <0>;
+		};
+		clk_alw_0_1: clk_alw_0_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <1>;
+		};
+		clk_alw_0_2: clk_alw_0_2 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <2>;
+		};
+		clk_alw_0_4: clk_alw_0_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <4>;
+		};
+		clk_alw_0_5: clk_alw_0_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <5>;
+		};
+		clk_alw_0_8: clk_alw_0_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0>;
+			port = <8>;
+		};
+
+		clk_alw_1_0: clk_alw_1_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <1>;
+			port = <0>;
+		};
+		clk_alw_1_1: clk_alw_1_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <1>;
+			port = <1>;
+		};
+		clk_alw_1_2: clk_alw_1_2 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <1>;
+			port = <2>;
+		};
+		clk_alw_1_8: clk_alw_1_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <1>;
+			port = <8>;
+		};
+		clk_alw_2_0: clk_alw_2_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <0>;
+		};
+		clk_alw_2_1: clk_alw_2_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <1>;
+		};
+		clk_alw_2_2: clk_alw_2_2 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <2>;
+		};
+		clk_alw_2_4: clk_alw_2_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <4>;
+		};
+		clk_alw_2_5: clk_alw_2_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <5>;
+		};
+		clk_alw_2_8: clk_alw_2_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <2>;
+			port = <8>;
+		};
+		clk_alw_6_8: clk_alw_6_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <6>;
+			port = <8>;
+		};
+
+		clk_alw_7_0: clk_alw_7_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <7>;
+			port = <0>;
+		};
+		clk_alw_7_3: clk_alw_7_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <7>;
+			port = <3>;
+		};
+		clk_alw_7_7: clk_alw_7_7 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <7>;
+			port = <7>;
+		};
+		clk_alw_8_0: clk_alw_8_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <8>;
+			port = <0>;
+		};
+		clk_alw_9_0: clk_alw_9_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <9>;
+			port = <0>;
+		};
+		clk_alw_a_0: clk_alw_a_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0a>;
+			port = <0>;
+		};
+		clk_alw_a_1: clk_alw_a_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0a>;
+			port = <1>;
+		};
+		clk_alw_b_0: clk_alw_b_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0b>;
+			port = <0>;
+		};
+		clk_alw_c_0: clk_alw_c_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0c>;
+			port = <0>;
+		};
+		clk_alw_c_1: clk_alw_c_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0c>;
+			port = <1>;
+		};
+		clk_alw_c_2: clk_alw_c_2 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0c>;
+			port = <2>;
+		};
+		clk_alw_e_0: clk_alw_e_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0e>;
+			port = <0>;
+		};
+		clk_alw_e_2: clk_alw_e_2 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0e>;
+			port = <2>;
+		};
+		clk_alw_e_3: clk_alw_e_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0e>;
+			port = <3>;
+		};
+		clk_alw_e_5: clk_alw_e_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0e>;
+			port = <5>;
+		};
+		clk_alw_f_8: clk_alw_f_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <0>;
+			domain = <0x0f>;
+			port = <8>;
+		};
+		clk_ddr3_0_0: clk_ddr3_0_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <1>;
+			domain = <0>;
+			port = <0>;
+		};
+
+		clk_main_0_0: clk_main_0_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0>;
+			port = <0>;
+		};
+		clk_main_0_8: clk_main_0_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0>;
+			port = <8>;
+		};
+		clk_main_1_3: clk_main_1_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <1>;
+			port = <3>;
+		};
+		clk_main_1_4: clk_main_1_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <1>;
+			port = <4>;
+		};
+		clk_main_2_0: clk_main_2_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <2>;
+			port = <0>;
+		};
+		clk_main_2_1: clk_main_2_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <2>;
+			port = <1>;
+		};
+		clk_main_2_3: clk_main_2_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <2>;
+			port = <3>;
+		};
+		clk_main_2_4: clk_main_2_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <2>;
+			port = <4>;
+		};
+
+		clk_main_2_7: clk_main_2_7 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <2>;
+			port = <7>;
+		};
+		clk_main_3_0: clk_main_3_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <3>;
+			port = <0>;
+		};
+		clk_main_3_3: clk_main_3_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <3>;
+			port = <3>;
+		};
+		clk_main_3_4: clk_main_3_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <3>;
+			port = <4>;
+		};
+		clk_main_3_5: clk_main_3_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <3>;
+			port = <5>;
+		};
+		clk_main_3_6: clk_main_3_6 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <3>;
+			port = <6>;
+		};
+		clk_main_4_0: clk_main_4_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <4>;
+			port = <0>;
+		};
+		clk_main_4_4: clk_main_4_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <4>;
+			port = <4>;
+		};
+		clk_main_4_5: clk_main_4_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <4>;
+			port = <5>;
+		};
+
+		clk_main_5_0: clk_main_5_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <5>;
+			port = <0>;
+		};
+		clk_main_5_3: clk_main_5_3 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <5>;
+			port = <3>;
+		};
+		clk_main_5_4: clk_main_5_4 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <5>;
+			port = <4>;
+		};
+		clk_main_5_5: clk_main_5_5 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <5>;
+			port = <5>;
+		};
+		clk_main_6_8: clk_main_6_8 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <6>;
+			port = <8>;
+		};
+
+		clk_main_7_0: clk_main_7_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <7>;
+			port = <0>;
+		};
+		clk_main_8_1: clk_main_8_1 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <8>;
+			port = <1>;
+		};
+		clk_main_9_0: clk_main_9_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <9>;
+			port = <0>;
+		};
+		clk_main_a_0: clk_main_a_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0xa>;
+			port = <0>;
+		};
+		clk_main_b_0: clk_main_b_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0xb>;
+			port = <0>;
+		};
+		clk_main_c_0: clk_main_c_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0xc>;
+			port = <0>;
+		};
+		clk_main_d_0: clk_main_d_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <2>;
+			domain = <0xd>;
+			port = <0>;
+		};
+
+		clk_usb_0_0: clk_usb_0_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <4>;
+			domain = <0>;
+			port = <0>;
+		};
+		clk_usb_1_0: clk_usb_1_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <4>;
+			domain = <1>;
+			port = <0>;
+		};
+		clk_usb_2_0: clk_usb_2_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <4>;
+			domain = <2>;
+			port = <0>;
+		};
+		clk_usb_3_0: clk_usb_3_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <4>;
+			domain = <3>;
+			port = <0>;
+		};
+
+		clk_fpdlink_0_0: clk_fpdlink_0_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <5>;
+			domain = <0>;
+			port = <0>;
+		};
+		clk_fpdlink_1_0: clk_fpdlink_1_0 {
+			#clock-cells = <0>;
+			compatible = "mb86s70,crg11_gate";
+			cntrlr = <5>;
+			domain = <1>;
+			port = <0>;
+		};
+	};
+
+	power: power_domain {
+ 		compatible = "fujitsu,mb86s7x-pd";
+		#power-domain-cells = <1>;
+		#address-cells = <2>;
+		#size-cells = <1>;
+		ranges;
+ 
+		pd_alwayson: genpd@0 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <0>;
+		};
+		pd_default: genpd@1 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <1>;
+		};
+		pd_offchip: genpd@2 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <2>;
+		};
+		pd_cpu_a7: genpd@3 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <3>;
+		};
+		pd_cpu_a7_c0: genpd@4 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <4>;
+		};
+		pd_cpu_a7_c1: genpd@5 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <5>;
+		};
+		pd_cpu_a7_scu: genpd@6 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <6>;
+		};
+		pd_cpu_a15: genpd@7 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <7>;
+		};
+		pd_cpu_a15_c0: genpd@8 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <8>;
+		};
+		pd_cpu_a15_c1: genpd@9 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <9>;
+		};
+		pd_cpu_a15_scu: genpd@10 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <10>;
+		};
+		pd_mali: genpd@11 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <11>;
+		};
+		pd_vpu: genpd@12 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <12>;
+		};
+		pd_jpeg: genpd@13 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <13>;
+		};
+		pd_unused14: genpd@14 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <14>;
+		};
+		pd_unused15: genpd@15 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <15>;
+		};
+		pd_unused16: genpd@16 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <16>;
+		};
+		pd_fpdlink: genpd@17 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <17>;
+		};
+		pd_unused18: genpd@18 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <18>;
+		};
+		pd_unused19: genpd@19 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <19>;
+		};
+		pd_pcie0: genpd@20 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <20>;
+		};
+		pd_pcie1: genpd@21 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <21>;
+		};
+		pd_unused22: genpd@22 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <22>;
+		};
+		pd_unused23: genpd@23 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <23>;
+		};
+		pd_usb3h0: genpd@24 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <24>;
+		};
+		pd_usb3h1: genpd@25 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <25>;
+			/*
+			 * here is a power domain dependency layout example.
+			 * pd_usb3h1 will power-on index 7 then index 8.
+			 */
+			master-domain-cell = <&pd_usb3h0>;
+		};
+		pd_usb2h: genpd@26 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <26>;
+		};
+		pd_usb2otg: genpd@27 {
+			compatible = "fujitsu,mb86s7x-pd-cell";
+			index = <27>;
+		};
+	};
+
+	gic: interrupt-controller@2c001000 {
+		compatible = "arm,cortex-a15-gic", "arm,cortex-a9-gic";
+		#interrupt-cells = <3>;
+		interrupt-controller;
+		reg = <0 0x2c001000 0x1000>,
+		      <0 0x2c002000 0x1000>,
+		      <0 0x2c004000 0x2000>,
+		      <0 0x2c006000 0x2000>;
+		interrupts = <1 9 0xf04>;
+		#gpio-cells = <2>;
+
+		gic-cpuif@0 {
+			compatible = "arm,gic-cpuif";
+			cpuif-id = <0>;
+			cpu = <&cpu0>;
+		};
+		gic-cpuif@1 {
+			compatible = "arm,gic-cpuif";
+			cpuif-id = <1>;
+			cpu = <&cpu1>;
+		};
+		gic-cpuif@2 {
+			compatible = "arm,gic-cpuif";
+			cpuif-id = <0x100>;
+			cpu = <&cpu2>;
+		};
+		gic-cpuif@3 {
+			compatible = "arm,gic-cpuif";
+			cpuif-id = <0x101>;
+			cpu = <&cpu3>;
+		};
+
+	};
+
+	pmu_a15 {
+		compatible = "arm,cortex-a15-pmu";
+		cluster  = <&cluster0>;
+		interrupts = <0 2 4>,
+			     <0 6 4>;
+		power-domains = <&power 3>;
+
+	};
+
+	pmu_a7 {
+		compatible = "arm,cortex-a7-pmu";
+		cluster  = <&cluster1>;
+		interrupts = <0 18 4>,
+			     <0 22 4>;
+		power-domains = <&power 3>;
+	};
+
+	ipcu@31a00000 {
+		#clock-cells = <1>;
+		compatible = "fujitsu,ipcu";
+		mboxes = <8>;
+		reg = <0 0x31A00000 0x960>;
+		interrupts = <0 152 4>, /* A15 CPU 0 */
+			     <0 153 4>, /* A15 CPU 1 */
+			     <0 154 4>, /* reserved */
+			     <0 155 4>, /* reserved */
+			     <0 156 4>, /* A7 CPU 0 */
+			     <0 157 4>, /* A7 CPU 1 */
+			     <0 158 4>, /* reserved */
+			     <0 159 4>; /* reserved */
+		clocks = <&clk_alw_0_8>;
+		power-domains = <&power 1>;
+	};
+
+	mhu@2b1f0000 {
+		compatible = "fujitsu,mhu";
+		reg = <0 0x2B1F0000 0x1000>;
+		interrupts = <0 36 4>, /* LP Non-Sec */
+			     <0 35 4>, /* HP Non-Sec */
+			     <0 37 4>; /* Secure */
+		clocks = <&clk_alw_0_8>;
+		clock-names = "clk";
+		power-domains = <&power 1>;
+	};
+
+	mali0: mali@2d000000 {
+		compatible = "fujitsu,malit6xx";
+		reg = <0 0x2d000000 0x10000>;
+		interrupts = <0 33 4>, /* Mali JOB */
+			<0 34 4>, /* Mali MMU */
+			<0 32 4>; /* Mali GPU */
+                interrupt-names = "JOB", "MMU", "GPU";
+		clocks = <&clk_main_7_0>, <&clk_main_2_0>, <&clk_main_0_0>, <&clk_main_4_0>;
+		clock-names = "g3d", "aclk_g3d", "mout_gpll", "fout_gpll";
+		#stream-id-cells = <0>;
+		power-domains = <&power 11>;
+		cci-control-port = <&cci_control2>;
+	};
+
+	exiu: interrupt-controller@310c0000 {
+		compatible = "fujitsu,mb8ac0300-exiu";
+		reg = <0 0x310c0000 0x10000>;
+		exiu_irq_base0 = <112>;
+		exiu_irq_base1 = <128>;
+		clocks = <&clk_alw_0_0>;
+	};
+
+	amba@0 {
+		compatible = "arm,amba-bus", "simple-bus";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		ranges;
+		uart0: serial@0x31040000 {
+			#clock-cells = <1>;
+			compatible = "arm,pl011", "arm,primecell";
+			arm,primecell-periphid = <0x00341011>;
+			reg = <0 0x31040000 0x100>;
+			interrupts = <0 320 0x4>;
+			clock-frequency = <7813000>;
+			clocks = <&clk_alw_f_8>;
+			clock-names = "apb_pclk";
+			power-domains = <&power 1>;
+		};
+	
+		uart1: serial@0x31050000 {
+			#clock-cells = <1>;
+			compatible = "arm,pl011", "arm,primecell";
+			arm,primecell-periphid = <0x00341011>;
+			reg = <0 0x31050000 0x100>;
+			interrupts = <0 321 0x4>;
+			clock-frequency = <7813000>;
+			clocks = <&clk_alw_f_8>;
+			clock-names = "apb_pclk";
+			power-domains = <&power 1>;
+		};
+	
+		uart2: serial@0x31060000 {
+			#clock-cells = <1>;
+			compatible = "arm,pl011", "arm,primecell";
+			arm,primecell-periphid = <0x00341011>;
+			reg = <0 0x31060000 0x100>;
+			interrupts = <0 322 0x4>;
+			clock-frequency = <7813000>;
+			clocks = <&clk_alw_f_8>;
+			clock-names = "apb_pclk";
+			power-domains = <&power 1>;
+		};
+	};
+
+	hdmac0: hdmac_hsio { /* USB2 */
+		compatible = "fujitsu,mb8ac0300-hdmac";
+		reg = <0 0x34080000 0x1000>;
+		interrupts = <0 128 0x4>,
+			     <0 129 0x4>;
+		clocks = <&clk_main_2_1>;
+		channels = <2>;
+		chip = <0>;
+		rotation = <1>;
+		power-domains = <&power 27>;
+	};
+
+	hdmac2: hdmac_mp1 { /* I2S */
+		compatible = "fujitsu,mb8ac0300-hdmac";
+		reg = <0 0x36400000 0x1000>;
+		interrupts = <0 120 0x4>,
+			     <0 121 0x4>,
+			     <0 122 0x4>,
+			     <0 123 0x4>;
+		clocks = <&clk_main_3_6>;
+		channels = <4>;
+		chip = <1>;
+		rotation = <1>;
+	};
+
+	dma330_hsio: dma330@33600000 {
+		compatible = "arm,pl330", "arm,primecell";
+		reg = <0 0x33600000 0x1000>;
+		interrupts =    <0 144 4>,
+				<0 145 4>,
+				<0 146 4>,
+				<0 147 4>;
+		clocks = <&clk_main_0_8>;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x0 0x80000000>;
+		clock-names = "apb_pclk";
+		copy-align = <128>;
+		#dma-cells = <1>;
+		#dma-channels = <8>;
+		#dma-requests = <1>;
+		power-domains = <&power 2>;
+	};
+
+	dma330_css: dma330@2f200000 {
+		compatible = "arm,pl330", "arm,primecell";
+		reg = <0 0x2f200000 0x1000>;
+		interrupts =    <0 136 4>,
+				<0 137 4>,
+				<0 138 4>,
+				<0 139 4>,
+				<0 140 4>,
+				<0 141 4>,
+				<0 142 4>,
+				<0 143 4>;
+		clocks = <&clk_main_0_8>;
+		copy-align = <128>;
+		clock-names = "apb_pclk";
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x0 0x80000000>;
+		#dma-cells = <1>;
+		#dma-channels = <8>;
+		#dma-requests = <32>;
+		power-domains = <&power 2>;
+	};
+
+	iommu_gpu0: mmu400@2b400000 {
+		status = "disabled";
+		compatible = "arm,mmu-400";
+		reg = <0 0x2b400000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 39 4>, <0 38 4>;
+		mmu-masters = <&mali0>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+	iommu_css_dmac: mmu400@2fc00000 {
+		status = "disabled";
+		compatible = "arm,mmu-400";
+		reg = <0 0x2fc00000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 65 4>, <0 64 4>;
+		mmu-masters = <&dma330_css 0 1>;
+		stream-ids = <0>;
+		calxeda,smmu-secure-config-access = <0 0xFFFFFFFF>;
+		power-domains = <&power 2>;
+	};
+	iommu_pcie0: mmu400@35200000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x35200000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 71 4>, <0 70 4>;
+		mmu-masters = <&pcie0 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+
+	iommu_pcie1_x2len: mmu400@35700000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x35700000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 63 4>, <0 62 4>;
+		mmu-masters = <&pcie1_x2len 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+
+	iommu_pcie1_x4len: mmu400@35300000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x35300000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 73 4>, <0 72 4>;
+		mmu-masters = <&pcie1_x4len 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+
+	iommu_usb30: mmu400@35400000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x35400000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 75 4>, <0 74 4>;
+		mmu-masters = <&usb3host0 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+	iommu_usb31: mmu400@34600000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x34600000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 422 4>, <0 421 4>;
+		mmu-masters = <&usb3host1 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+	iommu_hsio_ahb: mmu400@35500000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x35500000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 77 4>, <0 76 4>;
+		mmu-masters = <&usb2otg 0 1>;
+		stream-ids = <0>, <0>;
+		power-domains = <&power 2>;
+	};
+
+
+	iommu_hsio_ahb2: mmu400@0x34700000 {
+		compatible = "arm,mmu-400";
+		reg = <0 0x34700000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 403 4>, <0 402 4>;
+		mmu-masters = <&hcd21 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+
+	iommu_hsio_dmac: mmu400@35600000 {
+		status = "disabled";
+		compatible = "arm,mmu-400";
+		reg = <0 0x35600000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 79 4>, <0 78 4>;
+		mmu-masters = <&dma330_hsio 0 1>;
+		stream-ids = <0>;
+		power-domains = <&power 2>;
+	};
+	iommu_mpb: mmu400@37800000 {
+		status = "disabled";
+		compatible = "arm,mmu-400";
+		reg = <0 0x37800000 0x10000>;
+		#global-interrupts = <1>;
+		interrupts = <0 365 4>, <0 364 4>;
+		mmu-masters = <&sdhci1 0 1>, <&fdb0 0 1>, <&jpeg 0 1>, <&alsa 0 1>;
+		stream-ids = <0>, <0>, <0>;
+		power-domains = <&power 2>;
+	};
+
+	fdb0: fdb.0 {
+		compatible = "fujitsu,f_fdb",
+			     "simple-bus";
+
+		id = <0>;
+		#stream-id-cells = <2>;
+		iova-start = <0x90000000>;
+		iova-range = <0x0 0x50000000>;
+		#address-cells = <2>;
+		#size-cells = <1>;
+		ranges;
+
+		/* fpdlink */
+		videophy0: fplink@fb0 {
+			compatible = "fujitsu,f_fpdl_tx";
+			reg = <0 0x37300000 0x8000>;
+			selmap = <1>;
+			interrupts = <0 144 0x4>; /* lock interrupt */
+			sources = <1>; /* bitfield of fbs can be driven by */
+			clocks = <&clk_fpdlink_0_0>, <&clk_fpdlink_1_0>, <&clk_main_3_0>, <&clk_main_5_0>;
+			clock-names = "pllclk", "dpiclk", "pclk", "clk4";
+			power-domains = <&power 17>;
+			simple-bind = <&fb0>;
+		};
+
+		/* FPDLINK IRIS */
+		fb0: iris-fpdlink@fb0 {
+			compatible = "fujitsu,f-iris-fb";
+			reg = <0 0x37000000 0x8000>;
+			interrupts = <0 192 0x4>,
+				     <0 193 0x4>,
+				     <0 194 0x4>,
+				     <0 195 0x4>,
+				     <0 196 0x4>,
+				     <0 197 0x4>,
+				     <0 198 0x4>,
+				     <0 199 0x4>,
+				     <0 200 0x4>,
+				     <0 201 0x4>,
+				     <0 202 0x4>,
+				     <0 203 0x4>,
+				     <0 204 0x4>,
+				     <0 205 0x4>,
+				     <0 206 0x4>,
+				     <0 207 0x4>;
+			id = <0>;
+			simple = <0>;
+			/* EVB LCD timings / resolution */
+			mode = "800x480x32bpp";
+			clocks = <&clk_fpdlink_0_0>, <&clk_main_a_0>, <&clk_main_b_0>;
+			clock-names = "clk1", "clk2", "clk3";
+			power-domains = <&power 17>;
+		};
+
+		fdbdrm0: fdbdrm.0 {
+			status = "okay";
+			compatible = "fujitsu,fdb-drm";
+			bind = <&fb0>;
+			registrations = <2>; /* wait for this many children */
+			default-bpp = <32>; /* set to 16 if 16-bit LCD mode selected */
+
+		};
+	};
+
+	sdhci0: emmc@300c0000 {
+		compatible = "fujitsu,f_sdh30";
+		reg = <0 0x300c0000 0x1000>;
+		interrupts = <0 164 0x4>,
+			     <0 165 0x4>;
+		resume-detect-retry;
+		mmc-caps2 = <0x20>;
+		vendor-hs200 = <0x01000000>;
+		bus-width = <8>;
+		/* clocks = <&clk_alw_c_0>, <&clk_alw_b_0>; */
+		/* clock-names = "sd_sd4clk", "sd_bclk"; */
+		power-domains = <&power 1>;
+	};
+
+	sdhci1: sdio@36600000 {
+		compatible = "fujitsu,f_sdh30";
+		reg = <0 0x36600000 0x1000>;
+		interrupts = <0 172 0x4>,
+			     <0 173 0x4>;
+		resume-detect-retry;
+		/* clocks = <&clk_main_c_0>, <&clk_main_d_0>; */
+		/* clock-names = "sd_sd4clk", "sd_bclk"; */
+		#stream-id-cells = <2>;
+		iova-start = <0x90000000>;
+		iova-range = <0x0 0x50000000>;
+		power-domains = <&power 2>;
+	};
+
+	sdhci2: sdio@31410000 {
+		compatible = "fujitsu,f_sdh30";
+		reg = <0 0x31410000 0x1000>;
+		interrupts = <0 176 0x4>,
+			     <0 177 0x4>;
+		resume-detect-retry;
+		no-dma;
+		/* clocks = <&clk_main_c_0>, <&clk_main_d_0>; */
+		/* clock-names = "sd_sd4clk", "sd_bclk"; */
+		//#stream-id-cells = <2>;
+		//iova-start = <0x90000000>;
+		//iova-range = <0x0 0x50000000>;
+		power-domains = <&power 1>;
+	};
+
+	eth0: ethernet {
+		compatible = "fujitsu,mb86s73-ogma", "fujitsu,ogma";
+		reg = <0 0x31600000 0x10000>, <0 0x31618000 0x4000>, <0 0x3161c000 0x4000>;
+		interrupts = <0 163 0x4>;
+		clocks = <&clk_alw_0_8>;
+		phy-mode = "rgmii";
+		max-speed = <1000>;
+		max-frame-size = <9000>;
+		local-mac-address = [ a4 17 31 73 00 ed ];
+		phy-handle = <&ethphy0>;
+		power-domains = <&power 1>;
+
+		#address-cells = <1>;
+		#size-cells = <0>;
+
+		ethphy0: ethernet-phy@1 {
+			device_type = "ethernet-phy";
+			compatible = "ethernet-phy-ieee802.3-c22";
+			reg = <1>;
+		};
+	};
+
+
+	jpeg: jpeg {
+		compatible = "fujitsu,jpegtx";
+
+		reg = <0 0x36020000 0x10000>;
+		id = <0>;
+		interrupts = <0 190 0x4>;
+		clocks = <&clk_main_5_5>, <&clk_main_8_1>;
+		clock-names = "clk1", "clk2";
+		memory-size = <0x01000000>; /* 16MB */
+		#stream-id-cells = <2>;
+		iova-start = <0x90000000>;
+		iova-range = <0x0 0x50000000>;
+		power-domains = <&power 13>;
+	};
+	
+	mmdecode: mmdecode {
+		compatible = "fujitsu,wave320";
+		reg = <0 0x36000000 0x20000>;
+		id = <0>;
+		interrupts = <0 188 0x4>,
+			     <0 189 0x4>;
+		clocks = <&clk_main_5_5>, <&clk_main_8_1>;
+		clock-names = "clk1", "clk2";
+		memory-size = <0x04000000>; /* 64MB */
+		#stream-id-cells = <2>;
+		iova-start = <0x90000000>;
+		iova-range = <0x0 0x50000000>;
+		power-domains = <&power 12>;
+	};
+
+	i2s4: fsaif@36440000 {
+		compatible = "fujitsu,f_saif";
+
+		reg = <0 0x36440000 0x1000>;
+		interrupts = <0 184 0x4>;
+		clocks = <&clk_main_3_6>;
+		power-domains = <&power 2>;
+	};
+
+	i2s5: fsaif@36450000 {
+		compatible = "fujitsu,f_saif";
+
+		reg = <0 0x36450000 0x1000>;
+		interrupts = <0 185 0x4>;
+		clocks = <&clk_main_3_6>;
+		power-domains = <&power 2>;
+	};
+
+	alsa: mb86s70_wm8973 {
+		compatible = "fujitsu,mb86s70_wm8973";
+		reg = <0 0 0>; /* fake to stop xx.123 disease */
+		channel = <0>;
+		ms_mode = <0x4000>; // SND_SOC_DAIFMT_CBS_CFS
+		format = <1>; // SND_SOC_DAIFMT_I2S
+		use_eclk = <1>; // 0 = AHB
+		#stream-id-cells = <2>;
+		iova-start = <0x90000000>;
+		iova-range = <0x0 0x50000000>;
+	};
+
+	pcm0: mb8ac0300_pcm@0 {
+		compatible = "fujitsu,mb8ac0300_pcm";
+		reg = <0 0 0>; /* fake to stop xx.123 disease */
+		/* which hdmac chip we are hooked to */
+		hdmac = <1>;
+		/* which periperhal dma request lines we are hooked to */
+		hdmac-trig-pid-capture =  <0x11000000>; /* HDMACA_IS_IDREQ1H */
+		hdmac-trig-pid-playback = <0x12000000>; /* HDMACA_IS_IDREQ2H */
+		power-domains = <&power 1>;
+	};
+
+	pcie1_x2len:pcie1_x2len@0x33E00600 {
+		compatible = "fujitsu,sysoc-f_pcie_dmx";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		id = <2>;
+		ranges;
+		reg = <0 0x33E00600 0x64>, <0 0x33E00168 0x4>, <0 0x33F00000 0x60>;
+		clock_num = <0>;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 21>;
+
+		/* PCIE_1(X2) */
+		pcie1_x2len_core: pcie1_x2len_core@320C0000 {
+			compatible = "fujitsu,mb86s7x-pcie_dme-integration";
+			ven_dev_id = <0x10cf 0x2051>;
+			/* disable msi */
+			reg = <0 0x320C0000 0x20000>, <0 0x320E0000 0x1000>;
+			/* enable msi */
+			/* reg = <0 0x320C0000 0x20000>, <0 0x320E0000 0x1000>, <0 0x320E1200 0x1000>; */
+			interrupts = <0 450 4>, <0 451 4>, <0 452 4>, <0 453 4>,
+				<0 454 4>, <0 457 4>, <0 459 4>, <0 460 4>,
+				<0 461 4>, <0 462 4>, <0 434 4>;
+			dma_int_off = <6>;
+			pme_int_off = <4>;
+			/* 0: INTx, 1:MSI, 2:MSI-X */
+			int_type = <0>;
+			/* 0: 1_type, 1: 2_type, 2: 4_type ...  5: 32_type */
+			msi-multi = <0>;
+			clocks = <&clk_alw_c_0>,<&clk_alw_e_0>,<&clk_alw_c_2>;
+			clock-names = "pcie2_aclk","pcie2_pclk","pcie2_aclk_hsiob";
+			clock_num = <3>;
+			link_spd = <1>; /* 0=>2.5GT/s, 1=>2.5GT/s and 5.0GT/s */
+			link_wid = <4>; /* 1=> max land x1, 4=> max land x4 */
+			#address-cells = <3>;
+			#size-cells = <2>;
+			ranges = <0x01000000 0x0 0x00000000 0x0 0x5C000000 0 0x08000000
+					0x02000000 0x0 0x08000000 0x0 0x78000000 0 0x02000000
+					0x02000000 0x0 0x0A000000 0x0 0x7A000000 0 0x02000000
+					0x02000000 0x0 0x0C000000 0x0 0x7C000000 0 0x02000000
+					0x02000000 0x0 0x0E000000 0x0 0x7E000000 0 0x02000000
+					0x03000000 0x0 0x80000000 0x10 0x80000000 0 0x40000000
+					0x00000000 0x0 0xC0000000 0x0 0x32100000 0 0x00010000>;
+		};
+	};
+
+	pcie1_x4len:pcie1_x4len@0x33E00100 {
+		compatible = "fujitsu,sysoc-f_pcie_dmx";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		id = <1>;
+		ranges;
+		reg = <0 0x33E00100 0x64>,<0 0 0>, <0 0x33F00000 0x60>;
+		clocks = <&clk_alw_e_3>;
+		clock-names = "pcie1_hclk_i";
+		clock_num = <1>;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 21>;
+
+		/* PCIE_1(X4) or PCIE_1(X2) */
+		pcie1_x4len_core: pcie1_x4len_core@32080000 {
+			compatible = "fujitsu,mb86s7x-pcie_dme-integration";
+			ven_dev_id = <0x10cf 0x204f>;
+			/* disable msi */
+			reg = <0 0x32080000 0x20000>, <0 0x320A0000 0x1000>;
+			/* enable msi */
+			/* reg = <0 0x32080000 0x20000>, <0 0x320A0000 0x1000>, <0 0x320A1200 0x1000>; */
+			interrupts = <0 260 4>, <0 261 4>, <0 262 4>, <0 263 4>,
+				<0 264 4>, <0 267 4>, <0 269 4>, <0 270 4>,
+				<0 271 4>, <0 272 4>, <0 411 4>;
+			dma_int_off = <6>;
+			pme_int_off = <4>;
+			/* 0: INTx, 1:MSI, 2:MSI-X */
+			int_type = <0>;
+			/* 0: 1_type, 1: 2_type, 2: 4_type ...  5: 32_type */
+			msi-multi = <0>;
+			clocks = <&clk_alw_c_0>,<&clk_alw_e_0>,<&clk_alw_c_2>,<&clk_alw_7_3>;
+			clock-names = "pcie1_aclk","pcie1_pclk","pcie1_aclk_hsiob","pcie1_hclk_hsiob";
+			clock_num = <4>;
+			link_spd = <1>; /* 0=>2.5GT/s, 1=>2.5GT/s and 5.0GT/s */
+			link_wid = <4>; /* 1=> max land x1, 4=> max land x4 */
+			#address-cells = <3>;
+			#size-cells = <2>;
+			ranges = <0x01000000 0x0 0x00000000 0x0 0x58000000 0 0x08000000
+					0x02000000 0x0 0x00000000 0x0 0x70000000 0 0x02000000
+					0x02000000 0x0 0x02000000 0x0 0x72000000 0 0x02000000
+					0x02000000 0x0 0x04000000 0x0 0x74000000 0 0x02000000
+					0x02000000 0x0 0x06000000 0x0 0x76000000 0 0x02000000
+					0x03000000 0x0 0x40000000 0x6 0x40000000 0 0x40000000
+					0x00000000 0x0 0xC0000000 0x0 0x320F0000 0 0x00010000>;
+		};
+	};
+
+	pcie0:pcie0@0x33E00000{
+		compatible = "fujitsu,sysoc-f_pcie_dmx";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		id = <0>;
+		ranges;
+		reg = <0 0x33E00000 0x64>,<0 0 0>, <0 0x33F00000 0x60>;
+		clocks = <&clk_alw_e_2>;
+		clock-names = "pcie0_hclk_i";
+		clock_num = <1>;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 20>;
+
+		pcie0_core: pcie0_core@32000000 {
+			compatible = "fujitsu,mb86s7x-pcie_dme-integration";
+			ven_dev_id = <0x10cf 0x204c>;
+			/* disable msi */
+			reg = <0 0x32000000 0x20000>, <0 0x32020000 0x10000>;
+			/* enable msi */
+			/* reg = <0 0x32000000 0x20000>, <0 0x32020000 0x10000>, <0 0x32021200 0x1000>; */
+			interrupts = <0 240 4>, <0 241 4>, <0 242 4>, <0 243 4>,
+				<0 244 4>, <0 247 4>, <0 249 4>, <0 250 4>,
+				<0 251 4>, <0 252 4>, <0 407 4>;
+			dma_int_off = <6>;
+			pme_int_off = <4>;
+			/* 0: INTx, 1:MSI, 2:MSI-X */
+			int_type = <0>;
+			/* 0: 1_type, 1: 2_type, 2: 4_type ...  5: 32_type */
+			msi-multi = <0>;
+			clocks = <&clk_alw_c_0>,<&clk_alw_e_0>,<&clk_alw_c_1>,<&clk_alw_7_0>;
+			clock-names = "pcie0_aclk","pcie0_pclk","pcie0_aclk_hsiob","pcie0_hclk_hsiob";
+			clock_num = <2>;
+			link_spd = <1>; /* 0=>2.5GT/s, 1=>2.5GT/s and 5.0GT/s */
+			link_wid = <4>; /* 1=> max land x1, 4=> max land x4 */
+			/* ranges = <phys.hi phys.mid phys.low cpu range */
+			#address-cells = <3>;
+			#size-cells = <2>;
+			ranges = <0x01000000 0x0 0x00000000 0x0 0x50000000 0 0x08000000
+					0x02000000 0x0 0x00000000 0x0 0x60000000 0 0x04000000
+					0x02000000 0x0 0x02000000 0x0 0x64000000 0 0x04000000
+					0x02000000 0x0 0x0C000000 0x0 0x6C000000 0 0x04000000
+					0x03000000 0x0 0x40000000 0x4 0x40000000 0 0x40000000
+					0x03000000 0x0 0x80000000 0x4 0x80000000 0 0x40000000
+					0x00000000 0x0 0xC0000000 0x0 0x32060000 0 0x00010000>;
+	
+		};
+	};
+
+	usb3host0: mb86s70_usb3host0 {
+		compatible = "fujitsu,mb86s70-dwc3";
+		clocks = <&clk_alw_9_0>;
+		clock-names = "h_clk";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		ranges;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 24>;
+
+		dwc3@32200000 {
+			compatible = "synopsys,dwc3";
+			reg = <0 0x32200000 0x100000>;
+			interrupts = <0 292 0x4>,
+			             <0 294 0x4>;
+			disconnection-quirk;
+		};
+	};
+
+	usb3host1: mb86s70_usb3host1 {
+		compatible = "fujitsu,mb86s70-dwc3";
+		clocks = <&clk_alw_9_0>;
+		clock-names = "h_clk";
+		#address-cells = <2>;
+		#size-cells = <1>;
+		ranges;
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 25>;
+
+		dwc3@32200000 {
+			compatible = "synopsys,dwc3";
+			reg = <0 0x32300000 0x100000>;
+			interrupts = <0 412 0x4>,
+			             <0 414 0x4>;
+			disconnection-quirk;
+		};
+	};
+
+	usb2otg: usb2otg@0x34000000 {
+		compatible = "fujitsu,f_usb20hdc_otg";
+		reg = <0 0x34000000 0x10000>;
+		interrupts = <0 296 0x4>;
+		dma_dreq = <0x0e000000 0x0e000000>;
+		hdmac_channel = <0 1>;
+		clocks = <&clk_main_2_3>, <&clk_main_4_4>, <&clk_usb_1_0>;
+		clock-names = "h_clk", "p_clk", "p_cryclk";
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 27>;
+	};
+
+	hcd21: f_usb20ho_hcd {
+		compatible = "fujitsu,f_usb20ho_hcd";
+		reg = <0 0x34200000 0x80000>;
+		interrupts = <0 419 0x4>;
+		clocks = <&clk_main_2_4>, <&clk_main_4_5>, <&clk_usb_0_0>;
+		clock-names = "h_clk", "p_clk", "p_cryclk";
+		#stream-id-cells = <2>;
+		iova-start = <0x00000000>;
+		iova-range = <0x1 0x00000000>;
+		power-domains = <&power 26>;
+	};
+};
diff --git a/arch/arm/boot/dts/mb86s72eb.dts b/arch/arm/boot/dts/mb86s72eb.dts
new file mode 100644
index 0000000..2bc7821
--- /dev/null
+++ b/arch/arm/boot/dts/mb86s72eb.dts
@@ -0,0 +1,330 @@
+/*
+ * Copyright (C) 2014 Linaro, LTD
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+/dts-v1/;
+#include <dt-bindings/gpio/gpio.h>
+#include <dt-bindings/thermal/thermal.h>
+#include "mb86s72.dtsi"
+
+/ {
+	model = "Fujitsu MB86S72 EVB";
+	compatible = "fujitsu,mb86s72eb","fujitsu,mb86s72";
+
+	memory {
+		device_type = "memory";
+		reg = <0 0x80000000 0x80000000>;
+	};
+
+	chosen {
+		bootargs = "no_console_suspend shm_offset=2048 loglevel=4 console=ttyAMA0,115200 root=/dev/mmcblk1p1 rootfstype=ext4 rootwait rw pcie_ports=native ";
+
+		linux,initrd-start = <0xc0000000>;
+		linux,initrd-end =   <0xc0800000>;
+	};
+
+	gpio0: mb86s70_gpio0 {
+		compatible = "fujitsu,mb86s70-gpio";
+		reg = <0 0x31000000 0x10000>;
+		gpio-controller;
+		#gpio-cells = <2>;
+		clocks = <&clk_alw_2_1>;
+		base = <0>;
+	};
+	gpio1: mb86s70_gpio1 {
+		compatible = "fujitsu,mb86s70-gpio";
+		reg = <0 0x31010000 0x10000>;
+		gpio-controller;
+		#gpio-cells = <2>;
+		clocks = <&clk_alw_2_1>;
+		base = <32>;
+       };
+
+	videomodes@0 {
+		compatible = "video-modes";
+		mode0: mode@0 {
+			mode  = "800x480x32bpp";
+			hactive = <800>; /* htotal 928 (+128) */
+			vactive = <480>; /* Vtotal 525 (+95) */
+			hback-porch = <96>;
+			hfront-porch = <1>;
+			hsync-len = <31>;
+			vback-porch = <37>;
+			vfront-porch = <55>;
+			vsync-len = <3>;
+			flags = <0x25>; /* h low, v low, de high */
+			clock = <32800>; /* 32.8MHz link clock */
+			bpp = <32>;
+			width-mm = <163>;
+			height-mm = <92>;
+			/* scanout-rotation = <0>; */
+			red_offset      = <16>;
+			red_length      = <8>;
+			green_offset    = <8>;
+			green_length    = <8>;
+			blue_offset     = <0>;
+			blue_length     = <8>;
+			alpha_offset    = <0>;
+			alpha_length    = <0>;
+		};
+
+		mode1: mode@1 {
+			mode  = "800x480x16bpp";
+			hactive = <800>; /* htotal 928 (+128) */
+			vactive = <480>; /* Vtotal 525 (+95) */
+			hback-porch = <96>;
+			hfront-porch = <1>;
+			hsync-len = <31>;
+			vback-porch = <37>;
+			vfront-porch = <55>;
+			vsync-len = <3>;
+			flags = <0x25>; /* h low, v low, de high */
+			clock = <32800>; /* 32.8MHz link clock */
+			bpp = <16>;
+			width-mm = <163>;
+			height-mm = <92>;
+			/* scanout-rotation = <0>; */
+                        red_offset      = <11>;
+                        red_length      = <5>;
+                        green_offset    = <5>;
+                        green_length    = <6>;
+                        blue_offset     = <0>;
+                        blue_length     = <5>;
+                        alpha_offset    = <0>;
+                        alpha_length    = <0>;
+		}; 
+	};
+
+	i2c0 {
+		compatible = "fujitsu,f_i2c";
+		reg = <0 0x31200000 0x10000>;
+		interrupts = <0 312 0x4>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+		clock-frequency = <100000>;
+		clocks = <&clk_alw_2_1>;
+		power-domains = <&power 1>;
+
+		wm8973: wm8973@1a {
+                        compatible = "wolfson,wm8973";
+                        reg = <0x1a>;
+                };
+
+		/* default to fan always on high, even during suspend */
+		amc6821: amc6821@2e {
+			status = "ok";
+                        compatible = "ti,amc6821";
+                        reg = <0x2e>;
+			low_temp = <0x10>; // unreasonably low target temp == fan always on high
+			pwminv = <1>;      //  1, inverted PWM output
+			fan_char = <0x25>;
+                };
+		/* alternative fan profile: fan responds to temperature + off in suspend */
+		amc6821a: amc6821a@2e {
+			status = "disabled";
+                        compatible = "ti,amc6821";
+                        reg = <0x2e>;
+			low_temp = <0x41>; // 32 degrees C
+			pwminv = <1>;      //  1, inverted PWM output
+			fan_char = <0x25>;
+			off-in-suspend; // stop fan during suspend
+                };
+
+	};
+
+	i2c1 {
+		compatible = "fujitsu,f_i2c";
+
+		reg = <0 0x31210000 0x10000>;
+		interrupts = <0 313 0x4>;
+		#address-cells = <1>;
+		#size-cells = <0>;
+		clocks = <&clk_alw_2_1>;
+		clock-frequency = <100000>;
+		power-domains = <&power 1>;
+
+		touch: ili210x@41 {
+			compatible = "ili,ili210x", "ili210x";
+			reg = <0x41>;
+			interrupts = <0 102 0x4>; /* PD54 - EXTINT22 shared */
+		};
+
+		backlight: backlight@42 {
+			compatible = "apnet,b01-bl";
+			reg = <0x42>;
+			interrupts = <0 102 0x4>; /* PD54 - EXTINT22 shared */
+			key1 = <1>; /* KEY_ESC */
+			key2 = <59>; /* KEY_F1 */
+			key3 = <240>;  /* KEY_UNKNOWN */
+		};
+
+	};
+
+	hsspi: hsspi.0 {
+		compatible = "fujitsu,mb86s7x-hsspi";
+		reg = <0 0x30010000 0x1000>,
+		      <0 0x48000000 0x1000000>;
+		interrupts = <0 361 0x4>,
+			     <0 360 0x4>,
+			     <0 362 0x4>;
+		mode = <1>; // HS_SPI_DIRECT_MODE
+		read_operation = <1>; // HS_SPI_NORMAL_READ
+		write_operation = <5>; // HS_SPI_NORMAL_WRITE
+		bank_size = <0x01000000>;	// 16MB flash bank
+		num_chipselect = <2>;
+		addr_width = <3>;
+		dma_rx_dreq = <0x800000>; // XDDES_TF_DREQ6 !!! GUESSED
+		dma_tx_dreq = <0x900000>; // XDDES_TF_DREQ7 !!! GUESSED
+		dma_channel = <0>;
+		sync_on = <1>;
+		clock = <0>; // HS_SPI_HCLK
+		clocks = <&clk_alw_1_8>, <&clk_main_6_8>;
+		clock-names = "iHCLK", "iPCLK";
+		#address-cells = <1>;
+	        #size-cells = <0>;
+
+		flash@0 {
+			compatible = "n25q512a";
+			spi-max-frequency = <31250000>;
+			reg = <0>;
+
+			#address-cells = <1>;
+			#size-cells = <1>;
+
+			spipart0@0 {
+				label = "scb-spi-firmware";
+				reg = <0 0x1000000>;
+			};
+
+		    };
+
+		flash@1 {
+			compatible = "n25q512a";
+			spi-max-frequency = <31250000>;
+			reg = <1>;
+
+			#address-cells = <1>;
+			#size-cells = <1>;
+
+			spipart0@0 {
+				label = "spi-romfs";
+				reg = <0 0x100000>;
+			};
+  			spipart1@1 {
+				label = "spi-user";
+				reg = <0x100000 0xf00000>;
+			};
+		    };
+	};
+
+	gpio-flash@xcs4-gpio {
+		status = "disabled";
+		compatible = "gpio-control-nor";
+		reg = <0 0x2a4d0000 0x10000>; /* prmux */
+		#address-cells = <1>;
+		#size-cells = <1>;
+		bank-width = <2>;
+		device-width = <2>;
+		power-domains = <&power 1>;
+
+		gpios = <&gpio0 30 0>,	/* rdy PD30 */
+			 <&gpio0 8 0>, 	/* nce */
+			 <&gpio0 10 0>, 	/* noe */
+			 <&gpio0 13 0>, 	/* nwe */
+
+			/* 16 data */
+
+			<&gpio0 14 0>,
+			<&gpio0 15 0>,
+			<&gpio0 16 0>,
+			<&gpio0 17 0>,
+			<&gpio0 18 0>,
+			<&gpio0 19 0>,
+			<&gpio0 20 0>,
+			<&gpio0 21 0>,
+			<&gpio0 22 0>,
+			<&gpio0 23 0>,
+			<&gpio0 24 0>,
+			<&gpio0 25 0>,
+			<&gpio0 26 0>,
+			<&gpio0 27 0>,
+			<&gpio0 28 0>,
+			<&gpio0 29 0>,
+
+			/* as many address as we have (22) */
+
+			<&gpio1 0 0>,
+			<&gpio1 1 0>,
+			<&gpio1 2 0>,
+			<&gpio1 3 0>,
+			<&gpio1 4 0>,
+			<&gpio1 5 0>,
+			<&gpio1 6 0>,
+			<&gpio1 7 0>,
+			<&gpio1 8 0>,
+			<&gpio1 9 0>,
+			<&gpio1 10 0>,
+			<&gpio1 11 0>,
+			<&gpio1 12 0>,
+			<&gpio1 13 0>,
+			<&gpio1 14 0>,
+			<&gpio1 15 0>,
+			<&gpio1 16 0>,
+			<&gpio1 17 0>,
+			<&gpio1 18 0>,
+			<&gpio1 19 0>,
+			<&gpio1 20 0>,
+			<&gpio1 21 0>;
+
+		firmware@0 { /* 2MByte firmware region */
+			label = "firmware";
+			reg = <0 0x200000>;
+		};
+
+		romfs@200000 { /* only 8MByte is addressable by gpio */
+			label = "romfs";
+			reg = <0x200000 0x5e0000>;
+		};
+		
+	};
+
+	flash@XSC4 {
+		status = "disabled";
+		compatible = "spansion,s29gl128p", "cfi-flash";
+		reg = <0 0x40000000 0x01000000>;
+		bank-width = <2>;
+		device-width = <2>;
+		power-domains = <&power 1>;
+	};
+
+	/* Enable only UART0 of S73-evb */
+	serial@0x31040000 {
+		status = "okay";
+	};
+	serial@0x31050000 {
+		status = "disabled";
+	};
+	serial@0x31060000 {
+		status = "disabled";
+	};
+
+	timer0: timer0@31080000 {
+		#clock-cells = <1>;
+		compatible = "arm,sp804";
+		reg = <0 0x31080000 0x10000>;
+		interrupts = <0 324 4>,
+			     <0 325 4>;
+		clocks = <&clk_alw_6_8>;
+	};
+
+	timer {
+		compatible = "arm,armv7-timer";
+		clock-frequency = <125000000>;
+		interrupts = <1 13 0xf08>, /* PHYS Secure */
+			     <1 14 0xf08>; /* PHYS Non-Secure */
+	};
+};
diff --git a/arch/arm/common/Makefile b/arch/arm/common/Makefile
index 8c60f47..8e2714f 100644
--- a/arch/arm/common/Makefile
+++ b/arch/arm/common/Makefile
@@ -13,7 +13,12 @@ obj-$(CONFIG_SHARP_PARAM)	+= sharpsl_param.o
 obj-$(CONFIG_SHARP_SCOOP)	+= scoop.o
 obj-$(CONFIG_PCI_HOST_ITE8152)  += it8152.o
 obj-$(CONFIG_ARM_TIMER_SP804)	+= timer-sp.o
+obj-$(CONFIG_ARM_TIMER_SP804)	+= timer-sp-lt.o
 obj-$(CONFIG_MCPM)		+= mcpm_head.o mcpm_entry.o mcpm_platsmp.o vlock.o
+obj-$(CONFIG_BL_SWITCHER)	+= bL_switcher.o
+obj-$(CONFIG_BL_SWITCHER_DUMMY_IF) += bL_switcher_dummy_if.o
+
 AFLAGS_mcpm_head.o		:= -march=armv7-a
 AFLAGS_vlock.o			:= -march=armv7-a
+CFLAGS_REMOVE_mcpm_entry.o	= -pg
 obj-$(CONFIG_TI_PRIV_EDMA)	+= edma.o
diff --git a/arch/arm/common/bL_switcher.c b/arch/arm/common/bL_switcher.c
new file mode 100644
index 0000000..872ff26
--- /dev/null
+++ b/arch/arm/common/bL_switcher.c
@@ -0,0 +1,864 @@
+/*
+ * arch/arm/common/bL_switcher.c -- big.LITTLE cluster switcher core driver
+ *
+ * Created by:	Nicolas Pitre, March 2012
+ * Copyright:	(C) 2012  Linaro Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/atomic.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/interrupt.h>
+#include <linux/cpu_pm.h>
+#include <linux/cpu.h>
+#include <linux/cpumask.h>
+#include <linux/kthread.h>
+#include <linux/wait.h>
+#include <linux/time.h>
+#include <linux/clockchips.h>
+#include <linux/hrtimer.h>
+#include <linux/tick.h>
+#include <linux/notifier.h>
+#include <linux/mm.h>
+#include <linux/mutex.h>
+#include <linux/smp.h>
+#include <linux/spinlock.h>
+#include <linux/string.h>
+#include <linux/sysfs.h>
+#include <linux/irqchip/arm-gic.h>
+#include <linux/moduleparam.h>
+
+#include <asm/smp_plat.h>
+#include <asm/cacheflush.h>
+#include <asm/cputype.h>
+#include <asm/suspend.h>
+#include <asm/mcpm.h>
+#include <asm/bL_switcher.h>
+
+#define CREATE_TRACE_POINTS
+//#include <trace/events/power_cpu_migrate.h>
+
+
+/*
+ * Use our own MPIDR accessors as the generic ones in asm/cputype.h have
+ * __attribute_const__ and we don't want the compiler to assume any
+ * constness here as the value _does_ change along some code paths.
+ */
+
+static int read_mpidr(void)
+{
+	unsigned int id;
+	asm volatile ("mrc\tp15, 0, %0, c0, c0, 5" : "=r" (id));
+	return id & MPIDR_HWID_BITMASK;
+}
+
+/*
+ * Get a global nanosecond time stamp for tracing.
+ */
+static s64 get_ns(void)
+{
+	struct timespec ts;
+	getnstimeofday(&ts);
+	return timespec_to_ns(&ts);
+}
+
+/*
+ * bL switcher core code.
+ */
+
+static void bL_do_switch(void *_arg)
+{
+	unsigned ib_mpidr, ib_cpu, ib_cluster;
+	long volatile handshake, **handshake_ptr = _arg;
+
+	pr_debug("%s\n", __func__);
+
+	ib_mpidr = cpu_logical_map(smp_processor_id());
+	ib_cpu = MPIDR_AFFINITY_LEVEL(ib_mpidr, 0);
+	ib_cluster = MPIDR_AFFINITY_LEVEL(ib_mpidr, 1);
+
+	/* Advertise our handshake location */
+	if (handshake_ptr) {
+		handshake = 0;
+		*handshake_ptr = &handshake;
+	} else
+		handshake = -1;
+
+	/*
+	 * Our state has been saved at this point.  Let's release our
+	 * inbound CPU.
+	 */
+	mcpm_set_entry_vector(ib_cpu, ib_cluster, cpu_resume);
+	sev();
+
+	/*
+	 * From this point, we must assume that our counterpart CPU might
+	 * have taken over in its parallel world already, as if execution
+	 * just returned from cpu_suspend().  It is therefore important to
+	 * be very careful not to make any change the other guy is not
+	 * expecting.  This is why we need stack isolation.
+	 *
+	 * Fancy under cover tasks could be performed here.  For now
+	 * we have none.
+	 */
+
+	/*
+	 * Let's wait until our inbound is alive.
+	 */
+	while (!handshake) {
+		wfe();
+		smp_mb();
+	}
+
+	/* Let's put ourself down. */
+	mcpm_cpu_power_down();
+
+	/* should never get here */
+	BUG();
+}
+
+/*
+ * Stack isolation.  To ensure 'current' remains valid, we just use another
+ * piece of our thread's stack space which should be fairly lightly used.
+ * The selected area starts just above the thread_info structure located
+ * at the very bottom of the stack, aligned to a cache line, and indexed
+ * with the cluster number.
+ */
+#define STACK_SIZE 512
+extern void call_with_stack(void (*fn)(void *), void *arg, void *sp);
+static int bL_switchpoint(unsigned long _arg)
+{
+	unsigned int mpidr = read_mpidr();
+	unsigned int clusterid = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+	void *stack = current_thread_info() + 1;
+	stack = PTR_ALIGN(stack, L1_CACHE_BYTES);
+	stack += clusterid * STACK_SIZE + STACK_SIZE;
+	call_with_stack(bL_do_switch, (void *)_arg, stack);
+	BUG();
+}
+
+/*
+ * Generic switcher interface
+ */
+
+static unsigned int bL_gic_id[MAX_CPUS_PER_CLUSTER][MAX_NR_CLUSTERS];
+static int bL_switcher_cpu_pairing[NR_CPUS];
+
+/*
+ * bL_switch_to - Switch to a specific cluster for the current CPU
+ * @new_cluster_id: the ID of the cluster to switch to.
+ *
+ * This function must be called on the CPU to be switched.
+ * Returns 0 on success, else a negative status code.
+ */
+static int bL_switch_to(unsigned int new_cluster_id)
+{
+	unsigned int mpidr, this_cpu, that_cpu;
+	unsigned int ob_mpidr, ob_cpu, ob_cluster, ib_mpidr, ib_cpu, ib_cluster;
+	struct completion inbound_alive;
+	struct tick_device *tdev;
+	enum clock_event_mode tdev_mode;
+	long volatile *handshake_ptr;
+	int ipi_nr, ret;
+
+	this_cpu = smp_processor_id();
+	ob_mpidr = read_mpidr();
+	ob_cpu = MPIDR_AFFINITY_LEVEL(ob_mpidr, 0);
+	ob_cluster = MPIDR_AFFINITY_LEVEL(ob_mpidr, 1);
+	BUG_ON(cpu_logical_map(this_cpu) != ob_mpidr);
+
+	if (new_cluster_id == ob_cluster)
+		return 0;
+
+	that_cpu = bL_switcher_cpu_pairing[this_cpu];
+	ib_mpidr = cpu_logical_map(that_cpu);
+	ib_cpu = MPIDR_AFFINITY_LEVEL(ib_mpidr, 0);
+	ib_cluster = MPIDR_AFFINITY_LEVEL(ib_mpidr, 1);
+
+	pr_debug("before switch: CPU %d MPIDR %#x -> %#x\n",
+		 this_cpu, ob_mpidr, ib_mpidr);
+
+	this_cpu = smp_processor_id();
+
+	/* Close the gate for our entry vectors */
+	mcpm_set_entry_vector(ob_cpu, ob_cluster, NULL);
+	mcpm_set_entry_vector(ib_cpu, ib_cluster, NULL);
+
+	/* Install our "inbound alive" notifier. */
+	init_completion(&inbound_alive);
+	ipi_nr = register_ipi_completion(&inbound_alive, this_cpu);
+	ipi_nr |= ((1 << 16) << bL_gic_id[ob_cpu][ob_cluster]);
+	mcpm_set_early_poke(ib_cpu, ib_cluster, gic_get_sgir_physaddr(), ipi_nr);
+
+	/*
+	 * Let's wake up the inbound CPU now in case it requires some delay
+	 * to come online, but leave it gated in our entry vector code.
+	 */
+	ret = mcpm_cpu_power_up(ib_cpu, ib_cluster);
+	if (ret) {
+		pr_err("%s: mcpm_cpu_power_up() returned %d\n", __func__, ret);
+		return ret;
+	}
+
+	/*
+	 * Raise a SGI on the inbound CPU to make sure it doesn't stall
+	 * in a possible WFI, such as in bL_power_down().
+	 */
+	gic_send_sgi(bL_gic_id[ib_cpu][ib_cluster], 0);
+
+	/*
+	 * Wait for the inbound to come up.  This allows for other
+	 * tasks to be scheduled in the mean time.
+	 */
+	wait_for_completion(&inbound_alive);
+	mcpm_set_early_poke(ib_cpu, ib_cluster, 0, 0);
+
+	/*
+	 * From this point we are entering the switch critical zone
+	 * and can't sleep/schedule anymore.
+	 */
+	local_irq_disable();
+	local_fiq_disable();
+	//trace_cpu_migrate_begin(get_ns(), ob_mpidr);
+
+	/* redirect GIC's SGIs to our counterpart */
+	gic_migrate_target(bL_gic_id[ib_cpu][ib_cluster]);
+
+	tdev = tick_get_device(this_cpu);
+	if (tdev && !cpumask_equal(tdev->evtdev->cpumask, cpumask_of(this_cpu)))
+		tdev = NULL;
+	if (tdev) {
+		tdev_mode = tdev->evtdev->mode;
+		clockevents_set_mode(tdev->evtdev, CLOCK_EVT_MODE_SHUTDOWN);
+	}
+
+	ret = cpu_pm_enter();
+
+	/* we can not tolerate errors at this point */
+	if (ret)
+		panic("%s: cpu_pm_enter() returned %d\n", __func__, ret);
+
+	/*
+	 * Swap the physical CPUs in the logical map for this logical CPU.
+	 * This must be flushed to RAM as the resume code
+	 * needs to access it while the caches are still disabled.
+	 */
+	cpu_logical_map(this_cpu) = ib_mpidr;
+	cpu_logical_map(that_cpu) = ob_mpidr;
+	sync_cache_w(&cpu_logical_map(this_cpu));
+
+	/* Let's do the actual CPU switch. */
+	ret = cpu_suspend((unsigned long)&handshake_ptr, bL_switchpoint);
+	if (ret > 0)
+		panic("%s: cpu_suspend() returned %d\n", __func__, ret);
+
+	/* We are executing on the inbound CPU at this point */
+	mpidr = read_mpidr();
+	pr_debug("after switch: CPU %d MPIDR %#x\n", this_cpu, mpidr);
+	BUG_ON(mpidr != ib_mpidr);
+
+	mcpm_cpu_powered_up();
+
+	ret = cpu_pm_exit();
+
+	if (tdev) {
+		clockevents_set_mode(tdev->evtdev, tdev_mode);
+		clockevents_program_event(tdev->evtdev,
+					  tdev->evtdev->next_event, 1);
+	}
+
+	//trace_cpu_migrate_finish(get_ns(), ib_mpidr);
+	local_fiq_enable();
+	local_irq_enable();
+
+	*handshake_ptr = 1;
+	dsb_sev();
+
+	if (ret)
+		pr_err("%s exiting with error %d\n", __func__, ret);
+	return ret;
+}
+
+struct bL_thread {
+	spinlock_t lock;
+	struct task_struct *task;
+	wait_queue_head_t wq;
+	int wanted_cluster;
+	struct completion started;
+	bL_switch_completion_handler completer;
+	void *completer_cookie;
+};
+
+static struct bL_thread bL_threads[NR_CPUS];
+
+static int bL_switcher_thread(void *arg)
+{
+	struct bL_thread *t = arg;
+	struct sched_param param = { .sched_priority = 1 };
+	int cluster;
+	bL_switch_completion_handler completer;
+	void *completer_cookie;
+
+	sched_setscheduler_nocheck(current, SCHED_FIFO, &param);
+	complete(&t->started);
+
+	do {
+		if (signal_pending(current))
+			flush_signals(current);
+		wait_event_interruptible(t->wq,
+				t->wanted_cluster != -1 ||
+				kthread_should_stop());
+
+		spin_lock(&t->lock);
+		cluster = t->wanted_cluster;
+		completer = t->completer;
+		completer_cookie = t->completer_cookie;
+		t->wanted_cluster = -1;
+		t->completer = NULL;
+		spin_unlock(&t->lock);
+
+		if (cluster != -1) {
+			bL_switch_to(cluster);
+
+			if (completer)
+				completer(completer_cookie);
+		}
+	} while (!kthread_should_stop());
+
+	return 0;
+}
+
+static struct task_struct * bL_switcher_thread_create(int cpu, void *arg)
+{
+	struct task_struct *task;
+
+	task = kthread_create_on_node(bL_switcher_thread, arg,
+				      cpu_to_node(cpu), "kswitcher_%d", cpu);
+	if (!IS_ERR(task)) {
+		kthread_bind(task, cpu);
+		wake_up_process(task);
+	} else
+		pr_err("%s failed for CPU %d\n", __func__, cpu);
+	return task;
+}
+
+/*
+ * bL_switch_request_cb - Switch to a specific cluster for the given CPU,
+ *      with completion notification via a callback
+ *
+ * @cpu: the CPU to switch
+ * @new_cluster_id: the ID of the cluster to switch to.
+ * @completer: switch completion callback.  if non-NULL,
+ *	@completer(@completer_cookie) will be called on completion of
+ *	the switch, in non-atomic context.
+ * @completer_cookie: opaque context argument for @completer.
+ *
+ * This function causes a cluster switch on the given CPU by waking up
+ * the appropriate switcher thread.  This function may or may not return
+ * before the switch has occurred.
+ *
+ * If a @completer callback function is supplied, it will be called when
+ * the switch is complete.  This can be used to determine asynchronously
+ * when the switch is complete, regardless of when bL_switch_request()
+ * returns.  When @completer is supplied, no new switch request is permitted
+ * for the affected CPU until after the switch is complete, and @completer
+ * has returned.
+ */
+int bL_switch_request_cb(unsigned int cpu, unsigned int new_cluster_id,
+			 bL_switch_completion_handler completer,
+			 void *completer_cookie)
+{
+	struct bL_thread *t;
+
+	if (cpu >= ARRAY_SIZE(bL_threads)) {
+		pr_err("%s: cpu %d out of bounds\n", __func__, cpu);
+		return -EINVAL;
+	}
+
+	t = &bL_threads[cpu];
+
+	if (IS_ERR(t->task))
+		return PTR_ERR(t->task);
+	if (!t->task)
+		return -ESRCH;
+
+	spin_lock(&t->lock);
+	if (t->completer) {
+		spin_unlock(&t->lock);
+		return -EBUSY;
+	}
+	t->completer = completer;
+	t->completer_cookie = completer_cookie;
+	t->wanted_cluster = new_cluster_id;
+	spin_unlock(&t->lock);
+	wake_up(&t->wq);
+	return 0;
+}
+
+EXPORT_SYMBOL_GPL(bL_switch_request_cb);
+
+/*
+ * Detach an outstanding switch request.
+ *
+ * The switcher will continue with the switch request in the background,
+ * but the completer function will not be called.
+ *
+ * This may be necessary if the completer is in a kernel module which is
+ * about to be unloaded.
+ */
+void bL_switch_request_detach(unsigned int cpu,
+			      bL_switch_completion_handler completer)
+{
+	struct bL_thread *t;
+
+	if (cpu >= ARRAY_SIZE(bL_threads)) {
+		pr_err("%s: cpu %d out of bounds\n", __func__, cpu);
+		return;
+	}
+
+	t = &bL_threads[cpu];
+
+	if (IS_ERR(t->task) || !t->task)
+		return;
+
+	spin_lock(&t->lock);
+	if (t->completer == completer)
+		t->completer = NULL;
+	spin_unlock(&t->lock);
+}
+
+EXPORT_SYMBOL_GPL(bL_switch_request_detach);
+
+/*
+ * Activation and configuration code.
+ */
+
+static DEFINE_MUTEX(bL_switcher_activation_lock);
+static BLOCKING_NOTIFIER_HEAD(bL_activation_notifier);
+static unsigned int bL_switcher_active;
+static unsigned int bL_switcher_cpu_original_cluster[NR_CPUS];
+static cpumask_t bL_switcher_removed_logical_cpus;
+
+int bL_switcher_register_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_register(&bL_activation_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(bL_switcher_register_notifier);
+
+int bL_switcher_unregister_notifier(struct notifier_block *nb)
+{
+	return blocking_notifier_chain_unregister(&bL_activation_notifier, nb);
+}
+EXPORT_SYMBOL_GPL(bL_switcher_unregister_notifier);
+
+static int bL_activation_notify(unsigned long val)
+{
+	int ret;
+       
+	ret = blocking_notifier_call_chain(&bL_activation_notifier, val, NULL);
+	if (ret & NOTIFY_STOP_MASK)
+		pr_err("%s: notifier chain failed with status 0x%x\n",
+			__func__, ret);
+	return notifier_to_errno(ret);
+}
+
+static void bL_switcher_restore_cpus(void)
+{
+	int i;
+
+	for_each_cpu(i, &bL_switcher_removed_logical_cpus)
+		cpu_up(i);
+}
+
+static int bL_switcher_halve_cpus(void)
+{
+	int i, j, cluster_0, gic_id, ret;
+	unsigned int cpu, cluster, mask;
+	cpumask_t available_cpus;
+
+	/* First pass to validate what we have */
+	mask = 0;
+	for_each_online_cpu(i) {
+		cpu = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 0);
+		cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);
+		if (cluster >= 2) {
+			pr_err("%s: only dual cluster systems are supported\n", __func__);
+			return -EINVAL;
+		}
+		if (WARN_ON(cpu >= MAX_CPUS_PER_CLUSTER))
+			return -EINVAL;
+		mask |= (1 << cluster);
+	}
+	if (mask != 3) {
+		pr_info("%s: no CPU pairing possible\n", __func__);
+		return -EINVAL;
+	}
+
+	/*
+	 * Now let's do the pairing.  We match each CPU with another CPU
+	 * from a different cluster.  To get a uniform scheduling behavior
+	 * without fiddling with CPU topology and compute capacity data,
+	 * we'll use logical CPUs initially belonging to the same cluster.
+	 */
+	memset(bL_switcher_cpu_pairing, -1, sizeof(bL_switcher_cpu_pairing));
+	cpumask_copy(&available_cpus, cpu_online_mask);
+	cluster_0 = -1;
+	for_each_cpu(i, &available_cpus) {
+		int match = -1;
+		cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);
+		if (cluster_0 == -1)
+			cluster_0 = cluster;
+		if (cluster != cluster_0)
+			continue;
+		cpumask_clear_cpu(i, &available_cpus);
+		for_each_cpu(j, &available_cpus) {
+			cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(j), 1);
+			/*
+			 * Let's remember the last match to create "odd"
+			 * pairing on purpose in order for other code not
+			 * to assume any relation between physical and
+			 * logical CPU numbers.
+			 */
+			if (cluster != cluster_0)
+				match = j;
+		}
+		if (match != -1) {
+			bL_switcher_cpu_pairing[i] = match;
+			cpumask_clear_cpu(match, &available_cpus);
+			pr_info("CPU%d paired with CPU%d\n", i, match);
+		}
+	}
+
+	/*
+	 * Now we disable the unwanted CPUs i.e. everything that has no
+	 * pairing information (that includes the pairing counterparts).
+	 */ 
+	cpumask_clear(&bL_switcher_removed_logical_cpus);
+	for_each_online_cpu(i) {
+		cpu = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 0);
+		cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(i), 1);
+
+		/* Let's take note of the GIC ID for this CPU */
+		gic_id = gic_get_cpu_id(i);
+		if (gic_id < 0) {
+			pr_err("%s: bad GIC ID for CPU %d\n", __func__, i);
+			bL_switcher_restore_cpus();
+			return -EINVAL;
+		}
+		bL_gic_id[cpu][cluster] = gic_id;
+		pr_info("GIC ID for CPU %u cluster %u is %u\n",
+			cpu, cluster, gic_id);
+
+		if (bL_switcher_cpu_pairing[i] != -1) {
+			bL_switcher_cpu_original_cluster[i] = cluster;
+			continue;
+		}
+
+		ret = cpu_down(i);
+		if (ret) {
+			bL_switcher_restore_cpus();
+			return ret;
+		}
+		cpumask_set_cpu(i, &bL_switcher_removed_logical_cpus);
+	}
+
+	return 0;
+}
+
+/* Determine the logical CPU a given physical CPU is grouped on. */
+int bL_switcher_get_logical_index(u32 mpidr)
+{
+	int cpu;
+
+	if (!bL_switcher_active)
+		return -EUNATCH;
+
+	mpidr &= MPIDR_HWID_BITMASK;
+	for_each_online_cpu(cpu) {
+		int pairing = bL_switcher_cpu_pairing[cpu];
+		if (pairing == -1)
+			continue;
+		if ((mpidr == cpu_logical_map(cpu)) ||
+		    (mpidr == cpu_logical_map(pairing)))
+			return cpu;
+	}
+	return -EINVAL;
+}
+
+static void bL_switcher_trace_trigger_cpu(void *__always_unused info)
+{
+	//trace_cpu_migrate_current(get_ns(), read_mpidr());
+}
+
+int bL_switcher_trace_trigger(void)
+{
+	int ret;
+
+	preempt_disable();
+
+	bL_switcher_trace_trigger_cpu(NULL);
+	ret = smp_call_function(bL_switcher_trace_trigger_cpu, NULL, true);
+
+	preempt_enable();
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(bL_switcher_trace_trigger);
+
+static int bL_switcher_enable(void)
+{
+	int cpu, ret;
+
+	mutex_lock(&bL_switcher_activation_lock);
+	cpu_hotplug_driver_lock();
+	if (bL_switcher_active) {
+		cpu_hotplug_driver_unlock();
+		mutex_unlock(&bL_switcher_activation_lock);
+		return 0;
+	}
+
+	pr_info("big.LITTLE switcher initializing\n");
+
+	ret = bL_activation_notify(BL_NOTIFY_PRE_ENABLE);
+	if (ret)
+		goto error;
+
+	ret = bL_switcher_halve_cpus();
+	if (ret)
+		goto error;
+
+	bL_switcher_trace_trigger();
+
+	for_each_online_cpu(cpu) {
+		struct bL_thread *t = &bL_threads[cpu];
+		spin_lock_init(&t->lock);
+		init_waitqueue_head(&t->wq);
+		init_completion(&t->started);
+		t->wanted_cluster = -1;
+		t->task = bL_switcher_thread_create(cpu, t);
+	}
+
+	bL_switcher_active = 1;
+	bL_activation_notify(BL_NOTIFY_POST_ENABLE);
+	pr_info("big.LITTLE switcher initialized\n");
+	goto out;
+
+error:
+	pr_warning("big.LITTLE switcher initialization failed\n");
+	bL_activation_notify(BL_NOTIFY_POST_DISABLE);
+
+out:
+	cpu_hotplug_driver_unlock();
+	mutex_unlock(&bL_switcher_activation_lock);
+	return ret;
+}
+
+#ifdef CONFIG_SYSFS
+
+static void bL_switcher_disable(void)
+{
+	unsigned int cpu, cluster;
+	struct bL_thread *t;
+	struct task_struct *task;
+
+	mutex_lock(&bL_switcher_activation_lock);
+	cpu_hotplug_driver_lock();
+
+	if (!bL_switcher_active)
+		goto out;
+
+	if (bL_activation_notify(BL_NOTIFY_PRE_DISABLE) != 0) {
+		bL_activation_notify(BL_NOTIFY_POST_ENABLE);
+		goto out;
+	}
+
+	bL_switcher_active = 0;
+
+	/*
+	 * To deactivate the switcher, we must shut down the switcher
+	 * threads to prevent any other requests from being accepted.
+	 * Then, if the final cluster for given logical CPU is not the
+	 * same as the original one, we'll recreate a switcher thread
+	 * just for the purpose of switching the CPU back without any
+	 * possibility for interference from external requests.
+	 */
+	for_each_online_cpu(cpu) {
+		t = &bL_threads[cpu];
+		task = t->task;
+		t->task = NULL;
+		if (!task || IS_ERR(task))
+			continue;
+		kthread_stop(task);
+		/* no more switch may happen on this CPU at this point */
+		cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(cpu), 1);
+		if (cluster == bL_switcher_cpu_original_cluster[cpu])
+			continue;
+		init_completion(&t->started);
+		t->wanted_cluster = bL_switcher_cpu_original_cluster[cpu];
+		task = bL_switcher_thread_create(cpu, t);
+		if (!IS_ERR(task)) {
+			wait_for_completion(&t->started);
+			kthread_stop(task);
+			cluster = MPIDR_AFFINITY_LEVEL(cpu_logical_map(cpu), 1);
+			if (cluster == bL_switcher_cpu_original_cluster[cpu])
+				continue;
+		}
+		/* If execution gets here, we're in trouble. */
+		pr_crit("%s: unable to restore original cluster for CPU %d\n",
+			__func__, cpu);
+		pr_crit("%s: CPU %d can't be restored\n",
+			__func__, bL_switcher_cpu_pairing[cpu]);
+		cpumask_clear_cpu(bL_switcher_cpu_pairing[cpu],
+				  &bL_switcher_removed_logical_cpus);
+	}
+
+	bL_switcher_restore_cpus();
+	bL_switcher_trace_trigger();
+
+	bL_activation_notify(BL_NOTIFY_POST_DISABLE);
+
+out:
+	cpu_hotplug_driver_unlock();
+	mutex_unlock(&bL_switcher_activation_lock);
+}
+
+static ssize_t bL_switcher_active_show(struct kobject *kobj,
+		struct kobj_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%u\n", bL_switcher_active);
+}
+
+static ssize_t bL_switcher_active_store(struct kobject *kobj,
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int ret;
+
+	switch (buf[0]) {
+	case '0':
+		bL_switcher_disable();
+		ret = 0;
+		break;
+	case '1':
+		ret = bL_switcher_enable();
+		break;
+	default:
+		ret = -EINVAL;
+	}
+
+	return (ret >= 0) ? count : ret;
+}
+
+static ssize_t bL_switcher_trace_trigger_store(struct kobject *kobj,
+		struct kobj_attribute *attr, const char *buf, size_t count)
+{
+	int ret = bL_switcher_trace_trigger();
+
+	return ret ? ret : count;
+}
+
+static struct kobj_attribute bL_switcher_active_attr =
+	__ATTR(active, 0644, bL_switcher_active_show, bL_switcher_active_store);
+
+static struct kobj_attribute bL_switcher_trace_trigger_attr =
+	__ATTR(trace_trigger, 0200, NULL, bL_switcher_trace_trigger_store);
+
+static struct attribute *bL_switcher_attrs[] = {
+	&bL_switcher_active_attr.attr,
+	&bL_switcher_trace_trigger_attr.attr,
+	NULL,
+};
+
+static struct attribute_group bL_switcher_attr_group = {
+	.attrs = bL_switcher_attrs,
+};
+
+static struct kobject *bL_switcher_kobj;
+
+static int __init bL_switcher_sysfs_init(void)
+{
+	int ret;
+
+	bL_switcher_kobj = kobject_create_and_add("bL_switcher", kernel_kobj);
+	if (!bL_switcher_kobj)
+		return -ENOMEM;
+	ret = sysfs_create_group(bL_switcher_kobj, &bL_switcher_attr_group);
+	if (ret)
+		kobject_put(bL_switcher_kobj);
+	return ret;
+}
+
+#endif  /* CONFIG_SYSFS */
+
+bool bL_switcher_get_enabled(void)
+{
+	mutex_lock(&bL_switcher_activation_lock);
+
+	return bL_switcher_active;
+}
+EXPORT_SYMBOL_GPL(bL_switcher_get_enabled);
+
+void bL_switcher_put_enabled(void)
+{
+	mutex_unlock(&bL_switcher_activation_lock);
+}
+EXPORT_SYMBOL_GPL(bL_switcher_put_enabled);
+
+/*
+ * Veto any CPU hotplug operation while the switcher is active.
+ * We're just not ready to deal with that given the trickery involved.
+ */
+static int bL_switcher_hotplug_callback(struct notifier_block *nfb,
+					unsigned long action, void *hcpu)
+{
+	switch (action) {
+	case CPU_UP_PREPARE:
+	case CPU_DOWN_PREPARE:
+		if (bL_switcher_active)
+			return NOTIFY_BAD;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block bL_switcher_hotplug_notifier =
+        { &bL_switcher_hotplug_callback, NULL, 0 };
+
+#ifdef CONFIG_SCHED_HMP
+static bool no_bL_switcher = true;
+#else
+static bool no_bL_switcher;
+#endif
+core_param(no_bL_switcher, no_bL_switcher, bool, 0644);
+
+static int __init bL_switcher_init(void)
+{
+	int ret;
+
+	if (MAX_NR_CLUSTERS != 2) {
+		pr_err("%s: only dual cluster systems are supported\n", __func__);
+		return -EINVAL;
+	}
+
+	register_cpu_notifier(&bL_switcher_hotplug_notifier);
+
+	if (!no_bL_switcher) {
+		ret = bL_switcher_enable();
+		if (ret)
+			return ret;
+	}
+
+#ifdef CONFIG_SYSFS
+	ret = bL_switcher_sysfs_init();
+	if (ret)
+		pr_err("%s: unable to create sysfs entry\n", __func__);
+#endif
+
+	return 0;
+}
+
+late_initcall(bL_switcher_init);
diff --git a/arch/arm/common/bL_switcher_dummy_if.c b/arch/arm/common/bL_switcher_dummy_if.c
new file mode 100644
index 0000000..5e2dd19
--- /dev/null
+++ b/arch/arm/common/bL_switcher_dummy_if.c
@@ -0,0 +1,71 @@
+/*
+ * arch/arm/common/bL_switcher_dummy_if.c -- b.L switcher dummy interface
+ *
+ * Created by:	Nicolas Pitre, November 2012
+ * Copyright:	(C) 2012  Linaro Limited
+ *
+ * Dummy interface to user space for debugging purpose only.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/miscdevice.h>
+#include <asm/uaccess.h>
+#include <asm/bL_switcher.h>
+
+static ssize_t bL_switcher_write(struct file *file, const char __user *buf,
+			size_t len, loff_t *pos)
+{
+	unsigned char val[3];
+	unsigned int cpu, cluster;
+	int ret;
+
+	pr_debug("%s\n", __func__);
+
+	if (len < 3)
+		return -EINVAL;
+
+	if (copy_from_user(val, buf, 3))
+		return -EFAULT;
+
+	/* format: <cpu#>,<cluster#> */
+	if (val[0] < '0' || val[0] > '4' ||
+	    val[1] != ',' ||
+	    val[2] < '0' || val[2] > '1')
+		return -EINVAL;
+
+	cpu = val[0] - '0';
+	cluster = val[2] - '0';
+	ret = bL_switch_request(cpu, cluster);
+
+	return ret ? : len;
+}
+
+static const struct file_operations bL_switcher_fops = {
+	.write		= bL_switcher_write,
+	.owner	= THIS_MODULE,
+};
+
+static struct miscdevice bL_switcher_device = {
+        MISC_DYNAMIC_MINOR,
+        "b.L_switcher",
+        &bL_switcher_fops
+};
+
+static int __init bL_switcher_dummy_if_init(void)
+{
+	return misc_register(&bL_switcher_device);
+}
+
+static void __exit bL_switcher_dummy_if_exit(void)
+{
+	misc_deregister(&bL_switcher_device);
+}
+
+module_init(bL_switcher_dummy_if_init);
+module_exit(bL_switcher_dummy_if_exit);
diff --git a/arch/arm/common/mcpm_entry.c b/arch/arm/common/mcpm_entry.c
index 370236d..88ae918 100644
--- a/arch/arm/common/mcpm_entry.c
+++ b/arch/arm/common/mcpm_entry.c
@@ -12,11 +12,13 @@
 #include <linux/kernel.h>
 #include <linux/init.h>
 #include <linux/irqflags.h>
+#include <linux/cpu_pm.h>
 
 #include <asm/mcpm.h>
 #include <asm/cacheflush.h>
 #include <asm/idmap.h>
 #include <asm/cputype.h>
+#include <asm/suspend.h>
 
 extern unsigned long mcpm_entry_vectors[MAX_NR_CLUSTERS][MAX_CPUS_PER_CLUSTER];
 
@@ -27,6 +29,18 @@ void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr)
 	sync_cache_w(&mcpm_entry_vectors[cluster][cpu]);
 }
 
+extern unsigned long mcpm_entry_early_pokes[MAX_NR_CLUSTERS][MAX_CPUS_PER_CLUSTER][2];
+
+void mcpm_set_early_poke(unsigned cpu, unsigned cluster,
+			 unsigned long poke_phys_addr, unsigned long poke_val)
+{
+	unsigned long *poke = &mcpm_entry_early_pokes[cluster][cpu][0];
+	poke[0] = poke_phys_addr;
+	poke[1] = poke_val;
+	__cpuc_flush_dcache_area((void *)poke, 8);
+	outer_clean_range(__pa(poke), __pa(poke + 2));
+}
+
 static const struct mcpm_platform_ops *platform_ops;
 
 int __init mcpm_platform_register(const struct mcpm_platform_ops *ops)
@@ -113,6 +127,56 @@ int mcpm_cpu_powered_up(void)
 	return 0;
 }
 
+#ifdef CONFIG_ARM_CPU_SUSPEND
+
+static int __init nocache_trampoline(unsigned long _arg)
+{
+	void (*cache_disable)(void) = (void *)_arg;
+	unsigned int mpidr = read_cpuid_mpidr();
+	unsigned int cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+	unsigned int cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+	phys_reset_t phys_reset;
+
+	mcpm_set_entry_vector(cpu, cluster, cpu_resume);
+	setup_mm_for_reboot();
+
+	__mcpm_cpu_going_down(cpu, cluster);
+	BUG_ON(!__mcpm_outbound_enter_critical(cpu, cluster));
+	cache_disable();
+	__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);
+	__mcpm_cpu_down(cpu, cluster);
+
+	phys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);
+	phys_reset(virt_to_phys(mcpm_entry_point));
+	BUG();
+}
+
+int __init mcpm_loopback(void (*cache_disable)(void))
+{
+	int ret;
+
+	/*
+	 * We're going to soft-restart the current CPU through the
+	 * low-level MCPM code by leveraging the suspend/resume
+	 * infrastructure. Let's play it safe by using cpu_pm_enter()
+	 * in case the CPU init code path resets the VFP or similar.
+	 */
+	local_irq_disable();
+	local_fiq_disable();
+	ret = cpu_pm_enter();
+	if (!ret) {
+		ret = cpu_suspend((unsigned long)cache_disable, nocache_trampoline);
+		cpu_pm_exit();
+	}
+	local_fiq_enable();
+	local_irq_enable();
+	if (ret)
+		pr_err("%s returned %d\n", __func__, ret);
+	return ret;
+}
+
+#endif
+
 struct sync_struct mcpm_sync;
 
 /*
diff --git a/arch/arm/common/mcpm_head.S b/arch/arm/common/mcpm_head.S
index 8178705..f09556c 100644
--- a/arch/arm/common/mcpm_head.S
+++ b/arch/arm/common/mcpm_head.S
@@ -15,6 +15,7 @@
 
 #include <linux/linkage.h>
 #include <asm/mcpm.h>
+#include <asm/assembler.h>
 
 #include "vlock.h"
 
@@ -71,12 +72,19 @@ ENTRY(mcpm_entry_point)
 	 * position independent way.
 	 */
 	adr	r5, 3f
-	ldmia	r5, {r6, r7, r8, r11}
+	ldmia	r5, {r0, r6, r7, r8, r11}
+	add	r0, r5, r0			@ r0 = mcpm_entry_early_pokes
 	add	r6, r5, r6			@ r6 = mcpm_entry_vectors
 	ldr	r7, [r5, r7]			@ r7 = mcpm_power_up_setup_phys
 	add	r8, r5, r8			@ r8 = mcpm_sync
 	add	r11, r5, r11			@ r11 = first_man_locks
 
+	@ Perform an early poke, if any
+	add	r0, r0, r4, lsl #3
+	ldmia	r0, {r0, r1}
+	teq	r0, #0
+	strne	r1, [r0]
+
 	mov	r0, #MCPM_SYNC_CLUSTER_SIZE
 	mla	r8, r0, r10, r8			@ r8 = sync cluster base
 
@@ -195,7 +203,8 @@ mcpm_entry_gated:
 
 	.align	2
 
-3:	.word	mcpm_entry_vectors - .
+3:	.word	mcpm_entry_early_pokes - .
+	.word	mcpm_entry_vectors - 3b
 	.word	mcpm_power_up_setup_phys - 3b
 	.word	mcpm_sync - 3b
 	.word	first_man_locks - 3b
@@ -214,6 +223,10 @@ first_man_locks:
 ENTRY(mcpm_entry_vectors)
 	.space	4 * MAX_NR_CLUSTERS * MAX_CPUS_PER_CLUSTER
 
+	.type	mcpm_entry_early_pokes, #object
+ENTRY(mcpm_entry_early_pokes)
+	.space	8 * MAX_NR_CLUSTERS * MAX_CPUS_PER_CLUSTER
+
 	.type	mcpm_power_up_setup_phys, #object
 ENTRY(mcpm_power_up_setup_phys)
 	.space  4		@ set by mcpm_sync_init()
diff --git a/arch/arm/common/timer-sp-lt.c b/arch/arm/common/timer-sp-lt.c
new file mode 100644
index 0000000..a4922a3
--- /dev/null
+++ b/arch/arm/common/timer-sp-lt.c
@@ -0,0 +1,221 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+#include <linux/clk.h>
+#include <linux/clocksource.h>
+#include <linux/clockchips.h>
+#include <linux/err.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+
+#include <asm/localtimer.h>
+#include <asm/hardware/arm_timer.h>
+#include <asm/hardware/timer-sp.h>
+
+static int irq[4];
+static int instances;
+static void __iomem *base;
+static unsigned long rate;
+static struct clock_event_device *evtdev[4];
+
+#define timer_base(c)	(base + ((c) > 1 ? 0x10000:0) + (c % 2) * TIMER_2_BASE)
+
+static inline int to_cpu(struct clock_event_device *evt)
+{
+	int n;
+
+	for (n = 0; n < instances; n++)
+		if (evt == evtdev[n])
+			return n;
+
+	BUG();
+}
+
+static long __init sp804lt_get_clock_rate(struct clk *clk)
+{
+	int err;
+
+	err = clk_prepare(clk);
+	if (err) {
+		pr_err("sp804lt: clock failed to prepare: %d\n", err);
+		clk_put(clk);
+		return err;
+	}
+
+	err = clk_enable(clk);
+	if (err) {
+		pr_err("sp804lt: clock failed to enable: %d\n", err);
+		clk_unprepare(clk);
+		clk_put(clk);
+		return err;
+	}
+
+	rate = clk_get_rate(clk);
+	if (rate < 0) {
+		pr_err("sp804lt: clock failed to get rate: %ld\n", rate);
+		clk_disable(clk);
+		clk_unprepare(clk);
+		clk_put(clk);
+	}
+
+	return rate;
+}
+
+static irqreturn_t sp804lt_timer_interrupt(int irq, void *dev_id)
+{
+	struct clock_event_device *evt = dev_id;
+	unsigned int cpu = to_cpu(evt);
+
+	/* clear the interrupt */
+	writel(1, timer_base(cpu) + TIMER_INTCLR);
+
+	if (cpu == smp_processor_id())
+		evt->event_handler(evt);
+
+	return IRQ_HANDLED;
+}
+
+static struct irqaction sp804lt_timer_irq[] = {
+	[0] = {
+		.name = "sp804_lt0",
+		.flags = IRQF_TIMER | IRQF_NOBALANCING,
+		.handler = sp804lt_timer_interrupt,
+	},
+	[1] = {
+		.name = "sp804_lt1",
+		.flags = IRQF_TIMER | IRQF_NOBALANCING,
+		.handler = sp804lt_timer_interrupt,
+	},
+	[2] = {
+		.name = "sp804_lt2",
+		.flags = IRQF_TIMER | IRQF_NOBALANCING,
+		.handler = sp804lt_timer_interrupt,
+	},
+	[3] = {
+		.name = "sp804_lt3",
+		.flags = IRQF_TIMER | IRQF_NOBALANCING,
+		.handler = sp804lt_timer_interrupt,
+	},
+};
+
+static void sp804lt_set_mode(enum clock_event_mode mode,
+	struct clock_event_device *evt)
+{
+	unsigned int cpu = to_cpu(evt);
+	unsigned long ctrl = TIMER_CTRL_32BIT | TIMER_CTRL_IE;
+
+	writel(ctrl, timer_base(cpu) + TIMER_CTRL);
+
+	switch (mode) {
+	case CLOCK_EVT_MODE_PERIODIC:
+		writel(DIV_ROUND_CLOSEST(rate, HZ),
+				timer_base(cpu) + TIMER_LOAD);
+		ctrl |= TIMER_CTRL_PERIODIC | TIMER_CTRL_ENABLE;
+		break;
+
+	case CLOCK_EVT_MODE_ONESHOT:
+		/* period set, and timer enabled in 'next_event' hook */
+		ctrl |= TIMER_CTRL_ONESHOT;
+		break;
+
+	case CLOCK_EVT_MODE_UNUSED:
+	case CLOCK_EVT_MODE_SHUTDOWN:
+	default:
+		ctrl = 0;
+		break;
+	}
+
+	writel(ctrl, timer_base(cpu) + TIMER_CTRL);
+}
+
+static int sp804lt_set_next_event(unsigned long next,
+	struct clock_event_device *evt)
+{
+	unsigned int cpu = to_cpu(evt);
+	unsigned long ctrl = readl(timer_base(cpu) + TIMER_CTRL);
+
+	writel(next, timer_base(cpu) + TIMER_LOAD);
+	writel(ctrl | TIMER_CTRL_ENABLE, timer_base(cpu) + TIMER_CTRL);
+
+	return 0;
+}
+
+static int __cpuinit sp804lt_local_timer_setup(struct clock_event_device *evt)
+{
+	unsigned int cpu = smp_processor_id();
+
+	evtdev[cpu] = evt;
+
+	evt->irq = irq[cpu];
+	evt->name = "sp804_as_LT";
+	evt->features = CLOCK_EVT_FEAT_PERIODIC | CLOCK_EVT_FEAT_ONESHOT;
+	evt->rating = 200;
+	evt->set_mode = sp804lt_set_mode;
+	evt->set_next_event = sp804lt_set_next_event;
+	clockevents_config_and_register(evt, rate, 0xff, 0xffffffff);
+
+	sp804lt_timer_irq[cpu].dev_id = evt;
+	setup_irq(evt->irq, &sp804lt_timer_irq[cpu]);
+	irq_set_affinity(evt->irq, cpumask_of(cpu));
+
+	return 0;
+}
+
+static void sp804lt_local_timer_stop(struct clock_event_device *evt)
+{
+	unsigned int cpu = smp_processor_id();
+
+	remove_irq(evt->irq, &sp804lt_timer_irq[cpu]);
+	evt->set_mode(CLOCK_EVT_MODE_UNUSED, evt);
+}
+
+static struct local_timer_ops sp804lt_tick_ops __cpuinitdata = {
+	.setup	= sp804lt_local_timer_setup,
+	.stop	= sp804lt_local_timer_stop,
+};
+
+static void __init sp804lt_of_init(struct device_node *np)
+{
+	struct clk *clk;
+	int n;
+
+	clk = of_clk_get(np, 0);
+	if (WARN_ON(IS_ERR(clk)))
+		return;
+
+	if (sp804lt_get_clock_rate(clk) < 0)
+		return;
+
+	for (instances = 0; instances < ARRAY_SIZE(irq); instances++) {
+		irq[instances] = irq_of_parse_and_map(np, instances);
+		if (irq[instances] <= 0)
+			break;
+	}
+
+	base = of_iomap(np, 0);
+	if (WARN_ON(!base))
+		return;
+
+	/* Ensure timers are disabled */
+	for(n = 0; n < instances; n++)
+		writel(n, timer_base(n) + TIMER_CTRL);
+
+	local_timer_register(&sp804lt_tick_ops);
+}
+CLOCKSOURCE_OF_DECLARE(sp804lt, "arm,sp804lt", sp804lt_of_init);
diff --git a/arch/arm/common/timer-sp.c b/arch/arm/common/timer-sp.c
index ddc7407..840243c 100644
--- a/arch/arm/common/timer-sp.c
+++ b/arch/arm/common/timer-sp.c
@@ -28,6 +28,7 @@
 #include <linux/of.h>
 #include <linux/of_address.h>
 #include <linux/of_irq.h>
+#include <linux/syscore_ops.h>
 
 #include <asm/sched_clock.h>
 #include <asm/hardware/arm_timer.h>
@@ -208,6 +209,53 @@ void __init __sp804_clockevents_init(void __iomem *base, unsigned int irq, struc
 	clockevents_config_and_register(evt, rate, 0xf, 0xffffffff);
 }
 
+static void __iomem *_sp804_base;
+static u32 reg[6];
+
+static int sp804_suspend(void)
+{
+	unsigned long flags;
+
+	printk("suspend\n");
+	local_irq_save(flags);
+	reg[0] = readl(_sp804_base + TIMER_LOAD);
+	reg[1] = readl(_sp804_base + TIMER_VALUE);
+	reg[2] = readl(_sp804_base + TIMER_CTRL);
+	reg[3] = readl(_sp804_base + TIMER_2_BASE + TIMER_LOAD);
+	reg[4] = readl(_sp804_base + TIMER_2_BASE + TIMER_VALUE);
+	reg[5] = readl(_sp804_base + TIMER_2_BASE + TIMER_CTRL);
+	local_irq_restore(flags);
+
+	return 0;
+}
+
+static void sp804_resume(void)
+{
+	unsigned long flags;
+
+	local_irq_save(flags);
+	/* Disable timers */
+	writel(0, _sp804_base + TIMER_CTRL);
+	writel(0, _sp804_base + TIMER_2_BASE + TIMER_CTRL);
+
+	/* Program Load and Value regs */
+	writel(reg[0], _sp804_base + TIMER_LOAD);
+	writel(reg[1], _sp804_base + TIMER_VALUE);
+	writel(reg[3], _sp804_base + TIMER_2_BASE + TIMER_LOAD);
+	writel(reg[4], _sp804_base + TIMER_2_BASE + TIMER_VALUE);
+
+	/* Restore timers */
+	writel(reg[2], _sp804_base + TIMER_CTRL);
+	writel(reg[5], _sp804_base + TIMER_2_BASE + TIMER_CTRL);
+	local_irq_restore(flags);
+	printk("resume\n");
+}
+
+static struct syscore_ops sp804_syscore_ops = {
+	.suspend = sp804_suspend,
+	.resume = sp804_resume,
+};
+
 static void __init sp804_of_init(struct device_node *np)
 {
 	static bool initialized = false;
@@ -258,6 +306,9 @@ static void __init sp804_of_init(struct device_node *np)
 	}
 	initialized = true;
 
+	_sp804_base = base;
+	register_syscore_ops(&sp804_syscore_ops);
+
 	return;
 err:
 	iounmap(base);
diff --git a/arch/arm/include/asm/arch_timer.h b/arch/arm/include/asm/arch_timer.h
index 0704e0c..f8d5786 100644
--- a/arch/arm/include/asm/arch_timer.h
+++ b/arch/arm/include/asm/arch_timer.h
@@ -83,7 +83,7 @@ static inline u64 arch_counter_get_cntvct(void)
 	u64 cval;
 
 	isb();
-	asm volatile("mrrc p15, 1, %Q0, %R0, c14" : "=r" (cval));
+	asm volatile("mrrc p15, 0, %Q0, %R0, c14" : "=r" (cval));
 	return cval;
 }
 
@@ -99,7 +99,7 @@ static inline void arch_timer_set_cntkctl(u32 cntkctl)
 	asm volatile("mcr p15, 0, %0, c14, c1, 0" : : "r" (cntkctl));
 }
 
-static inline void arch_counter_set_user_access(void)
+static inline void __cpuinit arch_counter_set_user_access(void)
 {
 	u32 cntkctl = arch_timer_get_cntkctl();
 
diff --git a/arch/arm/include/asm/bL_switcher.h b/arch/arm/include/asm/bL_switcher.h
new file mode 100644
index 0000000..482383b
--- /dev/null
+++ b/arch/arm/include/asm/bL_switcher.h
@@ -0,0 +1,83 @@
+/*
+ * arch/arm/include/asm/bL_switcher.h
+ *
+ * Created by:  Nicolas Pitre, April 2012
+ * Copyright:   (C) 2012  Linaro Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef ASM_BL_SWITCHER_H
+#define ASM_BL_SWITCHER_H
+
+#include <linux/compiler.h>
+#include <linux/types.h>
+
+typedef void (*bL_switch_completion_handler)(void *cookie);
+
+int bL_switch_request_cb(unsigned int cpu, unsigned int new_cluster_id,
+			 bL_switch_completion_handler completer,
+			 void *completer_cookie);
+static inline int bL_switch_request(unsigned int cpu, unsigned int new_cluster_id)
+{
+	return bL_switch_request_cb(cpu, new_cluster_id, NULL, NULL);
+}
+
+/*
+ * Register here to be notified about runtime enabling/disabling of
+ * the switcher.
+ *
+ * The notifier chain is called with the switcher activation lock held:
+ * the switcher will not be enabled or disabled during callbacks.
+ * Callbacks must not call bL_switcher_{get,put}_enabled().
+ */
+#define BL_NOTIFY_PRE_ENABLE	0
+#define BL_NOTIFY_POST_ENABLE	1
+#define BL_NOTIFY_PRE_DISABLE	2
+#define BL_NOTIFY_POST_DISABLE	3
+
+#ifdef CONFIG_BL_SWITCHER
+
+void bL_switch_request_detach(unsigned int cpu,
+			      bL_switch_completion_handler completer);
+
+int bL_switcher_register_notifier(struct notifier_block *nb);
+int bL_switcher_unregister_notifier(struct notifier_block *nb);
+
+/*
+ * Use these functions to temporarily prevent enabling/disabling of
+ * the switcher.
+ * bL_switcher_get_enabled() returns true if the switcher is currently
+ * enabled.  Each call to bL_switcher_get_enabled() must be followed
+ * by a call to bL_switcher_put_enabled().  These functions are not
+ * recursive.
+ */
+bool bL_switcher_get_enabled(void);
+void bL_switcher_put_enabled(void);
+
+int bL_switcher_trace_trigger(void);
+int bL_switcher_get_logical_index(u32 mpidr);
+
+#else
+static void bL_switch_request_detach(unsigned int cpu,
+				     bL_switch_completion_handler completer) { }
+
+static inline int bL_switcher_register_notifier(struct notifier_block *nb)
+{
+	return 0;
+}
+
+static inline int bL_switcher_unregister_notifier(struct notifier_block *nb)
+{
+	return 0;
+}
+
+static inline bool bL_switcher_get_enabled(void) { return false; }
+static inline void bL_switcher_put_enabled(void) { }
+static inline int bL_switcher_trace_trigger(void) { return 0; }
+static inline int bL_switcher_get_logical_index(u32 mpidr) { return -EUNATCH; }
+#endif /* CONFIG_BL_SWITCHER */
+
+#endif
diff --git a/arch/arm/include/asm/cacheflush.h b/arch/arm/include/asm/cacheflush.h
index a25e62d..2059f01 100644
--- a/arch/arm/include/asm/cacheflush.h
+++ b/arch/arm/include/asm/cacheflush.h
@@ -437,4 +437,50 @@ static inline void __sync_cache_range_r(volatile void *p, size_t size)
 #define sync_cache_w(ptr) __sync_cache_range_w(ptr, sizeof *(ptr))
 #define sync_cache_r(ptr) __sync_cache_range_r(ptr, sizeof *(ptr))
 
+/*
+ * Disabling cache access for one CPU in an ARMv7 SMP system is tricky.
+ * To do so we must:
+ *
+ * - Clear the SCTLR.C bit to prevent further cache allocations
+ * - Flush the desired level of cache
+ * - Clear the ACTLR "SMP" bit to disable local coherency
+ *
+ * ... and so without any intervening memory access in between those steps,
+ * not even to the stack.
+ *
+ * WARNING -- After this has been called:
+ *
+ * - No ldrex/strex (and similar) instructions must be used.
+ * - The CPU is obviously no longer coherent with the other CPUs.
+ * - This is unlikely to work as expected if Linux is running non-secure.
+ *
+ * Note:
+ *
+ * - This is known to apply to several ARMv7 processor implementations,
+ *   however some exceptions may exist.  Caveat emptor.
+ *
+ * - The clobber list is dictated by the call to v7_flush_dcache_*.
+ *   fp is preserved to the stack explicitly prior disabling the cache
+ *   since adding it to the clobber list is incompatible with having
+ *   CONFIG_FRAME_POINTER=y.  ip is saved as well if ever r12-clobbering
+ *   trampoline are inserted by the linker and to keep sp 64-bit aligned.
+ */
+#define v7_exit_coherency_flush(level) \
+	asm volatile( \
+	"stmfd	sp!, {fp, ip} \n\t" \
+	"mrc	p15, 0, r0, c1, c0, 0	@ get SCTLR \n\t" \
+	"bic	r0, r0, #"__stringify(CR_C)" \n\t" \
+	"mcr	p15, 0, r0, c1, c0, 0	@ set SCTLR \n\t" \
+	"isb	\n\t" \
+	"bl	v7_flush_dcache_"__stringify(level)" \n\t" \
+	"clrex	\n\t" \
+	"mrc	p15, 0, r0, c1, c0, 1	@ get ACTLR \n\t" \
+	"bic	r0, r0, #(1 << 6)	@ disable local coherency \n\t" \
+	"mcr	p15, 0, r0, c1, c0, 1	@ set ACTLR \n\t" \
+	"isb	\n\t" \
+	"dsb	\n\t" \
+	"ldmfd	sp!, {fp, ip}" \
+	: : : "r0","r1","r2","r3","r4","r5","r6","r7", \
+	      "r9","r10","lr","memory" )
+
 #endif
diff --git a/arch/arm/include/asm/hardirq.h b/arch/arm/include/asm/hardirq.h
index 2740c2a..fe3ea77 100644
--- a/arch/arm/include/asm/hardirq.h
+++ b/arch/arm/include/asm/hardirq.h
@@ -5,7 +5,7 @@
 #include <linux/threads.h>
 #include <asm/irq.h>
 
-#define NR_IPI	6
+#define NR_IPI	8
 
 typedef struct {
 	unsigned int __softirq_pending;
diff --git a/arch/arm/include/asm/hardware/coresight.h b/arch/arm/include/asm/hardware/coresight.h
index 0cf7a6b..fc53019 100644
--- a/arch/arm/include/asm/hardware/coresight.h
+++ b/arch/arm/include/asm/hardware/coresight.h
@@ -17,15 +17,23 @@
 #define TRACER_ACCESSED_BIT	0
 #define TRACER_RUNNING_BIT	1
 #define TRACER_CYCLE_ACC_BIT	2
+#define TRACER_TRACE_DATA_BIT	3
+#define TRACER_TIMESTAMP_BIT	4
+#define TRACER_BRANCHOUTPUT_BIT	5
+#define TRACER_RETURN_STACK_BIT	6
 #define TRACER_ACCESSED		BIT(TRACER_ACCESSED_BIT)
 #define TRACER_RUNNING		BIT(TRACER_RUNNING_BIT)
 #define TRACER_CYCLE_ACC	BIT(TRACER_CYCLE_ACC_BIT)
+#define TRACER_TRACE_DATA	BIT(TRACER_TRACE_DATA_BIT)
+#define TRACER_TIMESTAMP	BIT(TRACER_TIMESTAMP_BIT)
+#define TRACER_BRANCHOUTPUT	BIT(TRACER_BRANCHOUTPUT_BIT)
+#define TRACER_RETURN_STACK	BIT(TRACER_RETURN_STACK_BIT)
 
 #define TRACER_TIMEOUT 10000
 
 #define etm_writel(t, v, x) \
-	(__raw_writel((v), (t)->etm_regs + (x)))
-#define etm_readl(t, x) (__raw_readl((t)->etm_regs + (x)))
+	(writel_relaxed((v), (t)->etm_regs + (x)))
+#define etm_readl(t, x) (readl_relaxed((t)->etm_regs + (x)))
 
 /* CoreSight Management Registers */
 #define CSMR_LOCKACCESS 0xfb0
@@ -43,7 +51,7 @@
 #define ETMCTRL_POWERDOWN	1
 #define ETMCTRL_PROGRAM		(1 << 10)
 #define ETMCTRL_PORTSEL		(1 << 11)
-#define ETMCTRL_DO_CONTEXTID	(3 << 14)
+#define ETMCTRL_CONTEXTIDSIZE(x) (((x) & 3) << 14)
 #define ETMCTRL_PORTMASK1	(7 << 4)
 #define ETMCTRL_PORTMASK2	(1 << 21)
 #define ETMCTRL_PORTMASK	(ETMCTRL_PORTMASK1 | ETMCTRL_PORTMASK2)
@@ -55,9 +63,12 @@
 #define ETMCTRL_DATA_DO_BOTH	(ETMCTRL_DATA_DO_DATA | ETMCTRL_DATA_DO_ADDR)
 #define ETMCTRL_BRANCH_OUTPUT	(1 << 8)
 #define ETMCTRL_CYCLEACCURATE	(1 << 12)
+#define ETMCTRL_TIMESTAMP_EN	(1 << 28)
+#define ETMCTRL_RETURN_STACK_EN	(1 << 29)
 
 /* ETM configuration code register */
 #define ETMR_CONFCODE		(0x04)
+#define ETMCCR_ETMIDR_PRESENT	BIT(31)
 
 /* ETM trace start/stop resource control register */
 #define ETMR_TRACESSCTRL	(0x18)
@@ -113,10 +124,25 @@
 #define ETMR_TRACEENCTRL	0x24
 #define ETMTE_INCLEXCL		BIT(24)
 #define ETMR_TRACEENEVT		0x20
-#define ETMCTRL_OPTS		(ETMCTRL_DO_CPRT | \
-				ETMCTRL_DATA_DO_ADDR | \
-				ETMCTRL_BRANCH_OUTPUT | \
-				ETMCTRL_DO_CONTEXTID)
+
+#define ETMR_VIEWDATAEVT	0x30
+#define ETMR_VIEWDATACTRL1	0x34
+#define ETMR_VIEWDATACTRL2	0x38
+#define ETMR_VIEWDATACTRL3	0x3c
+#define ETMVDC3_EXCLONLY	BIT(16)
+
+#define ETMCTRL_OPTS		(ETMCTRL_DO_CPRT)
+
+#define ETMR_ID			0x1e4
+#define ETMIDR_VERSION(x)	(((x) >> 4) & 0xff)
+#define ETMIDR_VERSION_3_1	0x21
+#define ETMIDR_VERSION_PFT_1_0	0x30
+
+#define ETMR_CCE		0x1e8
+#define ETMCCER_RETURN_STACK_IMPLEMENTED	BIT(23)
+#define ETMCCER_TIMESTAMPING_IMPLEMENTED	BIT(22)
+
+#define ETMR_TRACEIDR		0x200
 
 /* ETM management registers, "ETM Architecture", 3.5.24 */
 #define ETMMR_OSLAR	0x300
@@ -140,14 +166,16 @@
 #define ETBFF_TRIGIN		BIT(8)
 #define ETBFF_TRIGEVT		BIT(9)
 #define ETBFF_TRIGFL		BIT(10)
+#define ETBFF_STOPFL		BIT(12)
 
 #define etb_writel(t, v, x) \
-	(__raw_writel((v), (t)->etb_regs + (x)))
-#define etb_readl(t, x) (__raw_readl((t)->etb_regs + (x)))
+	(writel_relaxed((v), (t)->etb_regs + (x)))
+#define etb_readl(t, x) (readl_relaxed((t)->etb_regs + (x)))
 
-#define etm_lock(t) do { etm_writel((t), 0, CSMR_LOCKACCESS); } while (0)
-#define etm_unlock(t) \
-	do { etm_writel((t), CS_LAR_KEY, CSMR_LOCKACCESS); } while (0)
+#define etm_lock(t, id) \
+	do { etm_writel((t), (id), 0, CSMR_LOCKACCESS); } while (0)
+#define etm_unlock(t, id) \
+	do { etm_writel((t), (id), CS_LAR_KEY, CSMR_LOCKACCESS); } while (0)
 
 #define etb_lock(t) do { etb_writel((t), 0, CSMR_LOCKACCESS); } while (0)
 #define etb_unlock(t) \
diff --git a/arch/arm/include/asm/mach/arch.h b/arch/arm/include/asm/mach/arch.h
index 308ad7d..75bf079 100644
--- a/arch/arm/include/asm/mach/arch.h
+++ b/arch/arm/include/asm/mach/arch.h
@@ -8,6 +8,8 @@
  * published by the Free Software Foundation.
  */
 
+#include <linux/types.h>
+
 #ifndef __ASSEMBLY__
 
 struct tag;
@@ -16,8 +18,10 @@ struct pt_regs;
 struct smp_operations;
 #ifdef CONFIG_SMP
 #define smp_ops(ops) (&(ops))
+#define smp_init_ops(ops) (&(ops))
 #else
 #define smp_ops(ops) (struct smp_operations *)NULL
+#define smp_init_ops(ops) (bool (*)(void))NULL
 #endif
 
 struct machine_desc {
@@ -41,6 +45,7 @@ struct machine_desc {
 	unsigned char		reserve_lp2 :1;	/* never has lp2	*/
 	char			restart_mode;	/* default restart mode	*/
 	struct smp_operations	*smp;		/* SMP operations	*/
+	bool			(*smp_init)(void);
 	void			(*fixup)(struct tag *, char **,
 					 struct meminfo *);
 	void			(*reserve)(void);/* reserve mem blocks	*/
diff --git a/arch/arm/include/asm/mcpm.h b/arch/arm/include/asm/mcpm.h
index 0f7b762..33e1d18 100644
--- a/arch/arm/include/asm/mcpm.h
+++ b/arch/arm/include/asm/mcpm.h
@@ -42,6 +42,14 @@ extern void mcpm_entry_point(void);
 void mcpm_set_entry_vector(unsigned cpu, unsigned cluster, void *ptr);
 
 /*
+ * This sets an early poke i.e a value to be poked into some address
+ * from very early assembly code before the CPU is ungated.  The
+ * address must be physical, and if 0 then nothing will happen.
+ */
+void mcpm_set_early_poke(unsigned cpu, unsigned cluster,
+			 unsigned long poke_phys_addr, unsigned long poke_val);
+
+/*
  * CPU/cluster power operations API for higher subsystems to use.
  */
 
@@ -167,6 +175,22 @@ int __mcpm_cluster_state(unsigned int cluster);
 int __init mcpm_sync_init(
 	void (*power_up_setup)(unsigned int affinity_level));
 
+/**
+ * mcpm_loopback - make a run through the MCPM low-level code
+ *
+ * @cache_disable: pointer to function performing cache disabling
+ *
+ * This exercises the MCPM machinery by soft resetting the CPU and branching
+ * to the MCPM low-level entry code before returning to the caller.
+ * The @cache_disable function must do the necessary cache disabling to
+ * let the regular kernel init code turn it back on as if the CPU was
+ * hotplugged in. The MCPM state machine is set as if the cluster was
+ * initialized meaning the power_up_setup callback passed to mcpm_sync_init()
+ * will be invoked for all affinity levels. This may be useful to initialize
+ * some resources such as enabling the CCI that requires the cache to be off, or simply for testing purposes.
+ */
+int __init mcpm_loopback(void (*cache_disable)(void));
+
 void __init mcpm_smp_set_ops(void);
 
 #else
diff --git a/arch/arm/include/asm/memory.h b/arch/arm/include/asm/memory.h
index 584786f..af25e8e 100644
--- a/arch/arm/include/asm/memory.h
+++ b/arch/arm/include/asm/memory.h
@@ -100,23 +100,19 @@
 #define TASK_UNMAPPED_BASE	UL(0x00000000)
 #endif
 
-#ifndef PHYS_OFFSET
-#define PHYS_OFFSET 		UL(CONFIG_DRAM_BASE)
-#endif
-
 #ifndef END_MEM
 #define END_MEM     		(UL(CONFIG_DRAM_BASE) + CONFIG_DRAM_SIZE)
 #endif
 
 #ifndef PAGE_OFFSET
-#define PAGE_OFFSET		(PHYS_OFFSET)
+#define PAGE_OFFSET		PLAT_PHYS_OFFSET
 #endif
 
 /*
  * The module can be at any place in ram in nommu mode.
  */
 #define MODULES_END		(END_MEM)
-#define MODULES_VADDR		(PHYS_OFFSET)
+#define MODULES_VADDR		PAGE_OFFSET
 
 #define XIP_VIRT_ADDR(physaddr)  (physaddr)
 
@@ -157,6 +153,16 @@
 #endif
 #define ARCH_PGD_MASK		((1 << ARCH_PGD_SHIFT) - 1)
 
+/*
+ * PLAT_PHYS_OFFSET is the offset (from zero) of the start of physical
+ * memory.  This is used for XIP and NoMMU kernels, or by kernels which
+ * have their own mach/memory.h.  Assembly code must always use
+ * PLAT_PHYS_OFFSET and not PHYS_OFFSET.
+ */
+#ifndef PLAT_PHYS_OFFSET
+#define PLAT_PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
+#endif
+
 #ifndef __ASSEMBLY__
 
 /*
@@ -199,22 +205,15 @@ static inline unsigned long __phys_to_virt(unsigned long x)
 	return t;
 }
 #else
+
+#define PHYS_OFFSET	PLAT_PHYS_OFFSET
+
 #define __virt_to_phys(x)	((x) - PAGE_OFFSET + PHYS_OFFSET)
 #define __phys_to_virt(x)	((x) - PHYS_OFFSET + PAGE_OFFSET)
-#endif
-#endif
-#endif /* __ASSEMBLY__ */
 
-#ifndef PHYS_OFFSET
-#ifdef PLAT_PHYS_OFFSET
-#define PHYS_OFFSET	PLAT_PHYS_OFFSET
-#else
-#define PHYS_OFFSET	UL(CONFIG_PHYS_OFFSET)
 #endif
 #endif
 
-#ifndef __ASSEMBLY__
-
 /*
  * PFNs are used to describe any physical page; this means
  * PFN 0 == physical address 0.
@@ -291,7 +290,8 @@ static inline __deprecated void *bus_to_virt(unsigned long x)
 #define ARCH_PFN_OFFSET		PHYS_PFN_OFFSET
 
 #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
-#define virt_addr_valid(kaddr)	((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory)
+#define virt_addr_valid(kaddr)	(((unsigned long)(kaddr) >= PAGE_OFFSET && (unsigned long)(kaddr) < (unsigned long)high_memory) \
+					&& pfn_valid(__pa(kaddr) >> PAGE_SHIFT) )
 
 #endif
 
diff --git a/arch/arm/include/asm/mmu.h b/arch/arm/include/asm/mmu.h
index 6f18da0..64fd151 100644
--- a/arch/arm/include/asm/mmu.h
+++ b/arch/arm/include/asm/mmu.h
@@ -16,7 +16,7 @@ typedef struct {
 #ifdef CONFIG_CPU_HAS_ASID
 #define ASID_BITS	8
 #define ASID_MASK	((~0ULL) << ASID_BITS)
-#define ASID(mm)	((mm)->context.id.counter & ~ASID_MASK)
+#define ASID(mm)	((unsigned int)((mm)->context.id.counter & ~ASID_MASK))
 #else
 #define ASID(mm)	(0)
 #endif
diff --git a/arch/arm/include/asm/pgtable-3level-hwdef.h b/arch/arm/include/asm/pgtable-3level-hwdef.h
index 626989f..552a5f7 100644
--- a/arch/arm/include/asm/pgtable-3level-hwdef.h
+++ b/arch/arm/include/asm/pgtable-3level-hwdef.h
@@ -72,6 +72,7 @@
 #define PTE_TABLE_BIT		(_AT(pteval_t, 1) << 1)
 #define PTE_BUFFERABLE		(_AT(pteval_t, 1) << 2)		/* AttrIndx[0] */
 #define PTE_CACHEABLE		(_AT(pteval_t, 1) << 3)		/* AttrIndx[1] */
+#define PTE_AP2			(_AT(pteval_t, 1) << 7)		/* AP[2] */
 #define PTE_EXT_SHARED		(_AT(pteval_t, 3) << 8)		/* SH[1:0], inner shareable */
 #define PTE_EXT_AF		(_AT(pteval_t, 1) << 10)	/* Access Flag */
 #define PTE_EXT_NG		(_AT(pteval_t, 1) << 11)	/* nG */
diff --git a/arch/arm/include/asm/pgtable-3level.h b/arch/arm/include/asm/pgtable-3level.h
index 5689c18..c00ce4e 100644
--- a/arch/arm/include/asm/pgtable-3level.h
+++ b/arch/arm/include/asm/pgtable-3level.h
@@ -79,13 +79,13 @@
 #define L_PTE_PRESENT		(_AT(pteval_t, 3) << 0)		/* Present */
 #define L_PTE_FILE		(_AT(pteval_t, 1) << 2)		/* only when !PRESENT */
 #define L_PTE_USER		(_AT(pteval_t, 1) << 6)		/* AP[1] */
-#define L_PTE_RDONLY		(_AT(pteval_t, 1) << 7)		/* AP[2] */
 #define L_PTE_SHARED		(_AT(pteval_t, 3) << 8)		/* SH[1:0], inner shareable */
 #define L_PTE_YOUNG		(_AT(pteval_t, 1) << 10)	/* AF */
 #define L_PTE_XN		(_AT(pteval_t, 1) << 54)	/* XN */
-#define L_PTE_DIRTY		(_AT(pteval_t, 1) << 55)	/* unused */
-#define L_PTE_SPECIAL		(_AT(pteval_t, 1) << 56)	/* unused */
+#define L_PTE_DIRTY		(_AT(pteval_t, 1) << 55)
+#define L_PTE_SPECIAL		(_AT(pteval_t, 1) << 56)
 #define L_PTE_NONE		(_AT(pteval_t, 1) << 57)	/* PROT_NONE */
+#define L_PTE_RDONLY		(_AT(pteval_t, 1) << 58)	/* READ ONLY */
 
 #define PMD_SECT_VALID		(_AT(pmdval_t, 1) << 0)
 #define PMD_SECT_DIRTY		(_AT(pmdval_t, 1) << 55)
diff --git a/arch/arm/include/asm/pgtable.h b/arch/arm/include/asm/pgtable.h
index 76a52ae..2d4cf1c 100644
--- a/arch/arm/include/asm/pgtable.h
+++ b/arch/arm/include/asm/pgtable.h
@@ -214,12 +214,16 @@ static inline pte_t *pmd_page_vaddr(pmd_t pmd)
 
 #define pte_clear(mm,addr,ptep)	set_pte_ext(ptep, __pte(0), 0)
 
+#define pte_isset(pte, val)	((u32)(val) == (val) ? pte_val(pte) & (val) \
+						: !!(pte_val(pte) & (val)))
+#define pte_isclear(pte, val)	(!(pte_val(pte) & (val)))
+
 #define pte_none(pte)		(!pte_val(pte))
-#define pte_present(pte)	(pte_val(pte) & L_PTE_PRESENT)
-#define pte_write(pte)		(!(pte_val(pte) & L_PTE_RDONLY))
-#define pte_dirty(pte)		(pte_val(pte) & L_PTE_DIRTY)
-#define pte_young(pte)		(pte_val(pte) & L_PTE_YOUNG)
-#define pte_exec(pte)		(!(pte_val(pte) & L_PTE_XN))
+#define pte_present(pte)	(pte_isset((pte), L_PTE_PRESENT))
+#define pte_write(pte)		(pte_isclear((pte), L_PTE_RDONLY))
+#define pte_dirty(pte)		(pte_isset((pte), L_PTE_DIRTY))
+#define pte_young(pte)		(pte_isset((pte), L_PTE_YOUNG))
+#define pte_exec(pte)		(pte_isclear((pte), L_PTE_XN))
 #define pte_special(pte)	(0)
 
 #define pte_present_user(pte)  (pte_present(pte) && (pte_val(pte) & L_PTE_USER))
diff --git a/arch/arm/include/asm/pmu.h b/arch/arm/include/asm/pmu.h
index f24edad..0cd7824 100644
--- a/arch/arm/include/asm/pmu.h
+++ b/arch/arm/include/asm/pmu.h
@@ -62,9 +62,19 @@ struct pmu_hw_events {
 	raw_spinlock_t		pmu_lock;
 };
 
+struct cpupmu_regs {
+	u32 pmc;
+	u32 pmcntenset;
+	u32 pmuseren;
+	u32 pmintenset;
+	u32 pmxevttype[8];
+	u32 pmxevtcnt[8];
+};
+
 struct arm_pmu {
 	struct pmu	pmu;
 	cpumask_t	active_irqs;
+	cpumask_t	valid_cpus;
 	char		*name;
 	irqreturn_t	(*handle_irq)(int irq_num, void *dev);
 	void		(*enable)(struct perf_event *event);
@@ -81,6 +91,8 @@ struct arm_pmu {
 	int		(*request_irq)(struct arm_pmu *, irq_handler_t handler);
 	void		(*free_irq)(struct arm_pmu *);
 	int		(*map_event)(struct perf_event *event);
+	void		(*save_regs)(struct arm_pmu *, struct cpupmu_regs *);
+	void		(*restore_regs)(struct arm_pmu *, struct cpupmu_regs *);
 	int		num_events;
 	atomic_t	active_events;
 	struct mutex	reserve_mutex;
diff --git a/arch/arm/include/asm/psci.h b/arch/arm/include/asm/psci.h
index ce0dbe7..f0a8627 100644
--- a/arch/arm/include/asm/psci.h
+++ b/arch/arm/include/asm/psci.h
@@ -16,6 +16,10 @@
 
 #define PSCI_POWER_STATE_TYPE_STANDBY		0
 #define PSCI_POWER_STATE_TYPE_POWER_DOWN	1
+#define PSCI_POWER_STATE_AFFINITY_LEVEL0	0
+#define PSCI_POWER_STATE_AFFINITY_LEVEL1	1
+#define PSCI_POWER_STATE_AFFINITY_LEVEL2	2
+#define PSCI_POWER_STATE_AFFINITY_LEVEL3	3
 
 struct psci_power_state {
 	u16	id;
@@ -32,5 +36,22 @@ struct psci_operations {
 };
 
 extern struct psci_operations psci_ops;
+extern struct smp_operations psci_smp_ops;
 
+#ifdef CONFIG_ARM_PSCI
+void psci_init(void);
+bool psci_smp_available(void);
+#else
+static inline void psci_init(void) { }
+static inline bool psci_smp_available(void) { return false; }
+#endif
+
+#ifdef CONFIG_ARM_PSCI
+extern int __init psci_probe(void);
+#else
+static inline int psci_probe(void)
+{
+	return -ENODEV;
+}
+#endif
 #endif /* __ASM_ARM_PSCI_H */
diff --git a/arch/arm/kernel/bios32.c b/arch/arm/kernel/bios32.c
index b2ed73c..2e5d38c 100644
--- a/arch/arm/kernel/bios32.c
+++ b/arch/arm/kernel/bios32.c
@@ -537,6 +537,7 @@ void pci_common_init(struct hw_pci *hw)
 		pci_bus_add_devices(bus);
 	}
 }
+EXPORT_SYMBOL_GPL(pci_common_init);
 
 #ifndef CONFIG_PCI_HOST_ITE8152
 void pcibios_set_master(struct pci_dev *dev)
diff --git a/arch/arm/kernel/etm.c b/arch/arm/kernel/etm.c
index 8ff0ecd..27ad054 100644
--- a/arch/arm/kernel/etm.c
+++ b/arch/arm/kernel/etm.c
@@ -15,6 +15,7 @@
 #include <linux/init.h>
 #include <linux/types.h>
 #include <linux/io.h>
+#include <linux/slab.h>
 #include <linux/sysrq.h>
 #include <linux/device.h>
 #include <linux/clk.h>
@@ -37,26 +38,37 @@ MODULE_AUTHOR("Alexander Shishkin");
 struct tracectx {
 	unsigned int	etb_bufsz;
 	void __iomem	*etb_regs;
-	void __iomem	*etm_regs;
+	void __iomem	**etm_regs;
+	int		etm_regs_count;
 	unsigned long	flags;
 	int		ncmppairs;
 	int		etm_portsz;
+	int		etm_contextid_size;
+	u32		etb_fc;
+	unsigned long	range_start;
+	unsigned long	range_end;
+	unsigned long	data_range_start;
+	unsigned long	data_range_end;
+	bool		dump_initial_etb;
 	struct device	*dev;
 	struct clk	*emu_clk;
 	struct mutex	mutex;
 };
 
-static struct tracectx tracer;
+static struct tracectx tracer = {
+	.range_start = (unsigned long)_stext,
+	.range_end = (unsigned long)_etext,
+};
 
 static inline bool trace_isrunning(struct tracectx *t)
 {
 	return !!(t->flags & TRACER_RUNNING);
 }
 
-static int etm_setup_address_range(struct tracectx *t, int n,
+static int etm_setup_address_range(struct tracectx *t, int id, int n,
 		unsigned long start, unsigned long end, int exclude, int data)
 {
-	u32 flags = ETMAAT_ARM | ETMAAT_IGNCONTEXTID | ETMAAT_NSONLY | \
+	u32 flags = ETMAAT_ARM | ETMAAT_IGNCONTEXTID | ETMAAT_IGNSECURITY |
 		    ETMAAT_NOVALCMP;
 
 	if (n < 1 || n > t->ncmppairs)
@@ -72,95 +84,184 @@ static int etm_setup_address_range(struct tracectx *t, int n,
 		flags |= ETMAAT_IEXEC;
 
 	/* first comparator for the range */
-	etm_writel(t, flags, ETMR_COMP_ACC_TYPE(n * 2));
-	etm_writel(t, start, ETMR_COMP_VAL(n * 2));
+	etm_writel(t, id, flags, ETMR_COMP_ACC_TYPE(n * 2));
+	etm_writel(t, id, start, ETMR_COMP_VAL(n * 2));
 
 	/* second comparator is right next to it */
-	etm_writel(t, flags, ETMR_COMP_ACC_TYPE(n * 2 + 1));
-	etm_writel(t, end, ETMR_COMP_VAL(n * 2 + 1));
-
+	etm_writel(t, id, flags, ETMR_COMP_ACC_TYPE(n * 2 + 1));
+	etm_writel(t, id, end, ETMR_COMP_VAL(n * 2 + 1));
+
+	if (data) {
+		flags = exclude ? ETMVDC3_EXCLONLY : 0;
+		if (exclude)
+			n += 8;
+		etm_writel(t, id, flags | BIT(n), ETMR_VIEWDATACTRL3);
+	} else {
 	flags = exclude ? ETMTE_INCLEXCL : 0;
-	etm_writel(t, flags | (1 << n), ETMR_TRACEENCTRL);
+		etm_writel(t, id, flags | (1 << n), ETMR_TRACEENCTRL);
+	}
 
 	return 0;
 }
 
-static int trace_start(struct tracectx *t)
+static int trace_start_etm(struct tracectx *t, int id)
 {
 	u32 v;
 	unsigned long timeout = TRACER_TIMEOUT;
 
-	etb_unlock(t);
-
-	etb_writel(t, 0, ETBR_FORMATTERCTRL);
-	etb_writel(t, 1, ETBR_CTRL);
-
-	etb_lock(t);
-
-	/* configure etm */
 	v = ETMCTRL_OPTS | ETMCTRL_PROGRAM | ETMCTRL_PORTSIZE(t->etm_portsz);
+	v |= ETMCTRL_CONTEXTIDSIZE(t->etm_contextid_size);
 
 	if (t->flags & TRACER_CYCLE_ACC)
 		v |= ETMCTRL_CYCLEACCURATE;
 
-	etm_unlock(t);
+	if (t->flags & TRACER_BRANCHOUTPUT)
+		v |= ETMCTRL_BRANCH_OUTPUT;
+
+	if (t->flags & TRACER_TRACE_DATA)
+		v |= ETMCTRL_DATA_DO_ADDR;
+
+	if (t->flags & TRACER_TIMESTAMP)
+		v |= ETMCTRL_TIMESTAMP_EN;
+
+	if (t->flags & TRACER_RETURN_STACK)
+		v |= ETMCTRL_RETURN_STACK_EN;
 
-	etm_writel(t, v, ETMR_CTRL);
+	etm_unlock(t, id);
 
-	while (!(etm_readl(t, ETMR_CTRL) & ETMCTRL_PROGRAM) && --timeout)
+	etm_writel(t, id, v, ETMR_CTRL);
+
+	while (!(etm_readl(t, id, ETMR_CTRL) & ETMCTRL_PROGRAM) && --timeout)
 		;
 	if (!timeout) {
 		dev_dbg(t->dev, "Waiting for progbit to assert timed out\n");
-		etm_lock(t);
+		etm_lock(t, id);
 		return -EFAULT;
 	}
 
-	etm_setup_address_range(t, 1, (unsigned long)_stext,
-			(unsigned long)_etext, 0, 0);
-	etm_writel(t, 0, ETMR_TRACEENCTRL2);
-	etm_writel(t, 0, ETMR_TRACESSCTRL);
-	etm_writel(t, 0x6f, ETMR_TRACEENEVT);
+	if (t->range_start || t->range_end)
+		etm_setup_address_range(t, id, 1,
+					t->range_start, t->range_end, 0, 0);
+	else
+		etm_writel(t, id, ETMTE_INCLEXCL, ETMR_TRACEENCTRL);
+
+	etm_writel(t, id, 0, ETMR_TRACEENCTRL2);
+	etm_writel(t, id, 0, ETMR_TRACESSCTRL);
+	etm_writel(t, id, 0x6f, ETMR_TRACEENEVT);
+
+	etm_writel(t, id, 0, ETMR_VIEWDATACTRL1);
+	etm_writel(t, id, 0, ETMR_VIEWDATACTRL2);
+
+	if (t->data_range_start || t->data_range_end)
+		etm_setup_address_range(t, id, 2, t->data_range_start,
+					t->data_range_end, 0, 1);
+	else
+		etm_writel(t, id, ETMVDC3_EXCLONLY, ETMR_VIEWDATACTRL3);
+
+	etm_writel(t, id, 0x6f, ETMR_VIEWDATAEVT);
 
 	v &= ~ETMCTRL_PROGRAM;
 	v |= ETMCTRL_PORTSEL;
 
-	etm_writel(t, v, ETMR_CTRL);
+	etm_writel(t, id, v, ETMR_CTRL);
 
 	timeout = TRACER_TIMEOUT;
-	while (etm_readl(t, ETMR_CTRL) & ETMCTRL_PROGRAM && --timeout)
+	while (etm_readl(t, id, ETMR_CTRL) & ETMCTRL_PROGRAM && --timeout)
 		;
 	if (!timeout) {
 		dev_dbg(t->dev, "Waiting for progbit to deassert timed out\n");
-		etm_lock(t);
+		etm_lock(t, id);
 		return -EFAULT;
 	}
 
-	etm_lock(t);
+	etm_lock(t, id);
+	return 0;
+}
+
+static int trace_start(struct tracectx *t)
+{
+	int ret;
+	int id;
+	u32 etb_fc = t->etb_fc;
+
+	etb_unlock(t);
+
+	t->dump_initial_etb = false;
+	etb_writel(t, 0, ETBR_WRITEADDR);
+	etb_writel(t, etb_fc, ETBR_FORMATTERCTRL);
+	etb_writel(t, 1, ETBR_CTRL);
+
+	etb_lock(t);
+
+	/* configure etm(s) */
+	for (id = 0; id < t->etm_regs_count; id++) {
+		ret = trace_start_etm(t, id);
+		if (ret)
+			return ret;
+	}
 
 	t->flags |= TRACER_RUNNING;
 
 	return 0;
 }
-
-static int trace_stop(struct tracectx *t)
+static int trace_stop_etm(struct tracectx *t, int id)
 {
 	unsigned long timeout = TRACER_TIMEOUT;
 
-	etm_unlock(t);
+	etm_unlock(t, id);
 
-	etm_writel(t, 0x440, ETMR_CTRL);
-	while (!(etm_readl(t, ETMR_CTRL) & ETMCTRL_PROGRAM) && --timeout)
+	etm_writel(t, id, 0x440, ETMR_CTRL);
+	while (!(etm_readl(t, id, ETMR_CTRL) & ETMCTRL_PROGRAM) && --timeout)
 		;
 	if (!timeout) {
-		dev_dbg(t->dev, "Waiting for progbit to assert timed out\n");
-		etm_lock(t);
+		dev_err(t->dev,
+			"etm%d: Waiting for progbit to assert timed out\n",
+			id);
+		etm_lock(t, id);
 		return -EFAULT;
 	}
 
-	etm_lock(t);
+	etm_lock(t, id);
+	return 0;
+}
+
+static int trace_power_down_etm(struct tracectx *t, int id)
+{
+	unsigned long timeout = TRACER_TIMEOUT;
+	etm_unlock(t, id);
+	while (!(etm_readl(t, id, ETMR_STATUS) & ETMST_PROGBIT) && --timeout)
+		;
+	if (!timeout) {
+		dev_err(t->dev, "etm%d: Waiting for status progbit to assert timed out\n",
+			id);
+		etm_lock(t, id);
+		return -EFAULT;
+	}
+
+	etm_writel(t, id, 0x441, ETMR_CTRL);
+
+	etm_lock(t, id);
+	return 0;
+}
+
+static int trace_stop(struct tracectx *t)
+{
+	int id;
+	unsigned long timeout = TRACER_TIMEOUT;
+	u32 etb_fc = t->etb_fc;
+
+	for (id = 0; id < t->etm_regs_count; id++)
+		trace_stop_etm(t, id);
+
+	for (id = 0; id < t->etm_regs_count; id++)
+		trace_power_down_etm(t, id);
 
 	etb_unlock(t);
-	etb_writel(t, ETBFF_MANUAL_FLUSH, ETBR_FORMATTERCTRL);
+	if (etb_fc) {
+		etb_fc |= ETBFF_STOPFL;
+		etb_writel(t, t->etb_fc, ETBR_FORMATTERCTRL);
+	}
+	etb_writel(t, etb_fc | ETBFF_MANUAL_FLUSH, ETBR_FORMATTERCTRL);
 
 	timeout = TRACER_TIMEOUT;
 	while (etb_readl(t, ETBR_FORMATTERCTRL) &
@@ -185,24 +286,15 @@ static int trace_stop(struct tracectx *t)
 static int etb_getdatalen(struct tracectx *t)
 {
 	u32 v;
-	int rp, wp;
+	int wp;
 
 	v = etb_readl(t, ETBR_STATUS);
 
 	if (v & 1)
 		return t->etb_bufsz;
 
-	rp = etb_readl(t, ETBR_READADDR);
 	wp = etb_readl(t, ETBR_WRITEADDR);
-
-	if (rp > wp) {
-		etb_writel(t, 0, ETBR_READADDR);
-		etb_writel(t, 0, ETBR_WRITEADDR);
-
-		return 0;
-	}
-
-	return wp - rp;
+	return wp;
 }
 
 /* sysrq+v will always stop the running trace and leave it at that */
@@ -235,21 +327,18 @@ static void etm_dump(void)
 		printk("%08x", cpu_to_be32(etb_readl(t, ETBR_READMEM)));
 	printk(KERN_INFO "\n--- ETB buffer end ---\n");
 
-	/* deassert the overflow bit */
-	etb_writel(t, 1, ETBR_CTRL);
-	etb_writel(t, 0, ETBR_CTRL);
-
-	etb_writel(t, 0, ETBR_TRIGGERCOUNT);
-	etb_writel(t, 0, ETBR_READADDR);
-	etb_writel(t, 0, ETBR_WRITEADDR);
-
 	etb_lock(t);
 }
 
 static void sysrq_etm_dump(int key)
 {
+	if (!mutex_trylock(&tracer.mutex)) {
+		printk(KERN_INFO "Tracing hardware busy\n");
+		return;
+	}
 	dev_dbg(tracer.dev, "Dumping ETB buffer\n");
 	etm_dump();
+	mutex_unlock(&tracer.mutex);
 }
 
 static struct sysrq_key_op sysrq_etm_op = {
@@ -276,6 +365,10 @@ static ssize_t etb_read(struct file *file, char __user *data,
 	struct tracectx *t = file->private_data;
 	u32 first = 0;
 	u32 *buf;
+	int wpos;
+	int skip;
+	long wlength;
+	loff_t pos = *ppos;
 
 	mutex_lock(&t->mutex);
 
@@ -287,31 +380,39 @@ static ssize_t etb_read(struct file *file, char __user *data,
 	etb_unlock(t);
 
 	total = etb_getdatalen(t);
+	if (total == 0 && t->dump_initial_etb)
+		total = t->etb_bufsz;
 	if (total == t->etb_bufsz)
 		first = etb_readl(t, ETBR_WRITEADDR);
 
+	if (pos > total * 4) {
+		skip = 0;
+		wpos = total;
+	} else {
+		skip = (int)pos % 4;
+		wpos = (int)pos / 4;
+	}
+	total -= wpos;
+	first = (first + wpos) % t->etb_bufsz;
+
 	etb_writel(t, first, ETBR_READADDR);
 
-	length = min(total * 4, (int)len);
-	buf = vmalloc(length);
+	wlength = min(total, DIV_ROUND_UP(skip + (int)len, 4));
+	length = min(total * 4 - skip, (int)len);
+	buf = vmalloc(wlength * 4);
 
-	dev_dbg(t->dev, "ETB buffer length: %d\n", total);
+	dev_dbg(t->dev, "ETB read %ld bytes to %lld from %ld words at %d\n",
+		length, pos, wlength, first);
+	dev_dbg(t->dev, "ETB buffer length: %d\n", total + wpos);
 	dev_dbg(t->dev, "ETB status reg: %x\n", etb_readl(t, ETBR_STATUS));
-	for (i = 0; i < length / 4; i++)
+	for (i = 0; i < wlength; i++)
 		buf[i] = etb_readl(t, ETBR_READMEM);
 
-	/* the only way to deassert overflow bit in ETB status is this */
-	etb_writel(t, 1, ETBR_CTRL);
-	etb_writel(t, 0, ETBR_CTRL);
-
-	etb_writel(t, 0, ETBR_WRITEADDR);
-	etb_writel(t, 0, ETBR_READADDR);
-	etb_writel(t, 0, ETBR_TRIGGERCOUNT);
-
 	etb_lock(t);
 
-	length -= copy_to_user(data, buf, length);
+	length -= copy_to_user(data, (u8 *)buf + skip, length);
 	vfree(buf);
+	*ppos = pos + length;
 
 out:
 	mutex_unlock(&t->mutex);
@@ -348,47 +449,53 @@ static int etb_probe(struct amba_device *dev, const struct amba_id *id)
 	if (ret)
 		goto out;
 
+	mutex_lock(&t->mutex);
 	t->etb_regs = ioremap_nocache(dev->res.start, resource_size(&dev->res));
 	if (!t->etb_regs) {
 		ret = -ENOMEM;
 		goto out_release;
 	}
 
+	t->dev = &dev->dev;
+	t->dump_initial_etb = true;
 	amba_set_drvdata(dev, t);
 
+	etb_unlock(t);
+	t->etb_bufsz = etb_readl(t, ETBR_DEPTH);
+	dev_dbg(&dev->dev, "Size: %x\n", t->etb_bufsz);
+
+	/* make sure trace capture is disabled */
+	etb_writel(t, 0, ETBR_CTRL);
+	etb_writel(t, 0x1000, ETBR_FORMATTERCTRL);
+	etb_lock(t);
+	mutex_unlock(&t->mutex);
+
 	etb_miscdev.parent = &dev->dev;
 
 	ret = misc_register(&etb_miscdev);
 	if (ret)
 		goto out_unmap;
 
+	/* Get optional clock. Currently used to select clock source on omap3 */
 	t->emu_clk = clk_get(&dev->dev, "emu_src_ck");
-	if (IS_ERR(t->emu_clk)) {
+	if (IS_ERR(t->emu_clk))
 		dev_dbg(&dev->dev, "Failed to obtain emu_src_ck.\n");
-		return -EFAULT;
-	}
-
+	else
 	clk_enable(t->emu_clk);
 
-	etb_unlock(t);
-	t->etb_bufsz = etb_readl(t, ETBR_DEPTH);
-	dev_dbg(&dev->dev, "Size: %x\n", t->etb_bufsz);
-
-	/* make sure trace capture is disabled */
-	etb_writel(t, 0, ETBR_CTRL);
-	etb_writel(t, 0x1000, ETBR_FORMATTERCTRL);
-	etb_lock(t);
-
 	dev_dbg(&dev->dev, "ETB AMBA driver initialized.\n");
 
 out:
 	return ret;
 
 out_unmap:
+	mutex_lock(&t->mutex);
 	amba_set_drvdata(dev, NULL);
 	iounmap(t->etb_regs);
+	t->etb_regs = NULL;
 
 out_release:
+	mutex_unlock(&t->mutex);
 	amba_release_regions(dev);
 
 	return ret;
@@ -403,8 +510,10 @@ static int etb_remove(struct amba_device *dev)
 	iounmap(t->etb_regs);
 	t->etb_regs = NULL;
 
+	if (!IS_ERR(t->emu_clk)) {
 	clk_disable(t->emu_clk);
 	clk_put(t->emu_clk);
+	}
 
 	amba_release_regions(dev);
 
@@ -448,6 +557,9 @@ static ssize_t trace_running_store(struct kobject *kobj,
 		return -EINVAL;
 
 	mutex_lock(&tracer.mutex);
+	if (!tracer.etb_regs)
+		ret = -ENODEV;
+	else
 	ret = value ? trace_start(&tracer) : trace_stop(&tracer);
 	mutex_unlock(&tracer.mutex);
 
@@ -463,7 +575,11 @@ static ssize_t trace_info_show(struct kobject *kobj,
 {
 	u32 etb_wa, etb_ra, etb_st, etb_fc, etm_ctrl, etm_st;
 	int datalen;
+	int id;
+	int ret;
 
+	mutex_lock(&tracer.mutex);
+	if (tracer.etb_regs) {
 	etb_unlock(&tracer);
 	datalen = etb_getdatalen(&tracer);
 	etb_wa = etb_readl(&tracer, ETBR_WRITEADDR);
@@ -471,29 +587,39 @@ static ssize_t trace_info_show(struct kobject *kobj,
 	etb_st = etb_readl(&tracer, ETBR_STATUS);
 	etb_fc = etb_readl(&tracer, ETBR_FORMATTERCTRL);
 	etb_lock(&tracer);
+	} else {
+		etb_wa = etb_ra = etb_st = etb_fc = ~0;
+		datalen = -1;
+	}
 
-	etm_unlock(&tracer);
-	etm_ctrl = etm_readl(&tracer, ETMR_CTRL);
-	etm_st = etm_readl(&tracer, ETMR_STATUS);
-	etm_lock(&tracer);
-
-	return sprintf(buf, "Trace buffer len: %d\nComparator pairs: %d\n"
+	ret = sprintf(buf, "Trace buffer len: %d\nComparator pairs: %d\n"
 			"ETBR_WRITEADDR:\t%08x\n"
 			"ETBR_READADDR:\t%08x\n"
 			"ETBR_STATUS:\t%08x\n"
-			"ETBR_FORMATTERCTRL:\t%08x\n"
-			"ETMR_CTRL:\t%08x\n"
-			"ETMR_STATUS:\t%08x\n",
+			"ETBR_FORMATTERCTRL:\t%08x\n",
 			datalen,
 			tracer.ncmppairs,
 			etb_wa,
 			etb_ra,
 			etb_st,
-			etb_fc,
+			etb_fc
+			);
+
+	for (id = 0; id < tracer.etm_regs_count; id++) {
+		etm_unlock(&tracer, id);
+		etm_ctrl = etm_readl(&tracer, id, ETMR_CTRL);
+		etm_st = etm_readl(&tracer, id, ETMR_STATUS);
+		etm_lock(&tracer, id);
+		ret += sprintf(buf + ret, "ETMR_CTRL:\t%08x\n"
+			"ETMR_STATUS:\t%08x\n",
 			etm_ctrl,
 			etm_st
 			);
 }
+	mutex_unlock(&tracer.mutex);
+
+	return ret;
+}
 
 static struct kobj_attribute trace_info_attr =
 	__ATTR(trace_info, 0444, trace_info_show, NULL);
@@ -531,42 +657,260 @@ static ssize_t trace_mode_store(struct kobject *kobj,
 static struct kobj_attribute trace_mode_attr =
 	__ATTR(trace_mode, 0644, trace_mode_show, trace_mode_store);
 
+static ssize_t trace_contextid_size_show(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 char *buf)
+{
+	/* 0: No context id tracing, 1: One byte, 2: Two bytes, 3: Four bytes */
+	return sprintf(buf, "%d\n", (1 << tracer.etm_contextid_size) >> 1);
+}
+
+static ssize_t trace_contextid_size_store(struct kobject *kobj,
+					  struct kobj_attribute *attr,
+					  const char *buf, size_t n)
+{
+	unsigned int contextid_size;
+
+	if (sscanf(buf, "%u", &contextid_size) != 1)
+		return -EINVAL;
+
+	if (contextid_size == 3 || contextid_size > 4)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	tracer.etm_contextid_size = fls(contextid_size);
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+static struct kobj_attribute trace_contextid_size_attr =
+	__ATTR(trace_contextid_size, 0644,
+		trace_contextid_size_show, trace_contextid_size_store);
+
+static ssize_t trace_branch_output_show(struct kobject *kobj,
+					struct kobj_attribute *attr,
+					char *buf)
+{
+	return sprintf(buf, "%d\n", !!(tracer.flags & TRACER_BRANCHOUTPUT));
+}
+
+static ssize_t trace_branch_output_store(struct kobject *kobj,
+					 struct kobj_attribute *attr,
+					 const char *buf, size_t n)
+{
+	unsigned int branch_output;
+
+	if (sscanf(buf, "%u", &branch_output) != 1)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	if (branch_output) {
+		tracer.flags |= TRACER_BRANCHOUTPUT;
+		/* Branch broadcasting is incompatible with the return stack */
+		tracer.flags &= ~TRACER_RETURN_STACK;
+	} else {
+		tracer.flags &= ~TRACER_BRANCHOUTPUT;
+	}
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+static struct kobj_attribute trace_branch_output_attr =
+	__ATTR(trace_branch_output, 0644,
+		trace_branch_output_show, trace_branch_output_store);
+
+static ssize_t trace_return_stack_show(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  char *buf)
+{
+	return sprintf(buf, "%d\n", !!(tracer.flags & TRACER_RETURN_STACK));
+}
+
+static ssize_t trace_return_stack_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buf, size_t n)
+{
+	unsigned int return_stack;
+
+	if (sscanf(buf, "%u", &return_stack) != 1)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	if (return_stack) {
+		tracer.flags |= TRACER_RETURN_STACK;
+		/* Return stack is incompatible with branch broadcasting */
+		tracer.flags &= ~TRACER_BRANCHOUTPUT;
+	} else {
+		tracer.flags &= ~TRACER_RETURN_STACK;
+	}
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+static struct kobj_attribute trace_return_stack_attr =
+	__ATTR(trace_return_stack, 0644,
+		trace_return_stack_show, trace_return_stack_store);
+
+static ssize_t trace_timestamp_show(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  char *buf)
+{
+	return sprintf(buf, "%d\n", !!(tracer.flags & TRACER_TIMESTAMP));
+}
+
+static ssize_t trace_timestamp_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buf, size_t n)
+{
+	unsigned int timestamp;
+
+	if (sscanf(buf, "%u", &timestamp) != 1)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	if (timestamp)
+		tracer.flags |= TRACER_TIMESTAMP;
+	else
+		tracer.flags &= ~TRACER_TIMESTAMP;
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+static struct kobj_attribute trace_timestamp_attr =
+	__ATTR(trace_timestamp, 0644,
+		trace_timestamp_show, trace_timestamp_store);
+
+static ssize_t trace_range_show(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  char *buf)
+{
+	return sprintf(buf, "%08lx %08lx\n",
+			tracer.range_start, tracer.range_end);
+}
+
+static ssize_t trace_range_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buf, size_t n)
+{
+	unsigned long range_start, range_end;
+
+	if (sscanf(buf, "%lx %lx", &range_start, &range_end) != 2)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	tracer.range_start = range_start;
+	tracer.range_end = range_end;
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+
+static struct kobj_attribute trace_range_attr =
+	__ATTR(trace_range, 0644, trace_range_show, trace_range_store);
+
+static ssize_t trace_data_range_show(struct kobject *kobj,
+				  struct kobj_attribute *attr,
+				  char *buf)
+{
+	unsigned long range_start;
+	u64 range_end;
+	mutex_lock(&tracer.mutex);
+	range_start = tracer.data_range_start;
+	range_end = tracer.data_range_end;
+	if (!range_end && (tracer.flags & TRACER_TRACE_DATA))
+		range_end = 0x100000000ULL;
+	mutex_unlock(&tracer.mutex);
+	return sprintf(buf, "%08lx %08llx\n", range_start, range_end);
+}
+
+static ssize_t trace_data_range_store(struct kobject *kobj,
+				   struct kobj_attribute *attr,
+				   const char *buf, size_t n)
+{
+	unsigned long range_start;
+	u64 range_end;
+
+	if (sscanf(buf, "%lx %llx", &range_start, &range_end) != 2)
+		return -EINVAL;
+
+	mutex_lock(&tracer.mutex);
+	tracer.data_range_start = range_start;
+	tracer.data_range_end = (unsigned long)range_end;
+	if (range_end)
+		tracer.flags |= TRACER_TRACE_DATA;
+	else
+		tracer.flags &= ~TRACER_TRACE_DATA;
+	mutex_unlock(&tracer.mutex);
+
+	return n;
+}
+
+
+static struct kobj_attribute trace_data_range_attr =
+	__ATTR(trace_data_range, 0644,
+		trace_data_range_show, trace_data_range_store);
+
 static int etm_probe(struct amba_device *dev, const struct amba_id *id)
 {
 	struct tracectx *t = &tracer;
 	int ret = 0;
+	void __iomem **new_regs;
+	int new_count;
+	u32 etmccr;
+	u32 etmidr;
+	u32 etmccer = 0;
+	u8 etm_version = 0;
+
+	mutex_lock(&t->mutex);
+	new_count = t->etm_regs_count + 1;
+	new_regs = krealloc(t->etm_regs,
+				sizeof(t->etm_regs[0]) * new_count, GFP_KERNEL);
 
-	if (t->etm_regs) {
-		dev_dbg(&dev->dev, "ETM already initialized\n");
-		ret = -EBUSY;
+	if (!new_regs) {
+		dev_dbg(&dev->dev, "Failed to allocate ETM register array\n");
+		ret = -ENOMEM;
 		goto out;
 	}
+	t->etm_regs = new_regs;
 
 	ret = amba_request_regions(dev, NULL);
 	if (ret)
 		goto out;
 
-	t->etm_regs = ioremap_nocache(dev->res.start, resource_size(&dev->res));
-	if (!t->etm_regs) {
+	t->etm_regs[t->etm_regs_count] =
+		ioremap_nocache(dev->res.start, resource_size(&dev->res));
+	if (!t->etm_regs[t->etm_regs_count]) {
 		ret = -ENOMEM;
 		goto out_release;
 	}
 
-	amba_set_drvdata(dev, t);
+	amba_set_drvdata(dev, t->etm_regs[t->etm_regs_count]);
 
-	mutex_init(&t->mutex);
-	t->dev = &dev->dev;
-	t->flags = TRACER_CYCLE_ACC;
+	t->flags = TRACER_CYCLE_ACC | TRACER_TRACE_DATA | TRACER_BRANCHOUTPUT;
 	t->etm_portsz = 1;
+	t->etm_contextid_size = 3;
 
-	etm_unlock(t);
-	(void)etm_readl(t, ETMMR_PDSR);
+	etm_unlock(t, t->etm_regs_count);
+	(void)etm_readl(t, t->etm_regs_count, ETMMR_PDSR);
 	/* dummy first read */
-	(void)etm_readl(&tracer, ETMMR_OSSRR);
-
-	t->ncmppairs = etm_readl(t, ETMR_CONFCODE) & 0xf;
-	etm_writel(t, 0x440, ETMR_CTRL);
-	etm_lock(t);
+	(void)etm_readl(&tracer, t->etm_regs_count, ETMMR_OSSRR);
+
+	etmccr = etm_readl(t, t->etm_regs_count, ETMR_CONFCODE);
+	t->ncmppairs = etmccr & 0xf;
+	if (etmccr & ETMCCR_ETMIDR_PRESENT) {
+		etmidr = etm_readl(t, t->etm_regs_count, ETMR_ID);
+		etm_version = ETMIDR_VERSION(etmidr);
+		if (etm_version >= ETMIDR_VERSION_3_1)
+			etmccer = etm_readl(t, t->etm_regs_count, ETMR_CCE);
+	}
+	etm_writel(t, t->etm_regs_count, 0x441, ETMR_CTRL);
+	etm_writel(t, t->etm_regs_count, new_count, ETMR_TRACEIDR);
+	etm_lock(t, t->etm_regs_count);
 
 	ret = sysfs_create_file(&dev->dev.kobj,
 			&trace_running_attr.attr);
@@ -582,36 +926,101 @@ static int etm_probe(struct amba_device *dev, const struct amba_id *id)
 	if (ret)
 		dev_dbg(&dev->dev, "Failed to create trace_mode in sysfs\n");
 
-	dev_dbg(t->dev, "ETM AMBA driver initialized.\n");
+	ret = sysfs_create_file(&dev->dev.kobj,
+				&trace_contextid_size_attr.attr);
+	if (ret)
+		dev_dbg(&dev->dev,
+			"Failed to create trace_contextid_size in sysfs\n");
+
+	ret = sysfs_create_file(&dev->dev.kobj,
+				&trace_branch_output_attr.attr);
+	if (ret)
+		dev_dbg(&dev->dev,
+			"Failed to create trace_branch_output in sysfs\n");
+
+	if (etmccer & ETMCCER_RETURN_STACK_IMPLEMENTED) {
+		ret = sysfs_create_file(&dev->dev.kobj,
+					&trace_return_stack_attr.attr);
+		if (ret)
+			dev_dbg(&dev->dev,
+			      "Failed to create trace_return_stack in sysfs\n");
+	}
+
+	if (etmccer & ETMCCER_TIMESTAMPING_IMPLEMENTED) {
+		ret = sysfs_create_file(&dev->dev.kobj,
+					&trace_timestamp_attr.attr);
+		if (ret)
+			dev_dbg(&dev->dev,
+				"Failed to create trace_timestamp in sysfs\n");
+	}
+
+	ret = sysfs_create_file(&dev->dev.kobj, &trace_range_attr.attr);
+	if (ret)
+		dev_dbg(&dev->dev, "Failed to create trace_range in sysfs\n");
+
+	if (etm_version < ETMIDR_VERSION_PFT_1_0) {
+		ret = sysfs_create_file(&dev->dev.kobj,
+					&trace_data_range_attr.attr);
+		if (ret)
+			dev_dbg(&dev->dev,
+				"Failed to create trace_data_range in sysfs\n");
+	} else {
+		tracer.flags &= ~TRACER_TRACE_DATA;
+	}
+
+	dev_dbg(&dev->dev, "ETM AMBA driver initialized.\n");
+
+	/* Enable formatter if there are multiple trace sources */
+	if (new_count > 1)
+		t->etb_fc = ETBFF_ENFCONT | ETBFF_ENFTC;
+
+	t->etm_regs_count = new_count;
 
 out:
+	mutex_unlock(&t->mutex);
 	return ret;
 
 out_unmap:
 	amba_set_drvdata(dev, NULL);
-	iounmap(t->etm_regs);
+	iounmap(t->etm_regs[t->etm_regs_count]);
 
 out_release:
 	amba_release_regions(dev);
 
+	mutex_unlock(&t->mutex);
 	return ret;
 }
 
 static int etm_remove(struct amba_device *dev)
 {
-	struct tracectx *t = amba_get_drvdata(dev);
+	int i;
+	struct tracectx *t = &tracer;
+	void __iomem	*etm_regs = amba_get_drvdata(dev);
+
+	sysfs_remove_file(&dev->dev.kobj, &trace_running_attr.attr);
+	sysfs_remove_file(&dev->dev.kobj, &trace_info_attr.attr);
+	sysfs_remove_file(&dev->dev.kobj, &trace_mode_attr.attr);
+	sysfs_remove_file(&dev->dev.kobj, &trace_range_attr.attr);
+	sysfs_remove_file(&dev->dev.kobj, &trace_data_range_attr.attr);
 
 	amba_set_drvdata(dev, NULL);
 
-	iounmap(t->etm_regs);
+	mutex_lock(&t->mutex);
+	for (i = 0; i < t->etm_regs_count; i++)
+		if (t->etm_regs[i] == etm_regs)
+			break;
+	for (; i < t->etm_regs_count - 1; i++)
+		t->etm_regs[i] = t->etm_regs[i + 1];
+	t->etm_regs_count--;
+	if (!t->etm_regs_count) {
+		kfree(t->etm_regs);
 	t->etm_regs = NULL;
+	}
+	mutex_unlock(&t->mutex);
 
+	iounmap(etm_regs);
 	amba_release_regions(dev);
 
-	sysfs_remove_file(&dev->dev.kobj, &trace_running_attr.attr);
-	sysfs_remove_file(&dev->dev.kobj, &trace_info_attr.attr);
-	sysfs_remove_file(&dev->dev.kobj, &trace_mode_attr.attr);
-
 	return 0;
 }
 
@@ -620,6 +1029,10 @@ static struct amba_id etm_ids[] = {
 		.id	= 0x0003b921,
 		.mask	= 0x0007ffff,
 	},
+	{
+		.id	= 0x0003b950,
+		.mask	= 0x0007ffff,
+	},
 	{ 0, 0 },
 };
 
@@ -637,6 +1050,8 @@ static int __init etm_init(void)
 {
 	int retval;
 
+	mutex_init(&tracer.mutex);
+
 	retval = amba_driver_register(&etb_driver);
 	if (retval) {
 		printk(KERN_ERR "Failed to register etb\n");
diff --git a/arch/arm/kernel/head.S b/arch/arm/kernel/head.S
index 45e8935..e396eff 100644
--- a/arch/arm/kernel/head.S
+++ b/arch/arm/kernel/head.S
@@ -109,7 +109,7 @@ ENTRY(stext)
 	sub	r4, r3, r4			@ (PHYS_OFFSET - PAGE_OFFSET)
 	add	r8, r8, r4			@ PHYS_OFFSET
 #else
-	ldr	r8, =PHYS_OFFSET		@ always constant in this case
+	ldr	r8, =PLAT_PHYS_OFFSET		@ always constant in this case
 #endif
 
 	/*
@@ -343,7 +343,6 @@ __turn_mmu_on_loc:
 	.long	__turn_mmu_on_end
 
 #if defined(CONFIG_SMP)
-	__CPUINIT
 ENTRY(secondary_startup)
 	/*
 	 * Common entry point for secondary CPUs.
diff --git a/arch/arm/kernel/perf_event_v7.c b/arch/arm/kernel/perf_event_v7.c
index 039cffb..dd1ad0e 100644
--- a/arch/arm/kernel/perf_event_v7.c
+++ b/arch/arm/kernel/perf_event_v7.c
@@ -950,6 +950,51 @@ static void armv7_pmnc_dump_regs(struct arm_pmu *cpu_pmu)
 }
 #endif
 
+static void armv7pmu_save_regs(struct arm_pmu *cpu_pmu,
+					struct cpupmu_regs *regs)
+{
+	unsigned int cnt;
+	asm volatile("mrc p15, 0, %0, c9, c12, 0" : "=r" (regs->pmc));
+	if (!(regs->pmc & ARMV7_PMNC_E))
+		return;
+
+	asm volatile("mrc p15, 0, %0, c9, c12, 1" : "=r" (regs->pmcntenset));
+	asm volatile("mrc p15, 0, %0, c9, c14, 0" : "=r" (regs->pmuseren));
+	asm volatile("mrc p15, 0, %0, c9, c14, 1" : "=r" (regs->pmintenset));
+	asm volatile("mrc p15, 0, %0, c9, c13, 0" : "=r" (regs->pmxevtcnt[0]));
+	for (cnt = ARMV7_IDX_COUNTER0;
+			cnt <= ARMV7_IDX_COUNTER_LAST(cpu_pmu); cnt++) {
+		armv7_pmnc_select_counter(cnt);
+		asm volatile("mrc p15, 0, %0, c9, c13, 1"
+					: "=r"(regs->pmxevttype[cnt]));
+		asm volatile("mrc p15, 0, %0, c9, c13, 2"
+					: "=r"(regs->pmxevtcnt[cnt]));
+	}
+	return;
+}
+
+static void armv7pmu_restore_regs(struct arm_pmu *cpu_pmu,
+					struct cpupmu_regs *regs)
+{
+	unsigned int cnt;
+	if (!(regs->pmc & ARMV7_PMNC_E))
+		return;
+
+	asm volatile("mcr p15, 0, %0, c9, c12, 1" : : "r" (regs->pmcntenset));
+	asm volatile("mcr p15, 0, %0, c9, c14, 0" : : "r" (regs->pmuseren));
+	asm volatile("mcr p15, 0, %0, c9, c14, 1" : : "r" (regs->pmintenset));
+	asm volatile("mcr p15, 0, %0, c9, c13, 0" : : "r" (regs->pmxevtcnt[0]));
+	for (cnt = ARMV7_IDX_COUNTER0;
+			cnt <= ARMV7_IDX_COUNTER_LAST(cpu_pmu); cnt++) {
+		armv7_pmnc_select_counter(cnt);
+		asm volatile("mcr p15, 0, %0, c9, c13, 1"
+					: : "r"(regs->pmxevttype[cnt]));
+		asm volatile("mcr p15, 0, %0, c9, c13, 2"
+					: : "r"(regs->pmxevtcnt[cnt]));
+	}
+	asm volatile("mcr p15, 0, %0, c9, c12, 0" : : "r" (regs->pmc));
+}
+
 static void armv7pmu_enable_event(struct perf_event *event)
 {
 	unsigned long flags;
@@ -1223,6 +1268,8 @@ static void armv7pmu_init(struct arm_pmu *cpu_pmu)
 	cpu_pmu->start		= armv7pmu_start;
 	cpu_pmu->stop		= armv7pmu_stop;
 	cpu_pmu->reset		= armv7pmu_reset;
+	cpu_pmu->save_regs	= armv7pmu_save_regs;
+	cpu_pmu->restore_regs	= armv7pmu_restore_regs;
 	cpu_pmu->max_period	= (1LLU << 32) - 1;
 };
 
diff --git a/arch/arm/kernel/psci.c b/arch/arm/kernel/psci.c
index 3653164..0daf4f2 100644
--- a/arch/arm/kernel/psci.c
+++ b/arch/arm/kernel/psci.c
@@ -17,6 +17,7 @@
 
 #include <linux/init.h>
 #include <linux/of.h>
+#include <linux/string.h>
 
 #include <asm/compiler.h>
 #include <asm/errno.h>
@@ -26,6 +27,11 @@
 
 struct psci_operations psci_ops;
 
+/* Type of psci support. Currently can only be enabled or disabled */
+#define PSCI_SUP_DISABLED		0
+#define PSCI_SUP_ENABLED		1
+
+static unsigned int psci;
 static int (*invoke_psci_fn)(u32, u32, u32, u32);
 
 enum psci_function {
@@ -42,6 +48,7 @@ static u32 psci_function_id[PSCI_FN_MAX];
 #define PSCI_RET_EOPNOTSUPP		-1
 #define PSCI_RET_EINVAL			-2
 #define PSCI_RET_EPERM			-3
+#define PSCI_RET_EALREADYON		-4
 
 static int psci_to_linux_errno(int errno)
 {
@@ -54,6 +61,8 @@ static int psci_to_linux_errno(int errno)
 		return -EINVAL;
 	case PSCI_RET_EPERM:
 		return -EPERM;
+	case PSCI_RET_EALREADYON:
+		return -EAGAIN;
 	};
 
 	return -EINVAL;
@@ -158,15 +167,18 @@ static const struct of_device_id psci_of_match[] __initconst = {
 	{},
 };
 
-static int __init psci_init(void)
+void __init psci_init(void)
 {
 	struct device_node *np;
 	const char *method;
 	u32 id;
 
+	if (psci == PSCI_SUP_DISABLED)
+		return;
+
 	np = of_find_matching_node(NULL, psci_of_match);
 	if (!np)
-		return 0;
+		return;
 
 	pr_info("probing function IDs from device-tree\n");
 
@@ -206,6 +218,35 @@ static int __init psci_init(void)
 
 out_put_node:
 	of_node_put(np);
-	return 0;
+	return;
+}
+
+int __init psci_probe(void)
+{
+	struct device_node *np;
+	int ret = -ENODEV;
+
+	if (psci == PSCI_SUP_ENABLED) {
+		np = of_find_matching_node(NULL, psci_of_match);
+		if (np)
+			ret = 0;
+	}
+
+	of_node_put(np);
+	return ret;
+}
+
+static int __init early_psci(char *val)
+{
+	int ret = 0;
+
+	if (strcmp(val, "enable") == 0)
+		psci = PSCI_SUP_ENABLED;
+	else if (strcmp(val, "disable") == 0)
+		psci = PSCI_SUP_DISABLED;
+	else
+		ret = -EINVAL;
+
+	return ret;
 }
-early_initcall(psci_init);
+early_param("psci", early_psci);
diff --git a/arch/arm/kernel/psci_smp.c b/arch/arm/kernel/psci_smp.c
new file mode 100644
index 0000000..23a1142
--- /dev/null
+++ b/arch/arm/kernel/psci_smp.c
@@ -0,0 +1,84 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * Copyright (C) 2012 ARM Limited
+ *
+ * Author: Will Deacon <will.deacon@arm.com>
+ */
+
+#include <linux/init.h>
+#include <linux/irqchip/arm-gic.h>
+#include <linux/smp.h>
+#include <linux/of.h>
+
+#include <asm/psci.h>
+#include <asm/smp_plat.h>
+
+/*
+ * psci_smp assumes that the following is true about PSCI:
+ *
+ * cpu_suspend   Suspend the execution on a CPU
+ * @state        we don't currently describe affinity levels, so just pass 0.
+ * @entry_point  the first instruction to be executed on return
+ * returns 0  success, < 0 on failure
+ *
+ * cpu_off       Power down a CPU
+ * @state        we don't currently describe affinity levels, so just pass 0.
+ * no return on successful call
+ *
+ * cpu_on        Power up a CPU
+ * @cpuid        cpuid of target CPU, as from MPIDR
+ * @entry_point  the first instruction to be executed on return
+ * returns 0  success, < 0 on failure
+ *
+ * migrate       Migrate the context to a different CPU
+ * @cpuid        cpuid of target CPU, as from MPIDR
+ * returns 0  success, < 0 on failure
+ *
+ */
+
+extern void secondary_startup(void);
+
+static int __cpuinit psci_boot_secondary(unsigned int cpu,
+					 struct task_struct *idle)
+{
+	if (psci_ops.cpu_on)
+		return psci_ops.cpu_on(cpu_logical_map(cpu),
+				       __pa(secondary_startup));
+	return -ENODEV;
+}
+
+#ifdef CONFIG_HOTPLUG_CPU
+void __ref psci_cpu_die(unsigned int cpu)
+{
+       const struct psci_power_state ps = {
+               .type = PSCI_POWER_STATE_TYPE_POWER_DOWN,
+       };
+
+       if (psci_ops.cpu_off)
+               psci_ops.cpu_off(ps);
+
+       /* We should never return */
+       panic("psci: cpu %d failed to shutdown\n", cpu);
+}
+#else
+#define psci_cpu_die NULL
+#endif
+
+bool __init psci_smp_available(void)
+{
+	/* is cpu_on available at least? */
+	return (psci_ops.cpu_on != NULL);
+}
+
+struct smp_operations __initdata psci_smp_ops = {
+	.smp_boot_secondary	= psci_boot_secondary,
+	.cpu_die		= psci_cpu_die,
+};
diff --git a/arch/arm/kernel/return_address.c b/arch/arm/kernel/return_address.c
index fafedd8..13510d2 100644
--- a/arch/arm/kernel/return_address.c
+++ b/arch/arm/kernel/return_address.c
@@ -60,7 +60,7 @@ void *return_address(unsigned int level)
 #else /* if defined(CONFIG_FRAME_POINTER) && !defined(CONFIG_ARM_UNWIND) */
 
 #if defined(CONFIG_ARM_UNWIND)
-#warning "TODO: return_address should use unwind tables"
+/* #warning "TODO: return_address should use unwind tables" */
 #endif
 
 void *return_address(unsigned int level)
diff --git a/arch/arm/kernel/setup.c b/arch/arm/kernel/setup.c
index ac06bf0..a7f9b50 100644
--- a/arch/arm/kernel/setup.c
+++ b/arch/arm/kernel/setup.c
@@ -37,6 +37,7 @@
 #include <asm/cputype.h>
 #include <asm/elf.h>
 #include <asm/procinfo.h>
+#include <asm/psci.h>
 #include <asm/sections.h>
 #include <asm/setup.h>
 #include <asm/smp_plat.h>
@@ -261,6 +262,19 @@ static int cpu_has_aliasing_icache(unsigned int arch)
 	int aliasing_icache;
 	unsigned int id_reg, num_sets, line_size;
 
+#ifdef CONFIG_BIG_LITTLE
+	/*
+	 * We expect a combination of Cortex-A15 and Cortex-A7 cores.
+	 * A7 = VIPT aliasing I-cache
+	 * A15 = PIPT (non-aliasing) I-cache
+	 * To cater for this discrepancy, let's assume aliasing I-cache
+	 * all the time.  This means unneeded extra work on the A15 but
+	 * only ptrace is affected which is not performance critical.
+	 */
+	if ((read_cpuid_id() & 0xff0ffff0) == 0x410fc0f0)
+		return 1;
+#endif
+
 	/* PIPT caches never alias. */
 	if (icache_is_pipt())
 		return 0;
@@ -830,9 +844,15 @@ void __init setup_arch(char **cmdline_p)
 	unflatten_device_tree();
 
 	arm_dt_init_cpu_maps();
+	psci_init();
 #ifdef CONFIG_SMP
 	if (is_smp()) {
+		if (!mdesc->smp_init || !mdesc->smp_init()) {
+			if (psci_smp_available())
+				smp_set_ops(&psci_smp_ops);
+			else if (mdesc->smp)
 		smp_set_ops(mdesc->smp);
+		}
 		smp_init_cpus();
 	}
 #endif
@@ -906,6 +926,7 @@ static const char *hwcap_str[] = {
 	"vfpv4",
 	"idiva",
 	"idivt",
+	"vfpd32",
 	"lpae",
 	"evtstrm",
 	NULL
diff --git a/arch/arm/kernel/sleep.S b/arch/arm/kernel/sleep.S
index 987dcf3..9e9b915 100644
--- a/arch/arm/kernel/sleep.S
+++ b/arch/arm/kernel/sleep.S
@@ -4,6 +4,7 @@
 #include <asm/assembler.h>
 #include <asm/glue-cache.h>
 #include <asm/glue-proc.h>
+#include "entry-header.S"
 	.text
 
 /*
@@ -30,9 +31,8 @@ ENTRY(__cpu_suspend)
 	mov	r2, r5			@ virtual SP
 	ldr	r3, =sleep_save_sp
 #ifdef CONFIG_SMP
-	ALT_SMP(mrc p15, 0, lr, c0, c0, 5)
-	ALT_UP(mov lr, #0)
-	and	lr, lr, #15
+	get_thread_info	r5
+	ldr	lr, [r5, #TI_CPU] 	@ cpu logical index
 	add	r3, r3, lr, lsl #2
 #endif
 	bl	__cpu_suspend_save
@@ -81,11 +81,15 @@ ENDPROC(cpu_resume_after_mmu)
 	.data
 	.align
 ENTRY(cpu_resume)
+/*ARM_BE8(setend be)*/			@ ensure we are in BE mode
 #ifdef CONFIG_SMP
+	mov	r1, #0			@ fall-back logical index for UP
+	ALT_SMP(mrc p15, 0, r0, c0, c0, 5)
+	ALT_UP_B(1f)
+	bic	r0, #0xff000000
+	bl	cpu_logical_index 	@ return logical index in r1
+1:
 	adr	r0, sleep_save_sp
-	ALT_SMP(mrc p15, 0, r1, c0, c0, 5)
-	ALT_UP(mov r1, #0)
-	and	r1, r1, #15
 	ldr	r0, [r0, r1, lsl #2]	@ stack phys addr
 #else
 	ldr	r0, sleep_save_sp	@ stack phys addr
@@ -102,3 +106,20 @@ sleep_save_sp:
 	.rept	CONFIG_NR_CPUS
 	.long	0				@ preserve stack phys ptr here
 	.endr
+
+#ifdef CONFIG_SMP
+cpu_logical_index:
+	adr	r3, cpu_map_ptr
+	ldr	r2, [r3]
+	add	r3, r3, r2		@ virt_to_phys(__cpu_logical_map)
+	mov	r1, #0
+1:
+	ldr	r2, [r3, r1, lsl #2]
+	cmp	r2, r0
+	moveq	pc, lr
+	add	r1, r1, #1
+	b	1b
+
+cpu_map_ptr:
+	.long __cpu_logical_map - .
+#endif
diff --git a/arch/arm/mach-mb86s70/Kconfig b/arch/arm/mach-mb86s70/Kconfig
new file mode 100644
index 0000000..8488bf0
--- /dev/null
+++ b/arch/arm/mach-mb86s70/Kconfig
@@ -0,0 +1,34 @@
+config ARCH_MB86S70
+	bool "Fujitsu Semiconductor MB86S70"
+	select CPU_V7
+	select ARM_AMBA
+	select ARM_GIC
+#	select ARM_TIMER_SP804
+	select ARCH_REQUIRE_GPIOLIB
+	select ARCH_HAS_CPUFREQ
+	select ARCH_HAS_OPP
+	select PM_OPP
+	select USB_ARCH_HAS_EHCI
+	select USB_ARCH_HAS_XHCI
+	select BIG_LITTLE
+	select CLKDEV_LOOKUP
+	select COMMON_CLK
+	select GENERIC_CLOCKEVENTS
+	select GENERIC_GPIO
+	select HAVE_ARM_ARCH_TIMER
+	select HAVE_CLK
+	select HAVE_SMP
+	select MB8AC0300_EXIU
+	select MIGHT_HAVE_CACHE_L2X0
+	select ARM_CCI
+	select MIGHT_HAVE_PCI
+	select PCIEPORTBUS
+	select PINCTRL
+	select PINCTRL_MB86S70
+	select PM_GENERIC_DOMAINS if PM
+	select LOCAL_TIMERS
+	select ARCH_DMA_ADDR_T_64BIT
+	select ARCH_SELECT_MEMORY_MODEL
+	select ARCH_SPARSEMEM_ENABLE
+	help
+	  Support for Fujitsu Semiconductor's systems.
diff --git a/arch/arm/mach-mb86s70/Makefile b/arch/arm/mach-mb86s70/Makefile
new file mode 100644
index 0000000..a61174d
--- /dev/null
+++ b/arch/arm/mach-mb86s70/Makefile
@@ -0,0 +1,8 @@
+#
+# Makefile for the linux kernel.
+#
+ccflags-$(CONFIG_ARCH_MULTIPLATFORM) := -I$(srctree)/$(src)/include
+
+obj-y := sleep.o common.o devices.o headsmp.o scb_mhu.o mcpm.o mb86s70_pm_setup.o nonsec.o
+obj-$(CONFIG_PM_GENERIC_DOMAINS) += pm_domains.o
+CFLAGS_REMOVE_mcpm.o	= -pg
diff --git a/arch/arm/mach-mb86s70/Makefile.boot b/arch/arm/mach-mb86s70/Makefile.boot
new file mode 100644
index 0000000..f3835c4
--- /dev/null
+++ b/arch/arm/mach-mb86s70/Makefile.boot
@@ -0,0 +1 @@
+zreladdr-y	:= 0x80008000
diff --git a/arch/arm/mach-mb86s70/common.c b/arch/arm/mach-mb86s70/common.c
new file mode 100644
index 0000000..3eaedc1
--- /dev/null
+++ b/arch/arm/mach-mb86s70/common.c
@@ -0,0 +1,41 @@
+/*
+ * linux/arch/arm/mach-mb86s70/common.c
+ *
+ * Copyright (C) 2013 Linaro, LTD
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/init.h>
+#include <linux/irqchip.h>
+
+#include <asm/mach-types.h>
+#include <asm/mach/arch.h>
+
+#include "devices.h"
+
+static const char * const mb86s70_dt_match[] __initconst = {
+	"fujitsu,mb86s70",
+	"fujitsu,mb86s73",
+	"fujitsu,mb86s72",
+	NULL,
+};
+
+DT_MACHINE_START(MB86S70_DT, "Fujitsu MB86S70-based board")
+	.dt_compat	= mb86s70_dt_match,
+	.smp_init	= smp_init_ops(mb86s70_smp_init_ops),
+	.map_io		= mb86s70_dt_map_io,
+	.init_irq	= irqchip_init,
+	.init_time	= mb86s70_dt_timer_init,
+	.init_machine	= mb86s70_dt_init,
+MACHINE_END
diff --git a/arch/arm/mach-mb86s70/devices.c b/arch/arm/mach-mb86s70/devices.c
new file mode 100644
index 0000000..1a00c7c
--- /dev/null
+++ b/arch/arm/mach-mb86s70/devices.c
@@ -0,0 +1,122 @@
+#include <linux/device.h>
+#include <linux/io.h>
+#include <linux/smp.h>
+#include <linux/init.h>
+#include <linux/memblock.h>
+#include <linux/of_address.h>
+#include <linux/of_fdt.h>
+#include <linux/of_irq.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/mtd/physmap.h>
+#include <linux/clk-provider.h>
+#include <linux/clocksource.h>
+#include <linux/irqflags.h>
+
+#include <linux/platform_data/mb86s70-iomap.h>
+
+#include <asm/arch_timer.h>
+#include <asm/mach-types.h>
+#include <asm/sizes.h>
+#include <asm/smp_twd.h>
+#include <asm/mach/arch.h>
+#include <asm/mach/map.h>
+#include <asm/mach/time.h>
+#include <asm/hardware/arm_timer.h>
+#include <asm/hardware/cache-l2x0.h>
+#include <asm/hardware/timer-sp.h>
+#include <asm/mcpm.h>
+
+#include "devices.h"
+
+bool __init mb86s70_smp_init_ops(void)
+{
+	if (of_find_compatible_node(NULL, NULL, "arm,cci-400")) {
+		mcpm_smp_set_ops();
+		return true;
+	}
+
+	pr_err("%s:%d CCI DT node not found\n", __func__, __LINE__);
+	return false;
+}
+
+#define IOMAP_DEV(name) { \
+		.virtual = (unsigned long) MB86S70_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB86S70_##name##_PHYS), \
+		.length = MB86S70_##name##_SIZE, \
+		.type = MT_DEVICE, \
+	}
+#define IOMAP_SO(name) { \
+		.virtual = (unsigned long) MB86S70_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB86S70_##name##_PHYS), \
+		.length = MB86S70_##name##_SIZE, \
+		.type = MT_MEMORY_SO, \
+	}
+#define IOMAP_MEM(name) { \
+		.virtual = (unsigned long) MB86S70_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB86S70_##name##_PHYS), \
+		.length = MB86S70_##name##_SIZE, \
+		.type = MT_MEMORY, \
+	}
+
+static struct map_desc mb86s70_io_desc[] = {
+	IOMAP_DEV(UART0),
+	IOMAP_DEV(UART1),
+	IOMAP_DEV(MHU),
+	IOMAP_SO(ISRAM),
+	IOMAP_MEM(SSRAM),
+};
+
+void __init mb86s70_dt_map_io(void)
+{
+	iotable_init(mb86s70_io_desc, ARRAY_SIZE(mb86s70_io_desc));
+}
+
+void __init mb86s70_dt_timer_init(void)
+{
+	if (!irqs_disabled())
+		printk("%s:%d\n", __func__, __LINE__);
+	of_clk_init(NULL);
+	if (!irqs_disabled())
+		printk("%s:%d\n", __func__, __LINE__);
+	clocksource_of_init();
+	if (!irqs_disabled())
+		printk("%s:%d\n", __func__, __LINE__);
+/*	twd_local_timer_of_register(); */
+}
+
+//static struct platform_device *bL_cpufreq;
+
+void __init mb86s70_dt_init(void)
+{
+	unsigned int val;
+	void __iomem *sysoc = ioremap(0x37300000, SZ_8K);
+
+	writel(0x72000000, sysoc + 0x1008);
+
+	/* Init sysoc for HS_SPI
+	 * After power HSSPIn_CSCFG.BOOTEN bit is 1,AUXCTL.HSEL bit is 0.
+	 * At this setting only command sequencer mode can be used.
+	 * We want to use both of command sequencer mode and direct mode,
+	 * so we set:
+	 *   HSSPIn_CSCFG.BOOTEN: 0
+	 *   AUXCTL.HSEL        : 1
+	 * BOOTEN bit will set in HSSPI driver, so we only set AUXCTL.HSEL here.
+	 */
+	val = readl(sysoc + 8);
+	val |= 1 << 0;
+	writel(val, sysoc + 8);
+
+	iounmap(sysoc);
+
+	of_platform_populate(NULL, of_default_bus_match_table, NULL, NULL);
+
+	/* 
+	 * disabled until S70 SCB in LT binaries is able to deal with it
+	 *
+	 * bL_cpufreq = platform_device_alloc("arm-bL-cpufreq-dt", -1);
+	 * platform_device_add(bL_cpufreq);
+	 */
+}
+
diff --git a/arch/arm/mach-mb86s70/devices.h b/arch/arm/mach-mb86s70/devices.h
new file mode 100644
index 0000000..010768a
--- /dev/null
+++ b/arch/arm/mach-mb86s70/devices.h
@@ -0,0 +1,10 @@
+#ifndef __MACH_MB86S70_DEVICES_H__
+#define __MACH_MB86S70_DEVICES_H__
+
+bool __init mb86s70_smp_init_ops(void);
+void __init mb86s70_dt_map_io(void);
+void __init mb86s70_dt_timer_init(void);
+void __init mb86s70_dt_init(void);
+void __init mb86s704_secondary_startup(void);
+
+#endif
diff --git a/arch/arm/mach-mb86s70/headsmp.S b/arch/arm/mach-mb86s70/headsmp.S
new file mode 100644
index 0000000..8d1256e
--- /dev/null
+++ b/arch/arm/mach-mb86s70/headsmp.S
@@ -0,0 +1,39 @@
+/*
+ *  linux/arch/arm/mach-mb86s70/headsmp.S
+ *
+ *  Cloned from linux/arch/arm/plat-versatile/headsmp.S
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/linkage.h>
+#include <linux/init.h>
+
+	__CPUINIT
+
+/*
+ * Entry point for secondary CPUs. This provides a "holding pen" into which
+ * all secondary cores are held until we're ready for them to initialise.
+ */
+ENTRY(mb86s704_secondary_startup)
+	mrc	p15, 0, r0, c0, c0, 5
+	bic	r0, #0xff000000
+	adr	r4, 1f
+	ldmia	r4, {r5, r6}
+	sub	r4, r4, r5
+	add	r6, r6, r4
+pen:	ldr	r7, [r6]
+	cmp	r7, r0
+	bne	pen
+
+	/*
+	 * we've been released from the holding pen: secondary_stack
+	 * should now contain the SVC stack for this core
+	 */
+	b	secondary_startup
+ENDPROC(mb86s704_secondary_startup)
+
+	.align 2
+1:	.long	.
+	.long	pen_release
diff --git a/arch/arm/mach-mb86s70/mb86s70_pm_setup.S b/arch/arm/mach-mb86s70/mb86s70_pm_setup.S
new file mode 100644
index 0000000..1dadf01
--- /dev/null
+++ b/arch/arm/mach-mb86s70/mb86s70_pm_setup.S
@@ -0,0 +1,18 @@
+/*
+ * arch/arm/mach-mb86s70/mb86s70_pm_setup.S
+ *
+ * Shameless copy of dcscb_setup.S
+ * Created by:  Dave Martin, 2012-06-22
+ * Copyright:   (C) 2012-2013  Linaro Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/linkage.h>
+
+ENTRY(mb86s70_pm_power_up_setup)
+	b cci_enable_port_for_self
+	bx	lr
+ENDPROC(mb86s70_pm_power_up_setup)
diff --git a/arch/arm/mach-mb86s70/mcpm.c b/arch/arm/mach-mb86s70/mcpm.c
new file mode 100644
index 0000000..74902df
--- /dev/null
+++ b/arch/arm/mach-mb86s70/mcpm.c
@@ -0,0 +1,531 @@
+/*
+ * arch/arm/mach-mb86s70/mcpm.c
+ *
+ * Shameless copy of tc_pm.c
+ * Created by:	Nicolas Pitre, October 2012
+ * Copyright:	(C) 2012  Linaro Limited
+ * Some portions of this file were originally written by Achin Gupta
+ * Copyright:   (C) 2012  ARM Limited
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/irqchip/arm-gic.h>
+#include <linux/io.h>
+#include <linux/delay.h>
+#include <linux/pm.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/suspend.h>
+#include <linux/irq.h>
+#include <linux/interrupt.h>
+#include <linux/pinctrl/consumer.h>
+#include <linux/platform_device.h>
+
+#include <asm/mcpm.h>
+#include <asm/proc-fns.h>
+#include <asm/cacheflush.h>
+#include <asm/cputype.h>
+#include <asm/cp15.h>
+#include <asm/psci.h>
+#include <asm/system_misc.h>
+#include <asm/suspend.h>
+#include <asm/idmap.h>
+
+#include <linux/arm-cci.h>
+#include <linux/mailbox_controller.h>
+#include <linux/scb_mhu_api.h>
+#include <linux/reboot.h>
+#include <linux/cpu_pm.h>
+#include <linux/slab.h>
+#include <asm/cacheflush.h>
+
+#include <linux/platform_data/mb86s70-iomap.h>
+
+extern int skip_mhu;
+
+static arch_spinlock_t mb86s70_pm_lock = __ARCH_SPIN_LOCK_UNLOCKED;
+
+static atomic_t mb86s70_pm_use_count[4][2];
+
+static bool use_retention_mode;
+
+struct cmd_scb_version version_cmd;
+
+#define MB86S70_WFICOLOR_VIRT (MB86S70_ISRAM_VIRT + WFI_COLOR_REG_OFFSET)
+
+const struct cmd_scb_version *mb86s7x_get_scb_version(void)
+{
+	if (version_cmd.payload_size != sizeof(version_cmd))
+		return NULL;
+
+	return &version_cmd;
+}
+EXPORT_SYMBOL_GPL(mb86s7x_get_scb_version);
+
+static void mb86s70_set_wficolor(unsigned clstr, unsigned cpu, unsigned clr)
+{
+	u8 val;
+
+	if (clr & ~AT_WFI_COLOR_MASK)
+		return;
+
+	val = __raw_readb(MB86S70_WFICOLOR_VIRT + clstr * 2 + cpu);
+	val &= ~AT_WFI_COLOR_MASK;
+	val |= clr;
+	__raw_writeb(val, MB86S70_WFICOLOR_VIRT + clstr * 2 + cpu);
+}
+
+static int mb86s70_pm_power_up(unsigned int cpu, unsigned int cluster)
+{
+	struct cmd_cpu_control_gate cmd;
+	struct completion got_rsp;
+	int ret = 0;
+
+	local_irq_disable();
+	arch_spin_lock(&mb86s70_pm_lock);
+	switch (atomic_inc_return(&mb86s70_pm_use_count[cpu][cluster])) {
+	case 1:
+		cmd.payload_size = sizeof(cmd);
+		cmd.cluster_class = 0;
+		cmd.cluster_id = cluster;
+		cmd.cpu_id = cpu;
+		cmd.cpu_state = SCB_CPU_STATE_ON;
+
+		mbox_dbg("%s:%d CMD Pyld-%u Cl_Class-%u CL_ID-%u CPU_ID-%u STATE-%u}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cluster_class,
+			cmd.cluster_id,	cmd.cpu_id, cmd.cpu_state);
+
+		init_completion(&got_rsp);
+		if (skip_mhu) {
+			ret = 0;
+		} else {
+			mb86s70_set_wficolor(cluster, cpu, AT_WFI_DO_NOTHING);
+			arch_spin_unlock(&mb86s70_pm_lock);
+			local_irq_enable();
+			ret = mhu_send_packet(CMD_CPU_CLOCK_GATE_SET_REQ,
+					&cmd, sizeof(cmd), &got_rsp);
+		}
+		if (ret < 0) {
+			pr_err("%s:%d failed!\n", __func__, __LINE__);
+			return ret;
+		}
+
+		if (ret)
+			wait_for_completion(&got_rsp);
+
+		mbox_dbg("%s:%d REP Pyld-%u Cl_Class-%u CL_ID-%u CPU_ID-%u STATE-%u}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cluster_class,
+			cmd.cluster_id,	cmd.cpu_id, cmd.cpu_state);
+		break;
+	case 2:
+		/* This power up request has overtaken a power down request */
+		arch_spin_unlock(&mb86s70_pm_lock);
+		local_irq_enable();
+		break;
+	default:
+		/*
+		 * The only possible values are:
+		 * 0 = CPU down
+		 * 1 = CPU (still) up
+		 * 2 = CPU requested to be up before it had a chance
+		 *     to actually make itself down.
+		 * Any other value is a bug.
+		 */
+		BUG();
+	}
+
+	return 0;
+}
+
+static void mb86s70_pm_suspend(u64 suspend)
+{
+	unsigned int mpidr, cpu, cluster;
+	bool last_man = false, skip_wfi = false;
+
+	mpidr = read_cpuid_mpidr();
+	cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+
+	__mcpm_cpu_going_down(cpu, cluster);
+
+	local_irq_disable();
+	arch_spin_lock(&mb86s70_pm_lock);
+	BUG_ON(__mcpm_cluster_state(cluster) != CLUSTER_UP);
+
+	switch (atomic_dec_return(&mb86s70_pm_use_count[cpu][cluster])) {
+	case 1:
+		/*
+		 * Overtaken by a power up. Flush caches, exit coherency,
+		 * return & fake a reset
+		 */
+		skip_wfi = true;
+		break;
+	case 0:
+		/* If all other cores in the cluster were already down
+		 * AND this cpu is going to be down for long enough
+		 * then prepare for cluster power down.
+		 */
+		if (!atomic_read(&mb86s70_pm_use_count[0][cluster]) &&
+		    !atomic_read(&mb86s70_pm_use_count[1][cluster]) &&
+		    !atomic_read(&mb86s70_pm_use_count[2][cluster]) &&
+		    !atomic_read(&mb86s70_pm_use_count[3][cluster]))
+			last_man = true;
+		break;
+	default:
+		BUG();
+	}
+
+	if (!skip_wfi)
+		gic_cpu_if_down();
+
+	if (last_man && __mcpm_outbound_enter_critical(cpu, cluster)) {
+		arch_spin_unlock(&mb86s70_pm_lock);
+		local_irq_enable();
+
+		v7_exit_coherency_flush(all);
+
+		cci_disable_port_by_cpu(mpidr);
+
+		__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);
+	} else {
+		arch_spin_unlock(&mb86s70_pm_lock);
+		local_irq_enable();
+		v7_exit_coherency_flush(louis);
+	}
+
+	__mcpm_cpu_down(cpu, cluster);
+
+	/* Now we are prepared for power-down, do it: */
+	if (!skip_wfi) {
+		mb86s70_set_wficolor(cluster, cpu, AT_WFI_DO_POWEROFF);
+		wfi();
+	}
+
+	/* Not dead at this point?  Let our caller cope. */
+}
+
+static void mb86s70_pm_power_down(void)
+{
+	mb86s70_pm_suspend(0);
+}
+
+static const struct mcpm_platform_ops mb86s70_pm_power_ops = {
+	.power_up	= mb86s70_pm_power_up,
+	.power_down	= mb86s70_pm_power_down,
+	.suspend	= mb86s70_pm_suspend,
+};
+
+extern void cci_port_control(unsigned int port, int enable);
+extern void mb86s70_pm_power_up_setup(unsigned int affinity_level);
+extern void mb86s70_reboot(u32 delay);
+
+void mb86s70_restart(char mode, const char *unused)
+{
+	pr_err("%s\n", __func__);
+	/* Reboot immediately */
+	mb86s70_reboot(50);
+}
+
+static void mb86s70_poweroff(void)
+{
+	pr_err("%s\n", __func__);
+	/* Reboot never, remain dead */
+	mb86s70_reboot(~0);
+}
+
+static int mb86s70_pm_valid(suspend_state_t state)
+{
+	return (state == PM_SUSPEND_STANDBY) || (state == PM_SUSPEND_MEM);
+}
+
+extern void mb86s70_primary_reboot(void);
+typedef void (*phys_reset_t)(unsigned long);
+static int mb86s70_die(unsigned long arg)
+{
+	/* MCPM works with HW CPU identifiers */
+	unsigned int mpidr = read_cpuid_mpidr();
+	unsigned int cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+	unsigned int cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+	phys_reset_t phys_reset;
+	mcpm_set_entry_vector(cpu, cluster, cpu_resume);
+
+	setup_mm_for_reboot();
+
+	if (use_retention_mode) {
+		__mcpm_cpu_going_down(cpu, cluster);
+		local_irq_disable();
+		arch_spin_lock(&mb86s70_pm_lock);
+		gic_cpu_if_down();
+		__mcpm_outbound_enter_critical(cpu, cluster);
+		arch_spin_unlock(&mb86s70_pm_lock);
+		local_irq_enable();
+
+		v7_exit_coherency_flush(all);
+		cci_disable_port_by_cpu(mpidr);
+		__mcpm_outbound_leave_critical(cluster, CLUSTER_DOWN);
+		__mcpm_cpu_down(cpu, cluster);
+
+		/* Move WFI-color setting just before calling wfi() as a workaround for SCB firmware bug. */
+		mb86s70_set_wficolor(cluster, cpu, AT_WFI_DO_SUSPEND);
+
+		/* Return only to mb86s70_primary_reboot() via reset */
+		while (1)
+			asm("wfi");
+	}
+
+	asm("wfi");
+	/* Boot just like a secondary */
+	phys_reset = (phys_reset_t)(unsigned long)virt_to_phys(cpu_reset);
+	phys_reset(virt_to_phys(mcpm_entry_point));
+
+	return 0;
+}
+
+void set_secondary_entry(unsigned long secondary_entry);
+
+static void force_mvbar(void)
+{
+	struct cmd_scb_version capabitity_cmd;
+	struct completion got_rsp;
+	int ret;
+
+   capabitity_cmd.payload_size = sizeof(capabitity_cmd);
+
+	init_completion(&got_rsp);
+	
+	ret = mhu_send_packet(CMD_SCB_CAPABILITY_GET_REQ,
+				   &capabitity_cmd, sizeof(capabitity_cmd), &got_rsp);
+	if (ret < 0)
+		pr_err("%s:%d failed to get SCB version\n",
+						__func__, __LINE__);
+	if (ret)
+		wait_for_completion(&got_rsp);
+
+	/* force the MVBAR to VA of secure monitor vector */
+	if (capabitity_cmd.capabilities[0] & (1 << 2)) {
+		asm("ldr r0,	=0xfe100200\n" \
+				"mcr	p15, 0, r0, c12, c0, 1\n"
+				);
+	}
+}
+
+static int mb86s70_pm_enter(suspend_state_t state)
+{
+	unsigned int mpidr, cpu, cluster;
+	struct cmd_resume_entry_point_msg resume_entry_cmd;
+	struct completion got_rsp;
+	int ret;
+	mpidr = read_cpuid_mpidr();
+	cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+
+	switch (state) {
+	case PM_SUSPEND_STANDBY:
+		pr_err("STANDBY\n");
+		asm("wfi");
+		break;
+	case PM_SUSPEND_MEM:
+		pr_err("SUSPEND\n");
+		if (use_retention_mode) {
+
+			force_mvbar();
+			set_secondary_entry(0);
+
+			/* Set the target ads for primary after 'resume' from STR */
+			resume_entry_cmd.payload_size = sizeof(resume_entry_cmd);
+			resume_entry_cmd.resume_entry_point = virt_to_phys(mb86s70_primary_reboot);
+			resume_entry_cmd.resume_flag = 1;
+			
+			init_completion(&got_rsp);
+			
+			ret = mhu_send_packet(CMD_RESUME_ENTRY_POINT_SET_REQ,
+				   &resume_entry_cmd, sizeof(resume_entry_cmd), &got_rsp);
+
+			if (ret < 0) {
+				pr_err("%s:%d failed to set resume entry point, \
+					please update scb firmware.\n",
+								__func__, __LINE__);
+				BUG();
+			}
+			if (ret)
+				wait_for_completion(&got_rsp);
+
+		}
+
+		cpu_pm_enter();
+		cpu_suspend(0, mb86s70_die);
+		cpu_pm_exit();
+		break;
+	}
+	return 0;
+}
+
+static const struct platform_suspend_ops mb86s70_pm_ops = {
+	.valid		= mb86s70_pm_valid,
+	.enter		= mb86s70_pm_enter,
+};
+
+void set_secondary_entry(unsigned long secondary_entry);
+
+static int __init mb86s70_mcpm_init(void)
+{
+	struct completion got_rsp;
+	unsigned int mpidr, cpu, cluster;
+	struct device_node *np;
+	int ret;
+
+	np = of_find_compatible_node(NULL, NULL, "arm,cci-400");
+	if (!np)
+		return 0;
+
+	if (of_find_property(np, "retention-suspend", NULL))
+		use_retention_mode = true;
+
+	arm_pm_restart = mb86s70_restart;
+	pm_power_off = mb86s70_poweroff;
+
+	mpidr = read_cpuid_mpidr();
+	cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+	cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+	atomic_set(&mb86s70_pm_use_count[cpu][cluster], 1);
+	/* reset the wfi 'color' for primary cpu */
+	mb86s70_set_wficolor(cluster, cpu, AT_WFI_DO_NOTHING);
+	pr_info("Booting on cpu_%u cluster_%u\n", cpu, cluster);
+	boot_cpu_color_offset = cluster * 2 + cpu;
+
+	ret = mcpm_platform_register(&mb86s70_pm_power_ops);
+	if (!ret)
+		ret = mcpm_sync_init(mb86s70_pm_power_up_setup);
+	if (!ret) {
+		phys_cpu_reset = (u32) virt_to_phys(cpu_reset);
+		phys_mcpm_entry_point = (u32) virt_to_phys(mcpm_entry_point);
+		
+		/* use smc to set secondary entry address */
+		set_secondary_entry(phys_mcpm_entry_point);
+		flush_cache_all();
+		outer_flush_range(0, 16);
+		smp_wmb();
+                                             
+		version_cmd.payload_size = sizeof(version_cmd);
+		version_cmd.version = 0;
+		version_cmd.config_version = 0;
+
+		init_completion(&got_rsp);
+		if (skip_mhu)
+			ret = 0;
+		else
+			ret = mhu_send_packet(CMD_SCB_CAPABILITY_GET_REQ,
+				   &version_cmd, sizeof(version_cmd), &got_rsp);
+		if (ret < 0)
+			pr_err("%s:%d failed to get SCB version\n",
+							__func__, __LINE__);
+		if (ret)
+			wait_for_completion(&got_rsp);
+
+		pr_info("MB86S7x MCPM initialized: SCB version 0x%x:0x%x\n",
+			       version_cmd.version, version_cmd.config_version);
+	}
+
+	return ret;
+}
+early_initcall(mb86s70_mcpm_init);
+
+extern int exiu_irq_set_type(unsigned long irq_num, unsigned long type);
+
+#if 0
+static irqreturn_t s70_wakeup_handler(int irq, void *arg)
+{
+	return IRQ_HANDLED;
+}
+#endif
+
+static int mb86s70_evbpm_probe(struct platform_device *pdev)
+{
+	struct pinctrl_state *pins_default;
+	struct pinctrl *pinctrl;
+	struct resource *res;
+	int ret, irq;
+
+	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	irq = res->start;
+
+#if 0
+	/* set button to wakeup */
+	ret = exiu_irq_set_type(irq, IRQ_TYPE_EDGE_RISING);
+	if (ret) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+
+	ret = request_irq(irq, s70_wakeup_handler,
+				IRQF_ONESHOT, "WKUPirq", NULL);
+	if (ret) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+
+	ret = irq_set_irq_wake(irq, 1);
+	if (ret) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+#endif
+	pinctrl = devm_pinctrl_get(&pdev->dev);
+	if (IS_ERR(pinctrl)) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return PTR_ERR(pinctrl);
+	}
+
+	pins_default = pinctrl_lookup_state(pinctrl, PINCTRL_STATE_DEFAULT);
+	if (IS_ERR(pins_default)) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return PTR_ERR(pins_default);
+	}
+
+	ret = pinctrl_select_state(pinctrl, pins_default);
+	if (ret) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+
+	return 0;
+}
+
+static const struct of_device_id mb86s70_evbpm_dt_ids[] = {
+	{.compatible = "mb86s70evb,pm_ops"},
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, mb86s70_evbpm_dt_ids);
+
+static struct platform_driver mb86s70_evbpm_driver = {
+	.probe     = mb86s70_evbpm_probe,
+	.driver    = {
+		.name  = "mb86s70evb_pm_ops",
+		.owner = THIS_MODULE,
+		.of_match_table = mb86s70_evbpm_dt_ids,
+	},
+};
+
+static int __init mb86s70evb_pm_init(void)
+{
+	if (of_machine_is_compatible("fujitsu,mb8aa0350eb")) {
+		static const struct platform_device_info devinfo = {
+			.name = "s70-cpufreq",
+		};
+		platform_device_register_full(&devinfo);
+		pr_err("Populated s70-cpufreq\n");
+	}
+
+	suspend_set_ops(&mb86s70_pm_ops);
+
+	return platform_driver_register(&mb86s70_evbpm_driver);
+}
+late_initcall(mb86s70evb_pm_init);
diff --git a/arch/arm/mach-mb86s70/nonsec.S b/arch/arm/mach-mb86s70/nonsec.S
new file mode 100644
index 0000000..22802c2
--- /dev/null
+++ b/arch/arm/mach-mb86s70/nonsec.S
@@ -0,0 +1,16 @@
+#include <linux/linkage.h>
+
+.arch_extension sec
+
+ENTRY(set_secondary_entry)
+	stmfd   sp!, {r1-r11, lr}
+	mov r1, r0
+	ldr r0, =1
+	mrc p15, 0, r3, c1, c0, 0
+	mov r4, r3
+	and r3, #0xbfffffff
+	mcr p15, 0, r3, c1, c0, 0
+	smc #0
+	mcr p15, 0, r4, c1, c0, 0
+	ldmfd   sp!, {r1-r11, pc}
+ENDPROC(set_secondary_entry)
diff --git a/arch/arm/mach-mb86s70/pm_domains.c b/arch/arm/mach-mb86s70/pm_domains.c
new file mode 100644
index 0000000..761db24
--- /dev/null
+++ b/arch/arm/mach-mb86s70/pm_domains.c
@@ -0,0 +1,234 @@
+/*
+ * mb86s7x generic powerdomain support
+ * Copyright (C) 2014 Linaro, Ltd  Andy Green <andy.green@linaro.org>
+ *
+ * based on -->
+ *
+ * Exynos Generic power domain support.
+ *
+ * Copyright (c) 2012 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ *
+ * Implementation of Exynos specific power domain control which is used in
+ * conjunction with runtime-pm. Support for both device-tree and non-device-tree
+ * based power domain support is included.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+*/
+/* #define DEBUG */
+#include <linux/io.h>
+#include <linux/err.h>
+#include <linux/slab.h>
+#include <linux/pm_domain.h>
+#include <linux/delay.h>
+#include <linux/of_address.h>
+#include <linux/of_platform.h>
+#include <linux/sched.h>
+#include <linux/scb_mhu_api.h>
+
+/*
+ * mb86s7x specific wrapper around the generic power domain
+ */
+struct mb86s7x_pm_domain {
+	struct generic_pm_domain pd;
+	char const *name;
+	int powerdomain_index;
+};
+
+static int mb86s7x_pd_power(struct generic_pm_domain *domain, bool power_on)
+{
+	struct mb86s7x_pm_domain *pd;
+	struct cmd_powerdomain cmd;
+	struct completion got_rsp;
+	int ret;
+
+	pd = container_of(domain, struct mb86s7x_pm_domain, pd);
+
+	pr_info("%s: domain %s <- %d\n", __func__, pd->name, power_on);
+
+	if (skip_mhu)
+		return 0;
+
+	cmd.payload_size = sizeof(cmd);
+	cmd.powerdomain_index = pd->powerdomain_index;
+	cmd.state = power_on;
+
+	if (pd->powerdomain_index >= 0) {
+		pr_info("First powerdomain index1 set :%d .\n",
+			pd->powerdomain_index);
+		init_completion(&got_rsp);
+
+		ret = mhu_send_packet(CMD_POWERDOMAIN_SET_REQ,
+				&cmd, sizeof(cmd), &got_rsp);
+		if (ret < 0) {
+			pr_err("%s:%d failed to set powerdomain\n"
+				, __func__, __LINE__);
+			return ret;
+		}
+		if (ret)
+			wait_for_completion(&got_rsp);
+	}
+
+	return 0;
+}
+
+static int mb86s7x_pd_power_on(struct generic_pm_domain *domain)
+{
+	return mb86s7x_pd_power(domain, true);
+}
+
+static int mb86s7x_pd_power_off(struct generic_pm_domain *domain)
+{
+	return mb86s7x_pd_power(domain, false);
+}
+
+static struct genpd_onecell_data mb86s7x_pd;
+
+static int mb86s7x_add_sub_domain(struct device_node *master,
+			struct device_node *sub)
+{
+	struct mb86s7x_pm_domain *master_pd, *sub_pd;
+	int ret;
+
+	sub_pd = platform_get_drvdata(of_find_device_by_node(sub));
+	master_pd = platform_get_drvdata(of_find_device_by_node(master));
+
+	pr_debug("sub-domain:%d, master-domain:%d.\n"
+		, sub_pd->powerdomain_index, master_pd->powerdomain_index);
+
+	while (1) {
+		ret = pm_genpd_add_subdomain(&master_pd->pd, &sub_pd->pd);
+		if (ret != -EAGAIN)
+			break;
+		cond_resched();
+	}
+
+	return 0;
+}
+
+static int mb86s7x_pm_notifier_call(struct notifier_block *nb,
+				    unsigned long event, void *data)
+{
+	struct device *dev = data;
+
+	switch (event) {
+	case BUS_NOTIFY_BIND_DRIVER:
+		pm_genpd_dev_need_restore(dev, true);
+		break;
+	}
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block platform_nb = {
+	.notifier_call = mb86s7x_pm_notifier_call,
+};
+
+static __init int mb86s7x_pm_init_power_domain(void)
+{
+	struct device_node *np, *child = NULL, *master;
+	struct cmd_powerdomain cmd;
+	struct completion got_rsp;
+	int ret, pd_num;
+	struct platform_device *pdev;
+
+	pr_debug("mb86s7x power domain initialization start.\n");
+
+	np = of_find_compatible_node(NULL, NULL, "fujitsu,mb86s7x-pd");
+	if (np == NULL) {
+		pr_info("No mb86s7x power domains\n");
+		return -ENOMEM;
+	}
+
+	/* populate power-domain cell device */
+	pdev = of_find_device_by_node(np);
+	ret = of_platform_populate(np, NULL, NULL, &pdev->dev);
+	if (ret) {
+		pr_err("%s: missing power-domain-cell device\n", __func__);
+		return -ENOMEM;
+	}
+
+	/* count for power domain number */
+	pd_num = 0;
+	for  (child = of_get_next_child(np, NULL); child;
+			child = of_get_next_child(np, child))
+		pd_num++;
+
+	pr_debug("mb86s7x power domain count is %d.\n", pd_num);
+	mb86s7x_pd.domains = kmalloc(pd_num*sizeof(struct generic_pm_domain *),
+			GFP_KERNEL);
+	mb86s7x_pd.domain_num = pd_num;
+
+	pd_num = 0;
+	for  (child = of_get_next_child(np, NULL); child;
+					child = of_get_next_child(np, child)) {
+		struct mb86s7x_pm_domain *pd;
+		struct platform_device *child_pdev =
+				of_find_device_by_node(child);
+
+		pd = kzalloc(sizeof(*pd), GFP_KERNEL);
+		if (!pd) {
+			pr_err("%s: failed to allocate memory for domain-cell.\n"
+				, __func__);
+			return -ENOMEM;
+		}
+		pd->pd.name = kstrdup(np->name, GFP_KERNEL);
+		pd->pd.power_off = mb86s7x_pd_power_off;
+		pd->pd.power_on = mb86s7x_pd_power_on;
+		pd->pd.of_node = np; /*put parent's node*/
+		pd->name = pd->pd.name;
+		if (of_property_read_u32(child, "index"
+					, &pd->powerdomain_index))
+			pd->powerdomain_index = -1;
+		mb86s7x_pd.domains[pd_num] = &pd->pd;
+
+		/* sned power domain control cmd via mhu */
+		cmd.payload_size = sizeof(cmd);
+		cmd.powerdomain_index = pd->powerdomain_index;
+		cmd.state = 0;
+		if (!skip_mhu && pd->powerdomain_index >= 0) {
+			init_completion(&got_rsp);
+
+			ret = mhu_send_packet(CMD_POWERDOMAIN_GET_REQ,
+					&cmd, sizeof(cmd), &got_rsp);
+			if (ret < 0)
+				pr_err("%s:%d failed to get SCB version\n"
+							, __func__, __LINE__);
+			if (ret)
+				wait_for_completion(&got_rsp);
+		}
+
+		pm_genpd_init(&pd->pd, NULL, !cmd.state);
+		platform_set_drvdata(child_pdev, pd);
+
+		pr_debug("power domain %s starting.\n", child->name);
+		pd_num++;
+	}
+
+	/* register sub-domain if necessary */
+	for  (child = of_get_next_child(np, NULL); child;
+				child = of_get_next_child(np, child)) {
+		master = of_parse_phandle(child, "master-domain-cell", 0);
+		if (master == NULL)
+			continue;
+
+		mb86s7x_add_sub_domain(master, child);
+	}
+
+	if (of_genpd_add_provider(np, of_genpd_xlate_onecell, &mb86s7x_pd)) {
+		pr_err("mb86s7x power domain initialization failed.\n");
+		return -1;
+	}
+	bus_register_notifier(&platform_bus_type, &platform_nb);
+
+	pr_debug("mb86s7x power domain initialization end.\n");
+	return 0;
+}
+arch_initcall(mb86s7x_pm_init_power_domain);
+
+int __init mb86s7x_pm_late_initcall(void)
+{
+	pm_genpd_poweroff_unused();
+	return 0;
+}
diff --git a/arch/arm/mach-mb86s70/scb_mhu.c b/arch/arm/mach-mb86s70/scb_mhu.c
new file mode 100644
index 0000000..a2b3022
--- /dev/null
+++ b/arch/arm/mach-mb86s70/scb_mhu.c
@@ -0,0 +1,451 @@
+
+#include <linux/io.h>
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/mailbox.h>
+#include <linux/spinlock.h>
+#include <linux/of_irq.h>
+#include <linux/of_device.h>
+#include <linux/interrupt.h>
+#include <linux/scb_mhu_api.h>
+#include <linux/mailbox_client.h>
+
+#include <linux/platform_data/mb86s70-iomap.h>
+
+#define INTR_STAT_OFS	0x0
+#define INTR_SET_OFS	0x8
+#define INTR_CLR_OFS	0x10
+
+#define LPNONSEC	0
+#define HPNONSEC	1
+#define SECURE		2
+
+int __initdata shm_offset = 0x800;
+
+static void __iomem *cmd_from_scb = MB86S70_SHM_FROM_SCB_VIRT;
+static void __iomem *rsp_from_scb = MB86S70_SHM_FROM_SCB_VIRT + 0x100;
+static void __iomem *cmd_to_scb = MB86S70_SHM_FROM_SCB_VIRT + 0x200;
+static void __iomem *rsp_to_scb = MB86S70_SHM_FROM_SCB_VIRT + 0x300;
+static mhu_handler_t handler[MHU_NUM_CMDS];
+static DEFINE_SPINLOCK(fsm_lock);
+static LIST_HEAD(pending_xfers);
+static LIST_HEAD(free_xfers);
+static struct ipc_client mhu_cl;
+static struct completion fsm_rsp;
+static void *mhu_chan;
+static int do_xfer(void);
+
+static enum {
+	MHU_PARK = 0,
+	MHU_WRR, /* Waiting to get Remote's Reply */
+	MHU_WRL, /* Waiting to send Reply */
+	MHU_WRRL, /* WAIT_Ra && WAIT_Rb */
+	MHU_INVLD,
+} fsm_state;
+
+enum fsm_event {
+	EV_LC = 0, /* Local sent a command */
+	EV_RC, /* Remote sent a command */
+	EV_RR, /* Remote sent a reply */
+	EV_LR, /* Local sent a reply */
+};
+
+static int mhu_fsm[4][4] = {
+	[MHU_PARK] = {
+		[EV_LC] = MHU_WRR,
+		[EV_RC] = MHU_WRL,
+		[EV_RR] = MHU_INVLD,
+		[EV_LR] = MHU_INVLD,
+	},
+	[MHU_WRR] = {
+		[EV_LC] = MHU_INVLD,
+		[EV_RC] = MHU_WRRL,
+		[EV_RR] = MHU_PARK,
+		[EV_LR] = MHU_INVLD,
+	},
+	[MHU_WRL] = {
+		[EV_LC] = MHU_WRRL,
+		[EV_RC] = MHU_INVLD,
+		[EV_RR] = MHU_INVLD,
+		[EV_LR] = MHU_PARK,
+	},
+	[MHU_WRRL] = {
+		[EV_LC] = MHU_INVLD,
+		[EV_RC] = MHU_INVLD,
+		[EV_RR] = MHU_WRL,
+		[EV_LR] = MHU_WRR,
+	},
+};
+
+struct mhu_xfer {
+	int code;
+	int len;
+	void *buf;
+	struct completion *c;
+	struct list_head node;
+};
+static struct mhu_xfer *ax; /* stages of xfer */
+
+int skip_mhu = 0;
+EXPORT_SYMBOL_GPL(skip_mhu);
+
+static int __init skip_mhu_param(char *str)
+{
+	skip_mhu = 1;
+	return 1;
+}
+__setup("skipmhu", skip_mhu_param);
+
+static int __init set_shm_offset(char *str)
+{
+	get_option(&str, &shm_offset);
+
+	if (shm_offset < 0)
+		shm_offset = 0;
+
+	cmd_from_scb += shm_offset;
+	cmd_to_scb += shm_offset;
+	rsp_from_scb += shm_offset;
+	rsp_to_scb += shm_offset;
+
+	return 1;
+}
+__setup("shm_offset=", set_shm_offset);
+
+static int mhu_alloc_xfers(int n, struct list_head *list)
+{
+	struct mhu_xfer *x = kzalloc(n * sizeof(struct mhu_xfer), GFP_ATOMIC);
+	int i;
+
+	if (!x)
+		return -ENOMEM;
+	for (i = 0; i < n; i++)
+		list_add(&x[i].node, &free_xfers);
+	return 0;
+}
+
+static void got_data(u32 code)
+{
+	unsigned long flags;
+	struct completion *c = NULL;
+	mhu_handler_t hndlr = NULL;
+	int ev;
+
+	if (code & RESP_BIT)
+		ev = EV_RR;
+	else
+		ev = EV_RC;
+
+	spin_lock_irqsave(&fsm_lock, flags);
+	if (mhu_fsm[fsm_state][ev] == MHU_INVLD) {
+		spin_unlock_irqrestore(&fsm_lock, flags);
+		pr_err("State-%d EV-%d FSM Broken!\n", fsm_state, ev);
+		return;
+	}
+	fsm_state = mhu_fsm[fsm_state][ev];
+
+	if (code & RESP_BIT) {
+		c = ax->c;
+		memcpy_fromio(ax->buf, rsp_from_scb, ax->len);
+		list_move(&ax->node, &free_xfers);
+		ax = NULL;
+		if (c)
+			complete(c);
+	} else {
+		/* Find and dispatch relevant registered handler */
+		if (code < MHU_NUM_CMDS)
+			hndlr = handler[code];
+		if (hndlr)
+			hndlr(code, cmd_from_scb);
+		else
+			pr_err("No handler for CMD_%u\n", code);
+	}
+	spin_unlock_irqrestore(&fsm_lock, flags);
+}
+
+static void mhu_recv(void *data)
+{
+	if ((u32)data & RESP_BIT) {
+		/* Now that we got a reply to last TX, that
+		 * must mean the last TX was successful */
+		ipc_client_txdone(mhu_chan, XFER_OK);
+
+		ax->code = (u32)data; /* Save response */
+		complete(&fsm_rsp);
+	} else
+		got_data((u32)data);
+}
+
+static int do_xfer(void)
+{
+	struct mhu_xfer *x;
+	unsigned long flags;
+	int ev;
+	int code;
+
+	if (skip_mhu) {
+		WARN_ON(1);
+		return 0;
+	}
+
+	spin_lock_irqsave(&fsm_lock, flags);
+	if (list_empty(&pending_xfers)) {
+		void *_ch = NULL;
+		int cmd;
+
+		for (cmd = 0; cmd < MHU_NUM_CMDS && !handler[cmd]; cmd++)
+			;
+		/* Don't free channel if any user is listening */
+		if (cmd != MHU_NUM_CMDS) {
+			spin_unlock_irqrestore(&fsm_lock, flags);
+			return 0;
+		}
+
+		if (fsm_state == MHU_PARK) {
+			_ch = mhu_chan;
+			mhu_chan = NULL;
+		}
+
+		spin_unlock_irqrestore(&fsm_lock, flags);
+
+		if (_ch)
+			ipc_free_channel(_ch);
+
+		return 0;
+	}
+
+	x = list_first_entry(&pending_xfers, struct mhu_xfer, node);
+	code = x->code;
+
+	ev = code & RESP_BIT ? EV_LR : EV_LC;
+	if (mhu_fsm[fsm_state][ev] == MHU_INVLD) {
+		spin_unlock_irqrestore(&fsm_lock, flags);
+		mbox_dbg("%s:%d\n", __func__, __LINE__);
+		return 1;
+	}
+	list_del_init(&x->node);
+
+	/* Layout the SHM */
+	if (code & RESP_BIT)
+		memcpy_toio(rsp_to_scb, x->buf, x->len);
+	else
+		memcpy_toio(cmd_to_scb, x->buf, x->len);
+
+	if (ev == EV_LC)
+		ax = x;
+	else
+		list_move(&x->node, &free_xfers);
+	fsm_state = mhu_fsm[fsm_state][ev];
+
+	spin_unlock_irqrestore(&fsm_lock, flags);
+
+	/* Prefer mailbox API */
+	if (!mhu_chan) {
+		mhu_cl.tx_block = true;
+		mhu_cl.knows_txdone = true;
+		mhu_cl.rxcb = mhu_recv;
+		mhu_cl.chan_name = "f_mhu:HP_NonSec";
+		mhu_chan = ipc_request_channel(&mhu_cl);
+	}
+
+	if (mhu_chan) {
+		int ret;
+
+		init_completion(&fsm_rsp);
+		/* Send via generic api */
+		ret = ipc_send_message(mhu_chan, (void *)code);
+		if (!ret) {
+			pr_err("%s:%d CMD_%d Send Failed\n",
+					__func__, __LINE__, code);
+			BUG();
+		}
+		if (!(code & RESP_BIT)) {
+			ret = wait_for_completion_timeout(&fsm_rsp,
+						msecs_to_jiffies(1000));
+			if (!ret) {
+				pr_err("%s:%d CMD_%d Got No Reply\n",
+					__func__, __LINE__, code);
+				BUG();
+			}
+			got_data(ax->code);
+		}
+	} else {
+		void __iomem *tx_reg = MB86S70_MHU_VIRT + 0x120; /* HP-NonSec */
+		void __iomem *rx_reg = MB86S70_MHU_VIRT + 0x20; /* HP-NonSec */
+		u32 val, count;
+
+		/* Send via early-boot api */
+		val = __raw_readl(tx_reg + INTR_STAT_OFS);
+		if (val) {
+			pr_err("Last CMD not yet read by SCB\n");
+			__raw_writel(val, tx_reg + INTR_CLR_OFS);
+		}
+
+		__raw_writel(x->code, tx_reg + INTR_SET_OFS);
+
+		/* Wait until this message is read */
+		count = 0x1000000;
+		do {
+			cpu_relax();
+			val = __raw_readl(tx_reg + INTR_STAT_OFS);
+		} while (--count && val);
+		if (val)
+			pr_err("%s:%d SCB not listening!\n",
+				__func__, __LINE__);
+
+		if (!ax) {
+			/* A quick poll for pending remote cmd */
+			val = __raw_readl(rx_reg + INTR_STAT_OFS);
+			if (val) {
+				got_data(val);
+				__raw_writel(val, rx_reg + INTR_CLR_OFS);
+			}
+		} else {
+			do {
+retry:
+				/* Wait until we get reply */
+				count = 0x1000000;
+				do {
+					cpu_relax();
+					val = __raw_readl(rx_reg + INTR_STAT_OFS);
+				} while (--count && !val);
+
+				if (!val) {
+					pr_err("%s:%d SCB didn't reply\n",
+								__func__, __LINE__);
+					goto retry;
+				} else {
+					got_data(val);
+					__raw_writel(val, rx_reg + INTR_CLR_OFS);
+				}
+			} while(!(val & RESP_BIT));
+		}
+		if (list_empty(&pending_xfers))
+			return 0;
+	}
+
+	return do_xfer();
+}
+
+int mhu_hndlr_set(u32 cmd, mhu_handler_t hndlr)
+{
+	unsigned long flags;
+	int ret = -EINVAL;
+
+	spin_lock_irqsave(&fsm_lock, flags);
+	if (cmd < MHU_NUM_CMDS && !handler[cmd]) {
+		ret = 0;
+		handler[cmd] = hndlr;
+	}
+	spin_unlock_irqrestore(&fsm_lock, flags);
+
+	if (!mhu_chan) {
+		void *_ch;
+		mhu_cl.tx_block = true;
+		mhu_cl.knows_txdone = true;
+		mhu_cl.rxcb = mhu_recv;
+		mhu_cl.chan_name = "f_mhu:HP_NonSec";
+		_ch = ipc_request_channel(&mhu_cl);
+		if (_ch)
+			mhu_chan = _ch;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(mhu_hndlr_set);
+
+void mhu_hndlr_clr(u32 cmd, mhu_handler_t hndlr)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&fsm_lock, flags);
+
+	if (cmd < MHU_NUM_CMDS && handler[cmd] == hndlr)
+		handler[cmd] = NULL;
+
+	if (list_empty(&pending_xfers)) {
+		void *_ch = NULL;
+
+		for (cmd = 0; cmd < MHU_NUM_CMDS && !handler[cmd]; cmd++)
+			;
+		/* Don't free channel if any user is listening */
+		if (cmd != MHU_NUM_CMDS) {
+			spin_unlock_irqrestore(&fsm_lock, flags);
+			return;
+		}
+
+		if (fsm_state == MHU_PARK) {
+			_ch = mhu_chan;
+			mhu_chan = NULL;
+		}
+
+		spin_unlock_irqrestore(&fsm_lock, flags);
+
+		if (_ch)
+			ipc_free_channel(_ch);
+
+		return;
+	}
+	spin_unlock_irqrestore(&fsm_lock, flags);
+}
+EXPORT_SYMBOL_GPL(mhu_hndlr_clr);
+
+int mhu_send_packet(int code, void *buf, int len, struct completion *c)
+{
+	struct mhu_xfer *x;
+	unsigned long flags;
+
+	if (code & ~0xff) {
+		WARN_ON(1);
+		return -EINVAL;
+	}
+
+	if ((code & RESP_BIT) &&
+		fsm_state != MHU_WRRL && fsm_state != MHU_WRL) {
+		WARN_ON(1);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&fsm_lock, flags);
+
+	if (list_empty(&free_xfers) && mhu_alloc_xfers(5, &free_xfers)) {
+		spin_unlock_irqrestore(&fsm_lock, flags);
+		mbox_dbg("%s:%d OOM\n", __func__, __LINE__);
+		return -EAGAIN;
+	}
+
+	x = list_first_entry(&free_xfers, struct mhu_xfer, node);
+	x->code = code;
+	x->buf = buf;
+	x->len = len;
+	x->c = c;
+
+	if (code & RESP_BIT)
+		list_move(&x->node, &pending_xfers);
+	else
+		list_move_tail(&x->node, &pending_xfers);
+
+	spin_unlock_irqrestore(&fsm_lock, flags);
+
+	return do_xfer();
+}
+EXPORT_SYMBOL_GPL(mhu_send_packet);
+
+void mb86s70_reboot(u32 delay)
+{
+	void __iomem *tx_reg = MB86S70_MHU_VIRT + 0x120; /* HP-NonSec */
+	struct cmd_hard_reset cmd;
+	u32 val;
+
+	cmd.payload_size = sizeof(cmd);
+	cmd.delay = delay;
+
+	val = __raw_readl(tx_reg + INTR_STAT_OFS);
+	if (val) /* Flush anything pending */
+		__raw_writel(val, tx_reg + INTR_CLR_OFS);
+
+	memcpy_toio(cmd_to_scb, &cmd, sizeof(cmd));
+	__raw_writel(CMD_HARD_RESET_REQ, tx_reg + INTR_SET_OFS);
+}
+EXPORT_SYMBOL_GPL(mb86s70_reboot);
diff --git a/arch/arm/mach-mb86s70/sleep.S b/arch/arm/mach-mb86s70/sleep.S
new file mode 100644
index 0000000..d7e1185
--- /dev/null
+++ b/arch/arm/mach-mb86s70/sleep.S
@@ -0,0 +1,83 @@
+/*
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/linkage.h>
+
+#include <asm/memory.h>
+#include <asm/assembler.h>
+#include <asm/asm-offsets.h>
+#include <asm/cache.h>
+#include <linux/platform_data/mb86s70-iomap.h>
+
+/* From scb_mhu_api.h */
+#define AT_WFI_DO_NOTHING	0x0
+#define AT_WFI_DO_SUSPEND	0x1
+#define AT_WFI_DO_POWEROFF	0x2
+#define AT_WFI_COLOR_MASK	0x3
+#define WFI_COLOR_REG_OFFSET	0x3f00
+
+#define MB86S70_WFICOLOR_PHYS (MB86S70_ISRAM_PHYS + WFI_COLOR_REG_OFFSET)
+#define PHYS_TRMP_ADDR	(MB86S70_TRAMPOLINE_PHYS + SEC_RSTADDR_OFF)
+#define OFFSET_FIX	(PAGE_OFFSET - 0x80000000 /* PHYS_OFFSET */)
+.arch_extension sec
+
+ENTRY(mb86s70_primary_reboot)
+	ldr	r2, =OFFSET_FIX
+	/* Restore reset-address for the secondaries */
+	ldr	r0, =phys_mcpm_entry_point
+	sub	r0, r0, r2
+	ldr	r0, [r0]
+	
+	mov r1, r0
+	ldr r0, =1
+	mrc p15, 0, r3, c1, c0, 0
+	mov r4, r3
+	and r3, #0xbfffffff
+	mcr p15, 0, r3, c1, c0, 0
+	smc #0
+	mcr p15, 0, r4, c1, c0, 0
+
+	/* Reset wficolor for the secondaries */
+	ldr	r0, =boot_cpu_color_offset
+	sub	r0, r0, r2
+	ldr	r1, [r0]
+	ldr	r0, =MB86S70_WFICOLOR_PHYS
+	add	r0, r1
+
+	ldrb	r1, [r0]
+	bic	r1, r1, #AT_WFI_COLOR_MASK
+	orr	r1, r1, #AT_WFI_DO_NOTHING
+	strb	r1, [r0]
+
+	/* Do cpu_reset(mcpm_entry_point) */
+	ldr	r0, =phys_mcpm_entry_point
+	sub	r0, r0, r2
+	ldr	r0, [r0]
+	ldr	r1, =phys_cpu_reset
+	sub	r1, r1, r2
+	ldr	r1, [r1]
+	mov	pc, r1
+
+ENDPROC(mb86s70_primary_reboot)
+
+        .align  4
+	.globl phys_mcpm_entry_point
+phys_mcpm_entry_point:
+	.space	4
+	.globl phys_cpu_reset
+phys_cpu_reset:
+	.space	4
+	.globl boot_cpu_color_offset
+boot_cpu_color_offset:
+	.space	4
diff --git a/arch/arm/mach-mb8ac0300/Kconfig b/arch/arm/mach-mb8ac0300/Kconfig
new file mode 100644
index 0000000..fe6cd29
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/Kconfig
@@ -0,0 +1,22 @@
+config ARCH_MB8AC0300
+	bool "Fujitsu Semiconductor MB8AC0300" if ARCH_MULTI_V7
+	select ARM_GIC
+	select ARM_GLOBAL_TIMER
+	select ARM_TIMER_SP804
+	select CLKDEV_LOOKUP
+	select CLKSRC_OF
+	select COMMON_CLK
+	select GENERIC_CLOCKEVENTS
+	select GENERIC_TIME
+	select HAVE_ARM_SCU if SMP
+	select HAVE_ARM_TWD if LOCAL_TIMERS
+	select HAVE_SCHED_CLOCK
+	select HAVE_SMP
+	select MB8AC0300_EXIU
+	select MIGHT_HAVE_CACHE_L2X0
+	select SPARSE_IRQ
+	select HAVE_ARM_SCU
+	select HAVE_ARM_TWD
+	select ARCH_ENABLE_MEMORY_HOTPLUG
+	help
+	  Support for Fujitsu Semiconductor's MB8AC0300 based systems.
diff --git a/arch/arm/mach-mb8ac0300/Makefile b/arch/arm/mach-mb8ac0300/Makefile
new file mode 100644
index 0000000..5868448
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/Makefile
@@ -0,0 +1,6 @@
+obj-y += common.o io.o
+
+obj-$(CONFIG_SMP) += platsmp.o headsmp.o
+obj-$(CONFIG_PM) += pm.o hotplug.o
+obj-$(CONFIG_SUSPEND) += sleep.o
+obj-$(CONFIG_HIBERNATION) += hibernate.o hibernate_asm.o
diff --git a/arch/arm/mach-mb8ac0300/common.c b/arch/arm/mach-mb8ac0300/common.c
new file mode 100644
index 0000000..5754b91
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/common.c
@@ -0,0 +1,103 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/common.c
+ *
+ * Copyright (C) 2013 Linaro, LTD
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/platform_device.h>
+#include <linux/suspend.h>
+#include <linux/clocksource.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/irqchip.h>
+#include <linux/clkdev.h>
+#include <linux/clk-provider.h>
+
+#include <asm/io.h>
+#include <asm/irq.h>
+#include <asm/smp_scu.h>
+#include <asm/smp_twd.h>
+#include <asm/mach/map.h>
+#include <asm/mach/arch.h>
+#include <asm/mach/time.h>
+#include <asm/cacheflush.h>
+#include <asm/mach-types.h>
+#include <asm/hardware/timer-sp.h>
+#include <asm/hardware/arm_timer.h>
+#include <asm/hardware/cache-l2x0.h>
+
+#include <linux/platform_data/mb8ac0300-iomap.h>
+
+#include "devices.h"
+
+#define MB8AC0300_SYSOC_PHYS		0xfff70000
+#define MB8AC0300_SYSOC_SIZE		SZ_4K
+
+#define SYSOC_REG_AUXCTL		0x044
+#define SYSOC_AUXCTL_HSSPI_HSEL		(0x1 << 9)
+
+void __init mb8ac0300_init_timer(void)
+{
+	of_clk_init(NULL);
+	clocksource_of_init();
+}
+
+void __init mb8ac0300_init(void)
+{
+	unsigned int val;
+	void __iomem *sysoc =
+			ioremap(MB8AC0300_SYSOC_PHYS, MB8AC0300_SYSOC_SIZE);
+
+	/*
+	 * l2x0: 16 ways, CACHE_ID 0x000000c0, AUX_CTRL 0x72450000
+	 * Cache size: 524288 B
+	 */
+	l2x0_of_init((1 << L2X0_AUX_CTRL_EARLY_BRESP_SHIFT) |
+		    (1 << L2X0_AUX_CTRL_DATA_PREFETCH_SHIFT) |
+		    (1 << L2X0_AUX_CTRL_INSTR_PREFETCH_SHIFT) |
+		    (1 << L2X0_AUX_CTRL_SHARE_OVERRIDE_SHIFT), 0xFFFFFFFF);
+	of_platform_populate(NULL, of_default_bus_match_table, NULL, NULL);
+
+	/* Init sysoc for HS_SPI
+	 * After power HSSPIn_CSCFG.BOOTEN bit is 1,AUXCTL.HSEL bit is 0.
+	 * At this setting only command sequencer mode can be used.
+	 * We want to use both of command sequencer mode and direct mode,
+	 * so we set:
+	 *   HSSPIn_CSCFG.BOOTEN: 0
+	 *   AUXCTL.HSEL        : 1
+	 * BOOTEN bit will set in HSSPI driver, so we only set AUXCTL.HSEL here.
+	 */
+	val = readl(sysoc + SYSOC_REG_AUXCTL);
+	val |= SYSOC_AUXCTL_HSSPI_HSEL;
+	writel(val, sysoc + SYSOC_REG_AUXCTL);
+
+	iounmap(sysoc);
+
+#if defined(CONFIG_PM)
+	mb8ac0300_pm_init();
+#endif
+}
+
+static const char *mb8ac0300_dt_compat[] __initdata = {
+	"fujitsu,mb8ac0300",
+	"fujitsu,mb8ac0300eb",
+	NULL
+};
+
+extern struct smp_operations mb8ac0300_smp_ops;
+
+DT_MACHINE_START(MB8AC0300_DT, "Fujitsu Semiconductor MB8AC0300-EVB")
+	.smp		= smp_ops(mb8ac0300_smp_ops),
+	.map_io         = mb8ac0300_map_io,
+	.init_time	= mb8ac0300_init_timer,
+	.init_machine	= mb8ac0300_init,
+	.dt_compat	= mb8ac0300_dt_compat,
+MACHINE_END
diff --git a/arch/arm/mach-mb8ac0300/devices.h b/arch/arm/mach-mb8ac0300/devices.h
new file mode 100644
index 0000000..1c7d9d7
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/devices.h
@@ -0,0 +1,32 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/devices.h
+ *
+ * Copyright (C) 2013 Linaro, LTD
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+extern void __init mb8ac0300_map_io(void);
+extern void __init mb8ac0300_init_irq(void);
+extern void __init mb8ac0300_init_timer(void);
+extern void __init mb8ac0300_init(void);
+extern struct smp_operations mb8ac0300_smp_ops;
+extern void mb8ac0300_restart(char mode, const char *cmd);
+extern void crg11_restart(void);
+extern void mb8ac0300_secondary_startup(void);
+extern const int mb8ac0300_cpu_sleep_size;
+extern void mb8ac0300_cpu_save(long);
+extern void mb8ac0300_cpu_sleep(u32 *);
+extern int __init mb8ac0300_pm_init(void);
+extern const void __nosave_begin, __nosave_end;
+
diff --git a/arch/arm/mach-mb8ac0300/headsmp.S b/arch/arm/mach-mb8ac0300/headsmp.S
new file mode 100644
index 0000000..221a579
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/headsmp.S
@@ -0,0 +1,35 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/headsmp.S
+ *
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+
+#include <linux/linkage.h>
+#include <linux/init.h>
+
+	__CPUINIT	/* .txt */
+
+/*
+ * MB8AC0300 entry point for secondary CPU to jump from ROM code.
+ */
+ENTRY(mb8ac0300_secondary_startup)
+	mrc	p15, 0, r0, c0, c0, 5	@ Read Multiprocessor ID register
+	and	r0, r0, #15		@ get Core id
+	adr	r4, 1f
+	ldmia	r4, {r5, r6}
+	sub	r4, r4, r5
+	add	r6, r6, r4
+pen:	ldr	r7, [r6]		@ check the cpu ID
+	cmp	r7, r0			@ if released cpu is myself
+	bne	pen			@ release form pen
+
+	b	secondary_startup
+ENDPROC(mb8ac0300_secondary_startup)
+
+	.align
+1:	.long	.
+	.long	pen_release
diff --git a/arch/arm/mach-mb8ac0300/hibernate.c b/arch/arm/mach-mb8ac0300/hibernate.c
new file mode 100644
index 0000000..2e6024a
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/hibernate.c
@@ -0,0 +1,48 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/hibernate.c
+ *
+ * Copyright (C) 2012-2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/module.h>
+#include <linux/mm.h>
+
+#include "devices.h"
+
+/* Used in hibernate_asm.S */
+unsigned long saved_context_usr[10];	/* user r3 - r11, r13, r14 */
+unsigned long saved_context_svc[2];	/* SVC r13, r14 */
+unsigned long saved_cpsr;
+unsigned long saved_spsr_svc;
+
+
+/**
+ * pfn_is_nosave - check if given pfn is in the 'nosave' section
+ */
+int pfn_is_nosave(unsigned long pfn)
+{
+	unsigned long nosave_begin_pfn = __phys_to_pfn(__pa(&__nosave_begin));
+	unsigned long nosave_end_pfn = __phys_to_pfn(__pa(&__nosave_end));
+
+	return (pfn >= nosave_begin_pfn) && (pfn < nosave_end_pfn);
+}
+
+void save_processor_state(void)
+{
+}
+
+void restore_processor_state(void)
+{
+}
diff --git a/arch/arm/mach-mb8ac0300/hibernate_asm.S b/arch/arm/mach-mb8ac0300/hibernate_asm.S
new file mode 100644
index 0000000..5db7937
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/hibernate_asm.S
@@ -0,0 +1,171 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/hibernate_asm.S
+ *
+ * Copyright (C) 2012-2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/linkage.h>
+#include <asm/ptrace.h>
+#include <asm/cache.h>
+#include <asm/assembler.h>
+#include <asm/asm-offsets.h>
+
+	.text
+ENTRY(swsusp_arch_suspend)
+	/*
+	 * Save current program status register
+	 */
+	ldr     r3, .Lsaved_cpsr
+	mrs     r0, cpsr
+	str     r0, [r3]
+
+	/*
+	 * Mask cpsr mode
+	 */
+	mov     r1, r0
+	mvn     r2, #MODE_MASK
+	and	r1, r2
+
+	/*
+	 * Change to system(user) mode
+	 */
+	orr	r3, r1, #SYSTEM_MODE
+	msr     cpsr_c, r3
+
+	/*
+	 * Save User context
+	 */
+	ldr     r3, .Lsaved_context_usr
+	stmia   r3, {r4-r11, r13, r14}
+
+	/*
+	 * Change to SVC mode
+	 */
+	orr     r3, r1, #SVC_MODE
+	msr     cpsr_c, r3
+
+	/*
+	 * Save SVC context
+	 */
+	ldr	r3, .Lsaved_context_svc
+	stmia	r3, {r13 - r14}
+	ldr	r3, .Lsaved_spsr_svc
+	mrs	r2, spsr
+	str	r2, [r3]
+
+	/*
+	 * restore original mode before suspending.
+	 */
+	msr	cpsr_c, r0
+	b	swsusp_save
+ENDPROC(swsusp_arch_suspend)
+
+#define CP_COUNT (PAGE_SZ / (2 * L1_CACHE_BYTES) PLD( -1 ))
+
+ENTRY(swsusp_arch_resume)
+	/*
+	 * The following code is an assembly version of:
+	 *	struct pbe *pbe;
+	 *	for (pbe = restore_pblist; pbe != NULL; pbe = pbe->next)
+	 *		copy_page(pbe->orig_address, pbe->address);
+	 */
+
+	/*
+	 * Restore_pblist is the starting point for loaded pages
+	 */
+	ldr	r0, .Lrestore_pblist
+	ldr	r6, [r0]
+pbe_copy_loop:
+	ldr     r0, [r6, #4]	/* dst address (pbe->orig_address) */
+	ldr	r1, [r6]	/* src address (pbe->address) */
+
+	/* copy from arm/lib/copy_page.S */
+	PLD(	pld	[r1, #0]			)
+	PLD(	pld	[r1, #L1_CACHE_BYTES]		)
+	mov	r2, #CP_COUNT
+	ldmia	r1!, {r3, r4, ip, lr}
+1:	PLD(	pld	[r1, #2 * L1_CACHE_BYTES]	)
+	PLD(	pld	[r1, #3 * L1_CACHE_BYTES]	)
+2:
+	.rept   (2 * L1_CACHE_BYTES / 16 - 1)
+		stmia	r0!, {r3, r4, ip, lr}
+		ldmia	r1!, {r3, r4, ip, lr}
+	.endr
+	subs	r2, r2, #1
+	stmia	r0!, {r3, r4, ip, lr}
+	ldmgtia	r1!, {r3, r4, ip, lr}
+	bgt	1b
+	PLD(	ldmeqia r1!, {r3, r4, ip, lr}		)
+	PLD(	beq	2b				)
+
+	/* The last field of struct pbe is a pointer to the next pbe struct */
+	ldr	r6, [r6, #8]	/* pbe->next */
+	cmp	r6, #0
+	bne	pbe_copy_loop
+
+	/*
+	 * Flush TLB (Invalidate unified TLB unlocked entries)
+	 */
+	mov	r1, #0
+	mcr	p15, 0, r1, c8, c7, 0
+
+	/*
+	 * Restore CPU registers.
+	 */
+	mrs	r0, cpsr
+	mov	r1, r0
+	mvn	r2, #MODE_MASK
+	and	r1, r2
+
+	/*
+	 * Change to system(user) mode
+	 */
+	orr	r3, r1, #SYSTEM_MODE
+	msr	cpsr_c, r3
+
+	/*
+	 * Restore user mode context
+	 */
+	ldr	r3, .Lsaved_context_usr
+	ldmia	r3, {r4-r11, r13, r14}
+	ldr	r3, .Lsaved_cpsr
+	ldr	r2, [r3]
+	msr	cpsr_c, r2
+
+	/*
+	 * Change to SVC mode
+	 */
+	orr	r3, r1, #SVC_MODE
+	msr	cpsr_c, r3
+
+	/*
+	 * Restore SVC context
+	 */
+	ldr	r3, .Lsaved_context_svc
+	ldmia	r3, {r13-r14}
+	ldr	r3, .Lsaved_spsr_svc
+	ldr	r1, [r3]
+	msr	spsr_cxsf, r1
+
+	/* Set the return value */
+	mov	r0, #0
+	mov     pc, lr
+ENDPROC(swsusp_arch_resume)
+	.align	4
+.Lsaved_context_usr:		.long	saved_context_usr
+.Lsaved_context_svc:		.long	saved_context_svc
+.Lsaved_cpsr:			.long	saved_cpsr
+.Lsaved_spsr_svc:		.long   saved_spsr_svc
+.Lrestore_pblist:		.long	restore_pblist
diff --git a/arch/arm/mach-mb8ac0300/hotplug.c b/arch/arm/mach-mb8ac0300/hotplug.c
new file mode 100644
index 0000000..300d21d
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/hotplug.c
@@ -0,0 +1,149 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/hotplug.c
+ *
+ * Copyright (C) 2012-2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/errno.h>
+#include <linux/io.h>
+#include <linux/kernel.h>
+#include <linux/smp.h>
+#include <asm/cacheflush.h>
+#include <asm/cp15.h>
+
+#include <linux/irqchip/arm-gic.h>
+#include <linux/platform_data/mb8ac0300-irqs.h>
+#include <linux/platform_data/mb8ac0300-iomap.h>
+
+#include "devices.h"
+
+static inline void cpu_enter_lowpower(void)
+{
+	unsigned int v;
+
+	flush_cache_all();
+	asm volatile(
+	"	mcr	p15, 0, %1, c7, c5, 0\n" /* invalidate all i caches */
+	"	dsb\n"
+	/*
+	 * Turn off coherency
+	 */
+	"	mrc	p15, 0, %0, c1, c0, 1\n"
+	"	bic	%0, %0, %3\n"
+	"	mcr	p15, 0, %0, c1, c0, 1\n" /* ACTLR.SMP = 0 */
+	"	mrc	p15, 0, %0, c1, c0, 0\n"
+	"	bic	%0, %0, %2\n"
+	"	mcr	p15, 0, %0, c1, c0, 0\n" /* SCTLR.C = 0 */
+	  : "=&r" (v)
+	  : "r" (0), "Ir" (CR_C), "Ir" (0x40)
+	  : "cc");
+}
+
+static inline void cpu_leave_lowpower(void)
+{
+	unsigned int v;
+
+	asm volatile(
+	"	mrc	p15, 0, %0, c1, c0, 0\n"
+	"	orr	%0, %0, %1\n"
+	"	mcr	p15, 0, %0, c1, c0, 0\n" /* SCTLR.C = 1 */
+	"	mrc	p15, 0, %0, c1, c0, 1\n"
+	"	orr	%0, %0, %2\n"
+	"	mcr	p15, 0, %0, c1, c0, 1\n" /* ACTLR.SMP = 1 */
+	  : "=&r" (v)
+	  : "Ir" (CR_C), "Ir" (0x40)
+	  : "cc");
+}
+
+static inline void platform_do_lowpower(unsigned int cpu, int *spurious)
+{
+	/*
+	 * there is no power-control hardware on this platform, so all
+	 * we can do is put the core into WFI; this is safe as the calling
+	 * code will have already disabled interrupts
+	 */
+	for (;;) {
+		/*
+		 * here's the WFI
+		 */
+		asm("wfi");
+
+		barrier();
+		if (pen_release == cpu) {
+			/*
+			 * OK, proper wakeup, we're done
+			 */
+			break;
+	}
+
+		/*
+		 * Getting here, means that we have come out of WFI without
+		 * having been woken up - this shouldn't happen
+		 *
+		 * Just note it happening - when we're woken, we can report
+		 * its occurrence.
+		 */
+		(*spurious)++;
+	}
+}
+
+int platform_cpu_kill(unsigned int cpu)
+{
+	return 1;
+}
+
+/*
+ * platform-specific code to shutdown a CPU
+ *
+ * Called with IRQs disabled
+ */
+void __ref platform_cpu_die(unsigned int cpu)
+{
+	int spurious = 0;
+	unsigned int val;
+	void *base;
+
+	/*
+	 * we're ready for shutdown now, so do it
+	 */
+	cpu_enter_lowpower();
+	platform_do_lowpower(cpu, &spurious);
+
+	/*
+	 * FIXME: Clear IPI interrupt after wake up
+	 * to suppress an annoying message from do_IPI.
+	 */
+	base = (void *)MB8AC0300_GIC_CPU_VIRT;
+	val = readl_relaxed(base + GIC_CPU_INTACK);
+	writel_relaxed(val&0x3ff, base + GIC_CPU_EOI);
+
+	/*
+	 * bring this CPU back into the world of cache
+	 * coherency, and then restore interrupts
+	 */
+	cpu_leave_lowpower();
+
+	if (spurious)
+		pr_warn("CPU%u: %u spurious wakeup calls\n", cpu, spurious);
+}
+
+int platform_cpu_disable(unsigned int cpu)
+{
+	/*
+	 * we don't allow CPU 0 to be shutdown (it is still too special
+	 * e.g. clock tick interrupts)
+	 */
+	return cpu == 0 ? -EPERM : 0;
+}
diff --git a/arch/arm/mach-mb8ac0300/io.c b/arch/arm/mach-mb8ac0300/io.c
new file mode 100644
index 0000000..72df06e
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/io.c
@@ -0,0 +1,75 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/io.c
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+
+#include <asm/io.h>
+#include <asm/memory.h>
+#include <asm/mach/map.h>
+#include <asm/tlb.h>
+
+#include <linux/platform_data/mb8ac0300-iomap.h>
+
+void __iomem *mb8ac0300_scu_base;
+
+
+#define IOMAP_DEV(name) { \
+		.virtual = (unsigned long) MB8AC0300_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB8AC0300_##name##_PHYS), \
+		.length = MB8AC0300_##name##_SIZE, \
+		.type = MT_DEVICE, \
+	}
+#define IOMAP_MEM(name) { \
+		.virtual = (unsigned long) MB8AC0300_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB8AC0300_##name##_PHYS), \
+		.length = MB8AC0300_##name##_SIZE, \
+		.type = MT_MEMORY, \
+	}
+#define IOMAP_MEM_NONCACHED(name) { \
+		.virtual = (unsigned long) MB8AC0300_##name##_VIRT, \
+		.pfn = __phys_to_pfn(MB8AC0300_##name##_PHYS), \
+		.length = MB8AC0300_##name##_SIZE, \
+		.type = MT_MEMORY_NONCACHED, \
+	}
+
+static struct map_desc mb8ac0300_io_desc[] = {
+	IOMAP_DEV(UART0),
+	IOMAP_DEV(MRBC),
+	IOMAP_DEV(CORE),
+	IOMAP_DEV(CRG11_MAIN),
+	IOMAP_DEV(CRG11_DMC),
+	IOMAP_DEV(CRG11_GMAC),
+	IOMAP_DEV(CRG11_LCD),
+	IOMAP_DEV(DDRC),
+	IOMAP_MEM_NONCACHED(XSRAM),
+};
+
+/**
+ * mb8ac0300_map_io()
+ * make static mappings for initial devices
+ */
+void __init mb8ac0300_map_io(void)
+{
+	debug_ll_io_init();
+
+	mb8ac0300_scu_base = MB8AC0300_CORE_VIRT;
+
+	iotable_init(&mb8ac0300_io_desc[0], ARRAY_SIZE(mb8ac0300_io_desc));
+}
diff --git a/arch/arm/mach-mb8ac0300/platsmp.c b/arch/arm/mach-mb8ac0300/platsmp.c
new file mode 100644
index 0000000..ff4b053
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/platsmp.c
@@ -0,0 +1,160 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/platsmp.c
+ *
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+
+#include <linux/init.h>
+#include <linux/errno.h>
+#include <linux/device.h>
+#include <linux/smp.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/delay.h>
+#include <linux/jiffies.h>
+#include <linux/of_address.h>
+#include <linux/irqchip/arm-gic.h>
+
+#include <asm/cacheflush.h>
+#include <asm/mach-types.h>
+#include <asm/smp_scu.h>
+#include <asm/smp_plat.h>
+#include <asm/smp.h>
+
+#define MRBC_GPREG0		0x30
+
+static DEFINE_SPINLOCK(boot_lock);
+
+extern void mb8ac0300_secondary_startup(void);
+extern void __iomem *mb8ac0300_scu_base;
+static int ncores;
+
+static void __cpuinit mb8ac0300_secondary_init(unsigned int cpu)
+{
+	/*
+	 * Let the primary cpu know we're out of the pen.
+	 */
+	pen_release = -1;
+	smp_wmb();
+
+	/*
+	 * Synchronise with the boot thread.
+	 */
+	spin_lock(&boot_lock);
+	spin_unlock(&boot_lock);
+}
+
+static struct of_device_id mrbc_ids[]  = {
+	{ .compatible = "fujitsu,mb8ac0300-mrbc" },
+	{},
+};
+
+static int __cpuinit mb8ac0300_boot_secondary(unsigned int cpu,
+		struct task_struct *idle)
+{
+	unsigned long boot_vector;
+	void __iomem *mrbc_base;
+	struct device_node *np;
+	unsigned long timeout;
+
+	np = of_find_matching_node(NULL, mrbc_ids);
+	if (!np)
+		return -ENODEV;
+
+	mrbc_base = of_iomap(np, 0);
+	if (!mrbc_base)
+		return -ENOMEM;
+
+	/*
+	 * Set synchronisation state between this boot processor
+	 * and the secondary one
+	 */
+	spin_lock(&boot_lock);
+
+	pen_release = cpu_logical_map(cpu);
+
+	flush_cache_all();
+
+	/* set the reset vector to point to the secondary_startup routine */
+	boot_vector = virt_to_phys(mb8ac0300_secondary_startup);
+
+	/* set vetcor to SECOND_BOOT_ADD_REG for CPU1 to use */
+	writel(boot_vector, mrbc_base + MRBC_GPREG0);
+
+	smp_wmb();	/* barrier */
+
+	/* soft irq */
+	arch_send_wakeup_ipi_mask(cpumask_of(cpu));
+
+	/*
+	 * now the secondary core is starting up let it run its
+	 * calibrations, then wait for it to finish
+	 */
+
+	timeout = jiffies + HZ;
+	while (time_before(jiffies, timeout)) {
+		smp_rmb();
+		if (pen_release == -1)
+			break;
+		udelay(10);
+	}
+
+	spin_unlock(&boot_lock);
+
+	return pen_release != -1 ? -ENOSYS : 0;
+}
+
+static void __init mb8ac0300_smp_init_cpus(void)
+{
+	int i;
+
+	ncores = scu_get_core_count(mb8ac0300_scu_base);
+
+	if (ncores > nr_cpu_ids) {
+		pr_warn("%s(): ncores (%u) > configured (%u)\n",
+					__func__, ncores, nr_cpu_ids);
+		ncores = nr_cpu_ids;
+	}
+
+	for (i = 0; i < ncores; i++)
+		set_cpu_possible(i, true);
+}
+
+static void __init mb8ac0300_smp_prepare_cpus(unsigned int maxcpus)
+{
+	int i;
+
+	for (i = 0; i < maxcpus; i++)
+		set_cpu_present(i, true);
+
+	scu_enable(mb8ac0300_scu_base);
+}
+
+static void __cpuinitdata mb8ac0300_cpu_die(unsigned int cpu)
+{
+	/* we put the platform to just WFI */
+	for (;;) {
+		__asm__ __volatile__("dsb\n\t" "wfi\n\t"
+			: : : "memory");
+		if (pen_release == cpu_logical_map(cpu)) {
+			/*
+			 * OK, proper wakeup, we're done
+			 */
+			break;
+		}
+	}
+}
+
+struct smp_operations mb8ac0300_smp_ops __initdata = {
+	.smp_init_cpus		= mb8ac0300_smp_init_cpus,
+	.smp_prepare_cpus	= mb8ac0300_smp_prepare_cpus,
+	.smp_secondary_init	= mb8ac0300_secondary_init,
+	.smp_boot_secondary	= mb8ac0300_boot_secondary,
+#ifdef CONFIG_HOTPLUG_CPU
+	.cpu_die		= mb8ac0300_cpu_die,
+#endif
+};
diff --git a/arch/arm/mach-mb8ac0300/pm.c b/arch/arm/mach-mb8ac0300/pm.c
new file mode 100644
index 0000000..7b8f116
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/pm.c
@@ -0,0 +1,112 @@
+/* linux/arch/arm/mach-mb8ac0300/pm.c
+ *
+ * Copyright (C) 2012-2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/suspend.h>
+#include <linux/io.h>
+
+#include <asm/cacheflush.h>
+
+#include <linux/platform_data/mb8ac0300-irqs.h>
+#include <linux/platform_data/mb8ac0300-iomap.h>
+#include <linux/irqchip/irq-mb8ac0300.h>
+
+#include "devices.h"
+
+/**
+ * mb8ac0300_pm_valid
+ * @state: requested state
+ *
+ * Determine if we support the specified state
+ */
+static int mb8ac0300_pm_valid(suspend_state_t state)
+{
+	return (state == PM_SUSPEND_STANDBY) || (state == PM_SUSPEND_MEM);
+}
+
+static irqreturn_t mb8ac0300_wakeup_handler(int irq, void *arg)
+{
+	return IRQ_HANDLED;
+}
+
+/**
+ * mb8ac0300_pm_prepare
+ *
+ * Prepare the wakeup irq.
+ */
+static int mb8ac0300_pm_prepare(void)
+{
+	int ret;
+	/* set button A to wakeup */
+	ret = exiu_irq_set_type(IRQ_EXTINT21, IRQ_TYPE_EDGE_RISING);
+	if (ret)
+		return ret;
+	ret = request_irq(IRQ_EXTINT21, mb8ac0300_wakeup_handler,
+		IRQF_TRIGGER_RISING, "external irq", NULL);
+	if (ret)
+		return ret;
+	ret = irq_set_irq_wake(IRQ_EXTINT21, 1);
+	return ret;
+}
+
+/**
+  * mb8ac0300_pm_enter
+  * @state: requested state
+  */
+static int mb8ac0300_pm_enter(suspend_state_t state)
+{
+	pr_debug("Entered %s state:%d\n", __func__, state);
+
+	switch (state) {
+	case PM_SUSPEND_STANDBY:
+		/*
+		 * STANDBY mode just goes into WFI state
+		 */
+		asm("wfi");		/* wait for interrupt */
+		break;
+	case PM_SUSPEND_MEM:
+		/* virt to phys offset */
+		mb8ac0300_cpu_save(PHYS_OFFSET - PAGE_OFFSET);
+		break;
+	}
+	return 0;
+}
+
+static void mb8ac0300_pm_finish(void)
+{
+	free_irq(IRQ_EXTINT21, NULL);
+}
+
+static const struct platform_suspend_ops mb8ac0300_pm_ops = {
+	.valid		= mb8ac0300_pm_valid,
+	.prepare	= mb8ac0300_pm_prepare,
+	.enter		= mb8ac0300_pm_enter,
+	.finish		= mb8ac0300_pm_finish,
+};
+
+int __init mb8ac0300_pm_init(void)
+{
+	suspend_set_ops(&mb8ac0300_pm_ops);
+#ifdef CONFIG_SUSPEND
+	memcpy((u32 *)MB8AC0300_XSRAM_VIRT, (u32 *)mb8ac0300_cpu_sleep,
+					mb8ac0300_cpu_sleep_size);
+#endif
+	flush_cache_all();
+
+	return 0;
+}
diff --git a/arch/arm/mach-mb8ac0300/sleep.S b/arch/arm/mach-mb8ac0300/sleep.S
new file mode 100644
index 0000000..4206d67
--- /dev/null
+++ b/arch/arm/mach-mb8ac0300/sleep.S
@@ -0,0 +1,220 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/sleep.S
+ *
+ * Copyright (C) 2012-2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/linkage.h>
+#include <linux/platform_data/mb8ac0300-iomap.h>
+
+#define IOMEM(x) (x)
+
+#define	DDR3_SDRAMC_DENALI_CTL_00		0
+#define	DDR3_SDRAMC_DENALI_CTL_23		0x5c
+#define	DDR3_SDRAMC_DENALI_CTL_90		0x168
+#define	DDR3_SDRAMC_DENALI_CTL_92		0x170
+#define	DDR3_SDRAMC_REG_STR			0x400
+#define	DDR3_SDRAMC_REG_MCC			0x404
+#define	DDR3_SDRAMC_REG_RST			0x408
+#define DDR3_SDRAMC_REG_PWR0			0x40c
+#define DDR3_SDRAMC_REG_PWR1			0x410
+#define	DDR3_SDRAMC_REG_UP			0x440
+
+#define	DDR3_SDRAMC_DENALI_CTL_23_SREFRESH	1
+#define	DDR3_SDRAMC_DENALI_CTL_90_CKE_STATUS	1<<16
+#define	DDR3_SDRAMC_DENALI_CTL_92_CLK_DISABLE	1<<24
+#define	DDR3_SDRAMC_REG_STR_CONTROLLER_BUSY	0x8
+
+	.text
+	/*
+	 * mb8ac0300_cpu_save
+	 * r0 : v:p offset
+	 */
+
+ENTRY(mb8ac0300_cpu_save)
+	stmfd	sp!, { r4 - r11, lr }
+	/*
+	 * r1 virtual:physics offset
+	 * r3 virtual return function
+	 */
+	@mov	r1, r0			@ mb8ac0300 don't support dormant mode
+	@ldr	r3, =resume_with_mmu
+	@bl	cpu_suspend		@ we comment out cpu_suspend
+
+	ldr	r0, =MB8AC0300_DDRC_VIRT
+	ldr	r1, =MB8AC0300_XSRAM_VIRT
+	ldr	lr, =resume_with_mmu
+	mov	pc, r1
+
+	/*
+	 * return to the caller, after having the MMU
+	 * turned on, this restores the last bits from the stack
+	 */
+resume_with_mmu:
+
+	ldmfd	sp!, { r4 - r11, lr }
+	mov	pc, lr
+
+ENTRY(mb8ac0300_cpu_wakeup)
+	b	cpu_resume
+
+	/*
+	 * mb8ac0300_cpu_sleep
+	 * r0 : DDRC virtual address
+	 */
+ENTRY(mb8ac0300_cpu_sleep)
+l_ddrc_busy:
+	ldr	r1, [r0, #DDR3_SDRAMC_REG_STR]
+	ands	r2, r1, #DDR3_SDRAMC_REG_STR_CONTROLLER_BUSY
+	bne	l_ddrc_busy
+
+@ enter self-refresh mode
+	ldr	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_23]
+	orr	r2, r2, #DDR3_SDRAMC_DENALI_CTL_23_SREFRESH
+	str	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_23]
+l_cke_status_is_not_zero:
+	ldr	r1, [r0, #DDR3_SDRAMC_DENALI_CTL_90]
+	ands	r2, r1, #DDR3_SDRAMC_DENALI_CTL_92_CLK_DISABLE
+	bne	l_cke_status_is_not_zero
+
+@ disable DDR clock
+	ldr	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_92]
+	orr	r2, r2, #DDR3_SDRAMC_DENALI_CTL_92_CLK_DISABLE
+	str	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_92]
+
+@ DDR driver suspend
+	ldr	r1, [r0, #DDR3_SDRAMC_REG_PWR0]
+	ldr	r2, lc_ddrc_pwr0_val
+	orr	r1, r1, r2
+	str	r1, [r0, #DDR3_SDRAMC_REG_PWR0]
+
+@ I/O driver/receiver suspend
+	ldr	r1, [r0, #DDR3_SDRAMC_REG_PWR1]
+	ldr	r2, lc_ddrc_pwr1_val
+	orr	r1, r1, r2
+	str	r1, [r0, #DDR3_SDRAMC_REG_PWR1]
+
+@ into WFI sleep mode
+	wfi
+
+@ DDR driver resume
+	ldr	r1, [r0, #DDR3_SDRAMC_REG_PWR0]
+	ldr	r2, lc_ddrc_pwr0_val
+	bic	r1, r1, r2
+	str	r1, [r0, #DDR3_SDRAMC_REG_PWR0]
+
+@ I/O driver/receiver resume
+	ldr	r1, [r0, #DDR3_SDRAMC_REG_PWR1]
+	ldr	r2, lc_ddrc_pwr1_val
+	bic	r1, r1, r2
+	str	r1, [r0, #DDR3_SDRAMC_REG_PWR1]
+
+@ enable DDR clock
+	ldr	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_92]
+	bic	r2, r2, #DDR3_SDRAMC_DENALI_CTL_92_CLK_DISABLE
+	str	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_92]
+
+@ Reset Sequence
+	ldr	r2, lc_reg_rst1
+	str	r2, [r0, #DDR3_SDRAMC_REG_RST]
+
+ @ sleep 120ns,3*20=60cycle 120ns
+	mov	r2, #20
+l_sleep_1:
+	nop
+	nop
+	subs	r2, r2, #1
+	bne	l_sleep_1
+
+	ldr	r2, lc_reg_rst2
+	str	r2, [r0, #DDR3_SDRAMC_REG_RST]
+
+ @ sleep 66us,3*11k=33kcycle 66us
+	ldr	r2, lc_11000
+l_sleep_2:
+	nop
+	nop
+	subs	r2, r2, #1
+	bne	l_sleep_2
+
+	ldr	r2, lc_reg_rst3
+	str	r2, [r0, #DDR3_SDRAMC_REG_RST]
+
+ @ sleep 1500us,3*225k=675kcycle 1.5ms
+	ldr	r2, lc_225000
+l_sleep_3:
+	nop
+	nop
+	subs	r2, r2, #1
+	bne	l_sleep_3
+
+@ PZQ initial calibration
+	mov	r2, #0
+	str	r2, [r0, #DDR3_SDRAMC_REG_UP]
+
+@ sleep x ns(not strictly defined),3*10=30cycle 60ns
+	mov	r2, #10
+l_sleep_4:
+	nop
+	nop
+	subs	r2, r2, #1
+	bne	l_sleep_4
+
+	mov	r2, #0x1
+	str	r2, [r0, #DDR3_SDRAMC_REG_MCC]
+
+@ exit self-refresh mode
+	ldr	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_23]
+	bic	r2, r2, #DDR3_SDRAMC_DENALI_CTL_23_SREFRESH
+	str	r2, [r0, #DDR3_SDRAMC_DENALI_CTL_23]
+
+l_cke_status_is_zero:
+	ldr	r1, [r0, #DDR3_SDRAMC_DENALI_CTL_90]
+	ands	r2, r1, #DDR3_SDRAMC_DENALI_CTL_90_CKE_STATUS
+	beq	l_cke_status_is_zero
+
+	mov	pc, lr
+
+	.align 2
+lc_ddrc_pwr0_val:
+	.long	0x1ffffff
+			@ 0-14:Address(o_MA0-14) driver suspend
+			@15-17:Bank Address(o_MBA0-2) driver suspend
+			@18   :Write enable(o_MXWE) driver suspend
+			@19   :CAS(o_MXCAS) driver suspend
+			@20   :RAS(o_MXRAS) driver suspend
+			@21-22:CS(o_MXCS0-1) driver suspend
+			@23-24:Data(o_MODT0,1) driver suspend
+			@25-26:Clock enable(o_MCKE0-1) driver not suspend
+lc_ddrc_pwr1_val:
+	.long	0x10fff01
+			@    0:Clock(o_OTXCK) driver not suspend
+			@ 8-15:DQDQS driver suspend
+			@16-19:Loop back driver suspend
+			@24   :ZQ bias driver suspend
+lc_reg_rst1:
+	.long	0x0000F
+lc_reg_rst2:
+	.long	0x3000F
+lc_reg_rst3:
+	.long	0x7000F
+lc_225000:
+	.long	225000
+lc_11000:
+	.long	11000
+
+mb8ac0300_cpu_sleep_size:
+	.global	mb8ac0300_cpu_sleep_size
+	.long   . - mb8ac0300_cpu_sleep
diff --git a/arch/arm/mm/cache-v7.S b/arch/arm/mm/cache-v7.S
index 515b000..a84e053 100644
--- a/arch/arm/mm/cache-v7.S
+++ b/arch/arm/mm/cache-v7.S
@@ -146,18 +146,18 @@ flush_levels:
 	ldr	r7, =0x7fff
 	ands	r7, r7, r1, lsr #13		@ extract max number of the index size
 loop1:
-	mov	r9, r4				@ create working copy of max way size
+	mov	r9, r7				@ create working copy of max index
 loop2:
- ARM(	orr	r11, r10, r9, lsl r5	)	@ factor way and cache number into r11
- THUMB(	lsl	r6, r9, r5		)
+ ARM(	orr	r11, r10, r4, lsl r5	)	@ factor way and cache number into r11
+ THUMB(	lsl	r6, r4, r5		)
  THUMB(	orr	r11, r10, r6		)	@ factor way and cache number into r11
- ARM(	orr	r11, r11, r7, lsl r2	)	@ factor index number into r11
- THUMB(	lsl	r6, r7, r2		)
+ ARM(	orr	r11, r11, r9, lsl r2	)	@ factor index number into r11
+ THUMB(	lsl	r6, r9, r2		)
  THUMB(	orr	r11, r11, r6		)	@ factor index number into r11
 	mcr	p15, 0, r11, c7, c14, 2		@ clean & invalidate by set/way
-	subs	r9, r9, #1			@ decrement the way
+	subs	r9, r9, #1			@ decrement the index
 	bge	loop2
-	subs	r7, r7, #1			@ decrement the index
+	subs	r4, r4, #1			@ decrement the way
 	bge	loop1
 skip:
 	add	r10, r10, #2			@ increment cache number
diff --git a/arch/arm/mm/dma-mapping.c b/arch/arm/mm/dma-mapping.c
index 096179c..1ed6d94 100644
--- a/arch/arm/mm/dma-mapping.c
+++ b/arch/arm/mm/dma-mapping.c
@@ -25,7 +25,7 @@
 #include <linux/io.h>
 #include <linux/vmalloc.h>
 #include <linux/sizes.h>
-
+#include <linux/bootmem.h>
 #include <asm/memory.h>
 #include <asm/highmem.h>
 #include <asm/cacheflush.h>
@@ -38,6 +38,13 @@
 
 #include "mm.h"
 
+/* hack for MALI */
+void _dmac_flush_range(const void *a, const void *b)
+{
+	dmac_flush_range(a, b);
+}
+EXPORT_SYMBOL_GPL(_dmac_flush_range);
+
 /*
  * The DMA API is built upon the notion of "buffer ownership".  A buffer
  * is either exclusively owned by the CPU (and therefore may be accessed
@@ -157,9 +164,43 @@ struct dma_map_ops arm_coherent_dma_ops = {
 };
 EXPORT_SYMBOL(arm_coherent_dma_ops);
 
+static int __dma_supported(struct device *dev, u64 mask, bool warn)
+{
+	/*
+	 * If the mask allows for more memory than we can address,
+	 * and we actually have that much memory, then we must
+	 * indicate that DMA to this device is not supported.
+	 */
+	if (sizeof(mask) != sizeof(dma_addr_t) &&
+	    mask > (dma_addr_t)~0 &&
+	    dma_to_pfn(dev, ~0) > arm_dma_pfn_limit) {
+		if (warn) {
+			dev_warn(dev, "Coherent DMA mask %#llx is larger than dma_addr_t allows\n",
+				 mask);
+			dev_warn(dev, "Driver did not use or check the return value from dma_set_coherent_mask()?\n");
+		}
+		return 0;
+	}
+
+	/*
+	 * Translate the device's DMA mask to a PFN limit.  This
+	 * PFN number includes the page which we can DMA to.
+	 */
+	if (dma_to_pfn(dev, mask) < arm_dma_pfn_limit) {
+		if (warn)
+			dev_warn(dev, "Coherent DMA mask %#llx (pfn %#lx-%#lx) covers a smaller range of system memory than the DMA zone pfn 0x0-%#lx\n",
+				 mask,
+				 dma_to_pfn(dev, 0), dma_to_pfn(dev, mask) + 1,
+				 arm_dma_pfn_limit + 1);
+		return 0;
+	}
+
+	return 1;
+}
+
 static u64 get_coherent_dma_mask(struct device *dev)
 {
-	u64 mask = (u64)arm_dma_limit;
+	u64 mask = (u64)DMA_BIT_MASK(32);
 
 	if (dev) {
 		mask = dev->coherent_dma_mask;
@@ -173,13 +214,9 @@ static u64 get_coherent_dma_mask(struct device *dev)
 			return 0;
 		}
 
-		if ((~mask) & (u64)arm_dma_limit) {
-			dev_warn(dev, "coherent DMA mask %#llx is smaller "
-				 "than system GFP_DMA mask %#llx\n",
-				 mask, (u64)arm_dma_limit);
+		if (!__dma_supported(dev, mask, true))
 			return 0;
 		}
-	}
 
 	return mask;
 }
@@ -429,12 +466,21 @@ void __init dma_contiguous_remap(void)
 		map.type = MT_MEMORY_DMA_READY;
 
 		/*
-		 * Clear previous low-memory mapping
+		 * Clear previous low-memory mapping to ensure that the
+		 * TLB does not see any conflicting entries, then flush
+		 * the TLB of the old entries before creating new mappings.
+		 *
+		 * This ensures that any speculatively loaded TLB entries
+		 * (even though they may be rare) can not cause any problems,
+		 * and ensures that this code is architecturally compliant.
 		 */
 		for (addr = __phys_to_virt(start); addr < __phys_to_virt(end);
 		     addr += PMD_SIZE)
 			pmd_clear(pmd_off_k(addr));
 
+		flush_tlb_kernel_range(__phys_to_virt(start),
+				       __phys_to_virt(end));
+
 		iotable_init(&map, 1);
 	}
 }
@@ -994,9 +1040,7 @@ void arm_dma_sync_sg_for_device(struct device *dev, struct scatterlist *sg,
  */
 int dma_supported(struct device *dev, u64 mask)
 {
-	if (mask < (u64)arm_dma_limit)
-		return 0;
-	return 1;
+	return __dma_supported(dev, mask, false);
 }
 EXPORT_SYMBOL(dma_supported);
 
@@ -1088,6 +1132,8 @@ static struct page **__iommu_alloc_buffer(struct device *dev, size_t size,
 		unsigned long order = get_order(size);
 		struct page *page;
 
+		dev_dbg(dev, "%s %d\n", __func__, __LINE__);
+
 		page = dma_alloc_from_contiguous(dev, count, order);
 		if (!page)
 			goto error;
@@ -1219,7 +1265,7 @@ __iommu_create_mapping(struct device *dev, struct page **pages, size_t size)
 				break;
 
 		len = (j - i) << PAGE_SHIFT;
-		ret = iommu_map(mapping->domain, iova, phys, len, 0);
+		ret = iommu_map(mapping->domain, iova, phys, len, IOMMU_READ|IOMMU_WRITE|IOMMU_EXEC);
 		if (ret < 0)
 			goto fail;
 		iova += len;
@@ -1437,7 +1483,7 @@ static int __map_sg_chunk(struct device *dev, struct scatterlist *sg,
 			!dma_get_attr(DMA_ATTR_SKIP_CPU_SYNC, attrs))
 			__dma_page_cpu_to_dev(sg_page(s), s->offset, s->length, dir);
 
-		ret = iommu_map(mapping->domain, iova, phys, len, 0);
+		ret = iommu_map(mapping->domain, iova, phys, len, IOMMU_READ|IOMMU_WRITE|IOMMU_EXEC);
 		if (ret < 0)
 			goto fail;
 		count += len >> PAGE_SHIFT;
@@ -1641,8 +1687,7 @@ static dma_addr_t arm_coherent_iommu_map_page(struct device *dev, struct page *p
 	dma_addr = __alloc_iova(mapping, len);
 	if (dma_addr == DMA_ERROR_CODE)
 		return dma_addr;
-
-	ret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, 0);
+	ret = iommu_map(mapping->domain, dma_addr, page_to_phys(page), len, IOMMU_READ|IOMMU_WRITE|IOMMU_EXEC);
 	if (ret < 0)
 		goto fail;
 
diff --git a/arch/arm/mm/init.c b/arch/arm/mm/init.c
index dd5d5ae..0947e85 100644
--- a/arch/arm/mm/init.c
+++ b/arch/arm/mm/init.c
@@ -77,7 +77,7 @@ static int __init parse_tag_initrd2(const struct tag *tag)
 __tagtable(ATAG_INITRD2, parse_tag_initrd2);
 
 #ifdef CONFIG_OF_FLATTREE
-void __init early_init_dt_setup_initrd_arch(unsigned long start, unsigned long end)
+void __init early_init_dt_setup_initrd_arch(u64 start, u64 end)
 {
 	phys_initrd_start = start;
 	phys_initrd_size = end - start;
@@ -217,6 +217,7 @@ EXPORT_SYMBOL(arm_dma_zone_size);
  * so a successful GFP_DMA allocation will always satisfy this.
  */
 phys_addr_t arm_dma_limit;
+unsigned long arm_dma_pfn_limit;
 
 static void __init arm_adjust_dma_zone(unsigned long *size, unsigned long *hole,
 	unsigned long dma_size)
@@ -239,6 +240,7 @@ void __init setup_dma_zone(struct machine_desc *mdesc)
 		arm_dma_limit = PHYS_OFFSET + arm_dma_zone_size - 1;
 	} else
 		arm_dma_limit = 0xffffffff;
+	arm_dma_pfn_limit = arm_dma_limit >> PAGE_SHIFT;
 #endif
 }
 
@@ -632,6 +634,12 @@ void __init mem_init(void)
 	 * Since our memory may not be contiguous, calculate the
 	 * real number of pages we have in this system
 	 */
+#ifdef CONFIG_FLATMEM
+	printk(KERN_INFO "Memory : FLAT Memory");
+#endif /* CONFIG_FLATMEM */
+#ifdef CONFIG_SPARSEMEM
+	printk(KERN_INFO "Memory : SPARSE Memory");
+#endif	/* CONFIG_SPARSEMEM */
 	printk(KERN_INFO "Memory:");
 	num_physpages = 0;
 	for_each_memblock(memory, reg) {
diff --git a/arch/arm/mm/mm.h b/arch/arm/mm/mm.h
index d5a4e9a..d5a982d 100644
--- a/arch/arm/mm/mm.h
+++ b/arch/arm/mm/mm.h
@@ -81,8 +81,10 @@ extern __init void add_static_vm_early(struct static_vm *svm);
 
 #ifdef CONFIG_ZONE_DMA
 extern phys_addr_t arm_dma_limit;
+extern unsigned long arm_dma_pfn_limit;
 #else
 #define arm_dma_limit ((phys_addr_t)~0)
+#define arm_dma_pfn_limit (~0ul >> PAGE_SHIFT)
 #endif
 
 extern phys_addr_t arm_lowmem_limit;
diff --git a/arch/arm/mm/mmu.c b/arch/arm/mm/mmu.c
index ffcce29..8fbc59a 100644
--- a/arch/arm/mm/mmu.c
+++ b/arch/arm/mm/mmu.c
@@ -606,11 +606,25 @@ static void __init *early_alloc(unsigned long sz)
 	return early_alloc_aligned(sz, sz);
 }
 
-static pte_t * __init early_pte_alloc(pmd_t *pmd, unsigned long addr, unsigned long prot)
+static pte_t * __init early_pte_alloc(pmd_t *pmd)
+{
+	if (pmd_none(*pmd) || pmd_bad(*pmd))
+		return early_alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
+	return pmd_page_vaddr(*pmd);
+}
+
+static void __init early_pte_install(pmd_t *pmd, pte_t *pte, unsigned long prot)
 {
-	if (pmd_none(*pmd)) {
-		pte_t *pte = early_alloc(PTE_HWTABLE_OFF + PTE_HWTABLE_SIZE);
 		__pmd_populate(pmd, __pa(pte), prot);
+	BUG_ON(pmd_bad(*pmd));
+}
+
+static pte_t * __init early_pte_alloc_and_install(pmd_t *pmd,
+	unsigned long addr, unsigned long prot)
+{
+	if (pmd_none(*pmd)) {
+		pte_t *pte = early_pte_alloc(pmd);
+		early_pte_install(pmd, pte, prot);
 	}
 	BUG_ON(pmd_bad(*pmd));
 	return pte_offset_kernel(pmd, addr);
@@ -620,11 +634,17 @@ static void __init alloc_init_pte(pmd_t *pmd, unsigned long addr,
 				  unsigned long end, unsigned long pfn,
 				  const struct mem_type *type)
 {
-	pte_t *pte = early_pte_alloc(pmd, addr, type->prot_l1);
+	pte_t *start_pte = early_pte_alloc(pmd);
+	pte_t *pte = start_pte + pte_index(addr);
+
+	/* If replacing a section mapping, the whole section must be replaced */
+	BUG_ON(!pmd_none(*pmd) && pmd_bad(*pmd) && ((addr | end) & ~PMD_MASK));
+
 	do {
 		set_pte_ext(pte, pfn_pte(pfn, __pgprot(type->prot_pte)), 0);
 		pfn++;
 	} while (pte++, addr += PAGE_SIZE, addr != end);
+	early_pte_install(pmd, start_pte, type->prot_l1);
 }
 
 static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
@@ -656,7 +676,8 @@ static void __init __map_init_section(pmd_t *pmd, unsigned long addr,
 
 static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 				      unsigned long end, phys_addr_t phys,
-				      const struct mem_type *type)
+				      const struct mem_type *type,
+				      bool force_pages)
 {
 	pmd_t *pmd = pmd_offset(pud, addr);
 	unsigned long next;
@@ -673,7 +694,8 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 		 * aligned to a section boundary.
 		 */
 		if (type->prot_sect &&
-				((addr | next | phys) & ~SECTION_MASK) == 0) {
+				((addr | next | phys) & ~SECTION_MASK) == 0 &&
+				!force_pages) {
 			__map_init_section(pmd, addr, next, phys, type);
 		} else {
 			alloc_init_pte(pmd, addr, next,
@@ -686,15 +708,15 @@ static void __init alloc_init_pmd(pud_t *pud, unsigned long addr,
 }
 
 static void __init alloc_init_pud(pgd_t *pgd, unsigned long addr,
-				  unsigned long end, phys_addr_t phys,
-				  const struct mem_type *type)
+	unsigned long end, phys_addr_t phys, const struct mem_type *type,
+	bool force_pages)
 {
 	pud_t *pud = pud_offset(pgd, addr);
 	unsigned long next;
 
 	do {
 		next = pud_addr_end(addr, end);
-		alloc_init_pmd(pud, addr, next, phys, type);
+		alloc_init_pmd(pud, addr, next, phys, type, force_pages);
 		phys += next - addr;
 	} while (pud++, addr = next, addr != end);
 }
@@ -768,7 +790,7 @@ static void __init create_36bit_mapping(struct map_desc *md,
  * offsets, and we take full advantage of sections and
  * supersections.
  */
-static void __init create_mapping(struct map_desc *md)
+static void __init create_mapping(struct map_desc *md, bool force_pages)
 {
 	unsigned long addr, length, end;
 	phys_addr_t phys;
@@ -818,7 +840,7 @@ static void __init create_mapping(struct map_desc *md)
 	do {
 		unsigned long next = pgd_addr_end(addr, end);
 
-		alloc_init_pud(pgd, addr, next, phys, type);
+		alloc_init_pud(pgd, addr, next, phys, type, force_pages);
 
 		phys += next - addr;
 		addr = next;
@@ -840,7 +862,7 @@ void __init iotable_init(struct map_desc *io_desc, int nr)
 	svm = early_alloc_aligned(sizeof(*svm) * nr, __alignof__(*svm));
 
 	for (md = io_desc; nr; md++, nr--) {
-		create_mapping(md);
+		create_mapping(md, false);
 
 		vm = &svm->vm;
 		vm->addr = (void *)(md->virtual & PAGE_MASK);
@@ -961,7 +983,7 @@ void __init debug_ll_io_init(void)
 	map.virtual &= PAGE_MASK;
 	map.length = PAGE_SIZE;
 	map.type = MT_DEVICE;
-	create_mapping(&map);
+	create_mapping(&map, false);
 }
 #endif
 
@@ -1010,6 +1032,28 @@ void __init sanity_check_meminfo(void)
 		*bank = meminfo.bank[i];
 		size_limit = bank->size;
 
+#ifdef CONFIG_SPARSEMEM
+		if (pfn_to_section_nr(bank_pfn_start(bank)) !=
+		    pfn_to_section_nr(bank_pfn_end(bank) - 1)) {
+			phys_addr_t sz;
+			unsigned long start_pfn = bank_pfn_start(bank);
+			unsigned long end_pfn = SECTION_ALIGN_UP(start_pfn + 1);
+			sz = ((phys_addr_t)(end_pfn - start_pfn) << PAGE_SHIFT);
+
+			if (meminfo.nr_banks >= NR_BANKS) {
+				pr_crit("NR_BANKS too low, ignoring %lld bytes of memory\n",
+					(unsigned long long)(bank->size - sz));
+			} else {
+				memmove(bank + 1, bank,
+					(meminfo.nr_banks - i) * sizeof(*bank));
+				meminfo.nr_banks++;
+				bank[1].size -= sz;
+				bank[1].start = __pfn_to_phys(end_pfn);
+			}
+			bank->size = sz;
+		}
+#endif
+
 		if (bank->start >= vmalloc_limit)
 			highmem = 1;
 		else
@@ -1187,7 +1231,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	map.virtual = MODULES_VADDR;
 	map.length = ((unsigned long)_etext - map.virtual + ~SECTION_MASK) & SECTION_MASK;
 	map.type = MT_ROM;
-	create_mapping(&map);
+	create_mapping(&map, false);
 #endif
 
 	/*
@@ -1198,14 +1242,14 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	map.virtual = FLUSH_BASE;
 	map.length = SZ_1M;
 	map.type = MT_CACHECLEAN;
-	create_mapping(&map);
+	create_mapping(&map, false);
 #endif
 #ifdef FLUSH_BASE_MINICACHE
 	map.pfn = __phys_to_pfn(FLUSH_BASE_PHYS + SZ_1M);
 	map.virtual = FLUSH_BASE_MINICACHE;
 	map.length = SZ_1M;
 	map.type = MT_MINICLEAN;
-	create_mapping(&map);
+	create_mapping(&map, false);
 #endif
 
 	/*
@@ -1221,13 +1265,13 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 #else
 	map.type = MT_LOW_VECTORS;
 #endif
-	create_mapping(&map);
+	create_mapping(&map, false);
 
 	if (!vectors_high()) {
 		map.virtual = 0;
 		map.length = PAGE_SIZE * 2;
 		map.type = MT_LOW_VECTORS;
-		create_mapping(&map);
+		create_mapping(&map, false);
 	}
 
 	/* Now create a kernel read-only mapping */
@@ -1235,7 +1279,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 	map.virtual = 0xffff0000 + PAGE_SIZE;
 	map.length = PAGE_SIZE;
 	map.type = MT_LOW_VECTORS;
-	create_mapping(&map);
+	create_mapping(&map, false);
 
 	/*
 	 * Ask the machine support to map in the statically mapped devices.
@@ -1260,7 +1304,7 @@ static void __init devicemaps_init(struct machine_desc *mdesc)
 static void __init kmap_init(void)
 {
 #ifdef CONFIG_HIGHMEM
-	pkmap_page_table = early_pte_alloc(pmd_off_k(PKMAP_BASE),
+	pkmap_page_table = early_pte_alloc_and_install(pmd_off_k(PKMAP_BASE),
 		PKMAP_BASE, _PAGE_KERNEL_TABLE);
 
 #endif
@@ -1271,12 +1315,14 @@ static void __init kmap_init(void)
 static void __init map_lowmem(void)
 {
 	struct memblock_region *reg;
+	phys_addr_t start;
+	phys_addr_t end;
+	struct map_desc map;
 
 	/* Map all the lowmem memory banks. */
 	for_each_memblock(memory, reg) {
-		phys_addr_t start = reg->base;
-		phys_addr_t end = start + reg->size;
-		struct map_desc map;
+		start = reg->base;
+		end = start + reg->size;
 
 		if (end > arm_lowmem_limit)
 			end = arm_lowmem_limit;
@@ -1288,8 +1334,20 @@ static void __init map_lowmem(void)
 		map.length = end - start;
 		map.type = MT_MEMORY;
 
-		create_mapping(&map);
+		create_mapping(&map, false);
 	}
+
+//#ifdef CONFIG_DEBUG_RODATA
+//	start = __pa(_stext) & PMD_MASK;
+//	end = ALIGN(__pa(__end_rodata), PMD_SIZE);
+//
+//	map.pfn = __phys_to_pfn(start);
+//	map.virtual = __phys_to_virt(start);
+//	map.length = end - start;
+//	map.type = MT_MEMORY;
+//
+//	create_mapping(&map, true);
+//#endif
 }
 
 /*
diff --git a/arch/arm/mm/proc-v7-3level.S b/arch/arm/mm/proc-v7-3level.S
index 4d7f816..fdf9574 100644
--- a/arch/arm/mm/proc-v7-3level.S
+++ b/arch/arm/mm/proc-v7-3level.S
@@ -86,8 +86,13 @@ ENTRY(cpu_v7_set_pte_ext)
 	tst	rh, #1 << (57 - 32)		@ L_PTE_NONE
 	bicne	rl, #L_PTE_VALID
 	bne	1f
-	tst	rh, #1 << (55 - 32)		@ L_PTE_DIRTY
-	orreq	rl, #L_PTE_RDONLY
+
+	eor	ip, rh, #1 << (55 - 32)	@ toggle L_PTE_DIRTY in temp reg to
+					@ test for !L_PTE_DIRTY || L_PTE_RDONLY
+	tst	ip, #1 << (55 - 32) | 1 << (58 - 32)
+	orrne	rl, #PTE_AP2
+	biceq	rl, #PTE_AP2
+
 1:	strd	r2, r3, [r0]
 	ALT_SMP(W(nop))
 	ALT_UP (mcr	p15, 0, r0, c7, c10, 1)		@ flush_pte
diff --git a/arch/arm/mm/proc-v7.S b/arch/arm/mm/proc-v7.S
index c8dfdc6..658a5d3 100644
--- a/arch/arm/mm/proc-v7.S
+++ b/arch/arm/mm/proc-v7.S
@@ -92,37 +92,46 @@ ENDPROC(cpu_v7_dcache_clean_area)
 
 /* Suspend/resume support: derived from arch/arm/mach-s5pv210/sleep.S */
 .globl	cpu_v7_suspend_size
-.equ	cpu_v7_suspend_size, 4 * 8
+.equ	cpu_v7_suspend_size, 4 * 9
 #ifdef CONFIG_ARM_CPU_SUSPEND
 ENTRY(cpu_v7_do_suspend)
 	stmfd	sp!, {r4 - r10, lr}
 	mrc	p15, 0, r4, c13, c0, 0	@ FCSE/PID
 	mrc	p15, 0, r5, c13, c0, 3	@ User r/o thread ID
 	stmia	r0!, {r4 - r5}
+#ifdef CONFIG_MMU
 	mrc	p15, 0, r6, c3, c0, 0	@ Domain ID
+#ifdef CONFIG_ARM_LPAE
+	mrrc	p15, 1, r5, r7, c2	@ TTB 1
+#else
 	mrc	p15, 0, r7, c2, c0, 1	@ TTB 1
+#endif
 	mrc	p15, 0, r11, c2, c0, 2	@ TTB control register
+#endif
 	mrc	p15, 0, r8, c1, c0, 0	@ Control register
 	mrc	p15, 0, r9, c1, c0, 1	@ Auxiliary control register
 	mrc	p15, 0, r10, c1, c0, 2	@ Co-processor access control
-	stmia	r0, {r6 - r11}
+	stmia	r0, {r5 - r11}
 	ldmfd	sp!, {r4 - r10, pc}
 ENDPROC(cpu_v7_do_suspend)
 
 ENTRY(cpu_v7_do_resume)
 	mov	ip, #0
-	mcr	p15, 0, ip, c8, c7, 0	@ invalidate TLBs
 	mcr	p15, 0, ip, c7, c5, 0	@ invalidate I cache
 	mcr	p15, 0, ip, c13, c0, 1	@ set reserved context ID
 	ldmia	r0!, {r4 - r5}
 	mcr	p15, 0, r4, c13, c0, 0	@ FCSE/PID
 	mcr	p15, 0, r5, c13, c0, 3	@ User r/o thread ID
-	ldmia	r0, {r6 - r11}
+	ldmia	r0, {r5 - r11}
+#ifdef CONFIG_MMU
+	mcr	p15, 0, ip, c8, c7, 0	@ invalidate TLBs
 	mcr	p15, 0, r6, c3, c0, 0	@ Domain ID
-#ifndef CONFIG_ARM_LPAE
+#ifdef CONFIG_ARM_LPAE
+	mcrr	p15, 0, r1, ip, c2	@ TTB 0
+	mcrr	p15, 1, r5, r7, c2	@ TTB 1
+#else
 	ALT_SMP(orr	r1, r1, #TTB_FLAGS_SMP)
 	ALT_UP(orr	r1, r1, #TTB_FLAGS_UP)
-#endif
 	mcr	p15, 0, r1, c2, c0, 0	@ TTB 0
 	mcr	p15, 0, r7, c2, c0, 1	@ TTB 1
 	mcr	p15, 0, r11, c2, c0, 2	@ TTB control register
@@ -134,12 +143,18 @@ ENTRY(cpu_v7_do_resume)
 #else
 	mcrne	p15, 0, r9, c1, c0, 1	@ No, so write it
 #endif
+#endif
 1:
 	mcr	p15, 0, r10, c1, c0, 2	@ Co-processor access control
 	ldr	r4, =PRRR		@ PRRR
 	ldr	r5, =NMRR		@ NMRR
 	mcr	p15, 0, r4, c10, c2, 0	@ write PRRR
 	mcr	p15, 0, r5, c10, c2, 1	@ write NMRR
+#endif	/* CONFIG_MMU */
+	mrc	p15, 0, r4, c1, c0, 1	@ Read Auxiliary control register
+	teq	r4, r9			@ Is it already set?
+	mcrne	p15, 0, r9, c1, c0, 1	@ No, so write it
+	mcr	p15, 0, r10, c1, c0, 2	@ Co-processor access control
 	isb
 	dsb
 	mov	r0, r8			@ control register
diff --git a/drivers/Kconfig b/drivers/Kconfig
index eaea052..35834e0 100644
--- a/drivers/Kconfig
+++ b/drivers/Kconfig
@@ -168,6 +168,12 @@ source "drivers/ipack/Kconfig"
 
 source "drivers/reset/Kconfig"
 
-source "drivers/powercap/Kconfig"
+source "drivers/gator/Kconfig"
+
+source "drivers/gpu/kds/Kconfig"
+
+#source "drivers/gpu/ump/Kconfig"
+
+source "drivers/gpu/mali-t6xx/Kconfig"
 
 endmenu
diff --git a/drivers/base/Kconfig b/drivers/base/Kconfig
index 10cd80a..2f34cb6 100644
--- a/drivers/base/Kconfig
+++ b/drivers/base/Kconfig
@@ -196,6 +196,7 @@ config DMA_SHARED_BUFFER
 	bool
 	default n
 	select ANON_INODES
+	select KDS
 	help
 	  This option enables the framework for buffer-sharing between
 	  multiple drivers. A buffer is associated with a file using driver
diff --git a/drivers/base/dd.c b/drivers/base/dd.c
index 8a8d611..4e812a0 100644
--- a/drivers/base/dd.c
+++ b/drivers/base/dd.c
@@ -23,6 +23,7 @@
 #include <linux/kthread.h>
 #include <linux/wait.h>
 #include <linux/async.h>
+#include <linux/pm_domain.h>
 #include <linux/pm_runtime.h>
 #include <linux/pinctrl/devinfo.h>
 
@@ -287,6 +288,11 @@ static int really_probe(struct device *dev, struct device_driver *drv)
 
 	dev->driver = drv;
 
+	/* If using genpd, bind power domain now before probing */
+	ret = genpd_bind_domain(dev);
+	if (ret)
+		goto probe_failed;
+
 	/* If using pinctrl, bind pins now before probing */
 	ret = pinctrl_bind_pins(dev);
 	if (ret)
@@ -317,6 +323,7 @@ static int really_probe(struct device *dev, struct device_driver *drv)
 probe_failed:
 	devres_release_all(dev);
 	driver_sysfs_remove(dev);
+	genpd_unbind_domain(dev);
 	dev->driver = NULL;
 	dev_set_drvdata(dev, NULL);
 
@@ -530,7 +537,7 @@ static void __device_release_driver(struct device *dev)
 			blocking_notifier_call_chain(&dev->bus->p->bus_notifier,
 						     BUS_NOTIFY_UNBOUND_DRIVER,
 						     dev);
-
+		genpd_unbind_domain(dev);
 	}
 }
 
diff --git a/drivers/base/devres.c b/drivers/base/devres.c
index 507379e..545c4de 100644
--- a/drivers/base/devres.c
+++ b/drivers/base/devres.c
@@ -91,7 +91,8 @@ static __always_inline struct devres * alloc_dr(dr_release_t release,
 	if (unlikely(!dr))
 		return NULL;
 
-	memset(dr, 0, tot_size);
+	memset(dr, 0, offsetof(struct devres, data));
+
 	INIT_LIST_HEAD(&dr->node.entry);
 	dr->node.release = release;
 	return dr;
@@ -110,7 +111,7 @@ void * __devres_alloc(dr_release_t release, size_t size, gfp_t gfp,
 {
 	struct devres *dr;
 
-	dr = alloc_dr(release, size, gfp);
+	dr = alloc_dr(release, size, gfp | __GFP_ZERO);
 	if (unlikely(!dr))
 		return NULL;
 	set_node_dbginfo(&dr->node, name, size);
@@ -135,7 +136,7 @@ void * devres_alloc(dr_release_t release, size_t size, gfp_t gfp)
 {
 	struct devres *dr;
 
-	dr = alloc_dr(release, size, gfp);
+	dr = alloc_dr(release, size, gfp | __GFP_ZERO);
 	if (unlikely(!dr))
 		return NULL;
 	return dr->data;
@@ -745,58 +746,62 @@ void devm_remove_action(struct device *dev, void (*action)(void *), void *data)
 EXPORT_SYMBOL_GPL(devm_remove_action);
 
 /*
- * Managed kzalloc/kfree
+ * Managed kmalloc/kfree
  */
-static void devm_kzalloc_release(struct device *dev, void *res)
+static void devm_kmalloc_release(struct device *dev, void *res)
 {
 	/* noop */
 }
 
-static int devm_kzalloc_match(struct device *dev, void *res, void *data)
+static int devm_kmalloc_match(struct device *dev, void *res, void *data)
 {
 	return res == data;
 }
 
 /**
- * devm_kzalloc - Resource-managed kzalloc
+ * devm_kmalloc - Resource-managed kmalloc
  * @dev: Device to allocate memory for
  * @size: Allocation size
  * @gfp: Allocation gfp flags
  *
- * Managed kzalloc.  Memory allocated with this function is
+ * Managed kmalloc.  Memory allocated with this function is
  * automatically freed on driver detach.  Like all other devres
  * resources, guaranteed alignment is unsigned long long.
  *
  * RETURNS:
  * Pointer to allocated memory on success, NULL on failure.
  */
-void * devm_kzalloc(struct device *dev, size_t size, gfp_t gfp)
+void * devm_kmalloc(struct device *dev, size_t size, gfp_t gfp)
 {
 	struct devres *dr;
 
 	/* use raw alloc_dr for kmalloc caller tracing */
-	dr = alloc_dr(devm_kzalloc_release, size, gfp);
+	dr = alloc_dr(devm_kmalloc_release, size, gfp);
 	if (unlikely(!dr))
 		return NULL;
 
+	/*
+	 * This is named devm_kzalloc_release for historical reasons
+	 * The initial implementation did not support kmalloc, only kzalloc
+	 */
 	set_node_dbginfo(&dr->node, "devm_kzalloc_release", size);
 	devres_add(dev, dr->data);
 	return dr->data;
 }
-EXPORT_SYMBOL_GPL(devm_kzalloc);
+EXPORT_SYMBOL_GPL(devm_kmalloc);
 
 /**
  * devm_kfree - Resource-managed kfree
  * @dev: Device this memory belongs to
  * @p: Memory to free
  *
- * Free memory allocated with devm_kzalloc().
+ * Free memory allocated with devm_kmalloc().
  */
 void devm_kfree(struct device *dev, void *p)
 {
 	int rc;
 
-	rc = devres_destroy(dev, devm_kzalloc_release, devm_kzalloc_match, p);
+	rc = devres_destroy(dev, devm_kmalloc_release, devm_kmalloc_match, p);
 	WARN_ON(rc);
 }
 EXPORT_SYMBOL_GPL(devm_kfree);
diff --git a/drivers/base/dma-buf.c b/drivers/base/dma-buf.c
index 08fe897..3c04265 100644
--- a/drivers/base/dma-buf.c
+++ b/drivers/base/dma-buf.c
@@ -51,6 +51,7 @@ static int dma_buf_release(struct inode *inode, struct file *file)
 	BUG_ON(dmabuf->vmapping_counter);
 
 	dmabuf->ops->release(dmabuf);
+	kds_resource_term(&dmabuf->kds);
 
 	mutex_lock(&db_list.lock);
 	list_del(&dmabuf->list_node);
@@ -143,6 +144,8 @@ struct dma_buf *dma_buf_export_named(void *priv, const struct dma_buf_ops *ops,
 	list_add(&dmabuf->list_node, &db_list.head);
 	mutex_unlock(&db_list.lock);
 
+	kds_resource_init(&dmabuf->kds);
+
 	return dmabuf;
 }
 EXPORT_SYMBOL_GPL(dma_buf_export_named);
diff --git a/drivers/base/firmware_class.c b/drivers/base/firmware_class.c
index 00a5656..826f396 100644
--- a/drivers/base/firmware_class.c
+++ b/drivers/base/firmware_class.c
@@ -27,6 +27,7 @@
 #include <linux/pm.h>
 #include <linux/suspend.h>
 #include <linux/syscore_ops.h>
+#include <linux/reboot.h>
 
 #include <generated/utsrelease.h>
 
@@ -127,9 +128,11 @@ struct firmware_buf {
 	size_t size;
 #ifdef CONFIG_FW_LOADER_USER_HELPER
 	bool is_paged_buf;
+	bool need_uevent;
 	struct page **pages;
 	int nr_pages;
 	int page_array_size;
+	struct list_head pending_list;
 #endif
 	char fw_id[];
 };
@@ -171,6 +174,9 @@ static struct firmware_buf *__allocate_fw_buf(const char *fw_name,
 	strcpy(buf->fw_id, fw_name);
 	buf->fwc = fwc;
 	init_completion(&buf->completion);
+#ifdef CONFIG_FW_LOADER_USER_HELPER
+	INIT_LIST_HEAD(&buf->pending_list);
+#endif
 
 	pr_debug("%s: fw-%s buf=%p\n", __func__, fw_name, buf);
 
@@ -212,18 +218,6 @@ static int fw_lookup_and_allocate_buf(const char *fw_name,
 	return tmp ? 0 : -ENOMEM;
 }
 
-static struct firmware_buf *fw_lookup_buf(const char *fw_name)
-{
-	struct firmware_buf *tmp;
-	struct firmware_cache *fwc = &fw_cache;
-
-	spin_lock(&fwc->lock);
-	tmp = __fw_lookup_buf(fw_name);
-	spin_unlock(&fwc->lock);
-
-	return tmp;
-}
-
 static void __fw_free_buf(struct kref *ref)
 {
 	struct firmware_buf *buf = to_fwbuf(ref);
@@ -288,31 +282,35 @@ static noinline_for_stack long fw_file_size(struct file *file)
 	return st.size;
 }
 
-static bool fw_read_file_contents(struct file *file, struct firmware_buf *fw_buf)
+static int fw_read_file_contents(struct file *file, struct firmware_buf *fw_buf)
 {
 	long size;
 	char *buf;
+	int rc;
 
 	size = fw_file_size(file);
 	if (size <= 0)
-		return false;
+		return -EINVAL;
 	buf = vmalloc(size);
 	if (!buf)
-		return false;
-	if (kernel_read(file, 0, buf, size) != size) {
+		return -ENOMEM;
+	rc = kernel_read(file, 0, buf, size);
+	if (rc != size) {
+		if (rc > 0)
+			rc = -EIO;
 		vfree(buf);
-		return false;
+		return rc;
 	}
 	fw_buf->data = buf;
 	fw_buf->size = size;
-	return true;
+	return 0;
 }
 
-static bool fw_get_filesystem_firmware(struct device *device,
+static int fw_get_filesystem_firmware(struct device *device,
 				       struct firmware_buf *buf)
 {
 	int i;
-	bool success = false;
+	int rc = -ENOENT;
 	char *path = __getname();
 
 	for (i = 0; i < ARRAY_SIZE(fw_path); i++) {
@@ -327,14 +325,17 @@ static bool fw_get_filesystem_firmware(struct device *device,
 		file = filp_open(path, O_RDONLY, 0);
 		if (IS_ERR(file))
 			continue;
-		success = fw_read_file_contents(file, buf);
+		rc = fw_read_file_contents(file, buf);
 		fput(file);
-		if (success)
+		if (rc)
+			dev_warn(device, "firmware, attempted to load %s, but failed with error %d\n",
+				path, rc);
+		else
 			break;
 	}
 	__putname(path);
 
-	if (success) {
+	if (!rc) {
 		dev_dbg(device, "firmware: direct-loading firmware %s\n",
 			buf->fw_id);
 		mutex_lock(&fw_lock);
@@ -343,7 +344,7 @@ static bool fw_get_filesystem_firmware(struct device *device,
 		mutex_unlock(&fw_lock);
 	}
 
-	return success;
+	return rc;
 }
 
 /* firmware holds the ownership of pages */
@@ -446,10 +447,8 @@ static struct firmware_priv *to_firmware_priv(struct device *dev)
 	return container_of(dev, struct firmware_priv, dev);
 }
 
-static void fw_load_abort(struct firmware_priv *fw_priv)
+static void __fw_load_abort(struct firmware_buf *buf)
 {
-	struct firmware_buf *buf = fw_priv->buf;
-
 	/*
 	 * There is a small window in which user can write to 'loading'
 	 * between loading done and disappearance of 'loading'
@@ -457,8 +456,16 @@ static void fw_load_abort(struct firmware_priv *fw_priv)
 	if (test_bit(FW_STATUS_DONE, &buf->status))
 		return;
 
+	list_del_init(&buf->pending_list);
 	set_bit(FW_STATUS_ABORT, &buf->status);
 	complete_all(&buf->completion);
+}
+
+static void fw_load_abort(struct firmware_priv *fw_priv)
+{
+	struct firmware_buf *buf = fw_priv->buf;
+
+	__fw_load_abort(buf);
 
 	/* avoid user action after loading abort */
 	fw_priv->buf = NULL;
@@ -467,6 +474,25 @@ static void fw_load_abort(struct firmware_priv *fw_priv)
 #define is_fw_load_aborted(buf)	\
 	test_bit(FW_STATUS_ABORT, &(buf)->status)
 
+static LIST_HEAD(pending_fw_head);
+
+/* reboot notifier for avoid deadlock with usermode_lock */
+static int fw_shutdown_notify(struct notifier_block *unused1,
+			      unsigned long unused2, void *unused3)
+{
+	mutex_lock(&fw_lock);
+	while (!list_empty(&pending_fw_head))
+		__fw_load_abort(list_first_entry(&pending_fw_head,
+					       struct firmware_buf,
+					       pending_list));
+	mutex_unlock(&fw_lock);
+	return NOTIFY_DONE;
+}
+
+static struct notifier_block fw_shutdown_nb = {
+	.notifier_call = fw_shutdown_notify,
+};
+
 static ssize_t firmware_timeout_show(struct class *class,
 				     struct class_attribute *attr,
 				     char *buf)
@@ -509,8 +535,6 @@ static void fw_dev_release(struct device *dev)
 	struct firmware_priv *fw_priv = to_firmware_priv(dev);
 
 	kfree(fw_priv);
-
-	module_put(THIS_MODULE);
 }
 
 static int firmware_uevent(struct device *dev, struct kobj_uevent_env *env)
@@ -619,6 +643,7 @@ static ssize_t firmware_loading_store(struct device *dev,
 			 * is completed.
 			 * */
 			fw_map_pages_buf(fw_buf);
+			list_del_init(&fw_buf->pending_list);
 			complete_all(&fw_buf->completion);
 			break;
 		}
@@ -838,9 +863,6 @@ static int _request_firmware_load(struct firmware_priv *fw_priv, bool uevent,
 
 	dev_set_uevent_suppress(f_dev, true);
 
-	/* Need to pin this module until class device is destroyed */
-	__module_get(THIS_MODULE);
-
 	retval = device_add(f_dev);
 	if (retval) {
 		dev_err(f_dev, "%s: device_register failed\n", __func__);
@@ -853,13 +875,21 @@ static int _request_firmware_load(struct firmware_priv *fw_priv, bool uevent,
 		goto err_del_dev;
 	}
 
+	mutex_lock(&fw_lock);
+	list_add(&buf->pending_list, &pending_fw_head);
+	mutex_unlock(&fw_lock);
+
 	retval = device_create_file(f_dev, &dev_attr_loading);
 	if (retval) {
+		mutex_lock(&fw_lock);
+		list_del_init(&buf->pending_list);
+		mutex_unlock(&fw_lock);
 		dev_err(f_dev, "%s: device_create_file failed\n", __func__);
 		goto err_del_bin_attr;
 	}
 
 	if (uevent) {
+		buf->need_uevent = true;
 		dev_set_uevent_suppress(f_dev, false);
 		dev_dbg(f_dev, "firmware: requesting %s\n", buf->fw_id);
 		if (timeout != MAX_SCHEDULE_TIMEOUT)
@@ -895,6 +925,23 @@ static int fw_load_from_user_helper(struct firmware *firmware,
 	fw_priv->buf = firmware->priv;
 	return _request_firmware_load(fw_priv, uevent, timeout);
 }
+
+#ifdef CONFIG_PM_SLEEP
+/* kill pending requests without uevent to avoid blocking suspend */
+static void kill_requests_without_uevent(void)
+{
+	struct firmware_buf *buf;
+	struct firmware_buf *next;
+
+	mutex_lock(&fw_lock);
+	list_for_each_entry_safe(buf, next, &pending_fw_head, pending_list) {
+		if (!buf->need_uevent)
+			 __fw_load_abort(buf);
+	}
+	mutex_unlock(&fw_lock);
+}
+#endif
+
 #else /* CONFIG_FW_LOADER_USER_HELPER */
 static inline int
 fw_load_from_user_helper(struct firmware *firmware, const char *name,
@@ -907,6 +954,10 @@ fw_load_from_user_helper(struct firmware *firmware, const char *name,
 /* No abort during direct loading */
 #define is_fw_load_aborted(buf) false
 
+#ifdef CONFIG_PM_SLEEP
+static inline void kill_requests_without_uevent(void) { }
+#endif
+
 #endif /* CONFIG_FW_LOADER_USER_HELPER */
 
 
@@ -974,7 +1025,8 @@ _request_firmware_prepare(struct firmware **firmware_p, const char *name,
 	return 1; /* need to load */
 }
 
-static int assign_firmware_buf(struct firmware *fw, struct device *device)
+static int assign_firmware_buf(struct firmware *fw, struct device *device,
+				bool skip_cache)
 {
 	struct firmware_buf *buf = fw->priv;
 
@@ -991,7 +1043,7 @@ static int assign_firmware_buf(struct firmware *fw, struct device *device)
 	 * device may has been deleted already, but the problem
 	 * should be fixed in devres or driver core.
 	 */
-	if (device)
+	if (device && !skip_cache)
 		fw_add_devm_name(device, buf->fw_id);
 
 	/*
@@ -1047,11 +1099,18 @@ _request_firmware(const struct firmware **firmware_p, const char *name,
 		}
 	}
 
-	if (!fw_get_filesystem_firmware(device, fw->priv))
+	ret = fw_get_filesystem_firmware(device, fw->priv);
+	if (ret) {
+		dev_warn(device, "Direct firmware load failed with error %d\n",
+			 ret);
+		dev_warn(device, "Falling back to user helper\n");
 		ret = fw_load_from_user_helper(fw, name, device,
 					       uevent, nowait, timeout);
+	}
+
+	/* don't cache firmware handled without uevent */
 	if (!ret)
-		ret = assign_firmware_buf(fw, device);
+		ret = assign_firmware_buf(fw, device, !uevent);
 
 	usermodehelper_read_unlock();
 
@@ -1089,8 +1148,15 @@ int
 request_firmware(const struct firmware **firmware_p, const char *name,
                  struct device *device)
 {
-	return _request_firmware(firmware_p, name, device, true, false);
+	int ret;
+
+	/* Need to pin this module until return */
+	__module_get(THIS_MODULE);
+	ret = _request_firmware(firmware_p, name, device, true, false);
+	module_put(THIS_MODULE);
+	return ret;
 }
+EXPORT_SYMBOL(request_firmware);
 
 /**
  * release_firmware: - release the resource associated with a firmware image
@@ -1104,6 +1170,7 @@ void release_firmware(const struct firmware *fw)
 		kfree(fw);
 	}
 }
+EXPORT_SYMBOL(release_firmware);
 
 /* Async support */
 struct firmware_work {
@@ -1184,6 +1251,10 @@ request_firmware_nowait(
 	schedule_work(&fw_work->work);
 	return 0;
 }
+EXPORT_SYMBOL(request_firmware_nowait);
+
+#ifdef CONFIG_PM_SLEEP
+static ASYNC_DOMAIN_EXCLUSIVE(fw_cache_domain);
 
 /**
  * cache_firmware - cache one firmware image in kernel memory space
@@ -1199,7 +1270,7 @@ request_firmware_nowait(
  * Return !0 otherwise
  *
  */
-int cache_firmware(const char *fw_name)
+static int cache_firmware(const char *fw_name)
 {
 	int ret;
 	const struct firmware *fw;
@@ -1215,6 +1286,18 @@ int cache_firmware(const char *fw_name)
 	return ret;
 }
 
+static struct firmware_buf *fw_lookup_buf(const char *fw_name)
+{
+	struct firmware_buf *tmp;
+	struct firmware_cache *fwc = &fw_cache;
+
+	spin_lock(&fwc->lock);
+	tmp = __fw_lookup_buf(fw_name);
+	spin_unlock(&fwc->lock);
+
+	return tmp;
+}
+
 /**
  * uncache_firmware - remove one cached firmware image
  * @fw_name: the firmware image name
@@ -1226,7 +1309,7 @@ int cache_firmware(const char *fw_name)
  * Return !0 otherwise
  *
  */
-int uncache_firmware(const char *fw_name)
+static int uncache_firmware(const char *fw_name)
 {
 	struct firmware_buf *buf;
 	struct firmware fw;
@@ -1245,9 +1328,6 @@ int uncache_firmware(const char *fw_name)
 	return -EINVAL;
 }
 
-#ifdef CONFIG_PM_SLEEP
-static ASYNC_DOMAIN_EXCLUSIVE(fw_cache_domain);
-
 static struct fw_cache_entry *alloc_fw_cache_entry(const char *name)
 {
 	struct fw_cache_entry *fce;
@@ -1467,6 +1547,7 @@ static int fw_pm_notify(struct notifier_block *notify_block,
 	switch (mode) {
 	case PM_HIBERNATION_PREPARE:
 	case PM_SUSPEND_PREPARE:
+		kill_requests_without_uevent();
 		device_cache_fw_images();
 		break;
 
@@ -1529,6 +1610,7 @@ static int __init firmware_class_init(void)
 {
 	fw_cache_init();
 #ifdef CONFIG_FW_LOADER_USER_HELPER
+	register_reboot_notifier(&fw_shutdown_nb);
 	return class_register(&firmware_class);
 #else
 	return 0;
@@ -1542,15 +1624,10 @@ static void __exit firmware_class_exit(void)
 	unregister_pm_notifier(&fw_cache.pm_notify);
 #endif
 #ifdef CONFIG_FW_LOADER_USER_HELPER
+	unregister_reboot_notifier(&fw_shutdown_nb);
 	class_unregister(&firmware_class);
 #endif
 }
 
 fs_initcall(firmware_class_init);
 module_exit(firmware_class_exit);
-
-EXPORT_SYMBOL(release_firmware);
-EXPORT_SYMBOL(request_firmware);
-EXPORT_SYMBOL(request_firmware_nowait);
-EXPORT_SYMBOL_GPL(cache_firmware);
-EXPORT_SYMBOL_GPL(uncache_firmware);
diff --git a/drivers/base/power/domain.c b/drivers/base/power/domain.c
index 7072404..6397585 100644
--- a/drivers/base/power/domain.c
+++ b/drivers/base/power/domain.c
@@ -9,6 +9,7 @@
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/io.h>
+#include <linux/platform_device.h>
 #include <linux/pm_runtime.h>
 #include <linux/pm_domain.h>
 #include <linux/pm_qos.h>
@@ -2178,3 +2179,285 @@ void pm_genpd_init(struct generic_pm_domain *genpd,
 	list_add(&genpd->gpd_list_node, &gpd_list);
 	mutex_unlock(&gpd_list_lock);
 }
+
+#ifdef CONFIG_PM_GENERIC_DOMAINS_OF
+/*
+ * Device Tree based power domain providers.
+ *
+ * The code below implements generic device tree based power domain providers
+ * that bind device tree nodes with generic power domains registered in the
+ * system.
+ *
+ * Any driver that registers generic power domains and need to support binding
+ * of devices to these domains is supposed to register a power domain provider,
+ * which maps a power domain specifier retrieved from device tree to a power
+ * domain.
+ *
+ * Two simple mapping functions have been provided for convenience:
+ *  - of_genpd_xlate_simple() for 1:1 device tree node to domain mapping,
+ *  - of_genpd_xlate_onecell() for mapping of multiple domains per node
+ *    by index.
+ */
+
+/**
+ * struct of_genpd_provider - Power domain provider registration structure
+ * @link: Entry in global list of domain providers
+ * @node: Pointer to device tree node of domain provider
+ * @xlate: Provider-specific xlate callback mapping a set of specifier cells
+ *         into a power domain.
+ * @data: context pointer to be passed into @xlate callback
+ */
+struct of_genpd_provider {
+	struct list_head link;
+
+	struct device_node *node;
+	genpd_xlate_t xlate;
+	void *data;
+};
+
+/* List of registered power domain providers. */
+static LIST_HEAD(of_genpd_providers);
+/* Mutex to protect the list above. */
+static DEFINE_MUTEX(of_genpd_mutex);
+
+/**
+ * of_genpd_xlate_simple() - Xlate function for direct node-domain mapping
+ * @genpdspec: OF phandle args to map into a power domain
+ * @data: xlate function private data - pointer to struct generic_pm_domain
+ *
+ * This is a generic xlate function that can be used to model power domains
+ * that have their own device tree nodes. The private data of xlate function
+ * needs to be a valid pointer to struct generic_pm_domain.
+ */
+struct generic_pm_domain *of_genpd_xlate_simple(
+					struct of_phandle_args *genpdspec,
+					void *data)
+{
+	if (genpdspec->args_count != 0)
+		return ERR_PTR(-EINVAL);
+	return data;
+}
+EXPORT_SYMBOL_GPL(of_genpd_xlate_simple);
+
+/**
+ * of_genpd_xlate_onecell() - Xlate function for providers using single index.
+ * @genpdspec: OF phandle args to map into a power domain
+ * @data: xlate function private data - pointer to struct genpd_onecell_data
+ *
+ * This is a generic xlate function that can be used to model simple power
+ * domain controllers that have one device tree node and provide multiple
+ * power domains. A single cell is used as an index to an array of power
+ * domains specified in genpd_onecell_data struct when registering the
+ * provider.
+ */
+struct generic_pm_domain *of_genpd_xlate_onecell(
+					struct of_phandle_args *genpdspec,
+					void *data)
+{
+	struct genpd_onecell_data *genpd_data = data;
+	unsigned int idx = genpdspec->args[0];
+
+	if (genpdspec->args_count != 1)
+		return ERR_PTR(-EINVAL);
+
+	if (idx >= genpd_data->domain_num) {
+		pr_err("%s: invalid domain index %d\n", __func__, idx);
+		return ERR_PTR(-EINVAL);
+	}
+
+	return genpd_data->domains[idx];
+}
+EXPORT_SYMBOL_GPL(of_genpd_xlate_onecell);
+
+/**
+ * of_genpd_add_provider() - Register a domain provider for a node
+ * @np: Device node pointer associated with domain provider.
+ * @xlate: Callback for decoding domain from phandle arguments.
+ * @data: Context pointer for @genpd_src_get callback.
+ */
+int of_genpd_add_provider(struct device_node *np, genpd_xlate_t xlate,
+			  void *data)
+{
+	struct of_genpd_provider *cp;
+
+	cp = kzalloc(sizeof(*cp), GFP_KERNEL);
+	if (!cp)
+		return -ENOMEM;
+
+	cp->node = of_node_get(np);
+	cp->data = data;
+	cp->xlate = xlate;
+
+	mutex_lock(&of_genpd_mutex);
+	list_add(&cp->link, &of_genpd_providers);
+	mutex_unlock(&of_genpd_mutex);
+	pr_debug("Added domain provider from %s\n", np->full_name);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(of_genpd_add_provider);
+
+/**
+ * of_genpd_del_provider() - Remove a previously registered domain provider
+ * @np: Device node pointer associated with domain provider
+ */
+void of_genpd_del_provider(struct device_node *np)
+{
+	struct of_genpd_provider *cp;
+
+	mutex_lock(&of_genpd_mutex);
+	list_for_each_entry(cp, &of_genpd_providers, link) {
+		if (cp->node == np) {
+			list_del(&cp->link);
+			of_node_put(cp->node);
+			kfree(cp);
+			break;
+		}
+	}
+	mutex_unlock(&of_genpd_mutex);
+}
+EXPORT_SYMBOL_GPL(of_genpd_del_provider);
+
+/**
+ * of_genpd_get_from_provider() - Look-up power domain
+ * @genpdspec: OF phandle args to use for look-up
+ *
+ * Looks for domain provider under node specified by @genpdspec and if found
+ * uses xlate function of the provider to map phandle args to a power domain.
+ *
+ * Returns a valid pointer to struct generic_pm_domain on success or ERR_PTR()
+ * on failure.
+ */
+static struct generic_pm_domain *of_genpd_get_from_provider(
+					struct of_phandle_args *genpdspec)
+{
+	struct generic_pm_domain *genpd = ERR_PTR(-EPROBE_DEFER);
+	struct of_genpd_provider *provider;
+
+	mutex_lock(&of_genpd_mutex);
+
+	/* Check if we have such a provider in our array */
+	list_for_each_entry(provider, &of_genpd_providers, link) {
+		if (provider->node == genpdspec->np)
+			genpd = provider->xlate(genpdspec, provider->data);
+		if (!IS_ERR(genpd))
+			break;
+	}
+
+	mutex_unlock(&of_genpd_mutex);
+
+	return genpd;
+}
+
+/*
+ * Device<->domain binding using Device Tree look-up.
+ *
+ * The purpose of code below is to manage assignment of devices to their
+ * power domains in an automatic fashion, based on data read from device tree.
+ * The two functions, genpd_bind_domain() and genpd_unbind_domain() are
+ * intended to be called by higher level code that manages devices, i.e.
+ * really_probe() and __device_release_driver() to respectively bind and
+ * unbind device from its power domain.
+ *
+ * Both generic and legacy Samsung-specific DT bindings are supported to
+ * keep backwards compatibility with existing DTBs.
+ */
+
+/**
+ * genpd_bind_domain - Bind device to its power domain using Device Tree.
+ * @dev: Device to bind to its power domain.
+ *
+ * Tries to parse power domain specifier from device's OF node and if succeeds
+ * attaches the device to retrieved power domain.
+ *
+ * Returns 0 on success or negative error code otherwise.
+ */
+int genpd_bind_domain(struct device *dev)
+{
+	struct of_phandle_args pd_args;
+	struct generic_pm_domain *pd;
+	int ret;
+
+	if (!dev->of_node)
+		return 0;
+
+	ret = of_parse_phandle_with_args(dev->of_node, "power-domains",
+					"#power-domain-cells", 0, &pd_args);
+	if (ret < 0) {
+		if (ret != -ENOENT)
+			return ret;
+
+		/*
+		 * Try legacy Samsung-specific bindings
+		 * (for backwards compatibility of DT ABI)
+		 */
+		pd_args.args_count = 0;
+		pd_args.np = of_parse_phandle(dev->of_node,
+						"samsung,power-domain", 0);
+		if (!pd_args.np)
+			return 0;
+	}
+
+	pd = of_genpd_get_from_provider(&pd_args);
+	if (IS_ERR(pd)) {
+		if (PTR_ERR(pd) != -EPROBE_DEFER)
+			dev_err(dev, "failed to find power domain: %ld\n",
+				PTR_ERR(pd));
+		return PTR_ERR(pd);
+	}
+
+	dev_dbg(dev, "adding to power domain %s\n", pd->name);
+
+	while (1) {
+		ret = pm_genpd_add_device(pd, dev);
+		if (ret != -EAGAIN)
+			break;
+		cond_resched();
+	}
+
+	if (ret < 0) {
+		dev_err(dev, "failed to add to power domain %s: %d",
+			pd->name, ret);
+		return ret;
+	}
+
+	return 0;
+}
+
+/**
+ * genpd_unbind_domain - Unbind device from its power domain.
+ * @dev: Device to unbind from its power domain.
+ *
+ * Unbinds device from power domain previously bound to it.
+ *
+ * Returns 0 on success or negative error code otherwise.
+ */
+int genpd_unbind_domain(struct device *dev)
+{
+	struct generic_pm_domain *pd = dev_to_genpd(dev);
+	int ret;
+
+	if (!dev->of_node || IS_ERR(pd))
+		return 0;
+
+	dev_dbg(dev, "removing from power domain %s\n", pd->name);
+
+	while (1) {
+		ret = pm_genpd_remove_device(pd, dev);
+		if (ret != -EAGAIN)
+			break;
+		cond_resched();
+	}
+
+	if (ret < 0) {
+		dev_err(dev, "failed to remove from power domain %s: %d",
+			pd->name, ret);
+		return ret;
+	}
+
+	/* Check if domain can be powered off after removing this device. */
+	genpd_queue_power_off_work(pd);
+
+	return 0;
+}
+#endif
diff --git a/drivers/base/power/main.c b/drivers/base/power/main.c
index 5a9b656..1eeee09 100644
--- a/drivers/base/power/main.c
+++ b/drivers/base/power/main.c
@@ -28,7 +28,11 @@
 #include <linux/sched.h>
 #include <linux/async.h>
 #include <linux/suspend.h>
+#include <trace/events/power.h>
+#include <linux/cpufreq.h>
 #include <linux/cpuidle.h>
+#include <linux/timer.h>
+
 #include "../base.h"
 #include "power.h"
 
@@ -54,6 +58,12 @@ struct suspend_stats suspend_stats;
 static DEFINE_MUTEX(dpm_list_mtx);
 static pm_message_t pm_transition;
 
+struct dpm_watchdog {
+	struct device		*dev;
+	struct task_struct	*tsk;
+	struct timer_list	timer;
+};
+
 static int async_error;
 
 /**
@@ -384,6 +394,56 @@ static int dpm_run_callback(pm_callback_t cb, struct device *dev,
 	return error;
 }
 
+/**
+ * dpm_wd_handler - Driver suspend / resume watchdog handler.
+ *
+ * Called when a driver has timed out suspending or resuming.
+ * There's not much we can do here to recover so BUG() out for
+ * a crash-dump
+ */
+static void dpm_wd_handler(unsigned long data)
+{
+	struct dpm_watchdog *wd = (void *)data;
+	struct device *dev      = wd->dev;
+	struct task_struct *tsk = wd->tsk;
+
+	dev_emerg(dev, "**** DPM device timeout ****\n");
+	show_stack(tsk, NULL);
+
+	BUG();
+}
+
+/**
+ * dpm_wd_set - Enable pm watchdog for given device.
+ * @wd: Watchdog. Must be allocated on the stack.
+ * @dev: Device to handle.
+ */
+static void dpm_wd_set(struct dpm_watchdog *wd, struct device *dev)
+{
+	struct timer_list *timer = &wd->timer;
+
+	wd->dev = dev;
+	wd->tsk = get_current();
+
+	init_timer_on_stack(timer);
+	timer->expires = jiffies + HZ * 12;
+	timer->function = dpm_wd_handler;
+	timer->data = (unsigned long)wd;
+	add_timer(timer);
+}
+
+/**
+ * dpm_wd_clear - Disable pm watchdog.
+ * @wd: Watchdog to disable.
+ */
+static void dpm_wd_clear(struct dpm_watchdog *wd)
+{
+	struct timer_list *timer = &wd->timer;
+
+	del_timer_sync(timer);
+	destroy_timer_on_stack(timer);
+}
+
 /*------------------------- Resume routines -------------------------*/
 
 /**
@@ -570,6 +630,7 @@ static int device_resume(struct device *dev, pm_message_t state, bool async)
 	pm_callback_t callback = NULL;
 	char *info = NULL;
 	int error = 0;
+	struct dpm_watchdog wd;
 
 	TRACE_DEVICE(dev);
 	TRACE_RESUME(0);
@@ -585,6 +646,7 @@ static int device_resume(struct device *dev, pm_message_t state, bool async)
 	 * a resumed device, even if the device hasn't been completed yet.
 	 */
 	dev->power.is_prepared = false;
+	dpm_wd_set(&wd, dev);
 
 	if (!dev->power.is_suspended)
 		goto Unlock;
@@ -636,6 +698,7 @@ static int device_resume(struct device *dev, pm_message_t state, bool async)
 
  Unlock:
 	device_unlock(dev);
+	dpm_wd_clear(&wd);
 
  Complete:
 	complete_all(&dev->power.completion);
@@ -713,6 +776,8 @@ void dpm_resume(pm_message_t state)
 	mutex_unlock(&dpm_list_mtx);
 	async_synchronize_full();
 	dpm_show_time(starttime, state, NULL);
+
+	cpufreq_resume();
 }
 
 /**
@@ -1053,6 +1118,7 @@ static int __device_suspend(struct device *dev, pm_message_t state, bool async)
 	pm_callback_t callback = NULL;
 	char *info = NULL;
 	int error = 0;
+	struct dpm_watchdog wd;
 
 	dpm_wait_for_children(dev, async);
 
@@ -1076,6 +1142,8 @@ static int __device_suspend(struct device *dev, pm_message_t state, bool async)
 	if (dev->power.syscore)
 		goto Complete;
 
+	dpm_wd_set(&wd, dev);
+
 	device_lock(dev);
 
 	if (dev->pm_domain) {
@@ -1131,6 +1199,8 @@ static int __device_suspend(struct device *dev, pm_message_t state, bool async)
 
 	device_unlock(dev);
 
+	dpm_wd_clear(&wd);
+
  Complete:
 	complete_all(&dev->power.completion);
 	if (error)
@@ -1177,6 +1247,8 @@ int dpm_suspend(pm_message_t state)
 
 	might_sleep();
 
+	cpufreq_suspend();
+
 	mutex_lock(&dpm_list_mtx);
 	pm_transition = state;
 	async_error = 0;
diff --git a/drivers/base/regmap/regmap.c b/drivers/base/regmap/regmap.c
index 278c903..fd7cb92 100644
--- a/drivers/base/regmap/regmap.c
+++ b/drivers/base/regmap/regmap.c
@@ -966,7 +966,8 @@ int _regmap_raw_write(struct regmap *map, unsigned int reg,
 		unsigned int ival;
 		int val_bytes = map->format.val_bytes;
 		for (i = 0; i < val_len / val_bytes; i++) {
-			ival = map->format.parse_val(val + (i * val_bytes));
+			ival = map->format.parse_val((void *)
+						(val + (i * val_bytes)));
 			ret = regcache_write(map, reg + (i * map->reg_stride),
 					     ival);
 			if (ret) {
diff --git a/drivers/bus/Kconfig b/drivers/bus/Kconfig
index b05ecab..5286e2d 100644
--- a/drivers/bus/Kconfig
+++ b/drivers/bus/Kconfig
@@ -26,4 +26,11 @@ config OMAP_INTERCONNECT
 
 	help
 	  Driver to enable OMAP interconnect error handling driver.
+
+config ARM_CCI
+	bool "ARM CCI driver support"
+	depends on ARM
+	help
+	  Driver supporting the CCI cache coherent interconnect for ARM
+	  platforms.
 endmenu
diff --git a/drivers/bus/Makefile b/drivers/bus/Makefile
index 3c7b53c..670cea4 100644
--- a/drivers/bus/Makefile
+++ b/drivers/bus/Makefile
@@ -7,3 +7,5 @@ obj-$(CONFIG_OMAP_OCP2SCP)	+= omap-ocp2scp.o
 
 # Interconnect bus driver for OMAP SoCs.
 obj-$(CONFIG_OMAP_INTERCONNECT)	+= omap_l3_smx.o omap_l3_noc.o
+# CCI cache coherent interconnect for ARM platforms
+obj-$(CONFIG_ARM_CCI)		+= arm-cci.o
diff --git a/drivers/bus/arm-cci.c b/drivers/bus/arm-cci.c
new file mode 100644
index 0000000..9d2254c
--- /dev/null
+++ b/drivers/bus/arm-cci.c
@@ -0,0 +1,948 @@
+/*
+ * CCI cache coherent interconnect driver
+ *
+ * Copyright (C) 2013 ARM Ltd.
+ * Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed "as is" WITHOUT ANY WARRANTY of any
+ * kind, whether express or implied; without even the implied warranty
+ * of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/arm-cci.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/of_address.h>
+#include <linux/slab.h>
+
+#include <asm/cacheflush.h>
+#include <asm/irq_regs.h>
+#include <asm/pmu.h>
+#include <asm/smp_plat.h>
+
+#define DRIVER_NAME		"CCI"
+
+#define CCI_PORT_CTRL		0x0
+#define CCI_CTRL_STATUS		0xc
+
+#define CCI_ENABLE_SNOOP_REQ	0x1
+#define CCI_ENABLE_DVM_REQ	0x2
+#define CCI_ENABLE_REQ		(CCI_ENABLE_SNOOP_REQ | CCI_ENABLE_DVM_REQ)
+
+struct cci_nb_ports {
+	unsigned int nb_ace;
+	unsigned int nb_ace_lite;
+};
+
+enum cci_ace_port_type {
+	ACE_INVALID_PORT = 0x0,
+	ACE_PORT,
+	ACE_LITE_PORT,
+};
+
+struct cci_ace_port {
+	void __iomem *base;
+	unsigned long phys;
+	enum cci_ace_port_type type;
+	struct device_node *dn;
+};
+
+static struct cci_ace_port *ports;
+static unsigned int nb_cci_ports;
+
+static void __iomem *cci_ctrl_base;
+static unsigned long cci_ctrl_phys;
+
+#ifdef CONFIG_HW_PERF_EVENTS
+
+static void __iomem *cci_pmu_base;
+
+#define CCI400_PMCR		0x0100
+
+#define CCI400_PMU_CYCLE_CNTR_BASE    0x0000
+#define CCI400_PMU_CNTR_BASE(idx)     (CCI400_PMU_CYCLE_CNTR_BASE + (idx) * 0x1000)
+
+#define CCI400_PMCR_CEN          0x00000001
+#define CCI400_PMCR_RST          0x00000002
+#define CCI400_PMCR_CCR          0x00000004
+#define CCI400_PMCR_CCD          0x00000008
+#define CCI400_PMCR_EX           0x00000010
+#define CCI400_PMCR_DP           0x00000020
+#define CCI400_PMCR_NCNT_MASK    0x0000F800
+#define CCI400_PMCR_NCNT_SHIFT   11
+
+#define CCI400_PMU_EVT_SEL       0x000
+#define CCI400_PMU_CNTR          0x004
+#define CCI400_PMU_CNTR_CTRL     0x008
+#define CCI400_PMU_OVERFLOW      0x00C
+
+#define CCI400_PMU_OVERFLOW_FLAG 1
+
+enum cci400_perf_events {
+	CCI400_PMU_CYCLES = 0xFF
+};
+
+#define CCI400_PMU_EVENT_MASK   0xff
+#define CCI400_PMU_EVENT_SOURCE(event) ((event >> 5) & 0x7)
+#define CCI400_PMU_EVENT_CODE(event) (event & 0x1f)
+
+#define CCI400_PMU_EVENT_SOURCE_S0 0
+#define CCI400_PMU_EVENT_SOURCE_S4 4
+#define CCI400_PMU_EVENT_SOURCE_M0 5
+#define CCI400_PMU_EVENT_SOURCE_M2 7
+
+#define CCI400_PMU_EVENT_SLAVE_MIN 0x0
+#define CCI400_PMU_EVENT_SLAVE_MAX 0x13
+
+#define CCI400_PMU_EVENT_MASTER_MIN 0x14
+#define CCI400_PMU_EVENT_MASTER_MAX 0x1A
+
+#define CCI400_PMU_MAX_HW_EVENTS 5   /* CCI PMU has 4 counters + 1 cycle counter */
+
+#define CCI400_PMU_CYCLE_COUNTER_IDX 0
+#define CCI400_PMU_COUNTER0_IDX      1
+#define CCI400_PMU_COUNTER_LAST(cci_pmu) (CCI400_PMU_CYCLE_COUNTER_IDX + cci_pmu->num_events - 1)
+
+
+static struct perf_event *events[CCI400_PMU_MAX_HW_EVENTS];
+static unsigned long used_mask[BITS_TO_LONGS(CCI400_PMU_MAX_HW_EVENTS)];
+static struct pmu_hw_events cci_hw_events = {
+	.events    = events,
+	.used_mask = used_mask,
+};
+
+static int cci_pmu_validate_hw_event(u8 hw_event)
+{
+	u8 ev_source = CCI400_PMU_EVENT_SOURCE(hw_event);
+	u8 ev_code = CCI400_PMU_EVENT_CODE(hw_event);
+
+	if (ev_source <= CCI400_PMU_EVENT_SOURCE_S4 &&
+	    ev_code <= CCI400_PMU_EVENT_SLAVE_MAX)
+			return hw_event;
+	else if (CCI400_PMU_EVENT_SOURCE_M0 <= ev_source &&
+		   ev_source <= CCI400_PMU_EVENT_SOURCE_M2 &&
+		   CCI400_PMU_EVENT_MASTER_MIN <= ev_code &&
+		    ev_code <= CCI400_PMU_EVENT_MASTER_MAX)
+			return hw_event;
+
+	return -EINVAL;
+}
+
+static inline int cci_pmu_counter_is_valid(struct arm_pmu *cci_pmu, int idx)
+{
+	return CCI400_PMU_CYCLE_COUNTER_IDX <= idx &&
+		idx <= CCI400_PMU_COUNTER_LAST(cci_pmu);
+}
+
+static inline u32 cci_pmu_read_register(int idx, unsigned int offset)
+{
+	return readl_relaxed(cci_pmu_base + CCI400_PMU_CNTR_BASE(idx) + offset);
+}
+
+static inline void cci_pmu_write_register(u32 value, int idx, unsigned int offset)
+{
+	return writel_relaxed(value, cci_pmu_base + CCI400_PMU_CNTR_BASE(idx) + offset);
+}
+
+static inline void cci_pmu_disable_counter(int idx)
+{
+	cci_pmu_write_register(0, idx, CCI400_PMU_CNTR_CTRL);
+}
+
+static inline void cci_pmu_enable_counter(int idx)
+{
+	cci_pmu_write_register(1, idx, CCI400_PMU_CNTR_CTRL);
+}
+
+static inline void cci_pmu_select_event(int idx, unsigned long event)
+{
+	event &= CCI400_PMU_EVENT_MASK;
+	cci_pmu_write_register(event, idx, CCI400_PMU_EVT_SEL);
+}
+
+static u32 cci_pmu_get_max_counters(void)
+{
+	u32 n_cnts = (readl_relaxed(cci_ctrl_base + CCI400_PMCR) &
+		      CCI400_PMCR_NCNT_MASK) >> CCI400_PMCR_NCNT_SHIFT;
+
+	/* add 1 for cycle counter */
+	return n_cnts + 1;
+}
+
+static struct pmu_hw_events *cci_pmu_get_hw_events(void)
+{
+	return &cci_hw_events;
+}
+
+static int cci_pmu_get_event_idx(struct pmu_hw_events *hw, struct perf_event *event)
+{
+	struct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);
+	struct hw_perf_event *hw_event = &event->hw;
+	unsigned long cci_event = hw_event->config_base & CCI400_PMU_EVENT_MASK;
+	int idx;
+
+	if (cci_event == CCI400_PMU_CYCLES) {
+		if (test_and_set_bit(CCI400_PMU_CYCLE_COUNTER_IDX, hw->used_mask))
+			return -EAGAIN;
+
+                return CCI400_PMU_CYCLE_COUNTER_IDX;
+        }
+
+	for (idx = CCI400_PMU_COUNTER0_IDX; idx <= CCI400_PMU_COUNTER_LAST(cci_pmu); ++idx) {
+		if (!test_and_set_bit(idx, hw->used_mask))
+			return idx;
+	}
+
+	/* No counters available */
+	return -EAGAIN;
+}
+
+static int cci_pmu_map_event(struct perf_event *event)
+{
+	int mapping;
+	u8 config = event->attr.config & CCI400_PMU_EVENT_MASK;
+
+	if (event->attr.type < PERF_TYPE_MAX)
+		return -ENOENT;
+
+	/* 0xff is used to represent CCI Cycles */
+	if (config == 0xff)
+		mapping = config;
+	else
+		mapping = cci_pmu_validate_hw_event(config);
+
+	return mapping;
+}
+
+static int cci_pmu_request_irq(struct arm_pmu *cci_pmu, irq_handler_t handler)
+{
+	int irq, err, i = 0;
+	struct platform_device *pmu_device = cci_pmu->plat_device;
+
+	if (unlikely(!pmu_device))
+		return -ENODEV;
+
+	/* CCI exports 6 interrupts - 1 nERRORIRQ + 5 nEVNTCNTOVERFLOW (PMU)
+	   nERRORIRQ will be handled by secure firmware on TC2. So we
+	   assume that all CCI interrupts listed in the linux device
+	   tree are PMU interrupts.
+
+	   The following code should then be able to handle different routing
+	   of the CCI PMU interrupts.
+	*/
+	while ((irq = platform_get_irq(pmu_device, i)) > 0) {
+		err = request_irq(irq, handler, 0, "arm-cci-pmu", cci_pmu);
+		if (err) {
+			dev_err(&pmu_device->dev, "unable to request IRQ%d for ARM CCI PMU counters\n",
+				irq);
+			return err;
+		}
+		i++;
+	}
+
+	return 0;
+}
+
+static irqreturn_t cci_pmu_handle_irq(int irq_num, void *dev)
+{
+	struct arm_pmu *cci_pmu = (struct arm_pmu *)dev;
+	struct pmu_hw_events *events = cci_pmu->get_hw_events();
+	struct perf_sample_data data;
+	struct pt_regs *regs;
+	int idx;
+
+	regs = get_irq_regs();
+
+	/* Iterate over counters and update the corresponding perf events.
+	   This should work regardless of whether we have per-counter overflow
+	   interrupt or a combined overflow interrupt. */
+	for (idx = CCI400_PMU_CYCLE_COUNTER_IDX; idx <= CCI400_PMU_COUNTER_LAST(cci_pmu); idx++) {
+		struct perf_event *event = events->events[idx];
+		struct hw_perf_event *hw_counter;
+
+		if (!event)
+			continue;
+
+		hw_counter = &event->hw;
+
+		/* Did this counter overflow? */
+		if (!(cci_pmu_read_register(idx, CCI400_PMU_OVERFLOW) & CCI400_PMU_OVERFLOW_FLAG))
+			continue;
+		cci_pmu_write_register(CCI400_PMU_OVERFLOW_FLAG, idx, CCI400_PMU_OVERFLOW);
+
+		armpmu_event_update(event);
+		perf_sample_data_init(&data, 0, hw_counter->last_period);
+		if (!armpmu_event_set_period(event))
+			continue;
+
+		if (perf_event_overflow(event, &data, regs))
+			cci_pmu->disable(event);
+	}
+
+	irq_work_run();
+	return IRQ_HANDLED;
+}
+
+static void cci_pmu_free_irq(struct arm_pmu *cci_pmu)
+{
+	int irq, i = 0;
+	struct platform_device *pmu_device = cci_pmu->plat_device;
+
+	while ((irq = platform_get_irq(pmu_device, i)) > 0) {
+		free_irq(irq, cci_pmu);
+		i++;
+	}
+}
+
+static void cci_pmu_enable_event(struct perf_event *event)
+{
+	unsigned long flags;
+	struct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);
+	struct pmu_hw_events *events = cci_pmu->get_hw_events();
+	struct hw_perf_event *hw_counter = &event->hw;
+	int idx = hw_counter->idx;
+
+	if (unlikely(!cci_pmu_counter_is_valid(cci_pmu, idx))) {
+		dev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);
+		return;
+	}
+
+	raw_spin_lock_irqsave(&events->pmu_lock, flags);
+
+	/* Configure the event to count, unless you are counting cycles */
+	if (idx != CCI400_PMU_CYCLE_COUNTER_IDX)
+		cci_pmu_select_event(idx, hw_counter->config_base);
+
+	cci_pmu_enable_counter(idx);
+
+	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
+}
+
+static void cci_pmu_disable_event(struct perf_event *event)
+{
+	unsigned long flags;
+	struct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);
+	struct pmu_hw_events *events = cci_pmu->get_hw_events();
+	struct hw_perf_event *hw_counter = &event->hw;
+	int idx = hw_counter->idx;
+
+	if (unlikely(!cci_pmu_counter_is_valid(cci_pmu, idx))) {
+		dev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);
+		return;
+	}
+
+	raw_spin_lock_irqsave(&events->pmu_lock, flags);
+
+	cci_pmu_disable_counter(idx);
+
+	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
+}
+
+static void cci_pmu_start(struct arm_pmu *cci_pmu)
+{
+	u32 val;
+	unsigned long flags;
+	struct pmu_hw_events *events = cci_pmu->get_hw_events();
+
+	raw_spin_lock_irqsave(&events->pmu_lock, flags);
+
+	/* Enable all the PMU counters. */
+	val = readl(cci_ctrl_base + CCI400_PMCR) | CCI400_PMCR_CEN;
+	writel(val, cci_ctrl_base + CCI400_PMCR);
+
+	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
+}
+
+static void cci_pmu_stop(struct arm_pmu *cci_pmu)
+{
+	u32 val;
+	unsigned long flags;
+	struct pmu_hw_events *events = cci_pmu->get_hw_events();
+
+	raw_spin_lock_irqsave(&events->pmu_lock, flags);
+
+	/* Disable all the PMU counters. */
+	val = readl(cci_ctrl_base + CCI400_PMCR) & ~CCI400_PMCR_CEN;
+	writel(val, cci_ctrl_base + CCI400_PMCR);
+
+	raw_spin_unlock_irqrestore(&events->pmu_lock, flags);
+}
+
+static u32 cci_pmu_read_counter(struct perf_event *event)
+{
+	struct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);
+	struct hw_perf_event *hw_counter = &event->hw;
+	int idx = hw_counter->idx;
+	u32 value;
+
+	if (unlikely(!cci_pmu_counter_is_valid(cci_pmu, idx))) {
+		dev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);
+		return 0;
+	}
+	value = cci_pmu_read_register(idx, CCI400_PMU_CNTR);
+
+	return value;
+}
+
+static void cci_pmu_write_counter(struct perf_event *event, u32 value)
+{
+	struct arm_pmu *cci_pmu = to_arm_pmu(event->pmu);
+	struct hw_perf_event *hw_counter = &event->hw;
+	int idx = hw_counter->idx;
+
+	if (unlikely(!cci_pmu_counter_is_valid(cci_pmu, idx)))
+		dev_err(&cci_pmu->plat_device->dev, "Invalid CCI PMU counter %d\n", idx);
+	else
+		cci_pmu_write_register(value, idx, CCI400_PMU_CNTR);
+}
+
+static struct arm_pmu cci_pmu = {
+	.name             = DRIVER_NAME,
+	.max_period       = (1LLU << 32) - 1,
+	.get_hw_events    = cci_pmu_get_hw_events,
+	.get_event_idx    = cci_pmu_get_event_idx,
+	.map_event        = cci_pmu_map_event,
+	.request_irq      = cci_pmu_request_irq,
+	.handle_irq       = cci_pmu_handle_irq,
+	.free_irq         = cci_pmu_free_irq,
+	.enable           = cci_pmu_enable_event,
+	.disable          = cci_pmu_disable_event,
+	.start            = cci_pmu_start,
+	.stop             = cci_pmu_stop,
+	.read_counter     = cci_pmu_read_counter,
+	.write_counter    = cci_pmu_write_counter,
+};
+
+static int cci_pmu_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	cci_pmu_base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(cci_pmu_base))
+		return PTR_ERR(cci_pmu_base);
+
+	cci_pmu.plat_device = pdev;
+	cci_pmu.num_events = cci_pmu_get_max_counters();
+	raw_spin_lock_init(&cci_hw_events.pmu_lock);
+	cpumask_setall(&cci_pmu.valid_cpus);
+
+	return armpmu_register(&cci_pmu, -1);
+}
+
+static const struct of_device_id arm_cci_pmu_matches[] = {
+	{.compatible = "arm,cci-400-pmu"},
+	{},
+};
+
+static struct platform_driver cci_pmu_platform_driver = {
+	.driver = {
+		   .name = DRIVER_NAME,
+		   .of_match_table = arm_cci_pmu_matches,
+		  },
+	.probe = cci_pmu_probe,
+};
+
+static int __init cci_pmu_init(void)
+{
+	if (platform_driver_register(&cci_pmu_platform_driver))
+		WARN(1, "unable to register CCI platform driver\n");
+	return 0;
+}
+
+#else
+
+static int __init cci_pmu_init(void)
+{
+	return 0;
+}
+
+#endif /* CONFIG_HW_PERF_EVENTS */
+
+struct cpu_port {
+	u64 mpidr;
+	u32 port;
+};
+
+/*
+ * Use the port MSB as valid flag, shift can be made dynamic
+ * by computing number of bits required for port indexes.
+ * Code disabling CCI cpu ports runs with D-cache invalidated
+ * and SCTLR bit clear so data accesses must be kept to a minimum
+ * to improve performance; for now shift is left static to
+ * avoid one more data access while disabling the CCI port.
+ */
+#define PORT_VALID_SHIFT	31
+#define PORT_VALID		(0x1 << PORT_VALID_SHIFT)
+
+static inline void init_cpu_port(struct cpu_port *port, u32 index, u64 mpidr)
+{
+	port->port = PORT_VALID | index;
+	port->mpidr = mpidr;
+}
+
+static inline bool cpu_port_is_valid(struct cpu_port *port)
+{
+	return !!(port->port & PORT_VALID);
+}
+
+static inline bool cpu_port_match(struct cpu_port *port, u64 mpidr)
+{
+	return port->mpidr == (mpidr & MPIDR_HWID_BITMASK);
+}
+
+static struct cpu_port cpu_port[NR_CPUS];
+
+/**
+ * __cci_ace_get_port - Function to retrieve the port index connected to
+ *			a cpu or device.
+ *
+ * @dn: device node of the device to look-up
+ * @type: port type
+ *
+ * Return value:
+ *	- CCI port index if success
+ *	- -ENODEV if failure
+ */
+static int __cci_ace_get_port(struct device_node *dn, int type)
+{
+	int i;
+	bool ace_match;
+	struct device_node *cci_portn;
+
+	cci_portn = of_parse_phandle(dn, "cci-control-port", 0);
+	for (i = 0; i < nb_cci_ports; i++) {
+		ace_match = ports[i].type == type;
+		if (ace_match && cci_portn == ports[i].dn)
+			return i;
+	}
+	return -ENODEV;
+}
+
+int cci_ace_get_port(struct device_node *dn)
+{
+	return __cci_ace_get_port(dn, ACE_LITE_PORT);
+}
+EXPORT_SYMBOL_GPL(cci_ace_get_port);
+
+static void __init cci_ace_init_ports(void)
+{
+	int port, ac, cpu;
+	u64 hwid;
+	const u32 *cell;
+	struct device_node *cpun, *cpus;
+
+	cpus = of_find_node_by_path("/cpus");
+	if (WARN(!cpus, "Missing cpus node, bailing out\n"))
+		return;
+
+	if (WARN_ON(of_property_read_u32(cpus, "#address-cells", &ac)))
+		ac = of_n_addr_cells(cpus);
+
+	/*
+	 * Port index look-up speeds up the function disabling ports by CPU,
+	 * since the logical to port index mapping is done once and does
+	 * not change after system boot.
+	 * The stashed index array is initialized for all possible CPUs
+	 * at probe time.
+	 */
+	for_each_child_of_node(cpus, cpun) {
+		if (of_node_cmp(cpun->type, "cpu"))
+			continue;
+		cell = of_get_property(cpun, "reg", NULL);
+		if (WARN(!cell, "%s: missing reg property\n", cpun->full_name))
+			continue;
+
+		hwid = of_read_number(cell, ac);
+		cpu = get_logical_index(hwid & MPIDR_HWID_BITMASK);
+
+		if (cpu < 0 || !cpu_possible(cpu))
+			continue;
+		port = __cci_ace_get_port(cpun, ACE_PORT);
+		if (port < 0)
+			continue;
+
+		init_cpu_port(&cpu_port[cpu], port, cpu_logical_map(cpu));
+	}
+
+	for_each_possible_cpu(cpu) {
+		WARN(!cpu_port_is_valid(&cpu_port[cpu]),
+			"CPU %u does not have an associated CCI port\n",
+			cpu);
+	}
+}
+/*
+ * Functions to enable/disable a CCI interconnect slave port
+ *
+ * They are called by low-level power management code to disable slave
+ * interfaces snoops and DVM broadcast.
+ * Since they may execute with cache data allocation disabled and
+ * after the caches have been cleaned and invalidated the functions provide
+ * no explicit locking since they may run with D-cache disabled, so normal
+ * cacheable kernel locks based on ldrex/strex may not work.
+ * Locking has to be provided by BSP implementations to ensure proper
+ * operations.
+ */
+
+/**
+ * cci_port_control() - function to control a CCI port
+ *
+ * @port: index of the port to setup
+ * @enable: if true enables the port, if false disables it
+ */
+void notrace cci_port_control(unsigned int port, int enable)
+{
+	void __iomem *base = ports[port].base;
+
+	writel_relaxed(enable, base + CCI_PORT_CTRL);
+	/*
+	 * This function is called from power down procedures
+	 * and must not execute any instruction that might
+	 * cause the processor to be put in a quiescent state
+	 * (eg wfi). Hence, cpu_relax() can not be added to this
+	 * read loop to optimize power, since it might hide possibly
+	 * disruptive operations.
+	 */
+	while (readl_relaxed(cci_ctrl_base + CCI_CTRL_STATUS) & 0x1)
+			;
+}
+EXPORT_SYMBOL_GPL(cci_port_control);
+
+/**
+ * cci_disable_port_by_cpu() - function to disable a CCI port by CPU
+ *			       reference
+ *
+ * @mpidr: mpidr of the CPU whose CCI port should be disabled
+ *
+ * Disabling a CCI port for a CPU implies disabling the CCI port
+ * controlling that CPU cluster. Code disabling CPU CCI ports
+ * must make sure that the CPU running the code is the last active CPU
+ * in the cluster ie all other CPUs are quiescent in a low power state.
+ *
+ * Return:
+ *	0 on success
+ *	-ENODEV on port look-up failure
+ */
+int notrace cci_disable_port_by_cpu(u64 mpidr)
+{
+	int cpu;
+	bool is_valid;
+	for (cpu = 0; cpu < nr_cpu_ids; cpu++) {
+		is_valid = cpu_port_is_valid(&cpu_port[cpu]);
+		if (is_valid && cpu_port_match(&cpu_port[cpu], mpidr)) {
+			cci_port_control(cpu_port[cpu].port, false);
+			return 0;
+		}
+	}
+	return -ENODEV;
+}
+EXPORT_SYMBOL_GPL(cci_disable_port_by_cpu);
+
+/**
+ * cci_enable_port_for_self() - enable a CCI port for calling CPU
+ *
+ * Enabling a CCI port for the calling CPU implies enabling the CCI
+ * port controlling that CPU's cluster. Caller must make sure that the
+ * CPU running the code is the first active CPU in the cluster and all
+ * other CPUs are quiescent in a low power state  or waiting for this CPU
+ * to complete the CCI initialization.
+ *
+ * Because this is called when the MMU is still off and with no stack,
+ * the code must be position independent and ideally rely on callee
+ * clobbered registers only.  To achieve this we must code this function
+ * entirely in assembler.
+ *
+ * On success this returns with the proper CCI port enabled.  In case of
+ * any failure this never returns as the inability to enable the CCI is
+ * fatal and there is no possible recovery at this stage.
+ */
+asmlinkage void __naked cci_enable_port_for_self(void)
+{
+	asm volatile ("\n"
+
+"	mrc	p15, 0, r0, c0, c0, 5	@ get MPIDR value \n"
+"	and	r0, r0, #"__stringify(MPIDR_HWID_BITMASK)" \n"
+"	adr	r1, 5f \n"
+"	ldr	r2, [r1] \n"
+"	add	r1, r1, r2		@ &cpu_port \n"
+"	add	ip, r1, %[sizeof_cpu_port] \n"
+
+	/* Loop over the cpu_port array looking for a matching MPIDR */
+"1:	ldr	r2, [r1, %[offsetof_cpu_port_mpidr_lsb]] \n"
+"	cmp	r2, r0 			@ compare MPIDR \n"
+"	bne	2f \n"
+
+	/* Found a match, now test port validity */
+"	ldr	r3, [r1, %[offsetof_cpu_port_port]] \n"
+"	tst	r3, #"__stringify(PORT_VALID)" \n"
+"	bne	3f \n"
+
+	/* no match, loop with the next cpu_port entry */
+"2:	add	r1, r1, %[sizeof_struct_cpu_port] \n"
+"	cmp	r1, ip			@ done? \n"
+"	blo	1b \n"
+
+	/* CCI port not found -- cheaply try to stall this CPU */
+"cci_port_not_found: \n"
+"	wfi \n"
+"	wfe \n"
+"	b	cci_port_not_found \n"
+
+	/* Use matched port index to look up the corresponding ports entry */
+"3:	bic	r3, r3, #"__stringify(PORT_VALID)" \n"
+"	adr	r0, 6f \n"
+"	ldmia	r0, {r1, r2} \n"
+"	sub	r1, r1, r0 		@ virt - phys \n"
+"	ldr	r0, [r0, r2] 		@ *(&ports) \n"
+"	mov	r2, %[sizeof_struct_ace_port] \n"
+"	mla	r0, r2, r3, r0		@ &ports[index] \n"
+"	sub	r0, r0, r1		@ virt_to_phys() \n"
+
+	/* Enable the CCI port */
+"	ldr	r0, [r0, %[offsetof_port_phys]] \n"
+"	mov	r3, %[cci_enable_req]\n"		   
+"	str	r3, [r0, #"__stringify(CCI_PORT_CTRL)"] \n"
+
+	/* poll the status reg for completion */
+"	adr	r1, 7f \n"
+"	ldr	r0, [r1] \n"
+"	ldr	r0, [r0, r1]		@ cci_ctrl_base \n"
+"4:	ldr	r1, [r0, #"__stringify(CCI_CTRL_STATUS)"] \n"
+"	tst	r1, %[cci_control_status_bits] \n"			
+"	bne	4b \n"
+
+"	mov	r0, #0 \n"
+"	bx	lr \n"
+
+"	.align	2 \n"
+"5:	.word	cpu_port - . \n"
+"6:	.word	. \n"
+"	.word	ports - 6b \n"
+"7:	.word	cci_ctrl_phys - . \n"
+	: :
+	[sizeof_cpu_port] "i" (sizeof(cpu_port)),
+	[cci_enable_req] "i" cpu_to_le32(CCI_ENABLE_REQ),
+	[cci_control_status_bits] "i" cpu_to_le32(1),
+#ifndef __ARMEB__
+	[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)),
+#else
+	[offsetof_cpu_port_mpidr_lsb] "i" (offsetof(struct cpu_port, mpidr)+4),
+#endif
+	[offsetof_cpu_port_port] "i" (offsetof(struct cpu_port, port)),
+	[sizeof_struct_cpu_port] "i" (sizeof(struct cpu_port)),
+	[sizeof_struct_ace_port] "i" (sizeof(struct cci_ace_port)),
+	[offsetof_port_phys] "i" (offsetof(struct cci_ace_port, phys)) );
+
+	unreachable();
+}
+
+/**
+ * __cci_control_port_by_device() - function to control a CCI port by device
+ *				    reference
+ *
+ * @dn: device node pointer of the device whose CCI port should be
+ *      controlled
+ * @enable: if true enables the port, if false disables it
+ *
+ * Return:
+ *	0 on success
+ *	-ENODEV on port look-up failure
+ */
+int notrace __cci_control_port_by_device(struct device_node *dn, bool enable)
+{
+	int port;
+
+	if (!dn)
+		return -ENODEV;
+
+	port = __cci_ace_get_port(dn, ACE_LITE_PORT);
+	if (WARN_ONCE(port < 0, "node %s ACE lite port look-up failure\n",
+				dn->full_name))
+		return -ENODEV;
+	cci_port_control(port, enable);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(__cci_control_port_by_device);
+
+/**
+ * __cci_control_port_by_index() - function to control a CCI port by port index
+ *
+ * @port: port index previously retrieved with cci_ace_get_port()
+ * @enable: if true enables the port, if false disables it
+ *
+ * Return:
+ *	0 on success
+ *	-ENODEV on port index out of range
+ *	-EPERM if operation carried out on an ACE PORT
+ */
+int notrace __cci_control_port_by_index(u32 port, bool enable)
+{
+	if (port >= nb_cci_ports || ports[port].type == ACE_INVALID_PORT)
+		return -ENODEV;
+	/*
+	 * CCI control for ports connected to CPUS is extremely fragile
+	 * and must be made to go through a specific and controlled
+	 * interface (ie cci_disable_port_by_cpu(); control by general purpose
+	 * indexing is therefore disabled for ACE ports.
+	 */
+	if (ports[port].type == ACE_PORT)
+		return -EPERM;
+
+	cci_port_control(port, enable);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(__cci_control_port_by_index);
+
+static const struct cci_nb_ports cci400_ports = {
+	.nb_ace = 2,
+	.nb_ace_lite = 3
+};
+
+static const struct of_device_id arm_cci_matches[] = {
+	{.compatible = "arm,cci-400", .data = &cci400_ports },
+	{},
+};
+
+static const struct of_device_id arm_cci_ctrl_if_matches[] = {
+	{.compatible = "arm,cci-400-ctrl-if", },
+	{},
+};
+
+static int __init cci_probe(void)
+{
+	struct cci_nb_ports const *cci_config;
+	int ret, i, nb_ace = 0, nb_ace_lite = 0;
+	struct device_node *np, *cp;
+	struct resource res;
+	const char *match_str;
+	bool is_ace;
+
+	np = of_find_matching_node(NULL, arm_cci_matches);
+	if (!np)
+		return -ENODEV;
+
+	cci_config = of_match_node(arm_cci_matches, np)->data;
+	if (!cci_config)
+		return -ENODEV;
+
+	nb_cci_ports = cci_config->nb_ace + cci_config->nb_ace_lite;
+
+	ports = kcalloc(sizeof(*ports), nb_cci_ports, GFP_KERNEL);
+	if (!ports)
+		return -ENOMEM;
+
+	ret = of_address_to_resource(np, 0, &res);
+	if (!ret) {
+		cci_ctrl_base = ioremap(res.start, resource_size(&res));
+		cci_ctrl_phys =	res.start;
+	}
+	if (ret || !cci_ctrl_base) {
+		WARN(1, "unable to ioremap CCI ctrl\n");
+		ret = -ENXIO;
+		goto memalloc_err;
+	}
+
+	for_each_child_of_node(np, cp) {
+		if (!of_match_node(arm_cci_ctrl_if_matches, cp))
+			continue;
+
+		i = nb_ace + nb_ace_lite;
+
+		if (i >= nb_cci_ports)
+			break;
+
+		if (of_property_read_string(cp, "interface-type",
+					&match_str)) {
+			WARN(1, "node %s missing interface-type property\n",
+				  cp->full_name);
+			continue;
+		}
+		is_ace = strcmp(match_str, "ace") == 0;
+		if (!is_ace && strcmp(match_str, "ace-lite")) {
+			WARN(1, "node %s containing invalid interface-type property, skipping it\n",
+					cp->full_name);
+			continue;
+		}
+
+		ret = of_address_to_resource(cp, 0, &res);
+		if (!ret) {
+			ports[i].base = ioremap(res.start, resource_size(&res));
+			ports[i].phys = res.start;
+		}
+		if (ret || !ports[i].base) {
+			WARN(1, "unable to ioremap CCI port %d\n", i);
+			continue;
+		}
+
+		if (is_ace) {
+			if (WARN_ON(nb_ace >= cci_config->nb_ace))
+				continue;
+			ports[i].type = ACE_PORT;
+			++nb_ace;
+		} else {
+			if (WARN_ON(nb_ace_lite >= cci_config->nb_ace_lite))
+				continue;
+			ports[i].type = ACE_LITE_PORT;
+			++nb_ace_lite;
+		}
+		ports[i].dn = cp;
+	}
+
+	 /* initialize a stashed array of ACE ports to speed-up look-up */
+	cci_ace_init_ports();
+
+	/*
+	 * Multi-cluster systems may need this data when non-coherent, during
+	 * cluster power-up/power-down. Make sure it reaches main memory.
+	 */
+	sync_cache_w(&cci_ctrl_base);
+	sync_cache_w(&cci_ctrl_phys);
+	sync_cache_w(&ports);
+	sync_cache_w(&cpu_port);
+	__sync_cache_range_w(ports, sizeof(*ports) * nb_cci_ports);
+	pr_info("ARM CCI driver probed\n");
+	return 0;
+
+memalloc_err:
+
+	kfree(ports);
+	return ret;
+}
+
+static int cci_init_status = -EAGAIN;
+static DEFINE_MUTEX(cci_probing);
+
+static int __init cci_init(void)
+{
+	if (cci_init_status != -EAGAIN)
+		return cci_init_status;
+
+	mutex_lock(&cci_probing);
+	if (cci_init_status == -EAGAIN)
+		cci_init_status = cci_probe();
+	mutex_unlock(&cci_probing);
+	return cci_init_status;
+}
+
+/*
+ * To sort out early init calls ordering a helper function is provided to
+ * check if the CCI driver has beed initialized. Function check if the driver
+ * has been initialized, if not it calls the init function that probes
+ * the driver and updates the return value.
+ */
+bool __init cci_probed(void)
+{
+	return cci_init() == 0;
+}
+EXPORT_SYMBOL_GPL(cci_probed);
+
+early_initcall(cci_init);
+core_initcall(cci_pmu_init);
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("ARM CCI support");
diff --git a/drivers/char/Kconfig b/drivers/char/Kconfig
index ca307c3..4050d3b 100644
--- a/drivers/char/Kconfig
+++ b/drivers/char/Kconfig
@@ -6,6 +6,19 @@ menu "Character devices"
 
 source "drivers/tty/Kconfig"
 
+config DEVMEM
+	bool "Memory device driver"
+	default y
+	help
+	  The memory driver provides two character devices, mem and kmem, which
+	  provide access to the system's memory. The mem device is a view of
+	  physical memory, and each byte in the device corresponds to the
+	  matching physical address. The kmem device is the same as mem, but
+	  the addresses correspond to the kernel's virtual address space rather
+	  than physical memory. These devices are standard parts of a Linux
+	  system and most users should say Y here. You might say N if very
+	  security conscience or memory is tight.
+
 config DEVKMEM
 	bool "/dev/kmem virtual device support"
 	default y
@@ -612,5 +625,15 @@ config TILE_SROM
 	  device appear much like a simple EEPROM, and knows
 	  how to partition a single ROM for multiple purposes.
 
+config MIPILLI
+	tristate "MIPI Low Latency Interface"
+	help
+	  Low Latency Interface (LLI) is a point-to-point interconnect
+	  that allows two devices on separate chips to communicate as
+	  if a device attached to the remote chip is resident on the
+	  local chip. The connection between devices is at their
+	  respective interconnect level, using memory mapped transactions.
+
+source "drivers/char/mipi_lli/Kconfig"
 endmenu
 
diff --git a/drivers/char/Makefile b/drivers/char/Makefile
index ffebabf..768e4b2 100644
--- a/drivers/char/Makefile
+++ b/drivers/char/Makefile
@@ -53,10 +53,11 @@ obj-$(CONFIG_TELCLOCK)		+= tlclk.o
 obj-$(CONFIG_MWAVE)		+= mwave/
 obj-$(CONFIG_AGP)		+= agp/
 obj-$(CONFIG_PCMCIA)		+= pcmcia/
-
+obj-$(CONFIG_MIPILLI)		+= mipi_lli/
 obj-$(CONFIG_HANGCHECK_TIMER)	+= hangcheck-timer.o
 obj-$(CONFIG_TCG_TPM)		+= tpm/
 
+obj-$(CONFIG_DCC_TTY)		+= dcc_tty.o
 obj-$(CONFIG_PS3_FLASH)		+= ps3flash.o
 
 obj-$(CONFIG_JS_RTC)		+= js-rtc.o
diff --git a/drivers/char/mem.c b/drivers/char/mem.c
index 48a30a9..40ddab0 100644
--- a/drivers/char/mem.c
+++ b/drivers/char/mem.c
@@ -59,6 +59,7 @@ static inline int valid_mmap_phys_addr_range(unsigned long pfn, size_t size)
 }
 #endif
 
+#if defined(CONFIG_DEVMEM) || defined(CONFIG_DEVKMEM)
 #ifdef CONFIG_STRICT_DEVMEM
 static inline int range_is_allowed(unsigned long pfn, unsigned long size)
 {
@@ -84,7 +85,9 @@ static inline int range_is_allowed(unsigned long pfn, unsigned long size)
 	return 1;
 }
 #endif
+#endif
 
+#ifdef CONFIG_DEVMEM
 void __weak unxlate_dev_mem_ptr(unsigned long phys, void *addr)
 {
 }
@@ -211,6 +214,9 @@ static ssize_t write_mem(struct file *file, const char __user *buf,
 	*ppos += written;
 	return written;
 }
+#endif	/* CONFIG_DEVMEM */
+
+#if defined(CONFIG_DEVMEM) || defined(CONFIG_DEVKMEM)
 
 int __weak phys_mem_access_prot_allowed(struct file *file,
 	unsigned long pfn, unsigned long size, pgprot_t *vma_prot)
@@ -332,6 +338,7 @@ static int mmap_mem(struct file *file, struct vm_area_struct *vma)
 	}
 	return 0;
 }
+#endif	/* CONFIG_DEVMEM */
 
 #ifdef CONFIG_DEVKMEM
 static int mmap_kmem(struct file *file, struct vm_area_struct *vma)
@@ -726,6 +733,8 @@ static loff_t null_lseek(struct file *file, loff_t offset, int orig)
 	return file->f_pos = 0;
 }
 
+#if defined(CONFIG_DEVMEM) || defined(CONFIG_DEVKMEM) || defined(CONFIG_DEVPORT)
+
 /*
  * The memory devices use the full 32/64 bits of the offset, and so we cannot
  * check against negative addresses: they are ok. The return value is weird,
@@ -759,10 +768,14 @@ static loff_t memory_lseek(struct file *file, loff_t offset, int orig)
 	return ret;
 }
 
+#endif
+
+#if defined(CONFIG_DEVMEM) || defined(CONFIG_DEVKMEM) || defined(CONFIG_DEVPORT)
 static int open_port(struct inode *inode, struct file *filp)
 {
 	return capable(CAP_SYS_RAWIO) ? 0 : -EPERM;
 }
+#endif
 
 #define zero_lseek	null_lseek
 #define full_lseek      null_lseek
@@ -773,6 +786,7 @@ static int open_port(struct inode *inode, struct file *filp)
 #define open_kmem	open_mem
 #define open_oldmem	open_mem
 
+#ifdef CONFIG_DEVMEM
 static const struct file_operations mem_fops = {
 	.llseek		= memory_lseek,
 	.read		= read_mem,
@@ -781,6 +795,7 @@ static const struct file_operations mem_fops = {
 	.open		= open_mem,
 	.get_unmapped_area = get_unmapped_area_mem,
 };
+#endif
 
 #ifdef CONFIG_DEVKMEM
 static const struct file_operations kmem_fops = {
@@ -850,7 +865,9 @@ static const struct memdev {
 	const struct file_operations *fops;
 	struct backing_dev_info *dev_info;
 } devlist[] = {
+#ifdef CONFIG_DEVMEM
 	 [1] = { "mem", 0, &mem_fops, &directly_mappable_cdev_bdi },
+#endif
 #ifdef CONFIG_DEVKMEM
 	 [2] = { "kmem", 0, &kmem_fops, &directly_mappable_cdev_bdi },
 #endif
diff --git a/drivers/char/mipi_lli/Kconfig b/drivers/char/mipi_lli/Kconfig
new file mode 100644
index 0000000..77068ea
--- /dev/null
+++ b/drivers/char/mipi_lli/Kconfig
@@ -0,0 +1,17 @@
+#
+# MB86S70 MIPI_LLI device configuration
+#
+
+menu "MB86S70 MIPI_LLI devices"
+	depends on ARCH_MB86S70 && MIPILLI
+
+config MB86S70_MIPILLI
+	depends on ARCH_MB86S70 && MIPILLI
+	tristate "MB86S70 MIPILLI"
+	help
+	  MB86S70 MIPI-LLI SUPPLY, dirver for Fujitsu MIPI-LLI IP,
+	  selected if MB86S70 platform is defined.
+	  main function is to locally allocate a remote memory
+	  on another board via MIPI-LLI interface, this allocated
+	  memory can be used by usr-land application.
+endmenu
diff --git a/drivers/char/mipi_lli/Makefile b/drivers/char/mipi_lli/Makefile
new file mode 100644
index 0000000..0489479
--- /dev/null
+++ b/drivers/char/mipi_lli/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_MB86S70_MIPILLI) += mipi_lli.o
+mipi_lli-y := mipilli_mb86s70.o mipilli_major.o mipilli_test.o mipilli_registers.o mipilli_help.o
diff --git a/drivers/char/mipi_lli/mipilli_api.h b/drivers/char/mipi_lli/mipilli_api.h
new file mode 100644
index 0000000..355b0e9
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_api.h
@@ -0,0 +1,78 @@
+/*
+ * mipilli_api.h F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef MIPILLI_API_H_
+#define MIPILLI_API_H_
+
+#include <linux/delay.h>
+#include "mipilli_registers.h"
+#include "mipilli_mem.h"
+#include "mipilli_mb86s70.h"
+
+
+/* Main function */
+int mipilli_master_mount(u8 ch_num);
+int mipilli_slave_mount(u8 ch_num);
+int mipilli_master_unmount(u8 ch_num);
+int mipilli_config_update(u8 ch_num, u8 txlanes, u8 rxlanes);
+int mipilli_change_memmap(u32 from_addr, u32 to_addr, u32 size);
+int mipilli_write_signal(u32 v);
+u32 mipilli_read_signal(void);
+
+
+/* Help Function */
+void set_lo_cfg(struct lli_cfg *p_config);
+void set_timeout(struct lli_cfg *p_config);
+void set_lo_timeout(struct lli_cfg *p_config);
+void set_deskew_to(struct lli_cfg *p_config);
+void local_set_deskew_to(struct lli_cfg *p_config);
+void setup_error_detect(void);
+void set_err_wacfg(struct lli_cfg *pcfg, bool askpam, bool pamdef);
+void set_mphy_attr(struct lli_cfg *pcfg);
+bool update_shadow_remote(struct lli_cfg *pcfg, u8 txlan, u8 rxlan);
+bool phy_link_update(bool unmount, u8 ch_num);
+u32 worst_prep_length_in_si(struct lli_cfg *p_config);
+u32 worst_local_prep_length_in_si(struct lli_cfg *p_config);
+u32 worst_min_sleep_stall(struct lli_cfg *p_config);
+u32 worst_local_min_sleep_stall(struct lli_cfg *p_config);
+void set_lo_timeout(struct lli_cfg *p_config);
+u32 unit_conv_si_to_pclk(u32 sival, struct lli_cfg *p_config);
+u32 get_sync_len(u8 sync_lencap);
+u32 unit_conv_symbol_to_si(u32 symbolval);
+u32 unit_conv_si_to_symbol(u32 sival);
+u32 measure_nack_rtt(void);
+
+void mphy_write_reg(u32 addr_base, u8 targ, u8 val,
+u8 txlanes_s, u8 txlanes_d,
+u8 rxlanes_s, u8 rxlanes_d);
+
+
+bool phy_test_start(u8 test_lane, u8 ch_num);
+bool phy_test_stop(void);
+void roe_test(struct lli_cfg *p_config);
+bool slave_main_loop(u8 ch_num);
+
+/* Register access function */
+void lli_lo_wt(u32 f, u32 v);
+u32 lli_lo_rd(u32 v);
+void lli_rm_wt(u32 f, u32 v);
+u32 lli_rm_rd(u32 v);
+
+
+#endif /* MIPILLI_API_H_ */
diff --git a/drivers/char/mipi_lli/mipilli_help.c b/drivers/char/mipi_lli/mipilli_help.c
new file mode 100644
index 0000000..6bd3594
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_help.c
@@ -0,0 +1,1405 @@
+/*
+ * mipilli_help.c F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/io.h>
+#include "mipilli_api.h"
+
+void set_lo_cfg(struct lli_cfg *pcfg)
+{
+	u8 i = 0;
+
+	/* DBG("set_lo_cfg\n"); */
+	/*
+	 * mphy config not set here, use PLU
+	 * link config not set here, use PLU
+	 * PA config not set here, use PLU
+	*/
+
+	/* user PA config. exc. shadow register not set here, use PLU */
+	/* retry buf time */
+	lli_lo_wt(MIPI_USER_PA_RBTC, pcfg->usr_pacfg.rbtc);
+
+	/* NACKThreshold */
+	lli_lo_wt(MIPI_USER_PA_NACKT, pcfg->usr_pacfg.nackt);
+	lli_lo_wt(MIPI_USER_PA_DSKWTC, pcfg->usr_pacfg.dskwtc);
+	lli_lo_wt(MIPI_USER_PA_PATXSV, pcfg->usr_pacfg.patxsv);
+
+	/* LLI control config */
+	lli_lo_wt(CTRL_LLTCDISABLE, pcfg->lk_ctlcfg.ll_tc_disable);
+	lli_lo_wt(CTRL_BETCDISABLE, pcfg->lk_ctlcfg.be_tc_disable);
+	lli_lo_wt(CTRL_TLADDRMODE, pcfg->lk_ctlcfg.tl_addr_mode);
+
+	/* user control config */
+	lli_lo_wt(MIPI_USER_CTRL_CLDRSTAUTO, pcfg->usr_ctlcfg.cldrst_auto);
+	lli_lo_wt(MIPI_USER_CTRL_ROEAUTO, pcfg->usr_ctlcfg.roe_auto);
+	lli_lo_wt(MIPI_USER_CTRL_INTUM, pcfg->usr_ctlcfg.intum);
+	lli_lo_wt(MIPI_USER_CTRL_AXICFG, pcfg->usr_ctlcfg.axi_cofg);
+	lli_lo_wt(MIPI_USER_CTRL_TRANARB, pcfg->usr_ctlcfg.tranarb);
+	lli_lo_wt(MIPI_USER_CTRL_AXIAPBCTL, pcfg->usr_ctlcfg.axiapb_ctl);
+	lli_lo_wt(MIPI_USER_CTRL_RSTR, pcfg->usr_ctlcfg.rstr);
+
+
+	for (i = 0; i < 8; i++) {
+
+		lli_lo_wt(MIPI_USER_CTRL_MADREMAP1 + i * 4,
+		    pcfg->usr_ctlcfg.m_ad_remapConfig[i].remap);
+
+		lli_lo_wt(MIPI_USER_CTRL_MADREMAPUM1 + i * 4,
+		    pcfg->usr_ctlcfg.m_ad_remapConfig[i].unmask);
+
+		lli_lo_wt(MIPI_USER_CTRL_MADREMAPFLT1 + i * 4,
+		    pcfg->usr_ctlcfg.m_ad_remapConfig[i].filter);
+
+		lli_lo_wt(MIPI_USER_CTRL_SADREMAP1 + i * 4,
+		    pcfg->usr_ctlcfg.s_ad_remapConfig[i].remap);
+
+		lli_lo_wt(MIPI_USER_CTRL_SADREMAPUM1 + i * 4,
+		    pcfg->usr_ctlcfg.s_ad_remapConfig[i].unmask);
+
+		lli_lo_wt(MIPI_USER_CTRL_SADREMAPFLT1 + i * 4,
+		    pcfg->usr_ctlcfg.s_ad_remapConfig[i].filter);
+
+		lli_lo_wt(MIPI_USER_CTRL_SADPSTWUNM1 + i * 4,
+		    pcfg->usr_ctlcfg.s_ad_postwConfig[i].unmask);
+
+		lli_lo_wt(MIPI_USER_CTRL_SADPSTWFLT1 + i * 4,
+		    pcfg->usr_ctlcfg.s_ad_postwConfig[i].filter);
+	}
+
+
+	lli_lo_wt(MIPI_USER_CTRL_MAWUSRSW7_0,
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[7] << 28) |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[6] << 24) |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[5] << 20) |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[4] << 16) |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[3] << 12) |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[2] << 8)  |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[1] << 4)  |
+	    (pcfg->usr_ctlcfg.ll_m_awuserSwap[0] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_MAWUSRSW8,
+	    pcfg->usr_ctlcfg.ll_m_awuserSwap[8]);
+
+	lli_lo_wt(MIPI_USER_CTRL_MARUSRSW7_0,
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[7] << 28) |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[6] << 24) |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[5] << 20) |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[4] << 16) |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[3] << 12) |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[2] << 8)  |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[1] << 4)  |
+	    (pcfg->usr_ctlcfg.ll_m_aruserSwap[0] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_MARUSRSW8,
+	  pcfg->usr_ctlcfg.ll_m_aruserSwap[8]);
+
+	lli_lo_wt(MIPI_USER_CTRL_SAWUSRSW7_0,
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[7] << 28) |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[6] << 24) |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[5] << 20) |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[4] << 16) |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[3] << 12) |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[2] << 8)  |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[1] << 4)  |
+	    (pcfg->usr_ctlcfg.ll_s_awuserSwap[0] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_SAWUSRSW8,
+	  pcfg->usr_ctlcfg.ll_s_awuserSwap[8]);
+
+	lli_lo_wt(MIPI_USER_CTRL_SARUSRSW7_0,
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[7] << 28) |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[6] << 24) |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[5] << 20) |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[4] << 16) |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[3] << 12) |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[2] << 8)  |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[1] << 4)  |
+	    (pcfg->usr_ctlcfg.ll_s_aruserSwap[0] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_SARUSRSW8,
+	    pcfg->usr_ctlcfg.ll_s_aruserSwap[8]);
+
+	lli_lo_wt(MIPI_USER_CTRL_MAWUSREN,
+	    pcfg->usr_ctlcfg.mawusr_en);
+	lli_lo_wt(MIPI_USER_CTRL_MARUSREN,
+	    pcfg->usr_ctlcfg.marusr_en);
+	lli_lo_wt(MIPI_USER_CTRL_SAWUSREN,
+	    pcfg->usr_ctlcfg.sawusr_en);
+	lli_lo_wt(MIPI_USER_CTRL_SARUSREN,
+	    pcfg->usr_ctlcfg.sarusr_en);
+
+	for (i = 0; i < 5; i++) {
+		lli_lo_wt(MIPI_USER_CTRL_LSSTSSW5_0 + i * 4,
+		    (pcfg->usr_ctlcfg.sig_stat_sw[5 + i * 6] << 25) |
+		    (pcfg->usr_ctlcfg.sig_stat_sw[4 + i * 6] << 20) |
+		    (pcfg->usr_ctlcfg.sig_stat_sw[3 + i * 6] << 15) |
+		    (pcfg->usr_ctlcfg.sig_stat_sw[2 + i * 6] << 10) |
+		    (pcfg->usr_ctlcfg.sig_stat_sw[1 + i * 6] << 5) |
+		    (pcfg->usr_ctlcfg.sig_stat_sw[0 + i * 6] << 0));
+
+		lli_lo_wt(MIPI_USER_CTRL_LSSETSW5_0 + i * 4,
+		    (pcfg->usr_ctlcfg.sig_set_sw[5 + i * 6] << 25) |
+		    (pcfg->usr_ctlcfg.sig_set_sw[4 + i * 6] << 20) |
+		    (pcfg->usr_ctlcfg.sig_set_sw[3 + i * 6] << 15) |
+		    (pcfg->usr_ctlcfg.sig_set_sw[2 + i * 6] << 10) |
+		    (pcfg->usr_ctlcfg.sig_set_sw[1 + i * 6] << 5) |
+		    (pcfg->usr_ctlcfg.sig_set_sw[0 + i * 6] << 0));
+	}
+
+
+	lli_lo_wt(MIPI_USER_CTRL_LSSTSSW5_0 + i * 4,
+	    (pcfg->usr_ctlcfg.sig_set_sw[1 + i * 6] << 5) |
+	    (pcfg->usr_ctlcfg.sig_set_sw[0 + i * 6] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_LSSETSW5_0 + i * 4,
+	    (pcfg->usr_ctlcfg.sig_set_sw[1 + i * 6] << 5) |
+	    (pcfg->usr_ctlcfg.sig_set_sw[0 + i * 6] << 0));
+
+	lli_lo_wt(MIPI_USER_CTRL_LSSTSEN, pcfg->usr_ctlcfg.lssts_en);
+	lli_lo_wt(MIPI_USER_CTRL_LSSTSOR, pcfg->usr_ctlcfg.lssts_or);
+	lli_lo_wt(MIPI_USER_CTRL_LSSETEN, pcfg->usr_ctlcfg.lsset_en);
+	lli_lo_wt(MIPI_USER_CTRL_LSSETUM, pcfg->usr_ctlcfg.lsset_um);
+}
+
+
+
+void set_mphy_attr(struct lli_cfg *pcfg)
+{
+	u32 op_mode = 0;
+	u32 pwm_gear = 0;
+
+	pcfg->tx_cfg.amplitude = LARGE_AMPLITUDE;
+
+	if ((pcfg->tx_cfg.op_mode == LS_MODE) &&
+		(pcfg->tx_cfg.pwm_gear <= PWM_G4)) {
+		pcfg->tx_cfg.slew_rate = 0xC0;
+
+	} else {
+		pcfg->tx_cfg.slew_rate = 0x01;
+	}
+
+	pcfg->tx_cfg.sync_range = COARSE;
+	pcfg->tx_cfg.sync_length = 0x05;
+
+	if (pcfg->tx_cfg.op_mode == HS_MODE) {
+		if (pcfg->tx_cfg.hs_gear == HS_G2)
+			pcfg->tx_cfg.sync_length = 0x06;
+		else if (pcfg->tx_cfg.hs_gear == HS_G3)
+			pcfg->tx_cfg.sync_length = 0x07;
+	}
+
+	pcfg->tx_cfg.hs_prepare_length = 0x0F;
+	pcfg->tx_cfg.ls_prepare_length = 0x04;
+	pcfg->tx_cfg.lcc_enable = false;
+	pcfg->tx_cfg.hs_unterminated = 0x20;
+	pcfg->tx_cfg.hs_unterminated = false;
+
+	if ((pcfg->tx_cfg.op_mode == LS_MODE) &&
+		(pcfg->tx_cfg.pwm_gear >= PWM_G5)) {
+		pcfg->tx_cfg.ls_terminated = true;
+	} else {
+		pcfg->tx_cfg.ls_terminated = false;
+	}
+
+	pcfg->tx_cfg.min_activate_time = 0x08;
+	pcfg->tx_cfg.lsg6_sync_range = COARSE;
+	pcfg->tx_cfg.lsg6_sync_length = 0x01;
+
+	op_mode = pcfg->tx_cfg.op_mode;
+	pwm_gear = pcfg->tx_cfg.pwm_gear;
+	if ((op_mode == LS_MODE) && (pwm_gear >= PWM_G5))
+		pcfg->rx_cfg.ls_terminated = true;
+	else
+		pcfg->rx_cfg.ls_terminated = false;
+
+	pcfg->rx_cfg.hs_unterminated = false;
+}
+
+
+/*
+ * M-Phy can not
+ * communicate in PWM_G0 ~ PWM_G4 modes.
+ * Also, PWM_G5 has CDC(asynchronous) problem.
+ * Now, ignore PWM!!
+ * Therefore, setting HS-G1/G2
+ * must be done BEFORE MOUNTING.
+ * Both of Master/Slave have the same value.
+ * ToDo: In future, this bug will be fixed.
+ * These code must be removed
+ * when it is fixed.
+*/
+void set_err_wacfg(struct lli_cfg *pcfg,
+		bool askpam, bool pamdef)
+{
+
+	/* Only lane0 must be set here */
+	u32 updated_lanes = 0x1001;
+
+
+	lli_lo_wt(MIPI_MPHY0_TXMODE, pcfg->tx_cfg.op_mode);
+
+	lli_lo_wt(MIPI_MPHY0_TXHSRATE_SERIES, pcfg->tx_cfg.rate_series);
+
+	lli_lo_wt(MIPI_MPHY0_TXHSGEAR, pcfg->tx_cfg.hs_gear);
+
+	lli_lo_wt(MIPI_MPHY0_TXPWMGEAR, pcfg->tx_cfg.pwm_gear);
+
+	lli_lo_wt(MIPI_MPHY0_TXAMPLITUDE, pcfg->tx_cfg.amplitude);
+
+	lli_lo_wt(MIPI_MPHY0_TXHSSLEWRATE, pcfg->tx_cfg.slew_rate);
+
+	lli_lo_wt(MIPI_MPHY0_TXHSSYNC_LENGTH,
+	((pcfg->tx_cfg.sync_range << 6) | pcfg->tx_cfg.sync_length));
+
+	lli_lo_wt(MIPI_MPHY0_TXHSPREPLENGTH, pcfg->tx_cfg.hs_prepare_length);
+
+	lli_lo_wt(MIPI_MPHY0_TXLSPREPLENGTH, pcfg->tx_cfg.ls_prepare_length);
+
+	lli_lo_wt(MIPI_MPHY0_TXLCCENABLE, pcfg->tx_cfg.lcc_enable);
+
+	lli_lo_wt(MIPI_MPHY0_TXPWMCLOSEXT, pcfg->tx_cfg.hs_unterminated);
+
+	lli_lo_wt(MIPI_MPHY0_TXHSUNTERM, pcfg->tx_cfg.hs_unterminated);
+
+	lli_lo_wt(MIPI_MPHY0_TXLSTERM, pcfg->tx_cfg.ls_terminated);
+
+	lli_lo_wt(MIPI_MPHY0_TXMINACTIVATTIME, pcfg->tx_cfg.min_activate_time);
+
+	lli_lo_wt(MIPI_MPHY0_TXPWMG6G7SYNCLENGTH,
+	    (pcfg->tx_cfg.lsg6_sync_range << 6) |
+	    pcfg->tx_cfg.lsg6_sync_length);
+
+	lli_lo_wt(MIPI_MPHY0_RXMODE, pcfg->rx_cfg.op_mode);
+
+	lli_lo_wt(MIPI_MPHY0_RXHSRATE_SERIES, pcfg->rx_cfg.rate_series);
+
+	lli_lo_wt(MIPI_MPHY0_RXHSGEAR, pcfg->rx_cfg.hs_gear);
+
+	lli_lo_wt(MIPI_MPHY0_RXPWMGEAR, pcfg->rx_cfg.pwm_gear);
+
+	lli_lo_wt(MIPI_MPHY0_RXLSTERM, pcfg->rx_cfg.ls_terminated);
+
+	lli_lo_wt(MIPI_MPHY0_RXHSUNTERM, pcfg->rx_cfg.hs_unterminated);
+
+	/* Set LLI PA and User PA shadow here */
+
+	/* LLI PA config use PLU */
+	/* DBG_MSG("LLI PA configuration for WA\n"); */
+	lli_lo_wt(MIPI_LLI_PA_WTSTARTVAL, pcfg->pa_cfig.wt_start_value);
+
+	lli_lo_wt(MIPI_LLI_PA_MIN_SAVE_CONFIG,
+	    pcfg->pa_cfig.pa_min_save_config);
+
+	lli_lo_wt(MIPI_LLI_PA_WORSTCASE_RTT,
+	    pcfg->pa_cfig.pa_worst_case_rtt);
+
+	lli_lo_wt(MIPI_LLI_PA_DRV_TACTV_DUR,
+	    pcfg->pa_cfig.drive_tactive_duration);
+
+	lli_lo_wt(MIPI_LLI_PA_MK0INS_ENABLE,
+	    pcfg->pa_cfig.marker0_insertion);
+
+	lli_lo_wt(MIPI_LLI_PA_PHITRCVCOUNTEN,
+	    pcfg->pa_cfig.phit_rec_cont_en);
+
+	lli_lo_wt(MIPI_LLI_PA_PHITERRCOUNTEN,
+	    pcfg->pa_cfig.phit_err_cout_en);
+
+	lli_lo_wt(MIPI_LLI_PA_WORSTCASE_RTT,
+	    pcfg->pa_cfig.pa_worst_case_rtt);
+
+	lli_lo_wt(MIPI_USER_PA_NACKT,
+	    pcfg->usr_pacfg.nackt);
+
+	/* user PA config. shadow register use PLU */
+	/*DBG_MSG("User PA configuration for WA\n");*/
+	lli_lo_wt(MIPI_USER_PA_AGPRM, pcfg->usr_pacfg.aging_parameter);
+	/* ToDo: This value must be updated by PLU
+	 * To overcome this, debug register must be used!!
+	*/
+	set_lo_timeout(pcfg);
+	local_set_deskew_to(pcfg);
+
+	/* set PA config_update
+	 * Config_Update[11:0] shall control the M-TX [11:0].
+	 * Config_Update[23:12] shall control the M-RX [11:0].
+	 * DBG_MSG("Set PA config_update\n");
+	*/
+	lli_lo_wt(MIPI_LLI_PA_CONFIG_UPDATE, updated_lanes);
+}
+
+void mphy_write_reg(u32 addr_base, u8 targ, u8 val,
+	u8 txlanes_s, u8 txlanes_d,
+	u8 rxlanes_s, u8 rxlanes_d)
+{
+
+	s8 i = 0;
+
+	lli_lo_wt(MIPI_MPHY0_TXHIBERN8CTRL + i * 0x400, EXIT);
+	lli_rm_wt(MIPI_MPHY0_TXHIBERN8CTRL + i * 0x400, EXIT);
+
+	if (targ == MPHY_WRITE_TAG_LOCAL) {
+		for (i = txlanes_s; i < txlanes_d; i++)
+			lli_lo_wt(addr_base + i * 0x400, val);
+
+		for (i = rxlanes_s; i < rxlanes_d; i++)
+			lli_lo_wt(addr_base + i * 0x400, val);
+
+	} else if (targ == MPHY_WRITE_TAG_REMOTE) {
+		for (i = txlanes_s; i < txlanes_d; i++)
+			lli_rm_wt(addr_base + i * 0x400, val);
+
+		for (i = rxlanes_s; i < txlanes_d; i++)
+			lli_rm_wt(addr_base + i * 0x400, val);
+	} else {
+		for (i = txlanes_s; i < txlanes_d; i++) {
+			lli_rm_wt(addr_base + i * 0x400, val);
+			lli_lo_wt(addr_base + i * 0x400, val);
+		}
+
+		for (i = rxlanes_s; i < rxlanes_d; i++) {
+			lli_rm_wt(addr_base + i * 0x400, val);
+			lli_lo_wt(addr_base + i * 0x400, val);
+		}
+	}
+}
+
+
+bool
+update_shadow_remote(struct lli_cfg *pcfg, u8 txlan, u8 rxlan)
+{
+	bool result = true;
+
+	DBG_MSG("updateShadowAndRemote start\n");
+
+
+	/* DBG_MSG("Mphy TX configuration\n"); */
+	/* ---Mphy TX configuration --- */
+
+	mphy_write_reg(MIPI_MPHY0_TXMODE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.op_mode,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXHSRATE_SERIES,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.rate_series,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXHSGEAR,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.hs_gear,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXPWMGEAR,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.pwm_gear,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXAMPLITUDE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.amplitude,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXHSSLEWRATE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.slew_rate,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXSYNCSOURCE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.sync_source,
+	    0, txlan, 0, 0);
+
+
+	mphy_write_reg(MIPI_MPHY0_TXHSSYNC_LENGTH,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    (pcfg->tx_cfg.sync_range << 6)|
+	    pcfg->tx_cfg.sync_length,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXHSPREPLENGTH,
+	  MPHY_WRITE_TAG_LOCAL|
+	  MPHY_WRITE_TAG_REMOTE,
+	  pcfg->tx_cfg.hs_prepare_length,
+	  0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXLSPREPLENGTH,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.ls_prepare_length,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXLCCENABLE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.lcc_enable,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXPWMCLOSEXT,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.hs_unterminated,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXBYPASS8B10B,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.bypass8B10B,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXPOLARITY,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.polarity,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXHSUNTERM,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.hs_unterminated,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXLSTERM,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.ls_terminated,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXMINACTIVATTIME,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.min_activate_time,
+	    0, txlan, 0, 0);
+
+	mphy_write_reg(MIPI_MPHY0_TXPWMG6G7SYNCLENGTH,
+	    MPHY_WRITE_TAG_LOCAL |
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->tx_cfg.lsg6_sync_length,
+	    0, txlan, 0, 0);
+
+
+	/* --Mphy RX configuration-- */
+
+	/* DBG_MSG("Mphy RX configuration\n"); */
+
+	mphy_write_reg(MIPI_MPHY0_RXMODE,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.op_mode,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXHSRATE_SERIES,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.rate_series,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXHSGEAR,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.hs_gear,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXPWMGEAR,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.pwm_gear,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXLSTERM,
+	    MPHY_WRITE_TAG_LOCAL |
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.ls_terminated,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXHSUNTERM,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.hs_unterminated,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXBYPASS8B10B,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.bypass8B10B,
+	    0, 0, 0, rxlan);
+
+	mphy_write_reg(MIPI_MPHY0_RXTERMFORCEEN,
+	    MPHY_WRITE_TAG_LOCAL|
+	    MPHY_WRITE_TAG_REMOTE,
+	    pcfg->rx_cfg.force_term,
+	    0, 0, 0, rxlan);
+
+
+
+	/* LLI PA config use PLU */
+	/* DBG_MSG("LLI PA configuration\n"); */
+	lli_lo_wt(MIPI_LLI_PA_MK0INS_ENABLE, pcfg->pa_cfig.marker0_insertion);
+	lli_rm_wt(MIPI_LLI_PA_MK0INS_ENABLE, pcfg->pa_cfig.marker0_insertion);
+	lli_lo_wt(MIPI_LLI_PA_WTSTARTVAL, pcfg->pa_cfig.wt_start_value);
+	lli_rm_wt(MIPI_LLI_PA_WTSTARTVAL, pcfg->pa_cfig.wt_start_value);
+	lli_lo_wt(MIPI_LLI_PA_PHITRCVCOUNTEN, pcfg->pa_cfig.phit_rec_cont_en);
+	lli_rm_wt(MIPI_LLI_PA_PHITRCVCOUNTEN, pcfg->pa_cfig.phit_rec_cont_en);
+	lli_lo_wt(MIPI_LLI_PA_PHITERRCOUNTEN, pcfg->pa_cfig.phit_err_cout_en);
+	lli_rm_wt(MIPI_LLI_PA_PHITERRCOUNTEN, pcfg->pa_cfig.phit_err_cout_en);
+
+	lli_lo_wt(MIPI_LLI_PA_MIN_SAVE_CONFIG,
+	    pcfg->pa_cfig.pa_min_save_config);
+	lli_rm_wt(MIPI_LLI_PA_MIN_SAVE_CONFIG,
+	    pcfg->pa_cfig.pa_min_save_config);
+
+	lli_lo_wt(MIPI_LLI_PA_WORSTCASE_RTT, pcfg->pa_cfig.pa_worst_case_rtt);
+	lli_rm_wt(MIPI_LLI_PA_WORSTCASE_RTT, pcfg->pa_cfig.pa_worst_case_rtt);
+
+	lli_lo_wt(MIPI_LLI_PA_DRV_TACTV_DUR,
+	    pcfg->pa_cfig.drive_tactive_duration);
+	lli_rm_wt(MIPI_LLI_PA_DRV_TACTV_DUR,
+	    pcfg->pa_cfig.drive_tactive_duration);
+
+	lli_lo_wt(MIPI_LLI_PA_PHYTESTCONFIG,
+	    pcfg->pa_cfig.pa_phy_test_config_master);
+	lli_rm_wt(MIPI_LLI_PA_PHYTESTCONFIG,
+	    pcfg->pa_cfig.pa_phy_test_config_slasve);
+
+	/* user PA config. shadow register use PLU */
+	/* DBG_MSG("User PA configuration\n"); */
+	lli_lo_wt(MIPI_USER_PA_AGPRM, pcfg->usr_pacfg.aging_parameter);
+	lli_rm_wt(MIPI_USER_PA_AGPRM, pcfg->usr_pacfg.aging_parameter);
+	/* ToDo, check if Aging param may be set by SVC!! */
+
+	DBG_MSG("updateShadowAndRemote done successfully\n");
+
+	return result;
+}
+
+
+
+bool phy_link_update(bool unmount, u8 ch_num)
+{
+
+	/* bool result = true; */
+	u32 ret = 0, retry_cnt = RETRY_COUNTER;
+	u8 r = 0 , r1 = 0;
+
+	struct lli_data *the_lli;
+	/* DBG_MSG("PLU_start\n"); */
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	/* set PA config_update */
+	/* DBG_MSG("Set PA config_update\n"); */
+
+	/* Update Configuration Now */
+	if (!unmount)
+		lli_lo_wt(MIPI_LLI_PA_CSA_PA_SET, B_CSA_PA_LINKUPDATECONFIG);
+	else
+		lli_lo_wt(CTRL_CSASYSTEMCLR, B_CSA_LLI_MOUNT_CTRL);
+
+	ret = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+	INIT_COMPLETION(the_lli->msg_cmp);
+
+	if (!ret) {
+		/* DBG_MSG("PLU(Physical Link Update) failure(timeout)\n"); */
+		/* result = false; */
+		/* return result; */
+		return false;
+	}
+
+	if (the_lli->lli_interrupt_status & B_PLUFIN) {
+
+		/* DBG_MSG("Master Mount done successfully\n"); */
+
+		while (!r) {
+			r1 = lli_lo_rd(PA_CSA_PA_STATUS);
+			r = (r1 & B_CSA_PA_LINKUPDATECONFIG);
+			retry_cnt = retry_cnt - 1;
+			if (retry_cnt <= 0)
+				break;
+			/* msleep(20); */
+		}
+	}
+
+	/* result = retry_cnt > 0; */
+	/* DBG_MSG("PLU done successfully\n");*/
+	/* return result; */
+	return retry_cnt > 0;
+}
+
+
+/* return time conversion result,
+ * from SI to Tx/Rx_Symbolclk
+ */
+u32 unit_conv_si_to_symbol(u32 sival)
+{
+	u32 val = sival / 2;
+	/* round-up */
+	if ((val * 2) < sival)
+		val++;
+
+	return val;
+}
+
+/* return time coversion result,
+ * from Tx/Rx_Symbolclk to SI
+ */
+u32 unit_conv_symbol_to_si(u32 symbolval)
+{
+	u32 val = symbolval * 2;
+	return val;
+}
+
+/*
+ * return time conversion result, from SI to PCLK
+ * PCLK is assumed to be 40 MHz.
+ * Warning!! Master side can use this function.
+*/
+u32 unit_conv_si_to_pclk(u32 sival, struct lli_cfg *pcfg)
+{
+	static
+	u32 mode_freq_in_khz[14] = { /* Mode/Gear indexed */
+		10, 3000, 6000, 12000,/* LS-G0,1,2,3 */
+		24000, 48000, 96000, 192000,/* LS-G4,5,6,7 */
+		1248000, 1457600, 2496000, 2915200,/* HS-G1A, G1B, G2A, G2B */
+		4992000, 5830400,/* HS-G3A, G3B */
+	};
+
+	u8 index = 0;
+	enum MPHY_OP_MODE mode = pcfg->tx_cfg.op_mode;
+	u32 khz = 0;
+	u32 val = 0;
+
+	if (mode == HS_MODE) {
+		enum MPHY_HS_GEAR gear = pcfg->tx_cfg.hs_gear;
+		enum MPHY_HS_RATE_SERIES rate = pcfg->tx_cfg.rate_series;
+		index |= 0x08;
+		index |= (gear - 1) << 1;
+
+		if (rate == B)
+			index |= 0x01;
+
+	} else { /* LS */
+		enum MPHY_PWM_GEAR gear = pcfg->tx_cfg.pwm_gear;
+		index |= gear;
+	}
+
+	khz = mode_freq_in_khz[index];
+
+	#if 0
+	/* PCLK 40 MHz assumed */
+	u32 ratio = (khz / 10) / 40 * 1000;
+	u32 val = sival / ratio;
+	/* round-up */
+	if ((val * ratio) < sival)
+		val++;
+	#endif
+
+	val = (sival * (40 * 1000) * 10) / khz;
+	/* round-up */
+	if ((val * khz) < (sival * (40 * 1000) * 10))
+		val++;
+
+	return val;
+
+}
+
+
+u32 get_sync_len(u8 sync_lencap)
+{
+	u32 val = 0;
+
+	if ((sync_lencap & 0xC0) == 0x00)
+		val = sync_lencap & 0x0F;
+	else {
+
+		if ((sync_lencap & 0x0F) > 14)
+			val = 1 << 14;
+		else
+			val = 1 << (sync_lencap & 0x0F);
+
+	}
+	return val;
+}
+
+
+/*
+ * Plz ref mipi-m-phy :
+ * Table 6 PREPARE and SYNC Attribute and
+ * Dependent Parameter Values
+ * return sync_length in SI using config value and
+ * SVC returned value.
+ * Warning!! Only MOUNTED Master side can use this
+ * function.
+*/
+u32 worstsync_lengthInSI(struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+	u8 rxsynccap = 0;
+	u8 rxsynccaplocal = 0;
+	u32 sync_len = 0, sync_lenLocal = 0;
+
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+		if (pcfg->tx_cfg.pwm_gear < PWM_G6)
+			val = 0;
+		else {
+			rxsynccap = lli_rm_rd(MIPI_MPHY0_RXLSG67sync_len_CAP);
+			rxsynccaplocal =
+			  lli_lo_rd(MIPI_MPHY0_RXLSG67sync_len_CAP);
+
+			sync_len = get_sync_len(rxsynccap);
+			sync_lenLocal = get_sync_len(rxsynccaplocal);
+
+			if (sync_len > sync_lenLocal)
+				val = sync_len;
+			else
+				val = sync_lenLocal;
+		}
+	} else { /* HS */
+		switch (pcfg->tx_cfg.hs_gear) {
+		case HS_G1:
+			rxsynccap =
+			    lli_rm_rd(MIPI_MPHY0_RXHSG1sync_len_CAP);
+			rxsynccaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG1sync_len_CAP);
+			break;
+		case HS_G2:
+			rxsynccap = lli_rm_rd(MIPI_MPHY0_RXHSG2sync_len_CAP);
+			rxsynccaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG2sync_len_CAP);
+			break;
+		/* case HS_G3: */
+		default:
+			rxsynccap =
+			    lli_rm_rd(MIPI_MPHY0_RXHSG3sync_len_CAP);
+			rxsynccaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG3sync_len_CAP);
+			break;
+		}
+		sync_len = get_sync_len(rxsynccap);
+		sync_lenLocal = get_sync_len(rxsynccaplocal);
+
+		if (sync_len > sync_lenLocal)
+			val = sync_len;
+		else
+			val = sync_lenLocal;
+
+	}
+	return val;
+}
+
+static u32 worstLocalsync_lengthInSI(struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+	u8 rxsynccap = 0;
+	u32 sync_len = 0;
+
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+		if (pcfg->tx_cfg.pwm_gear < PWM_G6)
+			val = 0;
+		else {
+			rxsynccap = lli_lo_rd(MIPI_MPHY0_RXLSG67sync_len_CAP);
+			sync_len = get_sync_len(rxsynccap);
+			val = sync_len;
+		}
+	} else { /* HS */
+		switch (pcfg->tx_cfg.hs_gear) {
+		case HS_G1:
+			rxsynccap =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG1sync_len_CAP);
+			break;
+		case HS_G2:
+			rxsynccap =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG2sync_len_CAP);
+			break;
+		/* case HS_G3: */
+		default:
+			rxsynccap = lli_lo_rd(MIPI_MPHY0_RXHSG3sync_len_CAP);
+			break;
+		}
+		sync_len = get_sync_len(rxsynccap);
+		val = sync_len;
+	}
+
+	return val;
+}
+
+
+/*
+ * plz ref mipi-m-phy : Table 6 PREPARE
+ * and SYNC Attribute and Dependent Parameter Values
+*/
+u32 lspreplencaptosi(u8 cap, struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+
+	if (cap + pcfg->tx_cfg.pwm_gear >= 7)
+		val = 1 << (cap + pcfg->tx_cfg.pwm_gear - 7);
+	else
+		val = 1;
+
+	return val;
+}
+
+/*
+ * Table 6 PREPARE and SYNC Attribute
+ * and Dependent Parameter Values
+*/
+u32 hspreplencaptosi(u8 cap, struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+	val = cap * (1 << (pcfg->tx_cfg.hs_gear - 1));
+
+	return val;
+}
+
+/*
+ * return PrepareLength in SI using config value
+ * and SVC remote value.
+ * Warning!! Only MOUNTED Master side
+ * can use this function.
+*/
+u32 worst_prep_length_in_si(struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+	u8 rxprepcap = 0, rxprepcaplocal = 0;
+
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+		rxprepcap = lli_rm_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+
+		rxprepcaplocal = lli_lo_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+
+		if (rxprepcap < rxprepcaplocal)
+			rxprepcap = rxprepcaplocal;
+
+		val = lspreplencaptosi(rxprepcap, pcfg);
+	} else { /* HS */
+		switch (pcfg->tx_cfg.hs_gear) {
+		case HS_G1:
+			rxprepcap =
+			    lli_rm_rd(MIPI_MPHY0_RXHSG1PREPLEN_CAP);
+			rxprepcaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG1PREPLEN_CAP);
+			break;
+
+		case HS_G2:
+			rxprepcap = lli_rm_rd(MIPI_MPHY0_RXHSG2PREPLEN_CAP);
+			rxprepcaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG2PREPLEN_CAP);
+			break;
+
+		/* case HS_G3: */
+		default:
+			rxprepcap =
+			    lli_rm_rd(MIPI_MPHY0_RXHSG3PREPLEN_CAP);
+			rxprepcaplocal =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG3PREPLEN_CAP);
+			break;
+		}
+
+		if (rxprepcap < rxprepcaplocal)
+			rxprepcap = rxprepcaplocal;
+
+		val = hspreplencaptosi(rxprepcap, pcfg);
+	}
+	return val;
+}
+
+u32 worst_local_prep_length_in_si(struct lli_cfg *pcfg)
+{
+	u32 val = 0;
+	u8 rxprepcap = 0;
+
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+		rxprepcap =
+		    lli_lo_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+		val = lspreplencaptosi(rxprepcap, pcfg);
+	} else { /* HS */
+		switch (pcfg->tx_cfg.hs_gear) {
+		case HS_G1:
+			rxprepcap =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG1PREPLEN_CAP);
+			break;
+		case HS_G2:
+			rxprepcap =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG2PREPLEN_CAP);
+			break;
+		/* case HS_G3: */
+		default:
+			rxprepcap =
+			    lli_lo_rd(MIPI_MPHY0_RXHSG3PREPLEN_CAP);
+			break;
+		}
+		val = hspreplencaptosi(rxprepcap, pcfg);
+	}
+	return val;
+}
+
+/*
+ * SI:Symbol Interval (PHY layer)
+ * return Sleep/Stall_NoConfig in SI using config
+ * value and SVC remote value.
+ * Warning!! Only MOUNTED Master side can use this function.
+*/
+u32 worst_min_sleep_stall(struct lli_cfg *pcfg)
+{
+	u32 val = 0, val_next = 0;
+
+	/* Check Tx MinSleep */
+	val = lli_rm_rd(MIPI_MPHY0_TXMINSLEEP_CAP);
+	val_next = lli_lo_rd(MIPI_MPHY0_TXMINSLEEP_CAP);
+
+	if (val_next > val)
+		val = val_next;
+
+	/* Check Tx MinStall */
+	val_next = lli_rm_rd(MIPI_MPHY0_TXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	val_next = lli_lo_rd(MIPI_MPHY0_TXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	/* Check Rx MinSleep */
+	val_next = lli_rm_rd(MIPI_MPHY0_RXMINSLEEP_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	val_next = lli_lo_rd(MIPI_MPHY0_RXMINSLEEP_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	/* Check Rx MinStall */
+	val_next = lli_rm_rd(MIPI_MPHY0_RXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	val_next = lli_lo_rd(MIPI_MPHY0_RXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	return val;
+}
+
+
+u32 worst_local_min_sleep_stall(struct lli_cfg *pcfg)
+{
+	u32 val = 0, val_next = 0;
+
+	/* Check Minute Tx MinSleep */
+	val = lli_lo_rd(MIPI_MPHY0_TXMINSLEEP_CAP);
+
+	/* Check Minute Tx MinStall */
+	val_next = lli_lo_rd(MIPI_MPHY0_TXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	/* Check Minute Rx MinSleep */
+	val_next = lli_lo_rd(MIPI_MPHY0_RXMINSLEEP_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	/* Check Minute Rx MinStall */
+	val_next = lli_lo_rd(MIPI_MPHY0_RXMINSTALL_CAP);
+	if (val_next > val)
+		val = val_next;
+
+	return val;
+}
+
+void set_lo_timeout(struct lli_cfg *pcfg)
+{
+	u32 retry_time_si = 0;
+	u32 retry_time = 0;
+	u16 patxsvin_si = 0, nextpatxsv = 0;
+	u16 patxsv = 0;
+	u32 cldrsttim = 0;
+	u32 rval = 0;
+	u32 one_us = 0;
+	u32 svctime = 0;
+	u32 aging = 0;
+	u32 wtstartinsymbol = 0, wtstartinsi = 0;
+
+	retry_time_si =
+	    worst_local_min_sleep_stall(pcfg) +
+	    worst_local_prep_length_in_si(pcfg) +
+	    worstLocalsync_lengthInSI(pcfg) + (64 * 6) * 5;
+
+	retry_time = unit_conv_si_to_symbol(retry_time_si);
+	pcfg->usr_pacfg.rbtc = retry_time;
+	lli_lo_wt(MIPI_USER_PA_RBTC, retry_time);
+
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+		patxsvin_si = lli_lo_rd(MIPI_MPHY0_TXMINSLEEP_CAP);
+		nextpatxsv = lli_lo_rd(MIPI_MPHY0_RXMINSLEEP_CAP);
+		if (nextpatxsv > patxsvin_si)
+			patxsvin_si = nextpatxsv;
+
+	} else { /* HS_MODE */
+		patxsvin_si = lli_lo_rd(MIPI_MPHY0_TXMINSTALL_CAP);
+		nextpatxsv = lli_lo_rd(MIPI_MPHY0_RXMINSTALL_CAP);
+		if (nextpatxsv > patxsvin_si)
+			patxsvin_si = nextpatxsv;
+
+	}
+	patxsv = unit_conv_si_to_symbol(patxsvin_si);
+	pcfg->usr_pacfg.patxsv = patxsv;
+	lli_lo_wt(MIPI_USER_PA_PATXSV, (u32)patxsv);
+
+	/* Local ColdResetTimer */
+	cldrsttim = (pcfg->tx_cfg.min_activate_time * 100) + 3500;
+
+	one_us = 40; /* 40 MHz PCLK */
+
+	rval = lli_lo_rd(MIPI_USER_CTRL_CLDRSTAUTO);
+	rval = (rval & 0xFF) | (cldrsttim << 16) | (one_us << 8);
+
+	lli_lo_wt(MIPI_USER_CTRL_CLDRSTAUTO, rval);
+	svctime = 0xFFFF;
+
+	pcfg->usr_ctlcfg.rstr = svctime;
+	lli_lo_wt(MIPI_USER_CTRL_RSTR, svctime);
+
+	aging = 0x96;
+	wtstartinsymbol = aging * 2 + 600;
+	wtstartinsi = unit_conv_symbol_to_si(wtstartinsymbol);
+
+	pcfg->pa_cfig.wt_start_value = wtstartinsi;
+	lli_lo_wt(MIPI_LLI_PA_WTSTARTVAL, wtstartinsi);
+}
+
+/*
+ * Retry time , ColdResetTimer , svctime ,
+ * Aging Time , Window timer
+ * set timeout values.
+ * Warning!! Only MOUNTED Master side can call this.
+ * Warning!! PLU required after this call,
+ * as AgingParam is shadow
+*/
+void set_timeout(struct lli_cfg *pcfg)
+{
+	u32 retry_time_si = 0;
+	u32 retry_time = 0;
+	u16 patxsvin_si = 0, nextpatxsv = 0;
+	u16 patxsv = 0;
+	u32 cldrsttim = 0;
+	u32 rval = 0;
+	u32 one_us = 0;
+	u32 svctimeinsi = 0;
+	u32 svctime = 0;
+	u32 rttinsymbol = 0, rttin_si = 0;
+	u32 aging = 0;
+	u32 wtstartinsymbol = 0, wtstartinsi = 0;
+	u32 r0 = 0, r1 = 0, r2 = 0;
+
+	/* Retry time configure */
+	r0 = worst_min_sleep_stall(pcfg);
+	r1 = worst_prep_length_in_si(pcfg);
+	r2 = worstsync_lengthInSI(pcfg) + (64 * 6) * 5;
+
+	retry_time_si = r0 + r1 + r2;
+	retry_time = unit_conv_si_to_symbol(retry_time_si);
+
+	/* DBG_MSG("retry_time is now %08x\n", retry_time); */
+	pcfg->usr_pacfg.rbtc = retry_time;
+
+	lli_rm_wt(MIPI_USER_PA_RBTC, retry_time);
+	lli_lo_wt(MIPI_USER_PA_RBTC, retry_time);
+
+	/* Minimum time that should stay in SAVE configure */
+	if (pcfg->tx_cfg.op_mode == LS_MODE) {
+
+		patxsvin_si = lli_lo_rd(MIPI_MPHY0_TXMINSLEEP_CAP);
+		nextpatxsv = lli_rm_rd(MIPI_MPHY0_RXMINSLEEP_CAP);
+
+		if (nextpatxsv > patxsvin_si)
+			patxsvin_si = nextpatxsv;
+
+	} else { /* HS_MODE*/
+		patxsvin_si = lli_lo_rd(MIPI_MPHY0_TXMINSTALL_CAP);
+		nextpatxsv = lli_rm_rd(MIPI_MPHY0_RXMINSTALL_CAP);
+
+		if (nextpatxsv > patxsvin_si)
+			patxsvin_si = nextpatxsv;
+	}
+	patxsv = unit_conv_si_to_symbol(patxsvin_si);
+	/* DBG_MSG("patxsv is now %04x\n", patxsv); */
+	pcfg->usr_pacfg.patxsv = patxsv;
+
+	lli_rm_wt(MIPI_USER_PA_PATXSV, patxsv);
+	lli_lo_wt(MIPI_USER_PA_PATXSV , patxsv);
+
+	/* Remote ColdResetTimer */
+	cldrsttim = (lli_rm_rd(MIPI_MPHY0_TXMINACTIVATTIME) * 100) + 3500;
+	one_us = 40; /* 40 MHz PCLK*/
+	rval = lli_rm_rd(MIPI_USER_CTRL_CLDRSTAUTO);
+	rval = (rval & 0xFF) | (cldrsttim << 16) | (one_us << 8);
+
+	/* DBG_MSG("Remote CldRstAuto is now %08x\n", rval); */
+	pcfg->usr_ctlcfg.cldrst = rval;
+
+	lli_rm_wt(MIPI_USER_CTRL_CLDRSTAUTO, rval);
+
+	/* Local ColdResetTimer */
+
+	r0 = lli_lo_rd(MIPI_MPHY0_TXMINACTIVATTIME);
+	cldrsttim = (r0 * 100) + 3500;
+	/* 40 MHz PCLK */
+	one_us = 40;
+	rval = lli_lo_rd(MIPI_USER_CTRL_CLDRSTAUTO);
+	rval = (rval & 0xFF) | (cldrsttim << 16) | (one_us << 8);
+	/* DBG_MSG("Local CldRstAuto is now %08x\n", rval); */
+	lli_lo_wt(MIPI_USER_CTRL_CLDRSTAUTO, rval);
+
+	/* Round Trip Time */
+	rttinsymbol = measure_nack_rtt();
+	rttin_si = unit_conv_symbol_to_si(rttinsymbol);
+
+	r0 = worst_min_sleep_stall(pcfg);
+	r1 = worst_prep_length_in_si(pcfg);
+	r2 = worstsync_lengthInSI(pcfg);
+	svctimeinsi = (r0 + r1 + r2 + rttin_si) * 5;
+
+	/* Remote SVC Timeout Configure */
+	svctime = unit_conv_si_to_pclk(svctimeinsi, pcfg);
+	/* DBG_MSG("SVCTime is now %08x\n", svctime); */
+	pcfg->usr_ctlcfg.rstr = svctime;
+	lli_rm_wt(MIPI_USER_CTRL_RSTR, svctime);
+	lli_lo_wt(MIPI_USER_CTRL_RSTR, svctime);
+
+	/* Aging Param Configure */
+	aging = rttinsymbol + 10;
+	 /* value is too large for the 24bit register*/
+	if (aging & 0xFF000000)
+		aging = 0xFFFFFF;
+
+	/* DBG_MSG("AgingParam is now %08x\n", aging); */
+
+	pcfg->usr_pacfg.aging_parameter = aging;
+
+	lli_rm_wt(MIPI_USER_PA_AGPRM, aging);
+	lli_lo_wt(MIPI_USER_PA_AGPRM, aging);
+
+	wtstartinsymbol = aging * 2 + 600;
+	wtstartinsi = unit_conv_symbol_to_si(wtstartinsymbol);
+	/* DBG_MSG("WT_Start_value is now %08x\n", wtstartinsi); */
+	pcfg->pa_cfig.wt_start_value = wtstartinsi;
+	lli_rm_wt(MIPI_LLI_PA_WTSTARTVAL, wtstartinsi);
+	lli_lo_wt(MIPI_LLI_PA_WTSTARTVAL, wtstartinsi);
+}
+
+/* used in pre-mount procedure */
+void local_set_deskew_to(struct lli_cfg *pcfg)
+{
+
+	u32 deskew_time_si = 0;
+	u32 deskew_time = 0;
+	u32 r0 = 0, r1 = 0, r2 = 0;
+
+	if (pcfg->rx_cfg.op_mode == LS_MODE) {
+		if (pcfg->rx_cfg.pwm_gear < PWM_G6) { /* G0-G5 */
+			r0 = lli_lo_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+			deskew_time_si = lspreplencaptosi(r0, pcfg) + 100;
+		} else { /* G6, G7 */
+			r0 = lli_lo_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXLSG67sync_len_CAP);
+			deskew_time_si =
+			    lspreplencaptosi(r0, pcfg) +
+			    get_sync_len(r1) + 100;
+		}
+	} else { /* HS_MODE */
+		switch (pcfg->rx_cfg.hs_gear) {
+		case HS_G1:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG1PREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXHSG1sync_len_CAP);
+			deskew_time_si = hspreplencaptosi(r0, pcfg) +
+			    get_sync_len(r1) + 100;
+			break;
+
+		case HS_G2:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG2PREPLEN_CAP);
+			r1 = hspreplencaptosi(r0, pcfg) + 100;
+			r2 = lli_lo_rd(MIPI_MPHY0_RXHSG2sync_len_CAP);
+			deskew_time_si = r1 + get_sync_len(r2);
+			break;
+
+		/* case HS_G3: */
+		default:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG3PREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXHSG3sync_len_CAP);
+			deskew_time_si = 100 + hspreplencaptosi(r0, pcfg) +
+			get_sync_len(r1);
+			break;
+		}
+	}
+
+	deskew_time = unit_conv_si_to_symbol(deskew_time_si);
+	/* DBG_MSG("deskew_time(local) is now %08x\n", deskew_time); */
+	pcfg->usr_pacfg.dskwtc = deskew_time;
+	lli_lo_wt(MIPI_USER_PA_DSKWTC, deskew_time);
+}
+
+void set_deskew_to(struct lli_cfg *pcfg)
+{
+	u32 deskew_time_si = 0;
+	u32 deskew_time = 0;
+	u32 r0 = 0, r1 = 0;
+
+	if (pcfg->rx_cfg.op_mode == LS_MODE) {
+		r0 = lli_lo_rd(MIPI_MPHY0_RXLSPREPLEN_CAP);
+		r1 = lli_lo_rd(MIPI_MPHY0_RXLSG67sync_len_CAP);
+		/* G0-G5 */
+		if (pcfg->rx_cfg.pwm_gear < PWM_G6)
+			deskew_time_si = 100 + lspreplencaptosi(r0, pcfg);
+		else { /* G6, G7 */
+			deskew_time_si = 100 +
+			lspreplencaptosi(r0, pcfg) + get_sync_len(r1);
+		}
+	} else { /* HS_MODE */
+
+		switch (pcfg->rx_cfg.hs_gear) {
+		case HS_G1:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG1PREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXHSG1sync_len_CAP);
+			deskew_time_si = hspreplencaptosi(r0, pcfg) +
+			    get_sync_len(r1) + 100;
+			break;
+		case HS_G2:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG2PREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXHSG2sync_len_CAP);
+			deskew_time_si = hspreplencaptosi(r0, pcfg) +
+			    get_sync_len(r1) + 100;
+			break;
+		/* case HS_G3: */
+		default:
+			r0 = lli_lo_rd(MIPI_MPHY0_RXHSG3PREPLEN_CAP);
+			r1 = lli_lo_rd(MIPI_MPHY0_RXHSG3sync_len_CAP);
+			deskew_time_si =
+			    hspreplencaptosi(r0, pcfg) + get_sync_len(r1)
+			    + 100;
+			break;
+		}
+	}
+
+	deskew_time = unit_conv_si_to_symbol(deskew_time_si);
+	/*DBG_MSG("deskew_time is now %08x\n", deskew_time);*/
+	pcfg->usr_pacfg.dskwtc = deskew_time;
+	lli_rm_wt(MIPI_USER_PA_DSKWTC, deskew_time);
+	lli_lo_wt(MIPI_USER_PA_DSKWTC, deskew_time);
+}
+
+
+u32 measure_nack_rtt(void)
+{
+	u32 val = 0, intval = 0;
+	/* tries 10 times for precise measure*/
+	/* u32 measure = 10; */
+	u32 measure = 1;
+	u32 i = 0;
+	u32 max_val = 0;
+	u32 try = 100;
+
+	for (i = 0; i < measure; i++) {
+
+		lli_lo_wt(MIPI_USER_PA_NACKRTTMSR, NACK_RTTMSR_EN);
+		while (try > 0) {
+
+			intval = lli_lo_rd(MIPI_USER_CTRL_INTST);
+
+			if (intval)
+				lli_lo_wt(MIPI_USER_CTRL_INTCLR, intval);
+
+			val = lli_lo_rd(MIPI_USER_PA_NACKRTTMSR);
+
+			if (val & NACK_RTTMSR_DONE) {
+				val = lli_lo_rd(MIPI_USER_PA_NACKRTTMSRRSLT);
+				/* DBG_MSG("Measured Val: %08x\n", val); */
+				if (max_val < val)
+					max_val = val;
+				break;
+			}
+			try--;
+		}
+	}
+
+	val = max_val;
+	/* DBG_MSG("Maximum measured val: %08x\n", val); */
+
+	return val;
+}
+
+
+
+
+void setup_error_detect(void)
+{
+	u32 v = 0;
+
+	v = B_RETRYTIMEN | B_NACKCNTEN | B_DSKWTIMEN;
+	lli_lo_wt(MIPI_USER_PA_CNTCTL, v);
+}
+
+void lli_lo_wt(u32 f, u32 v)
+{
+	struct lli_dev *dev_lli;
+	struct lli_data *lli_ch;
+
+	dev_lli = gp_mb86s70_lli_dev;
+	lli_ch = &dev_lli->chan_data[ACT_LLI_CH];
+	writel(v, lli_ch->regs + (f | LOCAL_ADDR_BIT));
+}
+
+u32 lli_lo_rd(u32 v)
+{
+	struct lli_dev *dev_lli;
+	struct lli_data *lli_ch;
+
+	dev_lli = gp_mb86s70_lli_dev;
+	lli_ch = &dev_lli->chan_data[ACT_LLI_CH];
+	return readl(lli_ch->regs + (v | LOCAL_ADDR_BIT));
+}
+
+
+void lli_rm_wt(u32 f, u32 v)
+{
+	struct lli_dev *dev_lli;
+	struct lli_data *lli_ch;
+
+	dev_lli = gp_mb86s70_lli_dev;
+	lli_ch = &dev_lli->chan_data[ACT_LLI_CH];
+	writel(v, lli_ch->regs + (f & (~LOCAL_ADDR_BIT)));
+}
+
+u32 lli_rm_rd(u32 v)
+{
+	struct lli_dev *dev_lli;
+	struct lli_data *lli_ch;
+
+	dev_lli = gp_mb86s70_lli_dev;
+	lli_ch = &dev_lli->chan_data[ACT_LLI_CH];
+	return readl(lli_ch->regs + (v & (~LOCAL_ADDR_BIT)));
+}
diff --git a/drivers/char/mipi_lli/mipilli_major.c b/drivers/char/mipi_lli/mipilli_major.c
new file mode 100644
index 0000000..69680a8
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_major.c
@@ -0,0 +1,576 @@
+/*
+ * mipilli_major.c F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include "mipilli_api.h"
+
+#define master_mount() \
+	lli_lo_wt(CTRL_CSASYSTEMSET,\
+	(B_CSA_MASTER_NOT_SLAVE | B_CSA_LLI_MOUNT_CTRL));
+
+#define master_unmount() \
+	lli_lo_wt(CTRL_CSASYSTEMCLR,\
+	B_CSA_LLI_MOUNT_CTRL);
+
+
+#define if_mounted() \
+	((lli_lo_rd(CTRL_CSASYSTEMSTAT) & \
+	B_CSA_LLI_MOUNTED) ? true : false)
+
+#define if_link_update() \
+	((lli_lo_rd(PA_CSA_PA_STATUS) & \
+	B_CSA_PA_LINKUPDATECONFIG) ? true : false)
+
+#define dis_local_automode() \
+	lli_lo_wt(MIPI_LLI_PA_CSA_PA_CLEAR,\
+	B_CSA_PA_AUTO_MODE);
+
+#define dis_remote_automode() \
+	lli_rm_wt(MIPI_LLI_PA_CSA_PA_CLEAR,\
+	B_CSA_PA_AUTO_MODE);
+
+#define slave_unmount() \
+	lli_rm_wt(CTRL_CSASYSTEMCLR, \
+	B_CSA_LLI_MOUNT_CTRL);
+
+#define slave_mount() \
+do {\
+	lli_lo_wt(CTRL_CSASYSTEMCLR, \
+	B_CSA_MASTER_NOT_SLAVE); \
+	lli_lo_wt(CTRL_CSASYSTEMSET, \
+	B_CSA_LLI_MOUNT_CTRL);\
+} while (0)
+
+#define phy_link_upddate() \
+	lli_lo_wt(MIPI_LLI_PA_CSA_PA_SET,\
+	B_CSA_PA_LINKUPDATECONFIG);
+
+#define make_tx_hib8(a) \
+	mphy_write_reg(MIPI_MPHY0_TXHIBERN8CTRL,\
+	MPHY_WRITE_TAG_LOCAL | MPHY_WRITE_TAG_REMOTE,\
+	ENTER, 0,\
+	a->pa_cfig.active_tx_count, 0, 0);
+
+#define make_tx_exit_hib8(a) \
+	mphy_write_reg(MIPI_MPHY0_TXHIBERN8CTRL,\
+	MPHY_WRITE_TAG_LOCAL | MPHY_WRITE_TAG_REMOTE,\
+	EXIT, 0, a->pa_cfig.active_tx_count, 0, 0);
+
+
+#define make_rx_hib8(a) \
+	mphy_write_reg(MIPI_MPHY0_RXENTHIBE8,\
+	MPHY_WRITE_TAG_LOCAL, ENTER,\
+	0, 0, 0, a->pa_cfig.active_rx_count);
+
+#define make_rx_exit_hib8(a) \
+	mphy_write_reg(MIPI_MPHY0_RXENTHIBE8,\
+	MPHY_WRITE_TAG_LOCAL, EXIT, 0, 0, 0, a->pa_cfig.active_rx_count);
+
+
+int mipilli_write_signal(u32 v)
+{
+	lli_rm_wt(MIPI_LLI_SIGNAL_SIGNALSET, v);
+	return 0;
+}
+
+u32 mipilli_read_signal(void)
+{
+	u32 v = 0;
+
+	v = lli_lo_rd(MIPI_LLI_SIGNAL_SIGNALSTATE);
+	return v;
+}
+
+
+int mipilli_master_mount(u8 ch_num)
+{
+	u8 r = 0;
+	u8 retry_cnt = RETRY_COUNTER;
+	struct lli_data *the_lli;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	#if DBG_M_MOUNT
+	DBG_MSG("1.%s Master mount start\n",
+		the_lli->lli_chan_name);
+	#endif
+
+	master_mount();
+
+	#if DBG_M_MOUNT
+	DBG_MSG("2.Master Wait for INT_MNT\n");
+	#endif
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+	INIT_COMPLETION(the_lli->msg_cmp);
+
+	if (r == 0) {
+		ERR_MSG("Error->Master MNT_INT timeout\n");
+		return MASTER_MOUNT_MNTINT_TIMEOUT;
+	}
+
+	if (the_lli->lli_interrupt_status & B_MNT) {
+
+		DBG_MSG("3.Master detect MNT_INT\n");
+
+		#if DBG_M_MOUNT
+		while ((if_mounted() == false)) {
+			if (retry_cnt <= 0) {
+				ERR_MSG(
+				"Error->Master no detect CSA:MOUNTED\n");
+				r = MASTER_MOUNT_NO_CSA_MOUNTED;
+				the_lli->mount_stat = false;
+				ERR_MSG("Error->Master Mount done fail\n");
+				return r;
+			}
+			retry_cnt--;
+			/*msleep(20);*/
+		}
+		#endif
+
+		the_lli->mount_stat = true;
+
+		#ifdef DBG_M_MOUNT
+		DBG_MSG("4.Master detect CSA:MOUNTED\n");
+		DBG_MSG("5.Master Mount done successfully\n");
+		#endif
+		return MASTER_MOUNT_SUCCESS;
+
+	}
+
+	ERR_MSG("Error->Master no detect INT_MNT\n");
+	r = MASTER_MOUNT_NO_MNTINT;
+	the_lli->mount_stat = false;
+	ERR_MSG("Error->Master Mount done fail\n");
+	return r;
+}
+
+
+int mipilli_master_unmount(u8 ch_num)
+{
+	u32 r = 0;
+	u32 retry_cnt = RETRY_COUNTER;
+	u32 rx_lane = 0;
+	struct lli_data *the_lli;
+	struct lli_cfg *p_config;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	p_config = (struct lli_cfg *)the_lli->lli_defcfg;
+
+	#if DBG_M_UNMOUNT
+	DBG_MSG("1.Master Unmount start\n");
+	#endif
+
+	/* Disable Auto mode*/
+	#if DBG_M_UNMOUNT
+	DBG_MSG("2.Master disable auto mode\n");
+	#endif
+
+	dis_local_automode();
+	dis_remote_automode();
+
+	/*DBG_MSG("Make Tx Hibernate(in shadow)\n");*/
+
+	make_tx_hib8(p_config);
+
+	#if DBG_M_UNMOUNT
+	DBG_MSG("3.Set Un-mount\n");
+	#endif
+
+	#if DBG_M_UNMOUNT
+	DBG_MSG("Set Slave Unmount\n");
+	#endif
+
+	slave_unmount();
+
+	#if DBG_M_UNMOUNT
+	DBG_MSG("Set Master Unmount\n");
+	#endif
+
+	master_unmount();
+
+	#if DBG_M_UNMOUNT
+	DBG_MSG("4.Master Wait UnMNT_INT\n");
+	#endif
+
+	r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+	INIT_COMPLETION(the_lli->msg_cmp);
+	if (!r) {
+		ERR_MSG("Error->UnMNT INT Timeout\n");
+		r = MASTER_UNMOUNT_UNMNTINT_TIMEOUT;
+		return r;
+	}
+
+	if (the_lli->lli_interrupt_status & B_UNMNT) {
+
+		DBG_MSG("5.Master detect UnMNT_INT\n");
+
+		#if DBG_M_UNMOUNT
+		while ((lli_lo_rd(CTRL_CSASYSTEMSTAT) |
+				B_CSA_LLI_MOUNT_CTRL) > 0) {
+
+			retry_cnt--;
+			if (retry_cnt <= 0) {
+				r = MASTER_UNMOUNT_CSA_MOUNTED;
+				ERR_MSG(
+				"Err->Master CSA:MOUNT_CTRL!=0\n");
+				return r;
+			}
+			/*msleep(20);*/
+		}
+		#endif
+
+		#if DBG_M_UNMOUNT
+		DBG_MSG("6.Master no detect CSA:MOUNT_CTRL\n");
+		DBG_MSG("7.Master make Rx Hibernate\n");
+		#endif
+
+		make_rx_hib8(p_config);
+
+		/*set PA config_update*/
+		/*DBG_MSG("Set PA config_update\n");*/
+		rx_lane = ~(0xFFF << p_config->pa_cfig.active_rx_count);
+		lli_lo_wt(MIPI_LLI_PA_CONFIG_UPDATE, rx_lane << 12);
+	}
+
+	r = MASTER_UNMOUNT_SUCCESS;
+	the_lli->mount_stat = false;
+	DBG_MSG("8.Master unmount successfully\n");
+	return r;
+}
+
+/*8.4 LLI Mount Procedure*/
+int mipilli_slave_mount(u8 ch_num)
+{
+	u8 r = 0;
+	u32 retry_cnt = RETRY_COUNTER;
+	u32 int_s = 0;
+	struct lli_cfg *p_config;
+	struct lli_data *the_lli;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	p_config = (struct lli_cfg *)the_lli->lli_defcfg;
+
+	DBG_MSG("1.%s Slave mount start\n", the_lli->lli_chan_name);
+	DBG_MSG("2.Slave Wait for RXH8EXIT_INT\n");
+
+	while (retry_cnt > 0) {
+
+		r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+		INIT_COMPLETION(the_lli->msg_cmp);
+
+		int_s = the_lli->lli_interrupt_status & B_RXH8EXIT;
+		/*
+		if (!r) {
+			DBG_MSG("Error->RXH8EXIT_INT timeout\n");
+		}
+		*/
+		if ((r > 0) && (int_s > 0)) {
+			#ifdef DBG_S_MOUNT
+			DBG_MSG("3.Slave Get RXH8EXIT_INT\n");
+			#endif
+
+			retry_cnt = RETRY_COUNTER;
+			while ((lli_lo_rd(CTRL_CSASYSTEMSTAT) &
+				    B_CSA_PA_HIB8) > 0) {
+
+				retry_cnt--;
+				if (retry_cnt <= 0) {
+					ERR_MSG(
+					"Error->CSASYSTEMSTAT:HIB8=1\n");
+					return SLAVE_MOUNT_CSA_HIB8EXIT;
+				}
+				/*msleep(20);*/
+			}
+
+			#if DBG_S_MOUNT
+			DBG_MSG("4.CSASYSTEMSTAT:HIBERNATE==0\n");
+			#endif
+			slave_mount();
+			/*goto lab_slave_mount;*/
+			retry_cnt = RETRY_COUNTER;
+			break;
+		}
+		/*msleep(20);*/
+		retry_cnt--;
+	}
+
+	if (retry_cnt <= 0) {
+		r = SLAVE_MOUNT_HIB8EXIT_INT_TIME_OUT;
+		ERR_MSG("Error->Slave RXH8EXIT_INT timeout\n");
+		/*goto slave_mount_err;*/
+		the_lli->mount_stat = false;
+		ERR_MSG("Error->Slave Mount fail\n");
+		return r;
+	}
+
+/*lab_slave_mount:*/
+	#if DBG_S_MOUNT
+	DBG_MSG("5.Slave wait MNT_INT\n");
+	#endif
+	r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+	INIT_COMPLETION(the_lli->msg_cmp);
+
+	if (!r) {
+		r = SLAVE_MOUNT_MNTINT_TIMEOUT;
+		ERR_MSG("Error->Slave MNT_INT timerout!\n");
+		/*goto slave_mount_err;*/
+		the_lli->mount_stat = false;
+		ERR_MSG("Error->Slave Mount fail\n");
+		return r;
+	}
+
+	if (the_lli->lli_interrupt_status & B_MNT) {
+
+		#if DBG_S_MOUNT
+		DBG_MSG("6.Slave Get MNT_INT\n");
+		#endif
+
+		retry_cnt = RETRY_COUNTER;
+		while (if_mounted() == false) {
+			retry_cnt--;
+			if (retry_cnt <= 0) {
+				r = SLAVE_MOUNT_NO_CSA_MOUNTED;
+				ERR_MSG(
+				"Err->Slave MOUNTED not detected\n");
+				/*goto slave_mount_err;*/
+				the_lli->mount_stat = false;
+				ERR_MSG("Error->Slave Mount fail\n");
+				return r;
+			}
+			/*msleep(20);*/
+		}
+
+		#if DBG_S_MOUNT
+		DBG_MSG("7.Slave MOUNTED detected\n");
+		DBG_MSG("8.Slave Mount done successfully\n");
+		#endif
+		the_lli->mount_stat = true;
+		return SLAVE_MOUNT_SUCCESS;
+	}
+
+	/*slave_mount_err:*/
+	the_lli->mount_stat = false;
+	ERR_MSG("Error->Slave Mount fail\n");
+	return r;
+}
+
+/*
+ * Configuration Update and Lane Count Update implementation.
+ * Lane Count Update mipi_lli_spec 8.6
+ * Configuration Change Procedure
+ */
+int mipilli_config_update(u8 ch_num, u8 txlanes, u8 rxlanes)
+{
+	u32 retry_cnt = RETRY_COUNTER;
+	u32 r = 0;
+	u32 prev_auto_mode = 0;
+	u8 old_txlanes = 0;
+	u8 old_rxlanes = 0;
+	u32 act_tx_cnt = 0;
+	u32 act_rx_cnt = 0;
+
+	struct lli_data *the_lli;
+	struct lli_cfg *p_config;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	p_config = (struct lli_cfg *)the_lli->lli_defcfg;
+
+	#if DBG_CONFIG_UPDATE
+	DBG_MSG("1.ConfigUpdate start\n");
+	#endif
+
+	r = lli_lo_rd(PA_CSA_PA_STATUS);
+	/* prev_auto_mode = ((r & B_CSA_PA_AUTO_MODE) ? true : false); */
+
+	/* Disable Auto mode*/
+	#if DBG_CONFIG_UPDATE
+	DBG_MSG("2.Disable Local Auto Mode\n");
+	#endif
+
+	dis_local_automode();
+
+	#if DBG_CONFIG_UPDATE
+	DBG_MSG("3.Disable Remote Auto Mode\n");
+	#endif
+
+	dis_remote_automode();
+
+	/* Save old TX/RX lanes*/
+	old_txlanes = lli_lo_rd(MIPI_LLI_PA_ACTIVETXCOUNT);
+	old_rxlanes = lli_lo_rd(PA_ACT_RX_CNT);
+
+	/* for each shadow attributes except active lane counts.
+	   set local value and then set remote value using SVC
+	*/
+	r = update_shadow_remote(p_config, txlanes, rxlanes);
+
+	if (!r) {
+		r = CONFIG_UPDATE_SHADOW_FAIL;
+		/* goto config_update_err; */
+		ERR_MSG("Error:ConfigUpdate done fail\n");
+		return r;
+	}
+
+	if (old_txlanes < txlanes) {
+		/* Set new lanes to Hibernate8 state
+		 * Set previously unused lanes to go out of Hibernate8
+		*/
+		make_tx_exit_hib8(p_config);
+		/*
+		if (!result) {
+			goto configUpdate_Err;
+		}
+		*/
+	}
+
+	/* update Active_lane count if required*/
+	if (old_txlanes != txlanes) {
+		/*DBG_MSG("Update Master TX active lane count\n");*/
+		act_tx_cnt = p_config->pa_cfig.active_tx_count;
+		lli_lo_wt(MIPI_LLI_PA_ACTIVETXCOUNT, act_tx_cnt);
+		lli_rm_wt(PA_ACT_RX_CNT, act_tx_cnt);
+	}
+
+	if (old_rxlanes != rxlanes) {
+		/*DBG_MSG("Update Master RX active lane count\n");*/
+		act_rx_cnt = p_config->pa_cfig.active_rx_count;
+		lli_lo_wt(PA_ACT_RX_CNT, act_rx_cnt);
+		lli_rm_wt(MIPI_LLI_PA_ACTIVETXCOUNT, act_rx_cnt);
+	}
+
+	#if DBG_CONFIG_UPDATE
+	DBG_MSG("4.Start PLU Update\n");
+	#endif
+
+	phy_link_upddate();
+
+	#if DBG_CONFIG_UPDATE
+	DBG_MSG("5.Wait PLU INT\n");
+	#endif
+
+	r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+	INIT_COMPLETION(the_lli->msg_cmp);
+
+	if (r == 0) {
+		ERR_MSG("Error:PLU INT timeout\n");
+		r = CONFIG_UPDATE_PLU_INT_TIMEOUT;
+		ERR_MSG("Error:ConfigUpdate done fail\n");
+		return r;
+	}
+
+	if (the_lli->lli_interrupt_status & B_PLUFIN) {
+
+		DBG_MSG("6.PLU INT Detect\n");
+
+		while (if_link_update() == true) {
+			retry_cnt--;
+			if (retry_cnt <= 0) {
+				ERR_MSG(
+				"Error:PLUFin CSA LinkUpdateConfig=1\n");
+				r = CONFIG_UPDATE_CSA_LINKUPDATE;
+				break;
+			}
+			/*msleep(20);*/
+		}
+
+		if (retry_cnt > 0) {
+
+			#if DBG_CONFIG_UPDATE
+			DBG_MSG(
+			"7.PLUFin in CSA LinkUpdateConfig=0\n");
+			DBG_MSG(
+			"8.PLUFin in CSA LinkUpdateConfig success\n");
+			#endif
+			r = CONFIG_UPDATE_SUCCESS;
+		}
+	} else {
+		ERR_MSG("Error:PLU INT failure (No detect)\n");
+		r = CONFIG_UPDATE_NO_PLU_INT;
+	}
+
+	/* Set Auto mode if originally set*/
+	if (prev_auto_mode) {
+		/*DBG_MSG("restore Auto Mode\n");*/
+		lli_lo_wt(MIPI_LLI_PA_CSA_PA_SET, B_CSA_PA_AUTO_MODE);
+		lli_rm_wt(MIPI_LLI_PA_CSA_PA_SET, B_CSA_PA_AUTO_MODE);
+	}
+
+
+	if (r == CONFIG_UPDATE_SUCCESS) {
+		#if DBG_CONFIG_UPDATE
+		DBG_MSG("9.ConfigUpdate done successfully\n");
+		#endif
+		return r;
+	} else {
+		ERR_MSG("Error:ConfigUpdate done fail\n");
+	}
+
+	/* config_update_err: */
+	ERR_MSG("Error:ConfigUpdate done fail\n");
+	return r;
+}
+
+int mipilli_change_memmap(u32 from_addr, u32 to_addr, u32 size)
+{
+	u8 i;
+	u32 size_adjust = 0x01;
+	u32 addr_mask;
+
+	/* check if size is 2^n (n >= 12)*/
+	for (i = 0; i < 32; i++, size_adjust <<= 1) {
+		if (size_adjust >= size)
+			break;
+	}
+
+	if (size_adjust != size) {
+		/*
+		 * DBG_MSG(
+		 * "Warning!! changeMemmap size
+		 * must be 2^N(N>=12)!!\n");
+		 *
+		 */
+		size = size_adjust;
+		/*DBG_MSG("Adjust size, now it is %d Bytes\n", size);*/
+	}
+
+	addr_mask = size - 1;
+	if (from_addr & addr_mask) {
+		/*DBG_MSG(
+		 "Warning!! changeMemmap
+		 from_addr must be aligned to size!!\n");
+		 */
+		from_addr &= ~addr_mask;
+		/*DBG_MSG("Adjust from_addr, now it is %08x\n", from_addr);
+		 */
+	}
+
+	if (to_addr & addr_mask) {
+		/*DBG_MSG(
+		 * "Warning!! changeMemmap to_addr
+		 * must be aligned to size!!\n");
+		 */
+		to_addr &= ~addr_mask;
+		/*DBG_MSG("Adjust to_addr, now it is %08x\n", to_addr);*/
+	}
+
+	lli_lo_wt(MIPI_USER_CTRL_MADREMAPUM1, (~addr_mask) >> 12);
+	lli_lo_wt(MIPI_USER_CTRL_MADREMAPFLT1, from_addr >> 12);
+	lli_lo_wt(MIPI_USER_CTRL_MADREMAP1, to_addr >> 12);
+	return 0;
+}
diff --git a/drivers/char/mipi_lli/mipilli_mb86s70.c b/drivers/char/mipi_lli/mipilli_mb86s70.c
new file mode 100644
index 0000000..67c3e07
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_mb86s70.c
@@ -0,0 +1,935 @@
+/*
+ * mipilli_mb86s70.c F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/configfs.h>
+#include <linux/ioport.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/fs.h>
+#include <linux/ioctl.h>
+#include <linux/uaccess.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+#include <linux/mm.h>
+#include <asm/irq.h>
+#include <linux/timer.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/clk.h>
+#include <linux/of.h>
+#include <linux/uaccess.h>
+#include "mipilli_mb86s70.h"
+
+struct lli_dev *gp_mb86s70_lli_dev;
+u32 *glli_major;
+u32 glli_num;
+
+
+void en_lli_chan(u8 num)
+{
+	gp_mb86s70_lli_dev->act_lli_ch = num;
+}
+
+
+u32 lli_get_driver_version(struct lli_dev *dev_lli)
+{
+	return dev_lli->lli_drv_ver;
+}
+
+void init_mb86s70_lli_reg(struct lli_dev *dev_lli, u8 num)
+{
+	struct lli_data *lli_ch = NULL;
+
+	dev_lli->act_lli_ch = num;
+	lli_ch = &dev_lli->chan_data[num];
+	set_lo_cfg(lli_ch->lli_defcfg);
+}
+
+
+
+static irqreturn_t lli_irq(int irqno, void *dev_id)
+{
+	s16 i = 0;
+	struct lli_dev *dev_lli = (struct lli_dev *)dev_id;
+	struct lli_data *lli_ch = NULL;
+	u32 status = 0;
+	unsigned long flags = 0;
+	/* int ch_id = 0; */
+
+	while ((irqno != dev_lli->chan_data[i].irq) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num) {
+		DBG_MSG("lli_irq Err:irqno=%d\n", irqno);
+		spin_unlock_irqrestore(&lli_ch->mb86s70_irq_lock, flags);
+		return IRQ_HANDLED;
+	}
+
+	en_lli_chan(i);
+	lli_ch = &dev_lli->chan_data[i];
+
+	/* ch_id = MAJOR(lli_ch->devt); */
+	spin_lock_irqsave(&lli_ch->mb86s70_irq_lock, flags);
+	status = lli_lo_rd(MIPI_USER_CTRL_INTST);
+	lli_lo_wt(MIPI_USER_CTRL_INTCLR, status);
+
+	if (status & (B_SIGSET | B_SEQERR | B_CRCERR |
+			B_PHYSYMERR | B_FIFOOVF |
+			B_CRSTAST | B_RSTONERRDET |
+			B_MNTFTlERR)) {
+
+		#if DBG_FLAG
+
+		if (status & B_SIGSET)
+			DBG_MSG("B_SIGSET\n");
+
+		if (status & B_SEQERR)
+			DBG_MSG("SEQ Error\n");
+
+		if (status & B_CRCERR)
+			DBG_MSG("CRC Error\n");
+
+		if (status & B_PHYSYMERR)
+			DBG_MSG("PHY SYMBOL Error\n");
+
+		if (status & B_FIFOOVF)
+			DBG_MSG("FIFO of Data Link Layer is OverFlow\n");
+
+		if (status & B_CRSTAST)
+			DBG_MSG("Cool Reset\n");
+
+		if (status & B_RSTONERRDET)
+			DBG_MSG("Reset On Error\n");
+
+		if (status & B_MNTFTlERR)
+			DBG_MSG("Mount Fail\n");
+
+		#endif
+	} else {
+		complete(&lli_ch->msg_cmp);
+	}
+
+	spin_unlock_irqrestore(&lli_ch->mb86s70_irq_lock, flags);
+
+	return IRQ_HANDLED;
+}
+
+static int
+lli_open(struct inode *inode, struct file *file)
+{
+	int status = -1;
+	int i = 0;
+	int lli_major_num = 0;
+	struct lli_data *lli_ch = NULL;
+
+	file->private_data = (struct lli_dev *)gp_mb86s70_lli_dev;
+	gp_mb86s70_lli_dev->cur_lli_major_num = imajor(inode);
+	lli_major_num = imajor(inode);
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+	lli_ch = &gp_mb86s70_lli_dev->chan_data[i];
+
+	if (down_trylock(&(lli_ch->openf_sem)) == 0)
+		status = 0;
+
+	return status;
+}
+
+static int
+lli_release(struct inode *inode, struct file *file)
+{
+	s8 status = -1;
+	s8 i = 0;
+	u32 lli_major_num;
+	struct lli_data *lli_ch = NULL;
+	struct lli_dev *dev_lli = NULL;
+
+	dev_lli = (struct lli_dev *)file->private_data;
+	lli_major_num = imajor(inode);
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+	lli_ch = &gp_mb86s70_lli_dev->chan_data[i];
+
+
+	up(&(lli_ch->openf_sem));
+
+	status = 0;
+	return status;
+}
+
+static ssize_t
+lli_mmap(struct file *f, struct vm_area_struct *vma)
+{
+	int r = 0, i = 0;
+	int lli_major_num = 0;
+	u64 lli_mem_addr = 0;
+	struct lli_dev *dev_lli;
+
+
+	dev_lli = (struct lli_dev *)f->private_data;
+	lli_major_num = dev_lli->cur_lli_major_num;
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+
+	lli_mem_addr = LLI0_MEM_ADDR + LLI_MEM_OFF * i;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+
+	r = remap_pfn_range(vma, vma->vm_start,
+	    __phys_to_pfn(lli_mem_addr),
+	    vma->vm_end-vma->vm_start,
+	    vma->vm_page_prot);
+
+	if (r < 0) {
+		ERR_MSG("lli_mmap:Error->lli_mmap error\n");
+		return -1;
+	}
+
+	return 0;
+}
+
+
+static
+ssize_t lli_write(struct file *file,
+			const char __user *buf, size_t count, loff_t *ppos)
+{
+	s8 status = 0, i = 0;
+	u16 lli_major_num;
+	struct lli_dev *dev_lli;
+
+	dev_lli = (struct lli_dev *)file->private_data;
+	lli_major_num = dev_lli->cur_lli_major_num;
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+
+	if (down_trylock(&(dev_lli->buf_sem)) != 0) {
+		ERR_MSG("lli_write:Error down_trylock fail\n");
+		status = -1;
+	}
+
+	up(&(dev_lli->buf_sem));
+
+	return status;
+}
+
+
+
+
+static ssize_t
+lli_read(struct file *file, char __user *buf,
+		size_t count, loff_t *ppos)
+{
+	s8 status = 0, i = 0;
+	u16 lli_major_num;
+	struct lli_dev *dev_lli;
+
+	dev_lli = (struct lli_dev *)file->private_data;
+	lli_major_num = dev_lli->cur_lli_major_num;
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+
+	if (down_trylock(&(dev_lli->buf_sem)) != 0) {
+		ERR_MSG("lli_read:Error down_trylock fail\n");
+		status = -1;
+	}
+
+	up(&(dev_lli->buf_sem));
+	return status;
+}
+
+
+
+static long lli_ioctl(struct file *filp,
+				unsigned cmd, unsigned long arg)
+{
+	u32 lli_major_num;
+	struct ioctl_reg ioctl_iil_reg;
+	struct ioctl_memmap ioctl_iil_memap;
+	struct ioctl_cfg arg_lli_cfg;
+	struct lli_data *the_lli;
+	struct lli_dev *dev_lli;
+	s8 r = 0, i = 0;
+	u32 drv_ver;
+
+	dev_lli = (struct lli_dev *)filp->private_data;
+	lli_major_num = dev_lli->cur_lli_major_num;
+
+	while ((lli_major_num != glli_major[i]) && (i < glli_num)) {
+		i++;
+	};
+	if (i >= glli_num)
+		return -1;
+
+	en_lli_chan(i);
+	the_lli = &dev_lli->chan_data[ACT_LLI_CH];
+
+	switch (cmd) {
+
+	case LLI_IOCTL_M_MOUNT: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_M_MOUNT\n");
+		#endif
+
+		the_lli->master_or_slave = AS_MASTER;
+
+		if (the_lli->mount_stat == true) {
+			#ifdef DBG_IO_CTL
+			DBG_MSG("%s had mounted\n", the_lli->lli_chan_name);
+			#endif
+			return 1;
+		}
+
+		return mipilli_master_mount(ACT_LLI_CH);
+	}
+
+	case LLI_IOCTL_M_UNMOUNT: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_M_UNMOUNT\n");
+		#endif
+
+		if (the_lli->mount_stat == false) {
+			#ifdef DBG_IO_CTL
+			DBG_MSG("%s had un-mounted\n",
+			the_lli->lli_chan_name);
+			#endif
+			return 1;
+		}
+
+		return mipilli_master_unmount(ACT_LLI_CH);
+	}
+
+	case LLI_IOCTL_S_MOUNT: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_S_MOUNT\n");
+		#endif
+
+		the_lli->master_or_slave = AS_SLAVE;
+
+		if (the_lli->mount_stat == true) {
+			#ifdef DBG_IO_CTL
+			DBG_MSG("%s had mounted\n",
+			the_lli->lli_chan_name);
+			#endif
+			return 1;
+		}
+
+		return mipilli_slave_mount(/*the_lli*/ACT_LLI_CH);
+	}
+
+	/*
+	case LLI_IOCTL_S_MAIN_LOOP:{
+		return 0;
+	}
+
+	case LLI_IOCTL_SET_LOCAL_CONFIG:{
+		DBG_MSG("LLI_IOCTL_SET_LOCAL_CONFIG\n");
+		return 0;
+	}
+	*/
+	case LLI_IOCTL_CONFIG_UPDATE: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_CONFIG_UPDATE\n");
+		#endif
+
+		if (the_lli->mount_stat == false) {
+			#ifdef DBG_IO_CTL
+			ERR_MSG("%s had no mounted\n",
+			the_lli->lli_chan_name);
+			#endif
+			return -1;
+		}
+
+		if (the_lli->master_or_slave == AS_SLAVE) {
+			#ifdef DBG_IO_CTL
+			ERR_MSG("This channel is SLAVE\n");
+			#endif
+			return -1;
+		}
+
+		r = copy_from_user(&arg_lli_cfg,
+				(struct ioctl_cfg __user *)arg,
+				sizeof(struct ioctl_cfg));
+		if (r)
+			return -1;
+
+		/* read configure to user */
+		if (arg_lli_cfg.direct == 0) {
+			memcpy(&arg_lli_cfg.lli_defcfg,
+			    the_lli->lli_defcfg, sizeof(struct lli_cfg));
+
+			r = copy_to_user((struct ioctl_cfg __user *)arg,
+			    &arg_lli_cfg,
+			    sizeof(struct ioctl_cfg));
+			if (r)
+				return -1;
+
+			return 0;
+
+		} else {/* write configure to mipi-lli */
+			memcpy(the_lli->lli_defcfg, &arg_lli_cfg.lli_defcfg,
+				sizeof(struct lli_cfg));
+
+			r = mipilli_config_update(ACT_LLI_CH, TX_LANE_CNT,
+					RX_LANE_CNT);
+			if (r < 0)
+				return -1;
+			else
+				return 0;
+		}
+
+		r = mipilli_config_update(ACT_LLI_CH, TX_LANE_CNT, RX_LANE_CNT);
+		if (r < 0)
+			return -1;
+		else
+			return 0;
+	}
+
+	case LLI_IOCTL_CHANGE_MEMMAP: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_CHANGE_MEMMAP\n");
+		DBG_MSG("fromAddr=0x%x\n", ioctl_iil_memap.fromAddr);
+		DBG_MSG("toAddr=0x%x\n", ioctl_iil_memap.toAddr);
+		DBG_MSG("size=0x%x\n", ioctl_iil_memap.size);
+		#endif
+		r = copy_from_user(&ioctl_iil_memap,
+			    (struct ioctl_memmap __user *)arg,
+			    sizeof(struct ioctl_memmap));
+		if (r)
+			return -1;
+
+		return mipilli_change_memmap(ioctl_iil_memap.fromAddr,
+				    ioctl_iil_memap.toAddr,
+				    ioctl_iil_memap.size);
+	}
+
+	/*
+	case LLI_IOCTL_SET_REMOTE_SIG:{
+		signal_set_val = arg;
+		return mipilli_write_signal(signal_set_val);
+	}
+	*/
+
+	case LLI_IOCTL_WRITE_REG: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_WRITE_REG\n");
+		#endif
+
+		r = copy_from_user(&ioctl_iil_reg,
+			    (struct ioctl_reg __user *)arg,
+			    sizeof(struct ioctl_reg));
+
+		if (r > 0)
+			return -1;
+
+		if ((ioctl_iil_reg.reg_addr & LOCAL_ADDR_BIT))
+			lli_lo_wt(ioctl_iil_reg.reg_addr,
+			    ioctl_iil_reg.reg_data);
+		else {
+			if (the_lli->mount_stat == false) {
+				DBG_MSG("%s no mounte\n",
+				the_lli->lli_chan_name);
+				return -1;
+			}
+			lli_rm_wt(ioctl_iil_reg.reg_addr,
+			    ioctl_iil_reg.reg_data);
+		}
+
+		return 0;
+	}
+
+	case LLI_IOCTL_READ_REG: {
+
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_READ_REG\n");
+		#endif
+
+		r = copy_from_user(&ioctl_iil_reg,
+			    (struct ioctl_reg __user *)arg,
+			    sizeof(struct ioctl_reg));
+
+		if (r > 0)
+			return -1;
+
+		if ((ioctl_iil_reg.reg_addr & LOCAL_ADDR_BIT)) {
+
+			#ifdef DBG_IO_CTL
+			DBG_MSG(
+			    "ReadReg->Local reg_addr=0x%x reg_data=0x%x\n",
+			    ioctl_iil_reg.reg_addr,
+			    ioctl_iil_reg.reg_data);
+			#endif
+
+			ioctl_iil_reg.reg_data =
+			    lli_lo_rd(ioctl_iil_reg.reg_addr);
+
+		} else {
+
+			if (the_lli->mount_stat == false) {
+				DBG_MSG("%s had mounted\n",
+				the_lli->lli_chan_name);
+				return -1;
+			}
+
+			ioctl_iil_reg.reg_data =
+			    lli_rm_rd(ioctl_iil_reg.reg_addr);
+
+		}
+
+		r = copy_to_user((struct ioctl_reg __user *)arg,
+			    &ioctl_iil_reg,
+			    sizeof(struct ioctl_reg));
+		if (r > 0)
+			return -1;
+
+		return 0;
+	}
+
+	case LLI_IOCTL_GET_VERSION: {
+		#ifdef DBG_IO_CTL
+		DBG_MSG("LLI_IOCTL_GET_VERSION\n");
+		#endif
+		r = copy_from_user(&drv_ver,
+			    (unsigned long __user *)arg,
+			    sizeof(unsigned long));
+		if (r > 0)
+			return -1;
+
+		drv_ver = lli_get_driver_version(dev_lli);
+
+		r = copy_to_user((unsigned long __user *)arg,
+			    &drv_ver, sizeof(unsigned long));
+		if (r > 0)
+			return -1;
+
+		return 0;
+	}
+
+	default:
+		break;
+
+	}
+
+	return -EINVAL;
+}
+
+
+
+const struct file_operations lli_fops = {
+	.owner = THIS_MODULE,
+	.write = lli_write,
+	.read = lli_read,
+	.open = lli_open,
+	.release = lli_release,
+	.mmap = lli_mmap,
+	.unlocked_ioctl = lli_ioctl,
+};
+
+
+static int lli_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct resource lli_plat_res;
+	struct resource *lli_io_area;
+	struct lli_dev *mb86s70_lli_dev = NULL;
+	struct device_node *dev_np;
+
+	dev_t devt;
+	void __iomem *regs;
+	int ret = 0, i = 0;
+	int *irq_num = NULL;
+	int r = -ENODEV;
+	char class_name[10];
+	char driver_name[10];
+	char chardev_name[10];
+	char dev_node_n[10];
+	int clk_num = 0;
+	struct clk *lli_clk;
+	const char *irq_name;
+
+	while (1) {
+		sprintf(dev_node_n, "mipilli%d", i);
+		/* printk("device_name=%s\n", dev_node_n); */
+		dev_np = of_find_node_by_name(pdev->dev.of_node, dev_node_n);
+		if (!dev_np)
+			break;
+		i++;
+	}
+
+	if (i <= 0) {
+		ERR_MSG("can not find node in device tree\n");
+		goto out_err;
+	}
+
+	glli_num = i;
+
+	mb86s70_lli_dev =
+	    devm_kzalloc(dev, sizeof(struct lli_dev), GFP_KERNEL);
+
+	if (mb86s70_lli_dev == NULL) {
+		ERR_MSG("lli_probe:mb86s70_lli_dev create fail\n");
+		return -ENOMEM;
+	}
+
+	gp_mb86s70_lli_dev = mb86s70_lli_dev;
+	mb86s70_lli_dev->lli_drv_ver = MB86S70_DRV_VER;
+
+	mb86s70_lli_dev->chan_data =
+	    devm_kzalloc(dev, glli_num * sizeof(struct lli_data),
+		    GFP_KERNEL);
+
+	irq_num =
+		    devm_kzalloc(dev, glli_num * sizeof(int), GFP_KERNEL);
+
+	for (i = 0; i < glli_num; i++) {
+		mb86s70_lli_dev->chan_data[i].dev = NULL;
+		mb86s70_lli_dev->chan_data[i].regs = NULL;
+		mb86s70_lli_dev->chan_data[i].lli_class = NULL;
+		mb86s70_lli_dev->chan_data[i].lli_defcfg = NULL;
+		mb86s70_lli_dev->chan_data[i].ptr_slave_th = NULL;
+		mb86s70_lli_dev->chan_data[i].mount_stat = false;
+		mb86s70_lli_dev->chan_data[i].master_or_slave = AS_MASTER;
+	}
+
+	glli_major = devm_kzalloc(dev, glli_num * sizeof(u32),
+				    GFP_KERNEL);
+
+
+	/* ----------- Resource -------------- */
+	for (i = 0; i < glli_num; i++) {
+		sprintf(dev_node_n, "mipilli%d", i);
+		dev_np = of_find_node_by_name(pdev->dev.of_node, dev_node_n);
+
+		/* memery resource */
+		if (of_address_to_resource(dev_np, 0, &lli_plat_res)) {
+			ERR_MSG("%s can not get resource\n", dev_node_n);
+		} else {
+			lli_io_area =
+			    request_mem_region(lli_plat_res.start,
+				    lli_plat_res.end - lli_plat_res.start + 1,
+				    pdev->name);
+
+			if (lli_io_area == NULL) {
+				ERR_MSG("%s cannot request I/O\n", dev_node_n);
+				return -ENOMEM;
+			}
+			mb86s70_lli_dev->chan_data[i].rcs = lli_io_area;
+
+			regs = ioremap(lli_plat_res.start,
+				    lli_plat_res.end - lli_plat_res.start + 1);
+
+			if (!regs) {
+				ERR_MSG("%s ioremap fail\n", dev_node_n);
+				return -ENOMEM;
+			}
+
+			mb86s70_lli_dev->chan_data[i].regs = regs;
+		}
+
+		/* irq resource */
+		irq_num[i] = of_irq_to_resource(dev_np, 0, NULL);
+		if (irq_num[i] <= 0) {
+			ERR_MSG("%s ioremap fail\n", dev_node_n);
+			ret = -EINVAL;
+			goto out_err_2;
+		}
+
+		mb86s70_lli_dev->chan_data[i].irq = irq_num[i];
+
+		of_property_read_string(dev_np, "irq-name", &irq_name);
+
+		/* find the IRQ for this unit */
+		r = request_irq(mb86s70_lli_dev->chan_data[i].irq,
+			    lli_irq, IRQF_TRIGGER_RISING/*IRQF_SHARED*/,
+			    irq_name, mb86s70_lli_dev);
+		if (r != 0) {
+			ERR_MSG("lli:request_irq [%d] fail\n", i);
+			ret = -EINVAL;
+			goto out_err_2;
+		}
+
+		/* clock */
+		for (clk_num = 0; clk_num < LLI_CLK_NUM; clk_num++) {
+			lli_clk = of_clk_get(dev_np, clk_num);
+			if (IS_ERR(lli_clk)) {
+				dev_err(&pdev->dev,
+					"get clk err %d\n", clk_num);
+				goto out_err_2;
+			}
+			ret = clk_prepare_enable(lli_clk);
+			if (ret < 0) {
+				dev_err(&pdev->dev,
+					"enable clk err %d\n", clk_num);
+				goto out_err_2;
+			}
+			mb86s70_lli_dev->chan_data[i].clocks[clk_num] = lli_clk;
+		}
+		mb86s70_lli_dev->chan_data[i].dev = dev;
+	}
+
+	/* -------- Char I/F Init -------------- */
+	for (i = 0; i < glli_num; i++) {
+
+		sprintf(class_name, "lli-%d", i);
+
+		sprintf(mb86s70_lli_dev->chan_data[i].lli_chan_name, "%s",
+		    class_name);
+
+		mb86s70_lli_dev->chan_data[i].lli_class =
+		    class_create(THIS_MODULE, class_name);
+
+		if (IS_ERR(mb86s70_lli_dev->chan_data[i].lli_class)) {
+			ERR_MSG(
+			"lli_probe: chan_data[%d].lli_class Err\n", i);
+			goto out_err_2;
+		}
+
+		sprintf(chardev_name, "lli-%d", i);
+
+		r = alloc_chrdev_region(&devt, 0, 1, chardev_name);
+		if (r < 0) {
+			ERR_MSG("chardev_name =%s failed\n", chardev_name);
+			goto out_err_2;
+		}
+
+		mb86s70_lli_dev->chan_data[i].devt = devt;
+		glli_major[i] = MAJOR(mb86s70_lli_dev->chan_data[i].devt);
+
+		cdev_init(&mb86s70_lli_dev->chan_data[i].cdev, &lli_fops);
+		mb86s70_lli_dev->chan_data[i].cdev.owner = THIS_MODULE;
+
+		r = cdev_add(&mb86s70_lli_dev->chan_data[i].cdev, devt, 1);
+		if (r) {
+			dev_err(dev, "cdev_add() failed\n");
+			goto out_err_2;
+		}
+
+		sprintf(driver_name, "%s-%d", DRIVER_NAME, i);
+
+		mb86s70_lli_dev->chan_data[i].dev =
+			device_create(mb86s70_lli_dev->chan_data[i].lli_class,
+			    NULL, devt, NULL, "%s", driver_name);
+
+		if (IS_ERR(mb86s70_lli_dev->chan_data[i].dev)) {
+			ERR_MSG("driver_name =%s device_create Fail\n",
+			driver_name);
+			goto out_err_1;
+		}
+
+		mb86s70_lli_dev->chan_data[i].lli_defcfg = &lli_def_cfg;
+
+		/*initial spin_lock*/
+		spin_lock_init(&mb86s70_lli_dev->chan_data[i].mb86s70_irq_lock);
+		sema_init(&mb86s70_lli_dev->chan_data[i].openf_sem, 1);
+		init_completion(&mb86s70_lli_dev->chan_data[i].msg_cmp);
+	}
+
+	for (i = 0; i < glli_num; i++)
+		init_mb86s70_lli_reg(mb86s70_lli_dev, i);
+
+	sema_init(&mb86s70_lli_dev->buf_sem, 1);
+	platform_set_drvdata(pdev, mb86s70_lli_dev);
+
+	if (irq_num)
+		devm_kfree(dev, irq_num);
+
+	return ret;
+
+out_err_1:
+	for (i = 0; i < glli_num; i++) {
+		if (!IS_ERR(mb86s70_lli_dev->chan_data[i].lli_class)) {
+			class_destroy(mb86s70_lli_dev->chan_data[i].lli_class);
+			cdev_del(&gp_mb86s70_lli_dev->chan_data[i].cdev);
+		}
+	}
+
+	free_irq(mb86s70_lli_dev->chan_data[i].irq,
+		mb86s70_lli_dev);
+
+
+out_err_2:
+	for (i = 0; i < glli_num; i++) {
+		if (mb86s70_lli_dev->chan_data[i].regs)
+			iounmap(mb86s70_lli_dev->chan_data[i].regs);
+
+		for (clk_num = 0; clk_num < LLI_CLK_NUM; clk_num++) {
+			lli_clk = mb86s70_lli_dev->chan_data[i].clocks[clk_num];
+			if (lli_clk)
+				clk_disable_unprepare(lli_clk);
+		}
+	}
+
+	if (mb86s70_lli_dev->chan_data)
+		devm_kfree(dev, mb86s70_lli_dev->chan_data);
+
+	if (glli_major)
+		devm_kfree(dev, glli_major);
+
+	if (irq_num)
+		devm_kfree(dev, irq_num);
+
+out_err:
+	DBG_MSG("lli_probe error\n");
+	ret = -1;
+	return ret;
+}
+
+static int lli_remove(struct platform_device *pdev)
+{
+	int i = 0;
+	struct lli_dev *dev_lli;
+	struct lli_data *lli_ch;
+	int clk_num = 0;
+	struct clk *lli_clk;
+
+	dev_lli = platform_get_drvdata(pdev);
+	lli_ch = dev_lli->chan_data;
+
+	for (i = 0; i < glli_num; i++) {
+
+		if (lli_ch[i].regs)
+			iounmap(lli_ch[i].regs);
+
+		release_mem_region(lli_ch[i].rcs->start,
+			resource_size(lli_ch[i].rcs));
+
+		for (clk_num = 0; clk_num < LLI_CLK_NUM; clk_num++) {
+			lli_clk = lli_ch[i].clocks[clk_num];
+			if (lli_clk)
+				clk_disable_unprepare(lli_clk);
+		}
+
+		free_irq(lli_ch[i].irq, dev_lli);
+		device_destroy(lli_ch[i].lli_class, lli_ch[i].devt);
+		class_destroy(lli_ch[i].lli_class);
+		cdev_del(&lli_ch[i].cdev);
+		unregister_chrdev_region(lli_ch[i].devt, 1);
+	}
+
+	if (dev_lli->chan_data)
+		devm_kfree(&pdev->dev, dev_lli->chan_data);
+
+	if (dev_lli)
+		devm_kfree(&pdev->dev, dev_lli);
+
+	if (glli_major)
+		devm_kfree(&pdev->dev, glli_major);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+/*
+static int lli_suspend(struct device *dev)
+{
+	return 0;
+}
+
+
+static int lli_resume(struct device *dev)
+{
+	return 0;
+}
+*/
+#endif
+
+
+static const struct of_device_id f_mb86s70_mipilli_dt_ids[] = {
+	{ .compatible = "fujitsu,mb86s70-mipilli"},
+	{ /* sentinel  */ }
+};
+
+MODULE_DEVICE_TABLE(of, f_mb86s70_mipilli_dt_ids);
+
+static struct platform_driver lli_driver = {
+	.probe = lli_probe,
+	.remove = lli_remove,
+	/* .suspend = lli_suspend, */
+	/* .resume = lli_resume, */
+	.driver = {
+		.name = MODULE_NAME,
+		.owner = THIS_MODULE,
+		.of_match_table = f_mb86s70_mipilli_dt_ids,
+	},
+};
+
+
+static int __init mb86s70_lli_init(void)
+{
+	return platform_driver_register(&lli_driver);
+}
+
+static void __exit mb86s70_lli_cleanup(void)
+{
+	platform_driver_unregister(&lli_driver);
+}
+
+module_init(mb86s70_lli_init);
+module_exit(mb86s70_lli_cleanup);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION("MIPI LLI Driver for MB86S70");
+MODULE_ALIAS("platform:lli-mb86s70");
diff --git a/drivers/char/mipi_lli/mipilli_mb86s70.h b/drivers/char/mipi_lli/mipilli_mb86s70.h
new file mode 100644
index 0000000..b136b28
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_mb86s70.h
@@ -0,0 +1,173 @@
+/*
+ * mipilli_mb86s70.h F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef MIPILLI_MB86S70_H_
+#define MIPILLI_MB86S70_H_
+
+#include <linux/cdev.h>
+#include <linux/compat.h>
+#include <linux/kthread.h>
+#include <linux/spinlock.h>
+#include <linux/semaphore.h>
+#include "mipilli_api.h"
+#include "mipilli_registers.h"
+
+#define MB86S70_DRV_VER 1000
+#define DRIVER_NAME "lli"
+#define MODULE_NAME "mb86s70-mipilli"
+
+/* #define DBG_IO_CTL */
+#define DBG_PROB 1
+#define DBG_FLAG 1
+#define ERR_FLAG 1
+#define DBG_S_MOUNT 1
+#define DBG_M_UNMOUNT 1
+#define DBG_M_MOUNT 1
+#define DBG_CONFIG_UPDATE 1
+
+#if DBG_FLAG
+#define DBG_MSG(fmt, arg...) printk(fmt, ##arg)
+#else
+#define DBG_MSG(fmt, arg...)
+#endif
+
+#if ERR_FLAG
+#define ERR_MSG(fmt, arg...) printk(fmt, ##arg)
+#else
+#define ERR_MSG(fmt, arg...)
+#endif
+
+
+#define ACT_LLI_CH  (gp_mb86s70_lli_dev->act_lli_ch)
+#define AS_MASTER   1
+#define AS_SLAVE    0
+#define LOCAL_ADDR_BIT  0x8000
+#define RETRY_COUNTER   200
+#define MPHY_WRITE_TAG_LOCAL    0x01
+#define MPHY_WRITE_TAG_REMOTE   0x02
+#define LLI_CLK_NUM 3
+
+struct lli_data {
+	dev_t                devt;
+	struct device        *dev;
+	struct cdev          cdev;
+	struct lli_cfg       *lli_defcfg;
+	struct class         *lli_class;
+	struct completion    msg_cmp;
+	struct task_struct   *ptr_slave_th;
+	struct semaphore     openf_sem;
+	void __iomem         *regs;
+	struct resource      *rcs;
+	/* void __iomem        *lli_mem_if; */
+	u32                   irq;
+	u32                   lli_interrupt_status;
+	u8                    mount_stat;
+	u8                    master_or_slave; /* 0:master 1:slave */
+	spinlock_t            mb86s70_irq_lock;
+	char                 lli_chan_name[10];
+	struct clk           *clocks[3];
+};
+
+struct lli_dev {
+	s16                  lli_drv_ver;
+	s16                  cur_lli_major_num;
+	s8                   act_lli_ch;
+	struct semaphore    buf_sem;
+	struct lli_data    *chan_data;
+};
+
+struct ioctl_memmap {
+	u32 index;
+	u32 fromAddr;
+	u32 toAddr;
+	u32 size;
+};
+
+struct ioctl_cfg {
+	/*0:read form driver , 1:write to driver */
+	u8                direct;
+	struct lli_cfg  lli_defcfg;
+};
+
+struct ioctl_reg {
+	u32    reg_addr;
+	u32    reg_data;
+};
+
+
+extern struct lli_dev   *gp_mb86s70_lli_dev;
+
+
+
+#define TX_LANE_CNT 1
+#define RX_LANE_CNT 1
+#define CMP_TIMEOUT msecs_to_jiffies(10)
+
+
+
+/* IO_CONTROL_COMMAND */
+#define IOCTL_MAG   'P'
+
+#define LLI_IOCTL_M_MOUNT           _IO(IOCTL_MAG, 0) /*M*/
+#define LLI_IOCTL_M_UNMOUNT         _IO(IOCTL_MAG, 1) /*M*/
+#define LLI_IOCTL_PHY_TEST_START    _IO(IOCTL_MAG, 2) /*M*/
+#define LLI_IOCTL_PHY_TEST_STOP     _IO(IOCTL_MAG, 3) /*M*/
+#define LLI_IOCTL_S_MOUNT           _IO(IOCTL_MAG, 4) /*S*/
+#define LLI_IOCTL_S_MAIN_LOOP       _IO(IOCTL_MAG, 5) /*S*/
+#define LLI_IOCTL_SET_LOCAL_CONFIG  _IO(IOCTL_MAG, 6)
+#define LLI_IOCTL_CONFIG_UPDATE     _IO(IOCTL_MAG, 7)
+#define LLI_IOCTL_CHANGE_MEMMAP     _IO(IOCTL_MAG, 8)
+#define LLI_IOCTL_SET_REMOTE_SIG    _IO(IOCTL_MAG, 9)
+
+/* write SOC register */
+#define LLI_IOCTL_WRITE_REG _IOWR(IOCTL_MAG, 10, struct ioctl_reg)
+/* read SOC register */
+#define LLI_IOCTL_READ_REG _IOWR(IOCTL_MAG, 11, struct ioctl_reg)
+
+#define LLI_IOCTL_GET_VERSION _IOWR(IOCTL_MAG, 12, u32)
+/* only for test */
+#define LLI_IOCTL_MEM_TEST_CREATE _IOWR(IOCTL_MAG, 13, u32)
+
+
+#define MASTER_MOUNT_SUCCESS        0
+#define MASTER_MOUNT_MNTINT_TIMEOUT -1
+#define MASTER_MOUNT_NO_CSA_MOUNTED -2
+#define MASTER_MOUNT_NO_MNTINT      -3
+
+
+#define MASTER_UNMOUNT_SUCCESS          0
+#define MASTER_UNMOUNT_UNMNTINT_TIMEOUT -1
+#define MASTER_UNMOUNT_CSA_MOUNTED      -2
+#define MASTER_UNMOUNT_NO_MNTINT        -3
+
+
+#define SLAVE_MOUNT_SUCCESS         0
+#define SLAVE_MOUNT_MNTINT_TIMEOUT  -1
+#define SLAVE_MOUNT_NO_CSA_MOUNTED  -2
+#define SLAVE_MOUNT_CSA_HIB8EXIT    -3
+#define SLAVE_MOUNT_HIB8EXIT_INT_TIME_OUT   -4
+#define SLAVE_MOUNT_NO_MNTINT               -5
+
+#define CONFIG_UPDATE_SUCCESS           0
+#define CONFIG_UPDATE_PLU_INT_TIMEOUT   -1
+#define CONFIG_UPDATE_CSA_LINKUPDATE    -2
+#define CONFIG_UPDATE_SHADOW_FAIL       -3
+#define CONFIG_UPDATE_NO_PLU_INT        -4
+
+#endif
diff --git a/drivers/char/mipi_lli/mipilli_mem.h b/drivers/char/mipi_lli/mipilli_mem.h
new file mode 100644
index 0000000..1fa3e56
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_mem.h
@@ -0,0 +1,376 @@
+/*
+ * mipilli_mem.h F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#ifndef MIPILLI_MEM_H_
+#define MIPILLI_MEM_H_
+
+#define LLI_REG_BASE 0
+
+/*
+ * Remote LLI Attributes
+ * Local LLI Attributes must OR 0x00008000!!
+ */
+
+/*
+ * Registers are classified by usages:
+ * (O) Read-Only register/field,
+ * (P) Parameter, that can be written in any time unless prohibited,
+ * (C) Control, requires specific sequence to use.
+ * (OD) Read-Only for debug. ToDo should not be disclosed to users.
+ * (PD) Parameter for debug. ToDo should not be disclosed to users.
+ * (CD) Control for debug. ToDo should not be disclosed to users.
+ *
+ * Register/fields marked (P) is controlled by 'config' structure.
+ */
+
+/*
+ * M-PHY Lane0 attributes
+ * Lane1 to Lane15 must have address incremented by (Lane#*0x0400).
+ * All attributes are byte size.
+ * SVC available, PLU required for update.
+ */
+/* Ref M-PHY specification .pdf  Table 48 */
+#define MIPI_MPHY0_TXHSMODE_CAP     (LLI_REG_BASE + (0x01<<2))
+#define MIPI_MPHY0_TXHSGEAR_CAP     (LLI_REG_BASE + (0x02<<2))
+#define MIPI_MPHY0_TXPWMG0_CAP      (LLI_REG_BASE + (0x03<<2))
+#define MIPI_MPHY0_TXPWMGEAR_CAP    (LLI_REG_BASE + (0x04<<2))
+#define MIPI_MPHY0_TXAMPLITUDE_CAP  (LLI_REG_BASE + (0x05<<2))
+#define MIPI_MPHY0_TXEXTSYNC_CAP    (LLI_REG_BASE + (0x06<<2))
+#define MIPI_MPHY0_TXHSUNTERM_CAP   (LLI_REG_BASE + (0x07<<2))
+#define MIPI_MPHY0_TXTERM_CAP       (LLI_REG_BASE + (0x08<<2))
+#define MIPI_MPHY0_TXMINSLEEP_CAP   (LLI_REG_BASE + (0x09<<2))
+#define MIPI_MPHY0_TXMINSTALL_CAP   (LLI_REG_BASE + (0x0a<<2))
+#define MIPI_MPHY0_TXMINSAVECNF_CAP (LLI_REG_BASE + (0x0b<<2))
+#define MIPI_MPHY0_TXCLKSHARED_CAP  (LLI_REG_BASE + (0x0c<<2))
+#define MIPI_MPHY0_TXRELEASE_CAP    (LLI_REG_BASE + (0x0d<<2))
+#define MIPI_MPHY0_TXEDITION_CAP    (LLI_REG_BASE + (0x0e<<2))
+#define MIPI_MPHY0_TXHIB8TIME_CAP   (LLI_REG_BASE + (0x0f<<2))
+#define MIPI_MPHY0_TXMODE           (LLI_REG_BASE + (0x21<<2))
+#define MIPI_MPHY0_TXHSRATE_SERIES  (LLI_REG_BASE + (0x22<<2))
+#define MIPI_MPHY0_TXHSGEAR         (LLI_REG_BASE + (0x23<<2))
+#define MIPI_MPHY0_TXPWMGEAR        (LLI_REG_BASE + (0x24<<2))
+#define MIPI_MPHY0_TXAMPLITUDE      (LLI_REG_BASE + (0x25<<2))
+#define MIPI_MPHY0_TXHSSLEWRATE     (LLI_REG_BASE + (0x26<<2))
+#define MIPI_MPHY0_TXSYNCSOURCE     (LLI_REG_BASE + (0x27<<2))
+#define MIPI_MPHY0_TXHSSYNC_LENGTH  (LLI_REG_BASE + (0x28<<2))
+#define MIPI_MPHY0_TXHSPREPLENGTH   (LLI_REG_BASE + (0x29<<2))
+#define MIPI_MPHY0_TXLSPREPLENGTH   (LLI_REG_BASE + (0x2A<<2))
+#define MIPI_MPHY0_TXHIBERN8CTRL    (LLI_REG_BASE + (0x2B<<2))
+#define MIPI_MPHY0_TXLCCENABLE      (LLI_REG_BASE + (0x2C<<2))
+#define MIPI_MPHY0_TXPWMCLOSEXT     (LLI_REG_BASE + (0x2D<<2))
+#define MIPI_MPHY0_TXBYPASS8B10B    (LLI_REG_BASE + (0x2E<<2))
+#define MIPI_MPHY0_TXPOLARITY       (LLI_REG_BASE + (0x2F<<2))
+#define MIPI_MPHY0_TXHSUNTERM       (LLI_REG_BASE + (0x30<<2))
+#define MIPI_MPHY0_TXLSTERM         (LLI_REG_BASE + (0x31<<2))
+#define MIPI_MPHY0_TXLCCSEQUENCE    (LLI_REG_BASE + (0x32<<2))
+#define MIPI_MPHY0_TXMINACTIVATTIME (LLI_REG_BASE + (0x33<<2))
+#define MIPI_MPHY0_TXPWMG6G7SYNCLENGTH (LLI_REG_BASE + (0x34<<2))
+#define MIPI_MPHY0_TXFSMSTATE       (LLI_REG_BASE + (0x41<<2))
+
+/* OMC Attributes are omitted. */
+
+#define MIPI_MPHY0_RXHSMODE_CAP     (LLI_REG_BASE + (0x81<<2))
+#define MIPI_MPHY0_RXHSGEAR_CAP     (LLI_REG_BASE + (0x82<<2))
+#define MIPI_MPHY0_RXPWMG0_CAP      (LLI_REG_BASE + (0x83<<2))
+#define MIPI_MPHY0_RXPWMGEAR_CAP    (LLI_REG_BASE + (0x84<<2))
+#define MIPI_MPHY0_RXHSUNTERM_CAP   (LLI_REG_BASE + (0x85<<2))
+#define MIPI_MPHY0_RXLSTERM_CAP     (LLI_REG_BASE + (0x86<<2))
+#define MIPI_MPHY0_RXMINSLEEP_CAP   (LLI_REG_BASE + (0x87<<2))
+#define MIPI_MPHY0_RXMINSTALL_CAP   (LLI_REG_BASE + (0x88<<2))
+#define MIPI_MPHY0_RXMINSAVECNF_CAP (LLI_REG_BASE + (0x89<<2))
+#define MIPI_MPHY0_RXCLKSHARED_CAP  (LLI_REG_BASE + (0x8A<<2))
+#define MIPI_MPHY0_RXHSG1sync_len_CAP   (LLI_REG_BASE + (0x8B<<2))
+#define MIPI_MPHY0_RXHSG1PREPLEN_CAP    (LLI_REG_BASE + (0x8C<<2))
+#define MIPI_MPHY0_RXLSPREPLEN_CAP  (LLI_REG_BASE + (0x8D<<2))
+#define MIPI_MPHY0_RXPWMCLSLEN_CAP  (LLI_REG_BASE + (0x8E<<2))
+#define MIPI_MPHY0_RXMINACTTIME_CAP (LLI_REG_BASE + (0x8F<<2))
+#define MIPI_MPHY0_RXRELEASE_CAP    (LLI_REG_BASE + (0x90<<2))
+#define MIPI_MPHY0_RXEDITION_CAP    (LLI_REG_BASE + (0x91<<2))
+#define MIPI_MPHY0_RXHIB8TIME_CAP   (LLI_REG_BASE + (0x92<<2))
+#define MIPI_MPHY0_RXLSG67sync_len_CAP  (LLI_REG_BASE + (0x93<<2))
+#define MIPI_MPHY0_RXHSG2sync_len_CAP   (LLI_REG_BASE + (0x94<<2))
+#define MIPI_MPHY0_RXHSG3sync_len_CAP   (LLI_REG_BASE + (0x95<<2))
+#define MIPI_MPHY0_RXHSG2PREPLEN_CAP    (LLI_REG_BASE + (0x96<<2))
+#define MIPI_MPHY0_RXHSG3PREPLEN_CAP    (LLI_REG_BASE + (0x97<<2))
+#define MIPI_MPHY0_RXMODE               (LLI_REG_BASE + (0xA1<<2))
+#define MIPI_MPHY0_RXHSRATE_SERIES  (LLI_REG_BASE + (0xA2<<2))
+#define MIPI_MPHY0_RXHSGEAR         (LLI_REG_BASE + (0xA3<<2))
+#define MIPI_MPHY0_RXPWMGEAR        (LLI_REG_BASE + (0xA4<<2))
+#define MIPI_MPHY0_RXLSTERM         (LLI_REG_BASE + (0xA5<<2))
+#define MIPI_MPHY0_RXHSUNTERM       (LLI_REG_BASE + (0xA6<<2))
+#define MIPI_MPHY0_RXENTHIBE8       (LLI_REG_BASE + (0xA7<<2))
+#define MIPI_MPHY0_RXBYPASS8B10B    (LLI_REG_BASE + (0xA8<<2))
+#define MIPI_MPHY0_RXTERMFORCEEN    (LLI_REG_BASE + (0xA9<<2))
+#define MIPI_MPHY0_RXFSMSTATE       (LLI_REG_BASE + (0xC1<<2))
+
+/* OMC Attributes are omitted. */
+
+/*
+ * LLI PA Configuration registers
+ * All registers dword size.
+ * SVC available, PLU required for update(if shadow).
+ */
+#define MIPI_LLI_PA_TX_COUNT        (LLI_REG_BASE + 0x4000)
+#define MIPI_LLI_PA_RX_COUNT        (LLI_REG_BASE + 0x4004)
+#define MIPI_LLI_PA_MK0INS_ENABLE   (LLI_REG_BASE + 0x4100)
+#define MIPI_LLI_PA_WTSTARTVAL      (LLI_REG_BASE + 0x4104)
+#define MIPI_LLI_PA_PHITERRCOUNT    (LLI_REG_BASE + 0x4108)
+#define MIPI_LLI_PA_PHITRCVCOUNTEN  (LLI_REG_BASE + 0x410C)
+#define MIPI_LLI_PA_PHITERRCOUNTEN  (LLI_REG_BASE + 0x4110)
+#define MIPI_LLI_PA_PHITRCVCOUNTLSB (LLI_REG_BASE + 0x4114)
+#define MIPI_LLI_PA_PHITRCVCOUNTMSB (LLI_REG_BASE + 0x4118)
+#define MIPI_LLI_PA_PHITCLRCOUNT    (LLI_REG_BASE + 0x411C)
+#define MIPI_LLI_PA_ACTIVETXCOUNT   (LLI_REG_BASE + 0x4120)
+#define PA_ACT_RX_CNT               (LLI_REG_BASE + 0x4124)
+#define MIPI_LLI_PA_CONFIG_UPDATE   (LLI_REG_BASE + 0x4128)
+#define MIPI_LLI_PA_MIN_SAVE_CONFIG (LLI_REG_BASE + 0x412C)
+#define MIPI_LLI_PA_WORSTCASE_RTT   (LLI_REG_BASE + 0x4130)
+#define MIPI_LLI_PA_DRV_TACTV_DUR   (LLI_REG_BASE + 0x4134)
+#define PA_CSA_PA_STATUS            (LLI_REG_BASE + 0x4138)
+#define MIPI_LLI_PA_CSA_PA_SET      (LLI_REG_BASE + 0x413C)
+#define MIPI_LLI_PA_CSA_PA_CLEAR    (LLI_REG_BASE + 0x4140)
+#define MIPI_LLI_PA_PHYTESTCONFIG   (LLI_REG_BASE + 0x4148)
+#define MIPI_LLI_PA_PHYTESTRESULT0  (LLI_REG_BASE + 0x414C)
+#define MIPI_LLI_PA_PHYTESTRESULT1  (LLI_REG_BASE + 0x4150)
+#define MIPI_LLI_PA_PHYTESTRESULT2  (LLI_REG_BASE + 0x4154)
+#define MIPI_LLI_PA_PHYTESTRESULT3  (LLI_REG_BASE + 0x4158)
+#define MIPI_LLI_PA_PHYTESTRESULT4  (LLI_REG_BASE + 0x415C)
+#define MIPI_LLI_PA_PHYTESTRESULT5  (LLI_REG_BASE + 0x4160)
+#define MIPI_LLI_PA_PHYTESTRESULT6  (LLI_REG_BASE + 0x4164)
+#define MIPI_LLI_PA_PHYTESTRESULT7  (LLI_REG_BASE + 0x4168)
+#define MIPI_LLI_PA_PHYTESTRESULT8  (LLI_REG_BASE + 0x416C)
+#define MIPI_LLI_PA_PHYTESTRESULT9  (LLI_REG_BASE + 0x4170)
+#define MIPI_LLI_PA_PHYTESTRESULT10 (LLI_REG_BASE + 0x4174)
+#define MIPI_LLI_PA_PHYTESTRESULT11 (LLI_REG_BASE + 0x4178)
+
+
+
+/*
+ * User PA Configuration registers
+ * All registers dword size.
+ * SVC available, but not use it.
+ */
+#define MIPI_USER_PA_AGPRM          (LLI_REG_BASE + 0x4400)
+#define MIPI_USER_PA_NACKRTTMSR     (LLI_REG_BASE + 0x4404)
+#define NACK_RTTMSR_EN              (1 << 0)
+#define NACK_RTTMSR_DONE            (1 << 1)
+
+#define MIPI_USER_PA_NACKRTTMSRRSLT (LLI_REG_BASE + 0x4408)
+#define MIPI_USER_PA_CNTCTL         (LLI_REG_BASE + 0x4500)
+#define MIPI_USER_PA_RBTC           (LLI_REG_BASE + 0x4504)
+#define MIPI_USER_PA_NACKT          (LLI_REG_BASE + 0x4508)
+#define MIPI_USER_PA_DSKWTC         (LLI_REG_BASE + 0x450C)
+#define MIPI_USER_PA_PATXSV         (LLI_REG_BASE + 0x4520)
+#define MIPI_USER_PA_MPHYRB         (LLI_REG_BASE + 0x45F0)
+
+/*
+ * User PA Configuration for Debug
+ * ToDo: These should not be disclosed to users, it is evaluation purpose only.
+ *
+ */
+
+#define MIPI_USER_PA_DBG_CNTCTL             (LLI_REG_BASE + 0x4600)
+#define MIPI_USER_PA_DBG_ONEUSCNT           (LLI_REG_BASE + 0x4604)
+#define MIPI_USER_PA_DBG_PAMINSVCFGORVAL    (LLI_REG_BASE + 0x4608)
+#define MIPI_USER_PA_DBG_WORSTRTTORVAL      (LLI_REG_BASE + 0x460C)
+#define MIPI_USER_PA_DBG_CNTL1              (MIPI_LLI_REG_BASE + 0x4620)
+ /* (CD) ToDo, address 4620 correct??*/
+#define MIPI_USER_PA_DBG_CNTL2      (LLI_REG_BASE + 0x4624)
+#define MIPI_USER_PA_DBG_STATUS1    (LLI_REG_BASE + 0x4640)
+#define MIPI_USER_PA_DBG_STATUS2    (LLI_REG_BASE + 0x4644)
+#define MIPI_USER_PA_DBG_STATUS3    (LLI_REG_BASE + 0x4648)
+#define MIPI_USER_PA_DBG_STATUS4    (LLI_REG_BASE + 0x464C)
+#define MIPI_USER_PA_DBG_STATUS5    (LLI_REG_BASE + 0x4650)
+
+
+/*
+ * LLI Control registers
+ * All registers dword size.
+ * SVC available, but not use it.
+ */
+#define CTRL_LLINITPRSNT    (LLI_REG_BASE + 0x4800)
+#define CTRL_LLTRGTPRSNT    (LLI_REG_BASE + 0x4804)
+#define CTRL_BEINITPRSNT    (LLI_REG_BASE + 0x4808)
+#define CTRL_BETRGTPRSNT    (LLI_REG_BASE + 0x480C)
+#define CTRL_SVCTRGTPRSNT   (LLI_REG_BASE + 0x4810)
+#define CTRL_LLTCDISABLE    (LLI_REG_BASE + 0x4900)
+#define CTRL_BETCDISABLE    (LLI_REG_BASE + 0x4904)
+#define CTRL_CSASYSTEMSTAT  (LLI_REG_BASE + 0x4908)
+#define CTRL_CSASYSTEMSET   (LLI_REG_BASE + 0x490C)
+#define CTRL_CSASYSTEMCLR   (LLI_REG_BASE + 0x4910)
+#define CTRL_TLADDRMODE     (LLI_REG_BASE + 0x4914)
+
+/*
+ * User Control registers
+ * All registers dword size.
+ * SVC available, but not use it.
+ */
+#define MIPI_USER_CTRL_CLDRST       (LLI_REG_BASE + 0x5000)
+#define MIPI_USER_CTRL_CLDRSTAUTO   (LLI_REG_BASE + 0x5004)
+#define MIPI_USER_CTRL_ROE          (LLI_REG_BASE + 0x5008)
+#define MIPI_USER_CTRL_ROEAUTO      (LLI_REG_BASE + 0x500C)
+#define MIPI_USER_CTRL_INTST        (LLI_REG_BASE + 0x5020)
+#define MIPI_USER_CTRL_INTUM        (LLI_REG_BASE + 0x5024)
+#define MIPI_USER_CTRL_INTCLR       (LLI_REG_BASE + 0x5028)
+#define MIPI_USER_CTRL_AXICFG       (LLI_REG_BASE + 0x5040)
+#define MIPI_USER_CTRL_TRANARB      (LLI_REG_BASE + 0x5044)
+#define MIPI_USER_CTRL_AXIAPBCTL    (LLI_REG_BASE + 0x5048)
+#define MIPI_USER_CTRL_RSTR         (LLI_REG_BASE + 0x504C)
+#define MIPI_USER_CTRL_MADREMAP1    (LLI_REG_BASE + 0x5050)
+#define MIPI_USER_CTRL_MADREMAP2    (LLI_REG_BASE + 0x5054)
+#define MIPI_USER_CTRL_MADREMAP3    (LLI_REG_BASE + 0x5058)
+#define MIPI_USER_CTRL_MADREMAP4    (LLI_REG_BASE + 0x505C)
+#define MIPI_USER_CTRL_MADREMAP5    (LLI_REG_BASE + 0x5060)
+#define MIPI_USER_CTRL_MADREMAP6    (LLI_REG_BASE + 0x5064)
+#define MIPI_USER_CTRL_MADREMAP7    (LLI_REG_BASE + 0x5068)
+#define MIPI_USER_CTRL_MADREMAP8    (LLI_REG_BASE + 0x506C)
+#define MIPI_USER_CTRL_MADREMAPUM1  (LLI_REG_BASE + 0x5070)
+#define MIPI_USER_CTRL_MADREMAPUM2  (LLI_REG_BASE + 0x5074)
+#define MIPI_USER_CTRL_MADREMAPUM3  (LLI_REG_BASE + 0x5078)
+#define MIPI_USER_CTRL_MADREMAPUM4  (LLI_REG_BASE + 0x507C)
+#define MIPI_USER_CTRL_MADREMAPUM5  (LLI_REG_BASE + 0x5080)
+#define MIPI_USER_CTRL_MADREMAPUM6  (LLI_REG_BASE + 0x5084)
+#define MIPI_USER_CTRL_MADREMAPUM7  (LLI_REG_BASE + 0x5088)
+#define MIPI_USER_CTRL_MADREMAPUM8  (LLI_REG_BASE + 0x508C)
+#define MIPI_USER_CTRL_MADREMAPFLT1 (LLI_REG_BASE + 0x5090)
+#define MIPI_USER_CTRL_MADREMAPFLT2 (LLI_REG_BASE + 0x5094)
+#define MIPI_USER_CTRL_MADREMAPFLT3 (LLI_REG_BASE + 0x5098)
+#define MIPI_USER_CTRL_MADREMAPFLT4 (LLI_REG_BASE + 0x509C)
+#define MIPI_USER_CTRL_MADREMAPFLT5 (LLI_REG_BASE + 0x50A0)
+#define MIPI_USER_CTRL_MADREMAPFLT6 (LLI_REG_BASE + 0x50A4)
+#define MIPI_USER_CTRL_MADREMAPFLT7 (LLI_REG_BASE + 0x50A8)
+#define MIPI_USER_CTRL_MADREMAPFLT8 (LLI_REG_BASE + 0x50AC)
+#define MIPI_USER_CTRL_SADREMAP1    (LLI_REG_BASE + 0x50B0)
+#define MIPI_USER_CTRL_SADREMAP2    (LLI_REG_BASE + 0x50B4)
+#define MIPI_USER_CTRL_SADREMAP3    (LLI_REG_BASE + 0x50B8)
+#define MIPI_USER_CTRL_SADREMAP4    (LLI_REG_BASE + 0x50BC)
+#define MIPI_USER_CTRL_SADREMAP5    (LLI_REG_BASE + 0x50C0)
+#define MIPI_USER_CTRL_SADREMAP6    (LLI_REG_BASE + 0x50C4)
+#define MIPI_USER_CTRL_SADREMAP7    (LLI_REG_BASE + 0x50C8)
+#define MIPI_USER_CTRL_SADREMAP8    (LLI_REG_BASE + 0x50CC)
+#define MIPI_USER_CTRL_SADREMAPUM1  (LLI_REG_BASE + 0x50D0)
+#define MIPI_USER_CTRL_SADREMAPUM2  (LLI_REG_BASE + 0x50D4)
+#define MIPI_USER_CTRL_SADREMAPUM3  (LLI_REG_BASE + 0x50D8)
+#define MIPI_USER_CTRL_SADREMAPUM4  (LLI_REG_BASE + 0x50DC)
+#define MIPI_USER_CTRL_SADREMAPUM5  (LLI_REG_BASE + 0x50E0)
+#define MIPI_USER_CTRL_SADREMAPUM6  (LLI_REG_BASE + 0x50E4)
+#define MIPI_USER_CTRL_SADREMAPUM7  (LLI_REG_BASE + 0x50E8)
+#define MIPI_USER_CTRL_SADREMAPUM8  (LLI_REG_BASE + 0x50EC)
+#define MIPI_USER_CTRL_SADREMAPFLT1 (LLI_REG_BASE + 0x50F0)
+#define MIPI_USER_CTRL_SADREMAPFLT2 (LLI_REG_BASE + 0x50F4)
+#define MIPI_USER_CTRL_SADREMAPFLT3 (LLI_REG_BASE + 0x50F8)
+#define MIPI_USER_CTRL_SADREMAPFLT4 (LLI_REG_BASE + 0x50FC)
+#define MIPI_USER_CTRL_SADREMAPFLT5 (LLI_REG_BASE + 0x5100)
+#define MIPI_USER_CTRL_SADREMAPFLT6 (LLI_REG_BASE + 0x5104)
+#define MIPI_USER_CTRL_SADREMAPFLT7 (LLI_REG_BASE + 0x5108)
+#define MIPI_USER_CTRL_SADREMAPFLT8 (LLI_REG_BASE + 0x510C)
+#define MIPI_USER_CTRL_SADPSTWUNM1  (LLI_REG_BASE + 0x5110)
+#define MIPI_USER_CTRL_SADPSTWUNM2  (LLI_REG_BASE + 0x5114)
+#define MIPI_USER_CTRL_SADPSTWUNM3  (LLI_REG_BASE + 0x5118)
+#define MIPI_USER_CTRL_SADPSTWUNM4  (LLI_REG_BASE + 0x511C)
+#define MIPI_USER_CTRL_SADPSTWUNM5  (LLI_REG_BASE + 0x5120)
+#define MIPI_USER_CTRL_SADPSTWUNM6  (LLI_REG_BASE + 0x5124)
+#define MIPI_USER_CTRL_SADPSTWUNM7  (LLI_REG_BASE + 0x5128)
+#define MIPI_USER_CTRL_SADPSTWUNM8  (LLI_REG_BASE + 0x512C)
+#define MIPI_USER_CTRL_SADPSTWFLT1  (LLI_REG_BASE + 0x5130)
+#define MIPI_USER_CTRL_SADPSTWFLT2  (LLI_REG_BASE + 0x5134)
+#define MIPI_USER_CTRL_SADPSTWFLT3  (LLI_REG_BASE + 0x5138)
+#define MIPI_USER_CTRL_SADPSTWFLT4  (LLI_REG_BASE + 0x513C)
+#define MIPI_USER_CTRL_SADPSTWFLT5  (LLI_REG_BASE + 0x5140)
+#define MIPI_USER_CTRL_SADPSTWFLT6  (LLI_REG_BASE + 0x5144)
+#define MIPI_USER_CTRL_SADPSTWFLT7  (LLI_REG_BASE + 0x5148)
+#define MIPI_USER_CTRL_SADPSTWFLT8  (LLI_REG_BASE + 0x514C)
+#define MIPI_USER_CTRL_MAWUSRSW7_0  (LLI_REG_BASE + 0x5150)
+#define MIPI_USER_CTRL_MAWUSRSW8    (LLI_REG_BASE + 0x5154)
+#define MIPI_USER_CTRL_MARUSRSW7_0  (LLI_REG_BASE + 0x5158)
+#define MIPI_USER_CTRL_MARUSRSW8    (LLI_REG_BASE + 0x515C)
+#define MIPI_USER_CTRL_SAWUSRSW7_0  (LLI_REG_BASE + 0x5160)
+#define MIPI_USER_CTRL_SAWUSRSW8    (LLI_REG_BASE + 0x5164)
+#define MIPI_USER_CTRL_SARUSRSW7_0  (LLI_REG_BASE + 0x5168)
+#define MIPI_USER_CTRL_SARUSRSW8    (LLI_REG_BASE + 0x516C)
+#define MIPI_USER_CTRL_MAWUSREN     (LLI_REG_BASE + 0x5170)
+#define MIPI_USER_CTRL_MARUSREN     (LLI_REG_BASE + 0x5174)
+#define MIPI_USER_CTRL_SAWUSREN     (LLI_REG_BASE + 0x5178)
+#define MIPI_USER_CTRL_SARUSREN     (LLI_REG_BASE + 0x517C)
+#define MIPI_USER_CTRL_LSSTSSW5_0   (LLI_REG_BASE + 0x5200)
+#define MIPI_USER_CTRL_LSSTSSW11_6  (LLI_REG_BASE + 0x5204)
+#define MIPI_USER_CTRL_LSSTSSW17_12 (LLI_REG_BASE + 0x5208)
+#define MIPI_USER_CTRL_LSSTSSW23_18 (LLI_REG_BASE + 0x520C)
+#define MIPI_USER_CTRL_LSSTSSW29_24 (LLI_REG_BASE + 0x5210)
+#define MIPI_USER_CTRL_LSSTSSW31_30 (LLI_REG_BASE + 0x5214)
+#define MIPI_USER_CTRL_LSSETSW5_0   (LLI_REG_BASE + 0x5218)
+#define MIPI_USER_CTRL_LSSETSW11_6  (LLI_REG_BASE + 0x521C)
+#define MIPI_USER_CTRL_LSSETSW17_12 (LLI_REG_BASE + 0x5220)
+#define MIPI_USER_CTRL_LSSETSW23_18 (LLI_REG_BASE + 0x5224)
+#define MIPI_USER_CTRL_LSSETSW29_24 (LLI_REG_BASE + 0x5228)
+#define MIPI_USER_CTRL_LSSETSW31_30 (LLI_REG_BASE + 0x522C)
+#define MIPI_USER_CTRL_LSSTSEN      (LLI_REG_BASE + 0x5300)
+#define MIPI_USER_CTRL_LSSTSOR      (LLI_REG_BASE + 0x5340)
+#define MIPI_USER_CTRL_LSSETMR      (LLI_REG_BASE + 0x5400)
+#define MIPI_USER_CTRL_LSSETEN      (LLI_REG_BASE + 0x5440)
+#define MIPI_USER_CTRL_LSSETUM      (LLI_REG_BASE + 0x5480)
+
+
+/*
+ *  User Control for Debug
+ * ToDo: These should not be disclosed to users,
+ * it is evaluation purpose only.
+ *
+ */
+#define MIPI_USER_CTRL_DBG_TLDLDBGCNTL1   (LLI_REG_BASE + 0x5800)
+#define MIPI_USER_CTRL_DBG_TLDLDBGSTATUS1 (LLI_REG_BASE + 0x5840)
+#define MIPI_USER_CTRL_DBG_TLDLDBGSTATUS2 (LLI_REG_BASE + 0x5844)
+
+
+/*
+ * LLI Signaling Configuration registers
+ * All registers dword size.
+ * SVC available, but not use it.
+ *
+ */
+#define MIPI_LLI_SIGNAL_SIGREGNUM   (LLI_REG_BASE + 0x6000)
+#define MIPI_LLI_SIGNAL_SIGNALSTATE (LLI_REG_BASE + 0x6004)
+#define MIPI_LLI_SIGNAL_SIGNALSET   (LLI_REG_BASE + 0x6008)
+#define MIPI_LLI_SIGNAL_SIGNALCLEAR (LLI_REG_BASE + 0x600C)
+
+
+/*
+ * User Signaling Configuration registers
+ * SVC available, but not use it.
+ */
+#define MIPI_USER_SIGNAL_XXX (LLI_REG_BASE + 0x00)
+
+/*
+ * LLI Debug Signaling Configuration registers
+ * SVC available, but not use it.
+ */
+#define MIPI_LLI_DEBUG_SIGNAL_XXX (MIPI_LLI_REG_BASE + 0x00)
+
+/*
+ * User Debug Signaling Configuration registers
+ * SVC available, but not use it.
+ */
+#define MIPI_USER_DEBUG_SIGNAL_XXX (LLI_REG_BASE + 0x00)
+
+/* MIPI-LLI 0 Memory , MIPI-LLI 1 Memory */
+#define LLI0_MEM_ADDR 0x4000000000 /*0x4F_FFFF_FFFF*/
+#define LLI1_MEM_ADDR 0x5000000000 /*0x5F_FFFF_FFFF*/
+#define LLI_MEM_OFF (LLI0_MEM_ADDR - LLI1_MEM_ADDR)
+
+#endif /* MIPILLI_MEM_H_ */
diff --git a/drivers/char/mipi_lli/mipilli_registers.c b/drivers/char/mipi_lli/mipilli_registers.c
new file mode 100644
index 0000000..04827d1
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_registers.c
@@ -0,0 +1,267 @@
+/*
+ * mipilli_registers.c F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include "mipilli_registers.h"
+
+struct lli_cfg lli_def_cfg = {
+	/* ----------- M-PHY ----------- */
+	.tx_cfg = {
+		.op_mode            = LS_MODE,  /* TX/RX */
+		.rate_series        = A,        /* TX/RX */
+		.hs_gear            = HS_G1,    /* TX/RX HS-only*/
+		.pwm_gear           = PWM_G1,   /* TX/RX LS-only*/
+		.amplitude          = LARGE_AMPLITUDE,  /* TX */
+		.slew_rate          = 0,        /* TX HS-only */
+		.sync_source        = INTERNAL_SYNC,/* TX */
+		.sync_range         = COARSE,   /* TX HS-only */
+		.sync_length        = 15,       /* TX HS-only */
+		.hs_prepare_length  = 15,       /* TX HS-only */
+		.ls_prepare_length  = 10,       /* TX LS-only */
+		.lcc_enable         = true,     /* TX */
+		.hs_unterminated    = 32,       /* TX LS-only */
+		.bypass8B10B        = false,    /* TX/RX */
+		.polarity           = NORMAL,   /* TX */
+		.hs_unterminated    = false,    /* TX/RX HS-only*/
+		.ls_terminated      = false,    /* TX/RX LS-only*/
+		.min_activate_time  = 15,       /* TX */
+		.lsg6_sync_range    = COARSE,   /* TX LS-only */
+		.lsg6_sync_length   = 15,       /* TX LS-only */
+	},
+	.rx_cfg = {
+		.op_mode            = LS_MODE,  /* TX/RX */
+		.rate_series        = A,        /* TX/RX */
+		.hs_gear            = HS_G1,    /* TX/RX HS-only */
+		.pwm_gear           = PWM_G1,   /* TX/RX LS-only */
+		.hs_unterminated    = false,    /* TX/RX HS-only */
+		.ls_terminated      = false,    /* TX/RX LS-only */
+		.bypass8B10B        = false,    /* TX/RX */
+		.force_term         = false,    /* RX */
+	},
+
+	/* ------ M-PHY ------- */
+	.pa_cfig = {
+		.marker0_insertion          = false,
+		.wt_start_value             = 0,
+		.phit_rec_cont_en           = false,
+		.phit_err_cout_en           = false,
+		.active_tx_count            = 1,
+		.active_rx_count            = 1,
+		.pa_min_save_config         = 250,
+		.pa_worst_case_rtt          = 4096,
+		.drive_tactive_duration     = 10,
+		.csa_pa_set                 = 0,
+		.pa_phy_test_config_master  = 0,
+		.pa_phy_test_config_slasve  = 0,
+	},
+	.usr_pacfg = {
+		.aging_parameter = 0x96,
+		.rbtc   = 0,
+		.nackt  = 0,
+		.dskwtc = 0,
+		.patxsv = 3,
+	},
+	.lk_ctlcfg = {
+		.ll_tc_disable  = false,/* enable LL TC*/
+		.be_tc_disable  = false,/* disable BE TC*/
+		.tl_addr_mode   = false,/* 0: 36bit 1:40 bit*/
+	},
+	.usr_ctlcfg = {
+		/* .cldrst_auto = 0 << CLD_RST_TIM | 0 << CRST_ONE_US_CNT |
+			0 << CRST_AUTO_EN_2 | 0 << CRST_AUTO_EN,
+		*/
+		.cldrst_auto = 0,
+		.roe_auto   = 0,
+		.intum      = 0,
+		.axi_cofg   = 0,
+		.tranarb    = 0,
+		.axiapb_ctl = 0,
+		.rstr       = 0,
+
+		.m_ad_remapConfig = {
+			{/* [0]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [1]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [2]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [3]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [4]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [5]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [6]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [7]*/
+				.remap  = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+		},
+		.s_ad_remapConfig = {
+			{/* [0]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [1]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [2]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [3]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [4]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [5]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [6]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [7]*/
+				.remap = 0,
+				.unmask = 0,
+				.filter = 0,
+			},
+		},
+		.s_ad_postwConfig = {
+			{/* [0]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [1]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [2]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [3]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [4]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [5]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [6]*/
+				.unmask = 0,
+				.filter = 0,
+			},
+			{/* [7] */
+				.unmask = 0,
+				.filter = 0,
+			},
+		},
+		.ll_m_awuserSwap = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08,
+		},
+		.ll_m_aruserSwap = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08,
+		},
+		.ll_s_awuserSwap = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08,
+		},
+		.ll_s_aruserSwap = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08,
+		},
+		.mawusr_en = 0,
+		.marusr_en = 0,
+		.sawusr_en = 0,
+		.sarusr_en = 0,
+
+		.sig_stat_sw = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08, 0x09, 0x0A, 0x0B,
+			0x0C, 0x0D, 0x0E, 0x0F,
+			0x10, 0x11, 0x12, 0x13,
+			0x14, 0x15, 0x16, 0x17,
+			0x18, 0x19, 0x1A, 0x1B,
+			0x1C, 0x1D, 0x1E, 0x1F,
+		},
+
+		.sig_set_sw = {
+			0x00, 0x01, 0x02, 0x03,
+			0x04, 0x05, 0x06, 0x07,
+			0x08, 0x09, 0x0A, 0x0B,
+			0x0C, 0x0D, 0x0E, 0x0F,
+			0x10, 0x11, 0x12, 0x13,
+			0x14, 0x15, 0x16, 0x17,
+			0x18, 0x19, 0x1A, 0x1B,
+			0x1C, 0x1D, 0x1E, 0x1F,
+		},
+		.lssts_en = 0xffffffff,
+		.lssts_or = 0,
+		.lsset_en = 0xffffffff,
+		.lsset_um = 0xffffffff,
+	},
+};
+
diff --git a/drivers/char/mipi_lli/mipilli_registers.h b/drivers/char/mipi_lli/mipilli_registers.h
new file mode 100644
index 0000000..a484fe7
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_registers.h
@@ -0,0 +1,390 @@
+/*
+ * mipilli_registers.h F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+/*
+ * Address[14:0] Name
+ * 0x0000-0x3FFF PHY (see 6.4)
+ * 0x4000-0x43FF LLI PA Configuration (see 6.1.1)
+ * 0x4400-0x47FF User PA Configuration (*see 6.1.2)
+ * 0x4800-0x4FFF LLI Control (see 6.1.1)
+ * 0x5000-0x5FFF User Control (*see 6.1.3)
+ * 0x6000-0x63FF LLI Signaling Configuration (see 6.1.4)
+ * 0x6400-0x67FF User Signaling Configuration (Reserved)
+ * 0x6800-0x6BFF LLI Debug Signaling Configuration (Reserved)
+ * 0x6C00-0x6FFF User Debug Signaling Configuration (Reserved)
+ * 0x7000-0x7BFF Reserved
+ * 0x7C00-0x7FFF DDB(Not support) (Reserved)
+*/
+
+
+#ifndef MIPILLI_REGISTERS_H_
+#define MIPILLI_REGISTERS_H_
+
+
+#include <linux/module.h>
+
+
+enum MIPILLI_EVALPAT {
+	EVAL_WRITE,
+	EVAL_READ,
+	EVAL_WRITE_POST,
+	EVAL_UMOUNT,
+	EVAL_TEST,
+};
+
+enum MIPILLI_EVALMODE {
+	EVAL_HSG1A,
+	EVAL_HSG1B,
+	EVAL_HSG2A,
+	EVAL_HSG2B,
+};
+
+enum MIPILLI_MASTERSLAVE {
+	EVAL_MASTER,
+	EVAL_SLAVE,
+};
+
+enum MPHY_HS_GEAR_CAP {
+	HS_G1_ONLY = 1,
+	HS_G1_TO_G2 = 2,
+	HS_G1_TO_G3 = 3,
+};
+
+enum MPHY_PWM_GEAR_CAP {
+	PWM_G1_ONLY = 1,
+	PWM_G1_TO_G2 = 2,
+	PWM_G1_TO_G3 = 3,
+	PWM_G1_TO_G4 = 4,
+	PWM_G1_TO_G5 = 5,
+	PWM_G1_TO_G6 = 6,
+	PWM_G1_TO_G7 = 7,
+};
+
+enum MPHY_AMPLITUDE_CAP {
+	SMALL_AMPLITUDE_ONLY = 1,
+	LARGE_AMPLITUDE_ONLY = 2,
+	LARGE_AND_SMALL_AMPLITUDE = 3,
+};
+
+enum MPHY_OP_MODE {
+	LS_MODE = 1,
+	HS_MODE = 2,
+};
+
+enum MPHY_HS_RATE_SERIES {
+	A = 1,
+	B = 2,
+};
+
+enum MPHY_HS_GEAR {
+	HS_G1 = 1,
+	HS_G2 = 2,
+	HS_G3 = 3,
+};
+
+enum MPHY_PWM_GEAR {
+	PWM_G0 = 0,
+	PWM_G1 = 1,
+	PWM_G2 = 2,
+	PWM_G3 = 3,
+	PWM_G4 = 4,
+	PWM_G5 = 5,
+	PWM_G6 = 6,
+	PWM_G7 = 7,
+};
+
+enum MPHY_AMPLITUDE {
+	SMALL_AMPLITUDE = 1,
+	LARGE_AMPLITUDE = 2,
+};
+
+enum MPHY_SYNC_SOURCE {
+	INTERNAL_SYNC = 0,
+	EXTERNAL_SYNC = 1,
+};
+
+enum MPHY_SYNC_RANGE {
+	FINE = 0,
+	COARSE = 1,
+};
+
+enum MPHY_HIBERN8_CONTROL {
+	EXIT = 0,
+	ENTER = 1,
+};
+
+enum MPHY_POLARITY {
+	NORMAL = 0,
+	INVERTED = 1,
+};
+
+enum MPHY_STATE {
+	DISABLED = 0,
+	HIBERN8 = 1,
+	SLEEP = 2,
+	STALL = 3,
+	LS_BURST = 4,
+	HS_BURST = 5,
+	LINE_CFG = 6,
+};
+
+/* LLI attributes' enumerated value definitions */
+enum LLI_PA_TPVSTATE {
+	WAITING_FOR_NAK0 = 0,
+	PATTERN_COMPARISON_ON_GOING = 1,
+	END_OF_PATTERN_COMPARISON = 2,
+};
+
+#define CLD_RST_TIM     16
+#define CRST_ONE_US_CNT 8
+#define CRST_AUTO_EN_2  1
+#define CRST_AUTO_EN    0
+
+/* CSA System bit fields*/
+#define B_CSA_LLI_MOUNTED       0x01
+#define B_CSA_MASTER_NOT_SLAVE  0x02
+#define B_CSA_LLI_MOUNT_CTRL    0x04
+#define B_RESET_ON_ERROR_DETECT 0x08
+
+
+/* CSA PA bit fields */
+#define B_CSA_PA_AUTO_MODE          0x01
+#define B_CSA_PA_LINKUPDATECONFIG   0x02
+#define B_CSA_PA_TESTMODE           0x04
+#define B_CSA_PA_HIB8               0x08
+#define B_CSA_PA_PLU_RECEIVED       0x10
+
+/* INT fields */
+/* LLI INTERRUPT SHIT */
+#define B_RMTSVCTO          (1 << 16)
+#define B_SIGSET            (1 << 15)
+#define B_SEQERR            (1 << 14)
+#define B_CRCERR            (1 << 13)
+#define B_PHYSYMERR         (1 << 12)
+#define B_NACKRCVD          (1 << 11)
+#define B_RETRYBUFTO        (1 << 10)
+#define B_FIFOOVF           (1 << 9)
+#define B_DSKWTO            (1 << 8)
+#define B_CRSTAST           (1 << 7)
+#define B_RSTONERRDET   (1 << 6)
+#define B_PLUFIN        (1 << 5)
+#define B_PLURCV        (1 << 4)
+#define B_MNTFTlERR     (1 << 3)
+#define B_UNMNT         (1 << 2)
+#define B_MNT           (1 << 1)
+#define B_RXH8EXIT      (1 << 0)
+
+/* Test Configuration fields */
+#define B_PARAMSEL_CRPAT 0x01
+#define B_TESTRESULT_VEN 0x02
+#define B_TESTBURST_MODE 0x04
+#define B_SEQID_CHK_DIS  0x08
+
+/* AXI/APB Control Register fields */
+#define B_RMTSVCTOEN 0x100
+
+
+/* Counter Control Register fields */
+#define B_RETRYTIMEN 0x1
+#define B_NACKCNTEN 0x2
+#define B_DSKWTIMEN 0x4
+
+
+
+/*
+ * M-PHY configuration values 0x0000-0x3FFF PHY (see 6.4)
+ */
+struct mipilli_mphy_cfg {
+	enum MPHY_OP_MODE           op_mode;    /* TX/RX */
+	enum MPHY_HS_RATE_SERIES   rate_series;/* TX/RX */
+	enum MPHY_HS_GEAR           hs_gear;    /* TX/RX HS-only */
+	enum MPHY_PWM_GEAR          pwm_gear;   /* TX/RX LS-only */
+	enum MPHY_AMPLITUDE         amplitude;  /* TX */
+	enum MPHY_SYNC_SOURCE       sync_source;/* TX */
+	enum MPHY_SYNC_RANGE        sync_range; /* TX HS-only */
+	enum MPHY_POLARITY          polarity;   /* TX */
+	enum MPHY_SYNC_RANGE        lsg6_sync_range;/* TX LS-only */
+
+	u8 slew_rate;           /* TX HS-only */
+	u8 sync_length;         /* TX HS-only */
+	u8 hs_prepare_length;   /* TX HS-only */
+	u8 ls_prepare_length;   /* TX LS-only */
+	bool lcc_enable;        /* TX */
+	u8 burst_closureExt;    /* TX LS-only */
+	bool bypass8B10B;       /* TX/RX */
+	bool hs_unterminated;   /* TX/RX HS-only */
+	bool ls_terminated;     /* TX/RX LS-only */
+	u8 min_activate_time;   /* TX */
+	u8 lsg6_sync_length;    /* TX LS-only */
+	bool force_term;        /* RX */
+};
+
+
+/*
+ * 0x4000-0x43FF LLI PA Configuration (see 6.1.1)
+ * LLI PA configuration values
+ */
+
+struct mipilli_lk_pacfg {
+	u8 tx_count;    /* ro */
+	u8 rx_count;    /* ro */
+	bool marker0_insertion;
+	u16 wt_start_value;
+	bool phit_err_cont;     /* ro */
+	bool phit_rec_cont_en;
+	bool phit_err_cout_en;
+	u32 phit_rec_cont_lsb;  /* ro */
+	u32 phit_rec_cont_msb;  /* ro */
+	bool phit_clr_cont;
+	u8 active_tx_count;
+	u8 active_rx_count;
+	u32 config_update;
+	u8 pa_min_save_config;
+	u16 pa_worst_case_rtt;
+	u8 drive_tactive_duration;  /* default : 0x0a */
+	u8 csa_pa_status;
+	u8 csa_pa_set;
+	u8 csa_pa_clr;
+	u32 pa_phy_test_config_master;
+	u32 pa_phy_test_config_slasve;
+	u32 pa_phy_test_result0;    /* ro */
+	u32 pa_phy_test_result1;    /* ro */
+	u32 pa_phy_test_result2;    /* ro */
+	u32 pa_phy_test_result3;    /* ro */
+};
+
+
+
+
+/*
+ * 0x4400-0x47FF User PA Configuration (*see 6.1.2)
+ */
+struct mipilli_lk_ur_pacfg {
+	u32 aging_parameter;
+	u8 nack_rtt_msr;        /* default :0x00 */
+	u32 nack_rtt_msr_rslt;  /* default :0x00 */
+	u8 cnt_ctl;     /* default :0x00 */
+	u32 rbtc;       /* Retry Buffer Timeout */
+	u32 nackt;      /* NACKThreshold */
+	u32 dskwtc;     /* DeskewTimeout */
+	u16 patxsv;     /* PATXSave */
+	u32 mphyrb;     /* M-PHY Register Bank default:0x00 */
+};
+
+
+/*
+ * 0x4800-0x4FFF LLI Control (see 6.1.1)
+ */
+struct mipilli_lk_conl {
+	bool ll_initiator_present;  /* ro */
+	bool ii_target_present;     /* ro */
+	bool be_initiator_present;  /* ro */
+	bool be_Target_present;     /* ro */
+	bool svc_target_present;    /* ro */
+	bool ll_tc_disable;
+	bool be_tc_disable;
+	u8 csa_system_status;   /* ro */
+	u8 csa_system_set;      /* default : x */
+	u8 csa_system_clear;    /* default : x */
+	u8 tl_addr_mode;
+};
+
+
+
+
+
+/* 0x5000-0x5FFF User Control (*see 6.1.3) */
+
+struct mipilli_swcfg {
+	u32 remap;
+	u32 unmask;
+	u32 filter;
+};
+
+struct mipilli_lk_ur_conl {
+	bool cldrst;    /* default */
+	u32 cldrst_auto;
+	u8 roe;         /* default 0x00 */
+	u8 roe_auto;
+	u32 intst;      /* ro */
+	u32 intum;
+	u32 intclr;
+	u8 axi_cofg;
+	u8 tranarb;
+	u32 axiapb_ctl;
+	u32 rstr;       /* (Remote SVC Timeout) */
+
+	struct mipilli_swcfg m_ad_remapConfig[8];
+	struct mipilli_swcfg s_ad_remapConfig[8];
+	struct mipilli_swcfg s_ad_postwConfig[8];
+
+	u8 ll_m_awuserSwap[9];
+	u8 ll_m_aruserSwap[9];
+	u8 ll_s_awuserSwap[9];
+	u8 ll_s_aruserSwap[9];
+
+	u16 mawusr_en;
+	u16 marusr_en;
+	u16 sawusr_en;
+	u16 sarusr_en;
+
+	u8 sig_stat_sw[32];
+	u8 sig_set_sw[32];
+	u32 lssts_en;
+	u32 lssts_or;
+	u32 lsset_mr;   /* ro */
+	u32 lsset_en;
+	u32 lsset_um;
+
+};
+
+
+/* 0x6000-0x63FF LLI Signaling Configuration (see 6.1.4) */
+
+struct mipilli_lk_sigcfg {
+	u16 sig_reg_num;    /* ro */
+	u32 signal_status;  /* ro */
+	u32 signal_set;
+	u32 signal_clear;
+};
+
+
+struct lli_cfg {
+	/* M-PHY */
+	struct mipilli_mphy_cfg tx_cfg;
+	struct mipilli_mphy_cfg rx_cfg;
+	/* 6.5.LLI PA Configuration Attributes */
+	struct mipilli_lk_pacfg pa_cfig;
+	struct mipilli_lk_ur_pacfg  usr_pacfg;
+	struct mipilli_lk_conl      lk_ctlcfg;
+	struct mipilli_lk_ur_conl   usr_ctlcfg;
+	struct mipilli_lk_sigcfg    signal_config;
+};
+
+extern struct lli_cfg lli_def_cfg;
+
+/*
+ * 0x6400-0x67FF User Signaling Configuration (Reserved)
+ * 0x6800-0x6BFF LLI Debug Signaling Configuration (Reserved)
+ * 0x6C00-0x6FFF User Debug Signaling Configuration (Reserved)
+ * 0x7000-0x7BFF Reserved
+ * 0x7C00-0x7FFF DDB(Not support) (Reserved)
+*/
+#endif
diff --git a/drivers/char/mipi_lli/mipilli_test.c b/drivers/char/mipi_lli/mipilli_test.c
new file mode 100644
index 0000000..ed0f5ab
--- /dev/null
+++ b/drivers/char/mipi_lli/mipilli_test.c
@@ -0,0 +1,219 @@
+/*
+ * mipilli_test.c F_MIPILLI_LP Controller Driver
+ * Copyright (C) 2013 Fujitsu Semi, Ltd
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ *   This code is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; either version 2 of the License, or
+ *   (at your option) any later version.
+ *
+ *   This code is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   You should have received a copy of the GNU General Public License
+ *   along with this program; if not, write to the Free Software
+ *   Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+#include "mipilli_api.h"
+
+bool phy_test_start(u8 test_lane, u8 ch_num)
+{
+
+	bool result = true;
+	u32 txlane_bit, rxlane_bit;
+	u32 master_cfg, slave_cfg;
+	struct lli_data *the_lli;
+	struct lli_cfg *pcfg;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	pcfg = (struct lli_cfg *)the_lli->lli_defcfg;
+
+	/* DBG_MSG("Master test start start\n"); */
+
+	txlane_bit = ~(0xFFFFFFFF << test_lane);
+	rxlane_bit = ~(0xFFFFFFFF << test_lane);
+
+	master_cfg = (pcfg->pa_cfig.pa_phy_test_config_master |
+	    (txlane_bit << 4) | (rxlane_bit << 16));
+
+	slave_cfg = (pcfg->pa_cfig.pa_phy_test_config_slasve |
+	    (rxlane_bit << 4) | (txlane_bit << 16));
+
+	pcfg->pa_cfig.pa_phy_test_config_master = master_cfg;
+	pcfg->pa_cfig.pa_phy_test_config_slasve = slave_cfg;
+
+	mipilli_config_update(ACT_LLI_CH, test_lane, test_lane);
+
+	lli_lo_wt(MIPI_LLI_PA_CSA_PA_SET, B_CSA_PA_TESTMODE);
+	lli_rm_wt(MIPI_LLI_PA_CSA_PA_SET, B_CSA_PA_TESTMODE);
+
+
+	/* DBG_MSG("Master test start done successfully\n"); */
+
+	/*master_test_start_err:*/
+	return result;
+}
+
+
+bool phy_test_stop(void)
+{
+
+	bool result = true;
+	u8 r = 0;
+	u32 retry_cnt = RETRY_COUNTER;
+
+	/* DBG_MSG("Master test stop start\n"); */
+
+	lli_rm_wt(MIPI_LLI_PA_CSA_PA_CLEAR, B_CSA_PA_TESTMODE);
+	lli_lo_wt(MIPI_LLI_PA_CSA_PA_CLEAR, B_CSA_PA_TESTMODE);
+
+	while (r == 0) {
+		r = ~((lli_lo_rd(PA_CSA_PA_STATUS) & B_CSA_PA_TESTMODE));
+		retry_cnt = retry_cnt - 1;
+		if (retry_cnt <= 0)
+			break;
+
+		/* msleep(20); */
+	}
+
+	result = !(retry_cnt <= 0);
+
+	/* DBG_MSG("Phy test stop done successfully\n"); */
+
+	return result;
+}
+
+
+bool slave_main_loop(u8 ch_num)
+{
+	bool result = true;
+	/* u32 intval; */
+	u8 rx_lane, act_rx_cnt, r = 0;
+	u32 rx_lane_bit = 0;
+	u32 int_s = 0;
+	struct lli_data *the_lli;
+	struct lli_cfg *pcfg;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ch_num];
+	pcfg = (struct lli_cfg *)the_lli->lli_defcfg;
+
+	/* loop until serious error detection */
+	while (1) {
+		r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+		INIT_COMPLETION(the_lli->msg_cmp);
+		int_s = the_lli->lli_interrupt_status;
+
+		if (r > 0 && (int_s & B_CRSTAST)) {
+			/* DBG_MSG("ColdReset Detected\n"); */
+
+			/* Clear Reset */
+			lli_lo_wt(MIPI_USER_CTRL_CLDRST, 0);
+
+			/* ToDo: Check if this call can be here,
+			 * or after MOUNT.
+			*/
+
+			setup_error_detect();
+			set_lo_cfg(pcfg);
+			set_err_wacfg(pcfg, false, false);
+
+			/* DBG_MSG("Start Slave Mount Again\n"); */
+			mipilli_slave_mount(ACT_LLI_CH);
+		}
+
+		if (r > 0 && (int_s & B_UNMNT)) {
+
+			/* DBG_MSG("Slave unmounted,
+			   wait for next mounting\n");
+			*/
+
+			rx_lane = lli_lo_rd(PA_ACT_RX_CNT);
+
+			/* DBG_MSG("rx_lane: %02x\n",
+			   rx_lane);
+			 */
+
+			/* Make Rx Hibernate ????? */
+			act_rx_cnt = pcfg->pa_cfig.active_rx_count;
+			mphy_write_reg(
+				MIPI_MPHY0_RXENTHIBE8,
+				MPHY_WRITE_TAG_LOCAL,
+				ENTER, 0, 0, 0,
+				act_rx_cnt);
+
+			/* set PA config_update */
+			/* DBG_MSG("Set PA config_update\n"); */
+			rx_lane_bit = ~(0xFFF << rx_lane);
+
+			lli_lo_wt(MIPI_LLI_PA_CONFIG_UPDATE, rx_lane_bit << 12);
+
+			mipilli_slave_mount(ACT_LLI_CH);
+		}
+		/*
+		if (intval & B_RSTONERRDET){
+			DBG_MSG("ResetOnError Detected\n");
+		}
+		*/
+		#if DBG_FLAG
+		if (r == 0)
+			DBG_MSG("slave_main_loop:int timeout\n");
+		#endif
+		msleep(20);
+	}
+
+	return result;
+}
+
+
+/* ROE : Reset On Error */
+void roe_test(struct lli_cfg *pcfg)
+{
+	u32 int_s , try = RETRY_COUNTER;
+	u8 r = 0;
+	struct lli_data *the_lli;
+
+	the_lli = &gp_mb86s70_lli_dev->chan_data[ACT_LLI_CH];
+
+	/* Enable ROE */
+	lli_lo_wt(MIPI_USER_CTRL_ROE, 1);
+
+	while (try > 0) {
+
+		r = wait_for_completion_timeout(&the_lli->msg_cmp, CMP_TIMEOUT);
+		INIT_COMPLETION(the_lli->msg_cmp);
+		int_s = the_lli->lli_interrupt_status;
+
+		/* int_s = lli_lo_rd(MIPI_USER_CTRL_INTST);
+		if (int_s)
+			lli_lo_wt(MIPI_USER_CTRL_INTCLR, intval);
+		*/
+
+		if (r > 0 && (int_s & B_CRSTAST) > 0) {
+			/* DBG_MSG("ColdReset Detected\n"); */
+			/* DBG_MSG("Clear ColdReset Now\n"); */
+
+			/* Clear Reset */
+			lli_lo_wt(MIPI_USER_CTRL_CLDRST, 0);
+
+			/* DBG_MSG("Start Mounting now\n"); */
+
+			/* ToDo: Check if this call can be
+			 * here, or after MOUNT.
+			*/
+			setup_error_detect();
+			set_lo_cfg(pcfg);
+			set_err_wacfg(pcfg, false, false);
+			/* mipilli_master_mount(pcfg); */
+			break;
+		}
+		#if DBG_FLAG
+		if (r == 0)
+			DBG_MSG("roe_test:int timeout\n");
+		#endif
+		msleep(20);
+		try--;
+	}
+}
diff --git a/drivers/clk/Kconfig b/drivers/clk/Kconfig
index f5fc478..12236e3 100644
--- a/drivers/clk/Kconfig
+++ b/drivers/clk/Kconfig
@@ -42,7 +42,7 @@ config COMMON_CLK_WM831X
 
 config COMMON_CLK_VERSATILE
 	bool "Clock driver for ARM Reference designs"
-	depends on ARCH_INTEGRATOR || ARCH_REALVIEW || ARCH_VEXPRESS
+	depends on ARCH_INTEGRATOR || ARCH_REALVIEW || ARCH_VEXPRESS || ARM64
 	---help---
           Supports clocking on ARM Reference designs:
 	  - Integrator/AP and Integrator/CP
diff --git a/drivers/clk/Makefile b/drivers/clk/Makefile
index c94eee1..e9100b0 100644
--- a/drivers/clk/Makefile
+++ b/drivers/clk/Makefile
@@ -30,6 +30,8 @@ obj-$(CONFIG_ARCH_VT8500)	+= clk-vt8500.o
 obj-$(CONFIG_ARCH_ZYNQ)		+= zynq/
 obj-$(CONFIG_ARCH_TEGRA)	+= tegra/
 obj-$(CONFIG_PLAT_SAMSUNG)	+= samsung/
+obj-$(CONFIG_ARCH_MB8AC0300)	+= clk-mb8ac0300.o
+obj-$(CONFIG_ARCH_MB86S70)	+= clk-mb86s70.o
 obj-$(CONFIG_DAVINCI_CLKS)	+= davinci/
 obj-$(CONFIG_CLK_KEYSTONE_PLL)	+= keystone/
 obj-$(CONFIG_ARCH_OMAP)		+= ti/
diff --git a/drivers/clk/clk-mb86s70.c b/drivers/clk/clk-mb86s70.c
new file mode 100644
index 0000000..07952cf
--- /dev/null
+++ b/drivers/clk/clk-mb86s70.c
@@ -0,0 +1,429 @@
+/*
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/clkdev.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/clk-provider.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/scb_mhu_api.h>
+#include <linux/mailbox_client.h>
+
+#define to_crg_clk(p) container_of(p, struct crg_clk, hw)
+
+extern int skip_mhu;
+
+struct hack_rate {
+	unsigned clk_id;
+	unsigned long rate;
+	int gated;
+};
+
+static struct hack_rate brate[] = {
+	{(0<<8)|(0<<4)|(0<<0), 250000000, 0},
+	{(0<<8)|(0<<4)|(1<<0), 250000000, 0},
+	{(0<<8)|(0<<4)|(2<<0), 250000000, 0},
+	{(0<<8)|(0<<4)|(4<<0), 250000000, 0},
+	{(0<<8)|(0<<4)|(5<<0), 250000000, 0},
+	{(0<<8)|(1<<4)|(0<<0), 125000000, 0},
+	{(0<<8)|(1<<4)|(1<<0), 125000000, 0},
+	{(0<<8)|(1<<4)|(8<<0), 125000000, 0},
+	{(0<<8)|(2<<4)|(0<<0), 62500000, 0},
+	{(0<<8)|(2<<4)|(1<<0), 62500000, 0},
+	{(0<<8)|(2<<4)|(2<<0), 62500000, 0},
+	{(0<<8)|(2<<4)|(4<<0), 62500000, 0},
+	{(0<<8)|(2<<4)|(5<<0), 62500000, 0},
+	{(0<<8)|(2<<4)|(8<<0), 62500000, 0},
+	{(0<<8)|(6<<4)|(8<<0), 31250000, 0},
+	{(0<<8)|(7<<4)|(0<<0), 62500000, 0},
+	{(0<<8)|(8<<4)|(0<<0), 2000000, 0},
+	{(0<<8)|(10<<4)|(0<<0), 25000000, 0},
+	{(0<<8)|(10<<4)|(1<<0), 1200000000, 0},
+	{(1<<8)|(0<<4)|(0<<0), 1600000000, 0},
+	{(2<<8)|(0<<4)|(0<<0), 400000000, 0},
+	{(2<<8)|(0<<4)|(8<<0), 400000000, 0},
+	{(2<<8)|(1<<4)|(3<<0), 400000000, 0},
+	{(2<<8)|(1<<4)|(4<<0), 400000000, 0},
+	{(2<<8)|(2<<4)|(0<<0), 200000000, 0},
+	{(2<<8)|(2<<4)|(3<<0), 200000000, 0},
+	{(2<<8)|(2<<4)|(7<<0), 200000000, 0},
+	{(2<<8)|(3<<4)|(0<<0), 200000000, 0},
+	{(2<<8)|(3<<4)|(3<<0), 200000000, 0},
+	{(2<<8)|(3<<4)|(4<<0), 200000000, 0},
+	{(2<<8)|(3<<4)|(5<<0), 200000000, 0},
+	{(2<<8)|(3<<4)|(6<<0), 200000000, 0},
+	{(2<<8)|(4<<4)|(0<<0), 100000000, 0},
+	{(2<<8)|(4<<4)|(1<<0), 100000000, 0},
+	{(2<<8)|(5<<4)|(0<<0), 100000000, 0},
+	{(2<<8)|(5<<4)|(3<<0), 100000000, 0},
+	{(2<<8)|(5<<4)|(4<<0), 100000000, 0},
+	{(2<<8)|(5<<4)|(5<<0), 100000000, 0},
+	{(2<<8)|(7<<4)|(0<<0), 800000000, 0},
+	{(2<<8)|(8<<4)|(1<<0), 266666666, 0},
+	{(2<<8)|(9<<4)|(0<<0), 50000000, 0},
+	{(2<<8)|(11<<4)|(0<<0), 0, 0},
+};
+
+struct crg_clk {
+	struct clk_hw hw;
+	u8 cntrlr, domain, port;
+};
+
+static int crg_gate_control(struct clk_hw *hw, int en)
+{
+	struct crg_clk *crgclk = to_crg_clk(hw);
+	struct cmd_periclk_control cmd;
+	struct completion got_rsp;
+	int ret;
+
+	cmd.payload_size = sizeof(cmd);
+	cmd.cntrlr = crgclk->cntrlr;
+	cmd.domain = crgclk->domain;
+	cmd.port = crgclk->port;
+	cmd.en = en;
+	/* Port-8 is UngatedCLK */
+	if (cmd.port == 8)
+		return en ? 0 : -EINVAL;
+
+	mbox_dbg("%s:%d CMD Pyld-%u Cntrlr-%u Dom-%u Port-%u En-%u}\n",
+		__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+		cmd.domain, cmd.port, cmd.en);
+
+	init_completion(&got_rsp);
+	ret = mhu_send_packet(CMD_PERI_CLOCK_GATE_SET_REQ,
+					&cmd, sizeof(cmd), &got_rsp);
+	if (ret < 0) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+	if (ret)
+		wait_for_completion(&got_rsp);
+
+	mbox_dbg("%s:%d REP Pyld-%u Cntrlr-%u Dom-%u Port-%u En-%u}\n",
+		__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+		cmd.domain, cmd.port, cmd.en);
+
+	/* If the request was rejected */
+	if (cmd.en != en)
+		ret = -EINVAL;
+	else
+		ret = 0;
+
+	return ret;
+}
+
+static int crg_port_prepare(struct clk_hw *hw)
+{
+	return skip_mhu ? 0 : crg_gate_control(hw, 1);
+}
+
+static void crg_port_unprepare(struct clk_hw *hw)
+{
+	if (!skip_mhu)
+		crg_gate_control(hw, 0);
+}
+
+static int crg_rate_control(struct clk_hw *hw, int set, unsigned long *rate)
+{
+	struct crg_clk *crgclk = to_crg_clk(hw);
+	struct cmd_periclk_control cmd;
+	struct completion got_rsp;
+	int code, ret;
+
+	cmd.payload_size = sizeof(cmd);
+	cmd.cntrlr = crgclk->cntrlr;
+	cmd.domain = crgclk->domain;
+	cmd.port = crgclk->port;
+	cmd.freqency = *rate;
+
+	if (set) {
+		code = CMD_PERI_CLOCK_RATE_SET_REQ;
+		mbox_dbg("%s:%d CMD Pyld-%u Cntrlr-%u Dom-%u Port-%u Rate-SET %lluHz}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+			cmd.domain, cmd.port, cmd.freqency);
+	} else {
+		code = CMD_PERI_CLOCK_RATE_GET_REQ;
+		mbox_dbg("%s:%d CMD Pyld-%u Cntrlr-%u Dom-%u Port-%u Rate-GET}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+			cmd.domain, cmd.port);
+	}
+
+	if (skip_mhu) {
+		int i;
+		unsigned clk_id;
+		clk_id = (cmd.cntrlr << 8)|(cmd.domain << 4)|(cmd.port << 0);
+		for (i = 0; i < ARRAY_SIZE(brate); i++) {
+			mbox_dbg("%s:%d brate[i].clk_id=%x clk_id=%x\n",
+				__func__, __LINE__, brate[i].clk_id, clk_id);
+			if (brate[i].clk_id == clk_id) {
+				if (set)
+					brate[i].rate = *rate;
+				else
+					*rate = brate[i].rate;
+				return 0;
+			}
+		}
+		if (set)
+			return -EINVAL;
+		mbox_dbg("%s:%d Clock Cntrlr-%u Dom-%u Port-%u not found\n",
+			__func__, __LINE__, cmd.cntrlr,	cmd.domain, cmd.port);
+		*rate = 150000000; /* Random value */
+		return 0;
+	}
+
+	init_completion(&got_rsp);
+	ret = mhu_send_packet(code, &cmd, sizeof(cmd), &got_rsp);
+	if (ret < 0) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+	if (ret)
+		wait_for_completion(&got_rsp);
+
+	if (set)
+		mbox_dbg("%s:%d REP Pyld-%u Cntrlr-%u Dom-%u Port-%u Rate-SET %lluHz}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+			cmd.domain, cmd.port, cmd.freqency);
+	else
+		mbox_dbg("%s:%d REP Pyld-%u Cntrlr-%u Dom-%u Port-%u Rate-GOT %lluHz}\n",
+			__func__, __LINE__, cmd.payload_size, cmd.cntrlr,
+			cmd.domain, cmd.port, cmd.freqency);
+
+	*rate = cmd.freqency;
+	return 0;
+}
+
+static unsigned long crg_port_recalc_rate(struct clk_hw *hw,
+					unsigned long parent_rate)
+{
+	unsigned long rate;
+
+	crg_rate_control(hw, 0, &rate);
+
+	return rate;
+}
+
+static long
+crg_port_round_rate(struct clk_hw *hw, unsigned long rate, unsigned long *pr)
+{
+	return rate;
+}
+
+static int
+crg_port_set_rate(struct clk_hw *hw,
+	unsigned long rate, unsigned long parent_rate)
+{
+	return crg_rate_control(hw, 1, &rate);
+}
+
+const struct clk_ops crg_port_ops = {
+	.prepare = crg_port_prepare,
+	.unprepare = crg_port_unprepare,
+	.recalc_rate = crg_port_recalc_rate,
+	.round_rate = crg_port_round_rate,
+	.set_rate = crg_port_set_rate,
+};
+
+static void __init crg_port_init(struct device_node *node)
+{
+	struct clk_init_data init;
+	u32 cntrlr, domain, port;
+	struct crg_clk *crgclk;
+	struct clk *clk;
+	char clkp[20];
+	int rc;
+
+	rc = of_property_read_u32(node, "cntrlr", &cntrlr);
+	if (WARN_ON(rc))
+		return;
+	rc = of_property_read_u32(node, "domain", &domain);
+	if (WARN_ON(rc))
+		return;
+	rc = of_property_read_u32(node, "port", &port);
+	if (WARN_ON(rc))
+		return;
+
+	if (port > 7)
+		snprintf(clkp, 20, "UngatedCLK%d_%X", cntrlr, domain);
+	else
+		snprintf(clkp, 20, "CLK%d_%X_%d", cntrlr, domain, port);
+
+	clk = __clk_lookup(clkp);
+	if (clk)
+		return;
+
+	crgclk = kzalloc(sizeof(*crgclk), GFP_KERNEL);
+	if (!crgclk)
+		return;
+	init.name = clkp;
+	init.num_parents = 0;
+	init.ops = &crg_port_ops;
+	init.flags = CLK_IS_ROOT; /* | CLK_GET_RATE_NOCACHE */
+	crgclk->hw.init = &init;
+	crgclk->cntrlr = cntrlr;
+	crgclk->domain = domain;
+	crgclk->port = port;
+	clk = clk_register(NULL, &crgclk->hw);
+	if (IS_ERR(clk))
+		pr_err("%s:%d Error!\n", __func__, __LINE__);
+	else
+		pr_debug("Registered %s\n", clkp);
+
+	of_clk_add_provider(node, of_clk_src_simple_get, clk);
+	clk_register_clkdev(clk, clkp, NULL);
+}
+CLK_OF_DECLARE(crg11_gate, "mb86s70,crg11_gate", crg_port_init);
+
+struct cl_clk {
+	struct clk_hw hw;
+	int cluster;
+};
+
+#define to_clc_clk(clc) container_of(clc, struct cl_clk, hw)
+
+void mhu_cluster_rate(int cluster, unsigned long *rate, int get)
+{
+	struct cmd_cpu_control_rate cmd;
+	struct completion got_rsp;
+	int code, ret;
+
+	cmd.payload_size = sizeof(cmd);
+	cmd.cluster_class = 0;
+	cmd.cluster_id = cluster;
+	cmd.cpu_id = 0;
+	cmd.freqency = *rate;
+
+	if (skip_mhu) {
+		if (cmd.cluster_id == 0)
+			*rate = 1600000000;
+		else
+			*rate = 800000000;
+		return;
+	}
+
+	if (get)
+		code = CMD_CPU_CLOCK_RATE_GET_REQ;
+	else
+		code = CMD_CPU_CLOCK_RATE_SET_REQ;
+
+	mbox_dbg("%s:%d CMD Pyld-%u Cl_Class-%u CL_ID-%u CPU_ID-%u Freq-%llu}\n",
+		__func__, __LINE__, cmd.payload_size, cmd.cluster_class,
+		cmd.cluster_id,	cmd.cpu_id, cmd.freqency);
+
+	init_completion(&got_rsp);
+	ret = mhu_send_packet(code, &cmd, sizeof(cmd), &got_rsp);
+	if (ret < 0) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return;
+	}
+	if (ret)
+		wait_for_completion(&got_rsp);
+
+	mbox_dbg("%s:%d REP Pyld-%u Cl_Class-%u CL_ID-%u CPU_ID-%u Freq-%llu}\n",
+		__func__, __LINE__, cmd.payload_size, cmd.cluster_class,
+		cmd.cluster_id,	cmd.cpu_id, cmd.freqency);
+
+	*rate = cmd.freqency;
+}
+EXPORT_SYMBOL_GPL(mhu_cluster_rate);
+
+static unsigned long clc_recalc_rate(struct clk_hw *hw,
+		unsigned long unused)
+{
+	unsigned long rate;
+
+	mhu_cluster_rate(to_clc_clk(hw)->cluster, &rate, 1);
+	return rate;
+}
+
+static long clc_round_rate(struct clk_hw *hw, unsigned long rate,
+		unsigned long *unused)
+{
+	return rate;
+}
+
+static int clc_set_rate(struct clk_hw *hw, unsigned long rate,
+		unsigned long unused)
+{
+	unsigned long res = rate;
+
+	mhu_cluster_rate(to_clc_clk(hw)->cluster, &rate, 0);
+
+	return (res == rate) ? 0 : -EINVAL;
+}
+
+static struct clk_ops clk_clc_ops = {
+	.recalc_rate = clc_recalc_rate,
+	.round_rate = clc_round_rate,
+	.set_rate = clc_set_rate,
+};
+
+struct clk *mb86s70_clclk_register(const char *name, int cluster_id)
+{
+	struct clk_init_data init;
+	struct cl_clk *clc;
+	struct clk *clk;
+
+	clc = kzalloc(sizeof(*clc), GFP_KERNEL);
+	if (!clc) {
+		pr_err("could not allocate cl_clk\n");
+		return ERR_PTR(-ENOMEM);
+	}
+
+	clc->hw.init = &init;
+	clc->cluster = cluster_id;
+
+	init.name = name;
+	init.ops = &clk_clc_ops;
+	init.flags = CLK_IS_ROOT | CLK_GET_RATE_NOCACHE;
+	init.num_parents = 0;
+
+	clk = clk_register(NULL, &clc->hw);
+	if (!IS_ERR_OR_NULL(clk))
+		return clk;
+
+	pr_err("clk register failed\n");
+	kfree(clc);
+	return NULL;
+}
+
+void __init mb86s70_clclk_of_init(void)
+{
+	struct device_node *node = NULL;
+	char name[14] = "cpu-cluster.";
+	int cluster_id = 0, len;
+	struct clk *clk;
+	const u32 *val;
+
+	if (!of_find_compatible_node(NULL, NULL, "fujitsu,mhu"))
+		return;
+
+	while ((node = of_find_node_by_name(node, "cluster"))) {
+		val = of_get_property(node, "reg", &len);
+		if (val && len == 4)
+			cluster_id = be32_to_cpup(val);
+
+		name[12] = cluster_id + '0';
+		clk = mb86s70_clclk_register(name, cluster_id);
+		if (IS_ERR(clk))
+			return;
+
+		pr_debug("Registered clock '%s'\n", name);
+		clk_register_clkdev(clk, NULL, name);
+	}
+}
+CLK_OF_DECLARE(cpu_clk, "fujitsu,mhu", mb86s70_clclk_of_init);
diff --git a/drivers/clk/clk-mb8ac0300.c b/drivers/clk/clk-mb8ac0300.c
new file mode 100644
index 0000000..b5729a0
--- /dev/null
+++ b/drivers/clk/clk-mb8ac0300.c
@@ -0,0 +1,473 @@
+/*
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+#include <linux/clkdev.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/clk-provider.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+
+#include <asm/system_misc.h>
+
+/* Page-1298 of MB8AC0300_SP_R1.0J.pdf  */
+
+#define CRG_REG_CRPLC	0x0
+#define CRG_REG_CRRDY	0x4
+#define CRG_REG_CRSTP	0x8
+#define CRG_REG_CRIMA	0x10
+#define CRG_REG_CRPIC	0x14
+#define CRG_REG_CRRSC	0x20
+#define CRG_REG_CRSWR	0x24
+#define CRG_REG_CRRRS	0x28
+#define CRG_REG_CRRSM	0x2c
+#define CRG_REG_CRCDC	0x30
+
+#define CRG_OFF_CRDM	0x100
+#define CRG_OFF_CRLP	0x104
+
+#define CRPLC_FBMODE_MASK	0x3f
+#define CRPLC_FBMODE_SHFT	0
+#define CRPLC_PSMODE_MASK	0xf
+#define CRPLC_PSMODE_SHFT	8
+#define CRPLC_LUWMODE_MASK	0xf
+#define CRPLC_LUWMODE_SHFT	16
+#define CRPLC_PLLBYPASS		BIT(24)
+
+#define CRRDY_PLLRDY		BIT(0)
+#define CRRDY_PSRMNT		BIT(4)
+
+#define CRSTP_STOPEN		BIT(0)
+#define CRSTP_STOPMNT		BIT(1)
+
+#define CRIMA_RDYINTM		BIT(0)
+
+#define CRPIC_PLLRDYINT		BIT(0)
+
+#define CRRSC_ARSTMODE_MASK	0xf
+#define CRRSC_ARSTMODE_SHFT	0
+#define CRRSC_SWRSTM		BIT(8)
+#define CRRSC_WDRSTM		BIT(9)
+#define CRRSC_SRSTMODE_MASK	0xf
+#define CRRSC_SRSTMODE_SHFT	16
+
+#define CRSWR_SWRSTREQ		BIT(0)
+
+#define CRRRS_RRSTREQ_MASK	0xff
+#define CRRRS_RRSTREQ_SHFT	0
+
+#define CRRSM_WDRST		BIT(0)
+#define CRRSM_SWRST		BIT(1)
+#define CRRSM_SRST		BIT(2)
+#define CRRSM_PORESET		BIT(3)
+
+#define CRCDC_DCHREQ		BIT(0)
+
+#define CRDM_DIVMODE_MASK	0xff
+
+#define CRLP_CSYSREQ_RMASK	0xff
+#define CRLP_CSYSREQ_RSHFT	0
+#define CRLP_LPOWERHS_MASK	0xff
+#define CRLP_LPOWERHS_SHFT	8
+#define CRLP_CACTIVE_CMASK	0xff
+#define CRLP_CACTIVE_CSHFT	16
+#define CRLP_CEN_MASK		0xff
+#define CRLP_CEN_SHFT		24
+
+#define CRG11_NOT_PORT		8
+#define CRG11_NOT_DOMAIN	16
+
+#define to_crg_clk(p)		container_of(p, struct crg_clk, hw)
+
+struct crg_clk {
+	struct clk_hw hw;
+	struct crg11 *crg;
+	unsigned domain; /* For Divider Only */
+};
+
+struct crg11 {
+	unsigned id;
+	/* Base address of the controller instance */
+	void __iomem *reg_base;
+	/* Input clock rate to the PLL */
+	unsigned long xclk_rate;
+	/* Protect reg access */
+	spinlock_t lock;
+	struct list_head list;
+};
+
+static LIST_HEAD(crg11_list);
+static void __iomem *main_crg11;
+
+/* For now when we don't have separate power-control driver, use this one */
+void crg11_restart(char mode, const char *cmd)
+{
+	u32 val;
+
+	writel(~CRRRS_RRSTREQ_MASK, main_crg11 + CRG_REG_CRRRS);
+
+	val = readl(main_crg11 + CRG_REG_CRRSC);
+	val |= CRRSC_SWRSTM;
+	writel(val, main_crg11 + CRG_REG_CRRSC);
+
+	/* invoke software reset */
+	writel(CRSWR_SWRSTREQ, main_crg11 + CRG_REG_CRSWR);
+
+	while (1)
+	;
+}
+
+static unsigned long
+_get_gcd(unsigned long a, unsigned long b)
+{
+	unsigned long t;
+
+	if (a == b)
+		return a;
+
+	if (a < b) {
+		t = b;
+		b = a;
+		a = t;
+	}
+
+	while (1) {
+		t = b;
+		b = a % b;
+		a = t;
+		if (a == 1)
+			return 1;
+		if (b == 0)
+			return a;
+	}
+}
+
+static unsigned long
+crg11_pll_recalc_rate(struct clk_hw *hwclk,
+	unsigned long parent_rate)
+{
+	struct crg_clk *crgclk = to_crg_clk(hwclk);
+	void __iomem *reg = crgclk->crg->reg_base;
+	u32 m, n, val = readl(reg + CRG_REG_CRPLC);
+
+	/* PLLBYPASS mode */
+	if (val & CRPLC_PLLBYPASS)
+		return parent_rate;
+
+	m = (val >> CRPLC_FBMODE_SHFT) & CRPLC_FBMODE_MASK;
+	m <<= 1;
+	/* PLL Stopped */
+	if (m == 0)
+		return 0;
+
+	n = (val >> CRPLC_PSMODE_SHFT) & CRPLC_PSMODE_MASK;
+	n <<= 1;
+	if (n == 0)
+		n = 1;
+
+	return parent_rate / n * m;
+}
+
+static long
+crg11_pll_round_rate(struct clk_hw *hwclk,
+	unsigned long rate, unsigned long *prate)
+{
+	unsigned long r, m, n, parent_rate = *prate;
+
+	r = _get_gcd(rate, parent_rate);
+	m = rate / r;
+	n = parent_rate / r;
+
+	if (m > 126)
+		m = 126;
+	if (n > 30)
+		n = 30;
+
+	return parent_rate / n * m;
+}
+
+static int
+crg11_pll_set_rate(struct clk_hw *hwclk,
+	unsigned long rate, unsigned long parent_rate)
+{
+	struct crg_clk *crgclk = to_crg_clk(hwclk);
+	void __iomem *reg = crgclk->crg->reg_base;
+	u32 val = readl(reg + CRG_REG_CRPLC);
+	unsigned long r, m, n;
+
+	if (rate == 0) {
+		/* Can't simply stop while PLL is in use */
+		if (!(val & CRPLC_PLLBYPASS))
+			return -EINVAL;
+
+		/* FBMODE := 0 */
+		val &= ~(CRPLC_FBMODE_MASK << CRPLC_FBMODE_SHFT);
+		writel(val, reg + CRG_REG_CRPLC);
+		return 0;
+	}
+
+	r = _get_gcd(rate, parent_rate);
+	m = rate / r;
+	n = parent_rate / r;
+
+	val &= ~(CRPLC_FBMODE_MASK << CRPLC_FBMODE_SHFT);
+	val |= ((m & CRPLC_FBMODE_MASK) << CRPLC_FBMODE_SHFT);
+	val &= ~(CRPLC_PSMODE_MASK << CRPLC_PSMODE_SHFT);
+	val |= ((n & CRPLC_PSMODE_MASK) << CRPLC_PSMODE_SHFT);
+	writel(val, reg + CRG_REG_CRPLC);
+
+	/* Wait until PLL Ready */
+	do {
+		cpu_relax();
+	} while (!(readl(reg + CRG_REG_CRRDY) & CRRDY_PLLRDY));
+
+	return 0;
+}
+
+const struct clk_ops crg11_pll_ops = {
+	.recalc_rate = crg11_pll_recalc_rate,
+	.round_rate = crg11_pll_round_rate,
+	.set_rate = crg11_pll_set_rate,
+};
+
+static void __init crg11_mux_init(struct device_node *node)
+{
+	u32 phys_addr, clock_rate, id;
+	char *xclk, *pllout, *cclk;
+	const char *mux_parents[2];
+	struct clk_init_data init;
+	struct crg_clk *crgclk;
+	struct crg11 *crg;
+	struct clk *clk;
+	int rc;
+
+	rc = of_property_read_u32(node, "index", &id);
+	if (WARN_ON(rc))
+		return;
+
+	rc = of_property_read_u32(node, "phys_addr", &phys_addr);
+	if (WARN_ON(rc))
+		return;
+
+	rc = of_property_read_u32(node, "clock_rate", &clock_rate);
+	if (WARN_ON(rc))
+		return;
+
+	xclk = kzalloc(60, GFP_KERNEL);
+	pllout = xclk + 20;
+	cclk = xclk + 40;
+	mux_parents[0] = pllout;
+	mux_parents[1] = xclk;
+
+	crg = kzalloc(sizeof(*crg), GFP_KERNEL);
+	crg->reg_base = ioremap(phys_addr, SZ_4K);
+	crg->xclk_rate = clock_rate;
+	spin_lock_init(&crg->lock);
+	crg->id = id;
+	if (id == 0) { /* MAIN_CRG11 */
+		main_crg11 = crg->reg_base;
+		arm_pm_restart = crg11_restart;
+	}
+
+	/* Register XCLK */
+	snprintf(xclk, 20, "XCLK%d", crg->id);
+	clk = clk_register_fixed_rate(NULL, xclk,
+		NULL, CLK_IS_ROOT, crg->xclk_rate);
+	if (IS_ERR(clk))
+		pr_err("%s:%d Error!\n", __func__, __LINE__);
+	else
+		pr_debug("Registered %s\n", xclk);
+
+	/* Register PLL */
+	crgclk = kzalloc(sizeof(*crgclk), GFP_KERNEL);
+	if (crgclk == NULL)
+		return;
+
+	snprintf(pllout, 20, "PLL_Out%d", crg->id);
+	init.name = pllout;
+	init.ops = &crg11_pll_ops;
+	init.flags = CLK_GET_RATE_NOCACHE;
+	init.parent_names = (const char **)&xclk;
+	init.num_parents = 1;
+	crgclk->hw.init = &init;
+	crgclk->domain = CRG11_NOT_DOMAIN;
+	crgclk->crg = crg;
+	clk = clk_register(NULL, &crgclk->hw);
+	if (IS_ERR(clk))
+		pr_err("%s:%d Error!\n", __func__, __LINE__);
+	else
+		pr_debug("Registered %s\n", pllout);
+
+	/* Register MUX CCLK_O */
+	snprintf(cclk, 20, "CCLK%d", crg->id);
+	clk = clk_register_mux(NULL, cclk, mux_parents, 2, CLK_GET_RATE_NOCACHE,
+		crg->reg_base + CRG_REG_CRPLC, 24, 1, 0, &crg->lock);
+	if (IS_ERR(clk))
+		pr_err("%s:%d Error!\n", __func__, __LINE__);
+	else
+		pr_debug("Registered %s\n", cclk);
+
+	list_add(&crg->list, &crg11_list);
+	kfree(xclk);
+}
+CLK_OF_DECLARE(crg11_mux, "mb8ac0300,crg11_mux", crg11_mux_init);
+
+static const unsigned crg_cdr[] = {
+	1, 2, 3, 4, 6, 8, 9, 12, 18, 24, 27, 36, 54, 72, 108, 216,
+};
+
+static unsigned long
+crg11_div_recalc_rate(struct clk_hw *hwclk,
+	unsigned long parent_rate)
+{
+	struct crg_clk *crgclk = to_crg_clk(hwclk);
+	void __iomem *reg = crgclk->crg->reg_base;
+	unsigned dom = crgclk->domain;
+	u32 val = readl(reg + CRG_OFF_CRDM + 0x10 * dom);
+
+	return parent_rate / ((val & 0xff) + 1);
+}
+
+static long
+crg11_div_round_rate(struct clk_hw *hwclk,
+	unsigned long rate, unsigned long *prate)
+{
+	unsigned long div;
+	int i;
+
+	if (rate >= *prate)
+		return *prate;
+
+	div = *prate / rate;
+
+	for (i = 0; i < ARRAY_SIZE(crg_cdr); i++)
+		if (div < crg_cdr[i])
+			break;
+
+	return *prate / crg_cdr[i - 1];
+}
+
+static int
+crg11_div_set_rate(struct clk_hw *hwclk,
+	unsigned long rate, unsigned long parent_rate)
+{
+	struct crg_clk *crgclk = to_crg_clk(hwclk);
+	void __iomem *reg = crgclk->crg->reg_base;
+	unsigned long div = parent_rate / rate;
+	unsigned dom = crgclk->domain;
+	u32 val = readl(reg + CRG_OFF_CRDM + 0x10 * dom);
+
+	if (div - 1 == (val & 0xff)) /* Nothing new to do */
+		return 0;
+
+	val = (div - 1) & CRDM_DIVMODE_MASK;
+	writel(val, reg + CRG_OFF_CRDM + 0x10 * dom);
+
+	writel(CRCDC_DCHREQ, reg + CRG_REG_CRCDC);
+
+	/* Wait until Divider taken */
+	do {
+		cpu_relax();
+	} while (readl(reg + CRG_REG_CRCDC) & CRCDC_DCHREQ);
+
+	return 0;
+}
+
+const struct clk_ops crg11_div_ops = {
+	.recalc_rate = crg11_div_recalc_rate,
+	.round_rate = crg11_div_round_rate,
+	.set_rate = crg11_div_set_rate,
+};
+
+static void __init crg11_gate_init(struct device_node *node)
+{
+	char *ungclk, *clkp, *cclk;
+	struct clk_init_data init;
+	struct crg11 *crg = NULL;
+	u32 cntrlr, domain, port;
+	struct crg_clk *crgclk;
+	struct clk *clk;
+	bool found;
+	int rc;
+
+	rc = of_property_read_u32(node, "cntrlr", &cntrlr);
+	if (WARN_ON(rc))
+		return;
+	rc = of_property_read_u32(node, "domain", &domain);
+	if (WARN_ON(rc))
+		return;
+	rc = of_property_read_u32(node, "port", &port);
+	if (WARN_ON(rc))
+		return;
+
+	found = false;
+	list_for_each_entry(crg, &crg11_list, list) {
+		if (crg->id == cntrlr) {
+			found = true;
+			break;
+		}
+	}
+	if (!found) {
+		pr_err("CRG11_%d MUX not yet populated!\n", cntrlr);
+		return;
+	}
+
+	ungclk = kzalloc(60, GFP_KERNEL);
+	clkp = ungclk + 20;
+	cclk = ungclk + 40;
+
+	/* Look for UngatedCLK (port := 8) */
+	snprintf(ungclk, 20, "CLK%d_%X_8", crg->id, domain);
+	clk = __clk_lookup(ungclk);
+	if (clk == NULL) { /* First user of the domain */
+		snprintf(cclk, 20, "CCLK%d", crg->id);
+		clk = __clk_lookup(cclk);
+		if (WARN_ON(clk == NULL)) {
+			pr_err("CCLK%d not found!\n", crg->id);
+			return;
+		}
+
+		crgclk = kzalloc(sizeof(*crgclk), GFP_KERNEL);
+		if (!crgclk)
+			return;
+		init.name = ungclk;
+		init.ops = &crg11_div_ops;
+		init.flags = CLK_GET_RATE_NOCACHE;
+		init.parent_names = (const char **)&cclk;
+		init.num_parents = 1;
+		crgclk->hw.init = &init;
+		crgclk->domain = domain;
+		crgclk->crg = crg;
+		clk = clk_register(NULL, &crgclk->hw);
+		if (IS_ERR(clk))
+			pr_err("%s:%d Error!\n", __func__, __LINE__);
+		else
+			pr_debug("Registered %s\n", ungclk);
+		clk_register_clkdev(clk, ungclk, NULL);
+	}
+	if (port == 8)
+		goto exit;
+
+	snprintf(clkp, 20, "CLK%d_%X_%d", crg->id, domain, port);
+	pr_debug("\t\t%s\n", clkp);
+	clk = clk_register_gate(NULL, clkp, ungclk,
+		CLK_GET_RATE_NOCACHE | CLK_IGNORE_UNUSED,
+		crg->reg_base + CRG_OFF_CRLP + 0x10 * domain,
+		CRLP_CSYSREQ_RSHFT + port, 0, &crg->lock);
+	if (IS_ERR(clk))
+		pr_err("%s:%d Error!\n", __func__, __LINE__);
+	else
+		pr_debug("Registered %s\n", clkp);
+
+	clk_register_clkdev(clk, clkp, NULL);
+
+exit:
+	of_clk_add_provider(node, of_clk_src_simple_get, clk);
+	kfree(ungclk);
+}
+CLK_OF_DECLARE(crg11_gate, "mb8ac0300,crg11_gate", crg11_gate_init);
diff --git a/drivers/clocksource/Kconfig b/drivers/clocksource/Kconfig
index f151c6c..593485b 100644
--- a/drivers/clocksource/Kconfig
+++ b/drivers/clocksource/Kconfig
@@ -67,6 +67,21 @@ config ARM_ARCH_TIMER
 	bool
 	select CLKSRC_OF if OF
 
+config ARM_ARCH_TIMER_EVTSTREAM
+	bool "Support for ARM architected timer event stream generation"
+	default y if ARM_ARCH_TIMER
+	help
+	  This option enables support for event stream generation based on
+	  the ARM architected timer. It is used for waking up CPUs executing
+	  the wfe instruction at a frequency represented as a power-of-2
+	  divisor of the clock rate.
+	  The main use of the event stream is wfe-based timeouts of userspace
+	  locking implementations. It might also be useful for imposing timeout
+	  on wfe to safeguard against any programming errors in case an expected
+	  event is not generated.
+	  This must be disabled for hardware validation purposes to detect any
+	  hardware anomalies of missing events.
+
 config CLKSRC_METAG_GENERIC
 	def_bool y if METAG
 	help
diff --git a/drivers/clocksource/arm_arch_timer.c b/drivers/clocksource/arm_arch_timer.c
index 48b019f..8123f6c 100644
--- a/drivers/clocksource/arm_arch_timer.c
+++ b/drivers/clocksource/arm_arch_timer.c
@@ -13,16 +13,43 @@
 #include <linux/device.h>
 #include <linux/smp.h>
 #include <linux/cpu.h>
+#include <linux/cpu_pm.h>
 #include <linux/clockchips.h>
 #include <linux/interrupt.h>
 #include <linux/of_irq.h>
+#include <linux/of_address.h>
 #include <linux/io.h>
+#include <linux/slab.h>
 
 #include <asm/arch_timer.h>
 #include <asm/virt.h>
 
 #include <clocksource/arm_arch_timer.h>
 
+#define CNTTIDR		0x08
+#define CNTTIDR_VIRT(n)	(BIT(1) << ((n) * 4))
+
+#define CNTVCT_LO	0x08
+#define CNTVCT_HI	0x0c
+#define CNTFRQ		0x10
+#define CNTP_TVAL	0x28
+#define CNTP_CTL	0x2c
+#define CNTV_TVAL	0x38
+#define CNTV_CTL	0x3c
+
+#define ARCH_CP15_TIMER	BIT(0)
+#define ARCH_MEM_TIMER	BIT(1)
+static unsigned arch_timers_present __initdata;
+
+static void __iomem *arch_counter_base;
+
+struct arch_timer {
+	void __iomem *base;
+	struct clock_event_device evt;
+};
+
+#define to_arch_timer(e) container_of(e, struct arch_timer, evt)
+
 static u32 arch_timer_rate;
 
 enum ppi_nr {
@@ -38,6 +65,7 @@ static int arch_timer_ppi[MAX_TIMER_PPI];
 static struct clock_event_device __percpu *arch_timer_evt;
 
 static bool arch_timer_use_virtual = true;
+static bool arch_timer_mem_use_virtual;
 
 /*
  * Architected system timer support.
@@ -170,31 +198,47 @@ static int __cpuinit arch_timer_setup(struct clock_event_device *clk)
 	}
 
 	arch_counter_set_user_access();
+	if (IS_ENABLED(CONFIG_ARM_ARCH_TIMER_EVTSTREAM))
+		arch_timer_configure_evtstream();
 
 	return 0;
 }
 
-static int arch_timer_available(void)
+static void
+arch_timer_detect_rate(void __iomem *cntbase, struct device_node *np)
 {
-	u32 freq;
+	/* Who has more than one independent system counter? */
+	if (arch_timer_rate)
+		return;
 
-	if (arch_timer_rate == 0) {
-		freq = arch_timer_get_cntfrq();
+	/* Try to determine the frequency from the device tree or CNTFRQ */
+	if (of_property_read_u32(np, "clock-frequency", &arch_timer_rate)) {
+		if (cntbase)
+			arch_timer_rate = readl_relaxed(cntbase + CNTFRQ);
+		else
+			arch_timer_rate = arch_timer_get_cntfrq();
+	}
 
 		/* Check the timer frequency. */
-		if (freq == 0) {
+	if (arch_timer_rate == 0)
 			pr_warn("Architected timer frequency not available\n");
-			return -EINVAL;
 		}
 
-		arch_timer_rate = freq;
-	}
-
-	pr_info_once("Architected local timer running at %lu.%02luMHz (%s).\n",
+static void arch_timer_banner(unsigned type)
+{
+	pr_info("Architected %s%s%s timer(s) running at %lu.%02luMHz (%s%s%s).\n",
+		     type & ARCH_CP15_TIMER ? "cp15" : "",
+		     type == (ARCH_CP15_TIMER | ARCH_MEM_TIMER) ?  " and " : "",
+		     type & ARCH_MEM_TIMER ? "mmio" : "",
 		     (unsigned long)arch_timer_rate / 1000000,
 		     (unsigned long)(arch_timer_rate / 10000) % 100,
-		     arch_timer_use_virtual ? "virt" : "phys");
-	return 0;
+		     type & ARCH_CP15_TIMER ?
+			arch_timer_use_virtual ? "virt" : "phys" :
+			"",
+		     type == (ARCH_CP15_TIMER | ARCH_MEM_TIMER) ?  "/" : "",
+		     type & ARCH_MEM_TIMER ?
+			arch_timer_mem_use_virtual ? "virt" : "phys" :
+			"");
 }
 
 u32 arch_timer_get_rate(void)
@@ -237,6 +281,23 @@ struct timecounter *arch_timer_get_timecounter(void)
 	return &timecounter;
 }
 
+static void __init arch_counter_register(unsigned type)
+{
+	u64 start_count;
+
+	/* Register the CP15 based counter if we have one */
+	if (type & ARCH_CP15_TIMER)
+		arch_timer_read_counter = arch_counter_get_cntvct;
+	else
+		arch_timer_read_counter = arch_counter_get_cntvct_mem;
+
+	start_count = arch_timer_read_counter();
+	clocksource_register_hz(&clocksource_counter, arch_timer_rate);
+	cyclecounter.mult = clocksource_counter.mult;
+	cyclecounter.shift = clocksource_counter.shift;
+	timecounter_init(&timecounter, &cyclecounter, start_count);
+}
+
 static void __cpuinit arch_timer_stop(struct clock_event_device *clk)
 {
 	pr_debug("arch_timer_teardown disable IRQ%d cpu #%d\n",
@@ -276,27 +337,44 @@ static struct notifier_block arch_timer_cpu_nb __cpuinitdata = {
 	.notifier_call = arch_timer_cpu_notify,
 };
 
+#ifdef CONFIG_CPU_PM
+static unsigned int saved_cntkctl;
+static int arch_timer_cpu_pm_notify(struct notifier_block *self,
+				    unsigned long action, void *hcpu)
+{
+	if (action == CPU_PM_ENTER)
+		saved_cntkctl = arch_timer_get_cntkctl();
+	else if (action == CPU_PM_ENTER_FAILED || action == CPU_PM_EXIT)
+		arch_timer_set_cntkctl(saved_cntkctl);
+	return NOTIFY_OK;
+}
+
+static struct notifier_block arch_timer_cpu_pm_notifier = {
+	.notifier_call = arch_timer_cpu_pm_notify,
+};
+
+static int __init arch_timer_cpu_pm_init(void)
+{
+	return cpu_pm_register_notifier(&arch_timer_cpu_pm_notifier);
+}
+#else
+static int __init arch_timer_cpu_pm_init(void)
+{
+	return 0;
+}
+#endif
+
 static int __init arch_timer_register(void)
 {
 	int err;
 	int ppi;
 
-	err = arch_timer_available();
-	if (err)
-		goto out;
-
 	arch_timer_evt = alloc_percpu(struct clock_event_device);
 	if (!arch_timer_evt) {
 		err = -ENOMEM;
 		goto out;
 	}
 
-	clocksource_register_hz(&clocksource_counter, arch_timer_rate);
-	cyclecounter.mult = clocksource_counter.mult;
-	cyclecounter.shift = clocksource_counter.shift;
-	timecounter_init(&timecounter, &cyclecounter,
-			 arch_counter_get_cntvct());
-
 	if (arch_timer_use_virtual) {
 		ppi = arch_timer_ppi[VIRT_PPI];
 		err = request_percpu_irq(ppi, arch_timer_handler_virt,
@@ -325,11 +403,17 @@ static int __init arch_timer_register(void)
 	if (err)
 		goto out_free_irq;
 
+	err = arch_timer_cpu_pm_init();
+	if (err)
+		goto out_unreg_notify;
+
 	/* Immediately configure the timer on the boot CPU */
 	arch_timer_setup(this_cpu_ptr(arch_timer_evt));
 
 	return 0;
 
+out_unreg_notify:
+	unregister_cpu_notifier(&arch_timer_cpu_nb);
 out_free_irq:
 	if (arch_timer_use_virtual)
 		free_percpu_irq(arch_timer_ppi[VIRT_PPI], arch_timer_evt);
@@ -347,24 +431,77 @@ out:
 	return err;
 }
 
+static int __init arch_timer_mem_register(void __iomem *base, unsigned int irq)
+{
+	int ret;
+	irq_handler_t func;
+	struct arch_timer *t;
+
+	t = kzalloc(sizeof(*t), GFP_KERNEL);
+	if (!t)
+		return -ENOMEM;
+
+	t->base = base;
+	t->evt.irq = irq;
+	__arch_timer_setup(ARCH_MEM_TIMER, &t->evt);
+
+	if (arch_timer_mem_use_virtual)
+		func = arch_timer_handler_virt_mem;
+	else
+		func = arch_timer_handler_phys_mem;
+
+	ret = request_irq(irq, func, IRQF_TIMER, "arch_mem_timer", &t->evt);
+	if (ret) {
+		pr_err("arch_timer: Failed to request mem timer irq\n");
+		kfree(t);
+	}
+
+	return ret;
+}
+
+static const struct of_device_id arch_timer_of_match[] __initconst = {
+	{ .compatible   = "arm,armv7-timer",    },
+	{ .compatible   = "arm,armv8-timer",    },
+	{},
+};
+
+static const struct of_device_id arch_timer_mem_of_match[] __initconst = {
+	{ .compatible   = "arm,armv7-timer-mem", },
+	{},
+};
+
+static void __init arch_timer_common_init(void)
+{
+	unsigned mask = ARCH_CP15_TIMER | ARCH_MEM_TIMER;
+
+	/* Wait until both nodes are probed if we have two timers */
+	if ((arch_timers_present & mask) != mask) {
+		if (of_find_matching_node(NULL, arch_timer_mem_of_match) &&
+				!(arch_timers_present & ARCH_MEM_TIMER))
+			return;
+		if (of_find_matching_node(NULL, arch_timer_of_match) &&
+				!(arch_timers_present & ARCH_CP15_TIMER))
+			return;
+	}
+
+	arch_timer_banner(arch_timers_present);
+	arch_counter_register(arch_timers_present);
+	arch_timer_arch_init();
+}
+
 static void __init arch_timer_init(struct device_node *np)
 {
-	u32 freq;
 	int i;
 
-	if (arch_timer_get_rate()) {
+	if (arch_timers_present & ARCH_CP15_TIMER) {
 		pr_warn("arch_timer: multiple nodes in dt, skipping\n");
 		return;
 	}
 
-	/* Try to determine the frequency from the device tree or CNTFRQ */
-	if (!of_property_read_u32(np, "clock-frequency", &freq))
-		arch_timer_rate = freq;
-
+	arch_timers_present |= ARCH_CP15_TIMER;
 	for (i = PHYS_SECURE_PPI; i < MAX_TIMER_PPI; i++)
 		arch_timer_ppi[i] = irq_of_parse_and_map(np, i);
-
-	of_node_put(np);
+	arch_timer_detect_rate(NULL, np);
 
 	/*
 	 * If HYP mode is available, we know that the physical timer
@@ -385,7 +522,73 @@ static void __init arch_timer_init(struct device_node *np)
 	}
 
 	arch_timer_register();
-	arch_timer_arch_init();
+	arch_timer_common_init();
 }
 CLOCKSOURCE_OF_DECLARE(armv7_arch_timer, "arm,armv7-timer", arch_timer_init);
 CLOCKSOURCE_OF_DECLARE(armv8_arch_timer, "arm,armv8-timer", arch_timer_init);
+
+static void __init arch_timer_mem_init(struct device_node *np)
+{
+	struct device_node *frame, *best_frame = NULL;
+	void __iomem *cntctlbase, *base;
+	unsigned int irq;
+	u32 cnttidr;
+
+	arch_timers_present |= ARCH_MEM_TIMER;
+	cntctlbase = of_iomap(np, 0);
+	if (!cntctlbase) {
+		pr_err("arch_timer: Can't find CNTCTLBase\n");
+		return;
+	}
+
+	cnttidr = readl_relaxed(cntctlbase + CNTTIDR);
+	iounmap(cntctlbase);
+
+	/*
+	 * Try to find a virtual capable frame. Otherwise fall back to a
+	 * physical capable frame.
+	 */
+	for_each_available_child_of_node(np, frame) {
+		int n;
+
+		if (of_property_read_u32(frame, "frame-number", &n)) {
+			pr_err("arch_timer: Missing frame-number\n");
+			of_node_put(best_frame);
+			of_node_put(frame);
+			return;
+		}
+
+		if (cnttidr & CNTTIDR_VIRT(n)) {
+			of_node_put(best_frame);
+			best_frame = frame;
+			arch_timer_mem_use_virtual = true;
+			break;
+		}
+		of_node_put(best_frame);
+		best_frame = of_node_get(frame);
+	}
+
+	base = arch_counter_base = of_iomap(best_frame, 0);
+	if (!base) {
+		pr_err("arch_timer: Can't map frame's registers\n");
+		of_node_put(best_frame);
+		return;
+	}
+
+	if (arch_timer_mem_use_virtual)
+		irq = irq_of_parse_and_map(best_frame, 1);
+	else
+		irq = irq_of_parse_and_map(best_frame, 0);
+	of_node_put(best_frame);
+	if (!irq) {
+		pr_err("arch_timer: Frame missing %s irq",
+				arch_timer_mem_use_virtual ? "virt" : "phys");
+		return;
+	}
+
+	arch_timer_detect_rate(base, np);
+	arch_timer_mem_register(base, irq);
+	arch_timer_common_init();
+}
+CLOCKSOURCE_OF_DECLARE(armv7_arch_timer_mem, "arm,armv7-timer-mem",
+		       arch_timer_mem_init);
diff --git a/drivers/cpufreq/Kconfig b/drivers/cpufreq/Kconfig
index 534fcb8..9e1f7d9 100644
--- a/drivers/cpufreq/Kconfig
+++ b/drivers/cpufreq/Kconfig
@@ -102,6 +102,16 @@ config CPU_FREQ_DEFAULT_GOV_CONSERVATIVE
 	  Be aware that not all cpufreq drivers support the conservative
 	  governor. If unsure have a look at the help section of the
 	  driver. Fallback governor will be the performance governor.
+
+config CPU_FREQ_DEFAULT_GOV_INTERACTIVE
+	bool "interactive"
+	select CPU_FREQ_GOV_INTERACTIVE
+	help
+	  Use the CPUFreq governor 'interactive' as default. This allows
+	  you to get a full dynamic cpu frequency capable system by simply
+	  loading your cpufreq low-level hardware driver, using the
+	  'interactive' governor for latency-sensitive workloads.
+
 endchoice
 
 config CPU_FREQ_GOV_PERFORMANCE
@@ -160,6 +170,24 @@ config CPU_FREQ_GOV_ONDEMAND
 
 	  If in doubt, say N.
 
+config CPU_FREQ_GOV_INTERACTIVE
+	tristate "'interactive' cpufreq policy governor"
+	default n
+	help
+	  'interactive' - This driver adds a dynamic cpufreq policy governor
+	  designed for latency-sensitive workloads.
+
+	  This governor attempts to reduce the latency of clock
+	  increases so that the system is more responsive to
+	  interactive workloads.
+
+	  To compile this driver as a module, choose M here: the
+	  module will be called cpufreq_interactive.
+
+	  For details, take a look at linux/Documentation/cpu-freq.
+
+	  If in doubt, say N.
+
 config CPU_FREQ_GOV_CONSERVATIVE
 	tristate "'conservative' cpufreq governor"
 	depends on CPU_FREQ
@@ -201,7 +229,7 @@ source "drivers/cpufreq/Kconfig.x86"
 endmenu
 
 menu "ARM CPU frequency scaling drivers"
-depends on ARM
+depends on ARM || ARM64
 source "drivers/cpufreq/Kconfig.arm"
 endmenu
 
diff --git a/drivers/cpufreq/Kconfig.arm b/drivers/cpufreq/Kconfig.arm
index 6e57543..130b650 100644
--- a/drivers/cpufreq/Kconfig.arm
+++ b/drivers/cpufreq/Kconfig.arm
@@ -4,7 +4,8 @@
 
 config ARM_BIG_LITTLE_CPUFREQ
 	tristate "Generic ARM big LITTLE CPUfreq driver"
-	depends on ARM_CPU_TOPOLOGY && PM_OPP && HAVE_CLK
+	depends on (ARM_CPU_TOPOLOGY && BIG_LITTLE) || (ARM64 && SMP)
+	depends on PM_OPP && HAVE_CLK
 	help
 	  This enables the Generic CPUfreq driver for ARM big.LITTLE platforms.
 
@@ -15,6 +16,22 @@ config ARM_DT_BL_CPUFREQ
 	  This enables probing via DT for Generic CPUfreq driver for ARM
 	  big.LITTLE platform. This gets frequency tables from DT.
 
+config MB86S70_CPUFREQ
+	tristate "Fujistu S70 HMP cpufreq driver"
+	depends on ARCH_MB86S70 && !ARM_BIG_LITTLE_CPUFREQ
+	help
+	  This adds a cpufreq driver for MB86S70 frequency management, which
+	  has a quirk that CA15 cluster must be brought down before changing
+	  its freq. CA7 cluster is said to be good to need no such voodoo.
+
+config ARM_VEXPRESS_BL_CPUFREQ
+	tristate "ARM Vexpress big LITTLE CPUfreq driver"
+	select ARM_BIG_LITTLE_CPUFREQ
+	depends on VEXPRESS_SPC
+	help
+	  This enables the CPUfreq driver for ARM Vexpress big.LITTLE platform.
+	  If in doubt, say N.
+
 config ARM_EXYNOS_CPUFREQ
 	bool "SAMSUNG EXYNOS SoCs"
 	depends on ARCH_EXYNOS
diff --git a/drivers/cpufreq/Makefile b/drivers/cpufreq/Makefile
index 3b95322..f5942d6 100644
--- a/drivers/cpufreq/Makefile
+++ b/drivers/cpufreq/Makefile
@@ -72,6 +72,7 @@ obj-$(CONFIG_ARM_SA1100_CPUFREQ)	+= sa1100-cpufreq.o
 obj-$(CONFIG_ARM_SA1110_CPUFREQ)	+= sa1110-cpufreq.o
 obj-$(CONFIG_ARM_SPEAR_CPUFREQ)		+= spear-cpufreq.o
 obj-$(CONFIG_ARCH_TEGRA)		+= tegra-cpufreq.o
+obj-$(CONFIG_MB86S70_CPUFREQ)		+= mb86s70-cpufreq.o
 
 ##################################################################################
 # PowerPC platform drivers
diff --git a/drivers/cpufreq/arm_big_little.c b/drivers/cpufreq/arm_big_little.c
index 5d7f53f..26d7505 100644
--- a/drivers/cpufreq/arm_big_little.c
+++ b/drivers/cpufreq/arm_big_little.c
@@ -24,27 +24,148 @@
 #include <linux/cpufreq.h>
 #include <linux/cpumask.h>
 #include <linux/export.h>
+#include <linux/mutex.h>
 #include <linux/of_platform.h>
 #include <linux/opp.h>
 #include <linux/slab.h>
 #include <linux/topology.h>
 #include <linux/types.h>
+#include <asm/bL_switcher.h>
 
 #include "arm_big_little.h"
 
-/* Currently we support only two clusters */
-#define MAX_CLUSTERS	2
+#ifdef CONFIG_BL_SWITCHER
+bool bL_switching_enabled;
+#endif
+
+#define ACTUAL_FREQ(cluster, freq)	((cluster == A7_CLUSTER) ? freq << 1 : freq)
+#define VIRT_FREQ(cluster, freq)	((cluster == A7_CLUSTER) ? freq >> 1 : freq)
 
 static struct cpufreq_arm_bL_ops *arm_bL_ops;
 static struct clk *clk[MAX_CLUSTERS];
-static struct cpufreq_frequency_table *freq_table[MAX_CLUSTERS];
-static atomic_t cluster_usage[MAX_CLUSTERS] = {ATOMIC_INIT(0), ATOMIC_INIT(0)};
+static struct cpufreq_frequency_table *freq_table[MAX_CLUSTERS + 1];
+static atomic_t cluster_usage[MAX_CLUSTERS + 1] = {ATOMIC_INIT(0),
+	ATOMIC_INIT(0)};
+
+static unsigned int clk_big_min;	/* (Big) clock frequencies */
+static unsigned int clk_little_max;	/* Maximum clock frequency (Little) */
+
+static DEFINE_PER_CPU(unsigned int, physical_cluster);
+static DEFINE_PER_CPU(unsigned int, cpu_last_req_freq);
 
-static unsigned int bL_cpufreq_get(unsigned int cpu)
+static struct mutex cluster_lock[MAX_CLUSTERS];
+
+static unsigned int find_cluster_maxfreq(int cluster)
 {
-	u32 cur_cluster = cpu_to_cluster(cpu);
+	int j;
+	u32 max_freq = 0, cpu_freq;
+
+	for_each_online_cpu(j) {
+		cpu_freq = per_cpu(cpu_last_req_freq, j);
 
-	return clk_get_rate(clk[cur_cluster]) / 1000;
+		if ((cluster == per_cpu(physical_cluster, j)) &&
+				(max_freq < cpu_freq))
+			max_freq = cpu_freq;
+	}
+
+	pr_debug("%s: cluster: %d, max freq: %d\n", __func__, cluster,
+			max_freq);
+
+	return max_freq;
+}
+
+static unsigned int clk_get_cpu_rate(unsigned int cpu)
+{
+	u32 cur_cluster = per_cpu(physical_cluster, cpu);
+	u32 rate = clk_get_rate(clk[cur_cluster]) / 1000;
+
+	/* For switcher we use virtual A15 clock rates */
+	if (is_bL_switching_enabled())
+		rate = VIRT_FREQ(cur_cluster, rate);
+
+	pr_debug("%s: cpu: %d, cluster: %d, freq: %u\n", __func__, cpu,
+			cur_cluster, rate);
+
+	return rate;
+}
+
+static unsigned int bL_cpufreq_get_rate(unsigned int cpu)
+{
+	if (is_bL_switching_enabled()) {
+		pr_debug("%s: freq: %d\n", __func__, per_cpu(cpu_last_req_freq,
+					cpu));
+
+		return per_cpu(cpu_last_req_freq, cpu);
+	} else {
+		return clk_get_cpu_rate(cpu);
+	}
+}
+
+static unsigned int
+bL_cpufreq_set_rate(u32 cpu, u32 old_cluster, u32 new_cluster, u32 rate)
+{
+	u32 new_rate, prev_rate;
+	int ret;
+	bool bLs = is_bL_switching_enabled();
+
+	mutex_lock(&cluster_lock[new_cluster]);
+
+	if (bLs) {
+		prev_rate = per_cpu(cpu_last_req_freq, cpu);
+		per_cpu(cpu_last_req_freq, cpu) = rate;
+		per_cpu(physical_cluster, cpu) = new_cluster;
+
+		new_rate = find_cluster_maxfreq(new_cluster);
+		new_rate = ACTUAL_FREQ(new_cluster, new_rate);
+	} else {
+		new_rate = rate;
+	}
+
+	pr_debug("%s: cpu: %d, old cluster: %d, new cluster: %d, freq: %d\n",
+			__func__, cpu, old_cluster, new_cluster, new_rate);
+
+	ret = clk_set_rate(clk[new_cluster], new_rate * 1000);
+	if (WARN_ON(ret)) {
+		pr_err("clk_set_rate failed: %d, new cluster: %d\n", ret,
+				new_cluster);
+		if (bLs) {
+			per_cpu(cpu_last_req_freq, cpu) = prev_rate;
+			per_cpu(physical_cluster, cpu) = old_cluster;
+		}
+
+		mutex_unlock(&cluster_lock[new_cluster]);
+
+		return ret;
+	}
+
+	mutex_unlock(&cluster_lock[new_cluster]);
+
+	/* Recalc freq for old cluster when switching clusters */
+	if (old_cluster != new_cluster) {
+		pr_debug("%s: cpu: %d, old cluster: %d, new cluster: %d\n",
+				__func__, cpu, old_cluster, new_cluster);
+
+		/* Switch cluster */
+		bL_switch_request(cpu, new_cluster);
+
+		mutex_lock(&cluster_lock[old_cluster]);
+
+		/* Set freq of old cluster if there are cpus left on it */
+		new_rate = find_cluster_maxfreq(old_cluster);
+		new_rate = ACTUAL_FREQ(old_cluster, new_rate);
+
+		if (new_rate) {
+			pr_debug("%s: Updating rate of old cluster: %d, to freq: %d\n",
+					__func__, old_cluster, new_rate);
+
+			if (clk_set_rate(clk[old_cluster], new_rate * 1000))
+				pr_err("%s: clk_set_rate failed: %d, old cluster: %d\n",
+						__func__, ret, old_cluster);
+		}
+		mutex_unlock(&cluster_lock[old_cluster]);
+	}
+
+	return 0;
 }
 
 /* Validate policy frequency range */
@@ -60,12 +181,14 @@ static int bL_cpufreq_set_target(struct cpufreq_policy *policy,
 		unsigned int target_freq, unsigned int relation)
 {
 	struct cpufreq_freqs freqs;
-	u32 cpu = policy->cpu, freq_tab_idx, cur_cluster;
+	u32 cpu = policy->cpu, freq_tab_idx, cur_cluster, new_cluster,
+	    actual_cluster;
 	int ret = 0;
 
-	cur_cluster = cpu_to_cluster(policy->cpu);
+	cur_cluster = cpu_to_cluster(cpu);
+	new_cluster = actual_cluster = per_cpu(physical_cluster, cpu);
 
-	freqs.old = bL_cpufreq_get(policy->cpu);
+	freqs.old = bL_cpufreq_get_rate(cpu);
 
 	/* Determine valid target frequency using freq_table */
 	cpufreq_frequency_table_target(policy, freq_table[cur_cluster],
@@ -79,13 +202,21 @@ static int bL_cpufreq_set_target(struct cpufreq_policy *policy,
 	if (freqs.old == freqs.new)
 		return 0;
 
+	if (is_bL_switching_enabled()) {
+		if ((actual_cluster == A15_CLUSTER) &&
+				(freqs.new < clk_big_min)) {
+			new_cluster = A7_CLUSTER;
+		} else if ((actual_cluster == A7_CLUSTER) &&
+				(freqs.new > clk_little_max)) {
+			new_cluster = A15_CLUSTER;
+		}
+	}
+
 	cpufreq_notify_transition(policy, &freqs, CPUFREQ_PRECHANGE);
 
-	ret = clk_set_rate(clk[cur_cluster], freqs.new * 1000);
-	if (ret) {
-		pr_err("clk_set_rate failed: %d\n", ret);
+	ret = bL_cpufreq_set_rate(cpu, actual_cluster, new_cluster, freqs.new);
+	if (ret)
 		return ret;
-	}
 
 	policy->cur = freqs.new;
 
@@ -94,24 +225,116 @@ static int bL_cpufreq_set_target(struct cpufreq_policy *policy,
 	return ret;
 }
 
-static void put_cluster_clk_and_freq_table(struct device *cpu_dev)
+static inline u32 get_table_count(struct cpufreq_frequency_table *table)
 {
-	u32 cluster = cpu_to_cluster(cpu_dev->id);
+	int count;
+
+	for (count = 0; table[count].frequency != CPUFREQ_TABLE_END; count++)
+		;
+
+	return count;
+}
+
+/* get the minimum frequency in the cpufreq_frequency_table */
+static inline u32 get_table_min(struct cpufreq_frequency_table *table)
+{
+	int i;
+	uint32_t min_freq = ~0;
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++)
+		if (table[i].frequency < min_freq)
+			min_freq = table[i].frequency;
+	return min_freq;
+}
+
+/* get the maximum frequency in the cpufreq_frequency_table */
+static inline u32 get_table_max(struct cpufreq_frequency_table *table)
+{
+	int i;
+	uint32_t max_freq = 0;
+	for (i = 0; (table[i].frequency != CPUFREQ_TABLE_END); i++)
+		if (table[i].frequency > max_freq)
+			max_freq = table[i].frequency;
+	return max_freq;
+}
+
+static int merge_cluster_tables(void)
+{
+	int i, j, k = 0, count = 1;
+	struct cpufreq_frequency_table *table;
+
+	for (i = 0; i < MAX_CLUSTERS; i++)
+		count += get_table_count(freq_table[i]);
+
+	table = kzalloc(sizeof(*table) * count, GFP_KERNEL);
+	if (!table)
+		return -ENOMEM;
+
+	freq_table[MAX_CLUSTERS] = table;
+
+	/* Add in reverse order to get freqs in increasing order */
+	for (i = MAX_CLUSTERS - 1; i >= 0; i--) {
+		for (j = 0; freq_table[i][j].frequency != CPUFREQ_TABLE_END;
+				j++) {
+			table[k].frequency = VIRT_FREQ(i,
+					freq_table[i][j].frequency);
+			pr_debug("%s: index: %d, freq: %d\n", __func__, k,
+					table[k].frequency);
+			k++;
+		}
+	}
+
+	table[k].index = k;
+	table[k].frequency = CPUFREQ_TABLE_END;
+
+	pr_debug("%s: End, table: %p, count: %d\n", __func__, table, k);
+
+	return 0;
+}
+
+static void _put_cluster_clk_and_freq_table(struct device *cpu_dev)
+{
+	u32 cluster = topology_physical_package_id(cpu_dev->id);
+
+	if (!freq_table[cluster])
+		return;
 
-	if (!atomic_dec_return(&cluster_usage[cluster])) {
 		clk_put(clk[cluster]);
 		opp_free_cpufreq_table(cpu_dev, &freq_table[cluster]);
 		dev_dbg(cpu_dev, "%s: cluster: %d\n", __func__, cluster);
 	}
-}
 
-static int get_cluster_clk_and_freq_table(struct device *cpu_dev)
+static void put_cluster_clk_and_freq_table(struct device *cpu_dev)
 {
 	u32 cluster = cpu_to_cluster(cpu_dev->id);
-	char name[14] = "cpu-cluster.";
+	int i;
+
+	if (atomic_dec_return(&cluster_usage[cluster]))
+		return;
+
+	if (cluster < MAX_CLUSTERS)
+		return _put_cluster_clk_and_freq_table(cpu_dev);
+
+	for_each_present_cpu(i) {
+		struct device *cdev = get_cpu_device(i);
+		if (!cdev) {
+			pr_err("%s: failed to get cpu%d device\n", __func__, i);
+			return;
+		}
+
+		_put_cluster_clk_and_freq_table(cdev);
+	}
+
+	/* free virtual table */
+	kfree(freq_table[MAX_CLUSTERS]);
+}
+
+static int _get_cluster_clk_and_freq_table(struct device *cpu_dev)
+{
+	u32 cluster = topology_physical_package_id(cpu_dev->id);
+	char name[14] = "cpu-cluster.X";
 	int ret;
 
-	if (atomic_inc_return(&cluster_usage[cluster]) != 1)
+	if (freq_table[cluster])
 		return 0;
 
 	ret = arm_bL_ops->init_opp_table(cpu_dev);
@@ -149,6 +372,66 @@ atomic_dec:
 	return ret;
 }
 
+static int get_cluster_clk_and_freq_table(struct device *cpu_dev)
+{
+	u32 cluster = cpu_to_cluster(cpu_dev->id);
+	int i, ret;
+
+	if (atomic_inc_return(&cluster_usage[cluster]) != 1)
+		return 0;
+
+	if (cluster < MAX_CLUSTERS) {
+		ret = _get_cluster_clk_and_freq_table(cpu_dev);
+		if (ret)
+			atomic_dec(&cluster_usage[cluster]);
+		return ret;
+	}
+
+	/*
+	 * Get data for all clusters and fill virtual cluster with a merge of
+	 * both
+	 */
+	for_each_present_cpu(i) {
+		struct device *cdev = get_cpu_device(i);
+		if (!cdev) {
+			pr_err("%s: failed to get cpu%d device\n", __func__, i);
+			return -ENODEV;
+		}
+
+		ret = _get_cluster_clk_and_freq_table(cdev);
+		if (ret)
+			goto put_clusters;
+	}
+
+	ret = merge_cluster_tables();
+	if (ret)
+		goto put_clusters;
+
+	/* Assuming 2 cluster, set clk_big_min and clk_little_max */
+	clk_big_min = get_table_min(freq_table[0]);
+	clk_little_max = VIRT_FREQ(1, get_table_max(freq_table[1]));
+
+	pr_debug("%s: cluster: %d, clk_big_min: %d, clk_little_max: %d\n",
+			__func__, cluster, clk_big_min, clk_little_max);
+
+	return 0;
+
+put_clusters:
+	for_each_present_cpu(i) {
+		struct device *cdev = get_cpu_device(i);
+		if (!cdev) {
+			pr_err("%s: failed to get cpu%d device\n", __func__, i);
+			return -ENODEV;
+		}
+
+		_put_cluster_clk_and_freq_table(cdev);
+	}
+
+	atomic_dec(&cluster_usage[cluster]);
+
+	return ret;
+}
+
 /* Per-CPU initialization */
 static int bL_cpufreq_init(struct cpufreq_policy *policy)
 {
@@ -177,37 +460,30 @@ static int bL_cpufreq_init(struct cpufreq_policy *policy)
 
 	cpufreq_frequency_table_get_attr(freq_table[cur_cluster], policy->cpu);
 
+	if (cur_cluster < MAX_CLUSTERS) {
+		cpumask_copy(policy->cpus, topology_core_cpumask(policy->cpu));
+
+		per_cpu(physical_cluster, policy->cpu) = cur_cluster;
+	} else {
+		/* Assumption: during init, we are always running on A15 */
+		per_cpu(physical_cluster, policy->cpu) = A15_CLUSTER;
+	}
+
 	if (arm_bL_ops->get_transition_latency)
 		policy->cpuinfo.transition_latency =
 			arm_bL_ops->get_transition_latency(cpu_dev);
 	else
 		policy->cpuinfo.transition_latency = CPUFREQ_ETERNAL;
 
-	policy->cur = bL_cpufreq_get(policy->cpu);
+	policy->cur = clk_get_cpu_rate(policy->cpu);
 
-	cpumask_copy(policy->cpus, topology_core_cpumask(policy->cpu));
+	if (is_bL_switching_enabled())
+		per_cpu(cpu_last_req_freq, policy->cpu) = policy->cur;
 
 	dev_info(cpu_dev, "%s: CPU %d initialized\n", __func__, policy->cpu);
 	return 0;
 }
 
-static int bL_cpufreq_exit(struct cpufreq_policy *policy)
-{
-	struct device *cpu_dev;
-
-	cpu_dev = get_cpu_device(policy->cpu);
-	if (!cpu_dev) {
-		pr_err("%s: failed to get cpu%d device\n", __func__,
-				policy->cpu);
-		return -ENODEV;
-	}
-
-	put_cluster_clk_and_freq_table(cpu_dev);
-	dev_dbg(cpu_dev, "%s: Exited, cpu: %d\n", __func__, policy->cpu);
-
-	return 0;
-}
-
 /* Export freq_table to sysfs */
 static struct freq_attr *bL_cpufreq_attr[] = {
 	&cpufreq_freq_attr_scaling_available_freqs,
@@ -219,16 +495,47 @@ static struct cpufreq_driver bL_cpufreq_driver = {
 	.flags			= CPUFREQ_STICKY,
 	.verify			= bL_cpufreq_verify_policy,
 	.target			= bL_cpufreq_set_target,
-	.get			= bL_cpufreq_get,
+	.get			= bL_cpufreq_get_rate,
 	.init			= bL_cpufreq_init,
-	.exit			= bL_cpufreq_exit,
 	.have_governor_per_policy = true,
 	.attr			= bL_cpufreq_attr,
 };
 
+static int bL_cpufreq_switcher_notifier(struct notifier_block *nfb,
+					unsigned long action, void *_arg)
+{
+	pr_debug("%s: action: %ld\n", __func__, action);
+
+	switch (action) {
+	case BL_NOTIFY_PRE_ENABLE:
+	case BL_NOTIFY_PRE_DISABLE:
+		cpufreq_unregister_driver(&bL_cpufreq_driver);
+		break;
+
+	case BL_NOTIFY_POST_ENABLE:
+		set_switching_enabled(true);
+		cpufreq_register_driver(&bL_cpufreq_driver);
+		break;
+
+	case BL_NOTIFY_POST_DISABLE:
+		set_switching_enabled(false);
+		cpufreq_register_driver(&bL_cpufreq_driver);
+		break;
+
+	default:
+		return NOTIFY_DONE;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block bL_switcher_notifier = {
+	.notifier_call = bL_cpufreq_switcher_notifier,
+};
+
 int bL_cpufreq_register(struct cpufreq_arm_bL_ops *ops)
 {
-	int ret;
+	int ret, i;
 
 	if (arm_bL_ops) {
 		pr_debug("%s: Already registered: %s, exiting\n", __func__,
@@ -243,16 +550,29 @@ int bL_cpufreq_register(struct cpufreq_arm_bL_ops *ops)
 
 	arm_bL_ops = ops;
 
+	ret = bL_switcher_get_enabled();
+	set_switching_enabled(ret);
+
+	for (i = 0; i < MAX_CLUSTERS; i++)
+		mutex_init(&cluster_lock[i]);
+
 	ret = cpufreq_register_driver(&bL_cpufreq_driver);
 	if (ret) {
 		pr_info("%s: Failed registering platform driver: %s, err: %d\n",
 				__func__, ops->name, ret);
 		arm_bL_ops = NULL;
 	} else {
-		pr_info("%s: Registered platform driver: %s\n", __func__,
-				ops->name);
+		ret = bL_switcher_register_notifier(&bL_switcher_notifier);
+		if (ret) {
+			cpufreq_unregister_driver(&bL_cpufreq_driver);
+			arm_bL_ops = NULL;
+		} else {
+			pr_info("%s: Registered platform driver: %s\n",
+					__func__, ops->name);
+		}
 	}
 
+	bL_switcher_put_enabled();
 	return ret;
 }
 EXPORT_SYMBOL_GPL(bL_cpufreq_register);
@@ -265,9 +585,31 @@ void bL_cpufreq_unregister(struct cpufreq_arm_bL_ops *ops)
 		return;
 	}
 
+	bL_switcher_get_enabled();
+	bL_switcher_unregister_notifier(&bL_switcher_notifier);
 	cpufreq_unregister_driver(&bL_cpufreq_driver);
+	bL_switcher_put_enabled();
 	pr_info("%s: Un-registered platform driver: %s\n", __func__,
 			arm_bL_ops->name);
+
+	/* For saving table get/put on every cpu in/out */
+	if (is_bL_switching_enabled()) {
+		put_cluster_clk_and_freq_table(get_cpu_device(0));
+	} else {
+		int i;
+
+		for (i = 0; i < MAX_CLUSTERS; i++) {
+			struct device *cdev = get_cpu_device(i);
+			if (!cdev) {
+				pr_err("%s: failed to get cpu%d device\n",
+						__func__, i);
+				return;
+			}
+
+			put_cluster_clk_and_freq_table(cdev);
+		}
+	}
+
 	arm_bL_ops = NULL;
 }
 EXPORT_SYMBOL_GPL(bL_cpufreq_unregister);
diff --git a/drivers/cpufreq/arm_big_little.h b/drivers/cpufreq/arm_big_little.h
index 79b2ce1..4f5a03d 100644
--- a/drivers/cpufreq/arm_big_little.h
+++ b/drivers/cpufreq/arm_big_little.h
@@ -23,6 +23,20 @@
 #include <linux/device.h>
 #include <linux/types.h>
 
+/* Currently we support only two clusters */
+#define A15_CLUSTER	0
+#define A7_CLUSTER	1
+#define MAX_CLUSTERS	2
+
+#ifdef CONFIG_BL_SWITCHER
+extern bool bL_switching_enabled;
+#define is_bL_switching_enabled()		bL_switching_enabled
+#define set_switching_enabled(x) 		(bL_switching_enabled = (x))
+#else
+#define is_bL_switching_enabled()		false
+#define set_switching_enabled(x) 		do { } while (0)
+#endif
+
 struct cpufreq_arm_bL_ops {
 	char name[CPUFREQ_NAME_LEN];
 	int (*get_transition_latency)(struct device *cpu_dev);
@@ -36,7 +50,8 @@ struct cpufreq_arm_bL_ops {
 
 static inline int cpu_to_cluster(int cpu)
 {
-	return topology_physical_package_id(cpu);
+	return is_bL_switching_enabled() ? MAX_CLUSTERS:
+		topology_physical_package_id(cpu);
 }
 
 int bL_cpufreq_register(struct cpufreq_arm_bL_ops *ops);
diff --git a/drivers/cpufreq/cpufreq.c b/drivers/cpufreq/cpufreq.c
index e8f5422..0397ae9 100644
--- a/drivers/cpufreq/cpufreq.c
+++ b/drivers/cpufreq/cpufreq.c
@@ -17,6 +17,8 @@
 
 #define pr_fmt(fmt) KBUILD_MODNAME ": " fmt
 
+#include <asm/cputime.h>
+#include <linux/kernel.h>
 #include <linux/cpu.h>
 #include <linux/cpufreq.h>
 #include <linux/delay.h>
@@ -27,6 +29,7 @@
 #include <linux/mutex.h>
 #include <linux/slab.h>
 #include <linux/syscore_ops.h>
+#include <linux/suspend.h>
 #include <linux/tick.h>
 #include <trace/events/power.h>
 
@@ -47,6 +50,14 @@ static LIST_HEAD(cpufreq_policy_list);
 static DEFINE_PER_CPU(char[CPUFREQ_NAME_LEN], cpufreq_cpu_governor);
 #endif
 
+/* Flag to suspend/resume CPUFreq governors */
+static bool cpufreq_suspended;
+
+static inline bool has_target(void)
+{
+	return cpufreq_driver->target;
+}
+
 /*
  * cpu_policy_rwsem is a per CPU reader-writer semaphore designed to cure
  * all cpufreq/hotplug/workqueue/etc related lock issues.
@@ -169,6 +180,51 @@ static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
 
 	return cputime_to_usecs(idle_time);
 }
+EXPORT_SYMBOL_GPL(have_governor_per_policy);
+
+struct kobject *get_governor_parent_kobj(struct cpufreq_policy *policy)
+{
+	if (have_governor_per_policy())
+		return &policy->kobj;
+	else
+		return cpufreq_global_kobject;
+}
+EXPORT_SYMBOL_GPL(get_governor_parent_kobj);
+
+static inline u64 get_cpu_idle_time_jiffy(unsigned int cpu, u64 *wall)
+{
+	u64 idle_time;
+	u64 cur_wall_time;
+	u64 busy_time;
+
+	cur_wall_time = jiffies64_to_cputime64(get_jiffies_64());
+
+	busy_time = kcpustat_cpu(cpu).cpustat[CPUTIME_USER];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SYSTEM];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_IRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_SOFTIRQ];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_STEAL];
+	busy_time += kcpustat_cpu(cpu).cpustat[CPUTIME_NICE];
+
+	idle_time = cur_wall_time - busy_time;
+	if (wall)
+		*wall = cputime_to_usecs(cur_wall_time);
+
+	return cputime_to_usecs(idle_time);
+}
+
+u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, int io_busy)
+{
+	u64 idle_time = get_cpu_idle_time_us(cpu, io_busy ? wall : NULL);
+
+	if (idle_time == -1ULL)
+		return get_cpu_idle_time_jiffy(cpu, wall);
+	else if (!io_busy)
+		idle_time += get_cpu_iowait_time_us(cpu, wall);
+
+	return idle_time;
+}
+EXPORT_SYMBOL_GPL(get_cpu_idle_time);
 
 u64 get_cpu_idle_time(unsigned int cpu, u64 *wall, int io_busy)
 {
@@ -1492,23 +1548,32 @@ static struct subsys_interface cpufreq_interface = {
 /**
  * cpufreq_bp_suspend - Prepare the boot CPU for system suspend.
  *
- * This function is only executed for the boot processor.  The other CPUs
- * have been put offline by means of CPU hotplug.
+ * Called during system wide Suspend/Hibernate cycles for suspending governors
+ * as some platforms can't change frequency after this point in suspend cycle.
+ * Because some of the devices (like: i2c, regulators, etc) they use for
+ * changing frequency are suspended quickly after this point.
  */
-static int cpufreq_bp_suspend(void)
+void cpufreq_suspend(void)
 {
-	int ret = 0;
-
-	int cpu = smp_processor_id();
 	struct cpufreq_policy *policy;
+	int cpu;
+
+	if (!cpufreq_driver)
+		return;
+
+	if (!has_target())
+		return;
 
-	pr_debug("suspending cpu %u\n", cpu);
+	pr_debug("%s: Suspending Governors\n", __func__);
+
+	for_each_possible_cpu(cpu) {
+		if (!cpu_online(cpu))
+			continue;
 
-	/* If there's no policy for the boot CPU, we have nothing to do. */
 	policy = cpufreq_cpu_get(cpu);
-	if (!policy)
-		return 0;
 
+		if (__cpufreq_governor(policy, CPUFREQ_GOV_STOP))
+			pr_err("%s: Failed to stop governor for policy: %p\n",
 	if (cpufreq_driver->suspend) {
 		ret = cpufreq_driver->suspend(policy);
 		if (ret)
@@ -1516,18 +1581,10 @@ static int cpufreq_bp_suspend(void)
 					"step on CPU %u\n", policy->cpu);
 	}
 
-	cpufreq_cpu_put(policy);
 	return ret;
 }
 
 /**
- * cpufreq_bp_resume - Restore proper frequency handling of the boot CPU.
- *
- *	1.) resume CPUfreq hardware support (cpufreq_driver->resume())
- *	2.) schedule call cpufreq_update_policy() ASAP as interrupts are
- *	    restored. It will verify that the current freq is in sync with
- *	    what we believe it to be. This is a bit later than when it
- *	    should be, but nonethteless it's better than calling
  *	    cpufreq_driver->get() here which might re-enable interrupts...
  *
  * This function is only executed for the boot CPU.  The other CPUs have not
@@ -1535,37 +1592,43 @@ static int cpufreq_bp_suspend(void)
  */
 static void cpufreq_bp_resume(void)
 {
-	int ret = 0;
-
-	int cpu = smp_processor_id();
 	struct cpufreq_policy *policy;
+	int cpu;
 
-	pr_debug("resuming cpu %u\n", cpu);
+	if (!cpufreq_driver)
+		return;
 
-	/* If there's no policy for the boot CPU, we have nothing to do. */
-	policy = cpufreq_cpu_get(cpu);
-	if (!policy)
+	if (!has_target())
 		return;
 
-	if (cpufreq_driver->resume) {
-		ret = cpufreq_driver->resume(policy);
-		if (ret) {
-			printk(KERN_ERR "cpufreq: resume failed in ->resume "
-					"step on CPU %u\n", policy->cpu);
-			goto fail;
-		}
-	}
+	pr_debug("%s: Resuming Governors\n", __func__);
 
-	schedule_work(&policy->update);
+	cpufreq_suspended = false;
 
-fail:
-	cpufreq_cpu_put(policy);
-}
+	for_each_possible_cpu(cpu) {
+		if (!cpu_online(cpu))
+			continue;
 
-static struct syscore_ops cpufreq_syscore_ops = {
-	.suspend	= cpufreq_bp_suspend,
-	.resume		= cpufreq_bp_resume,
-};
+		policy = cpufreq_cpu_get(cpu);
+
+		if (__cpufreq_governor(policy, CPUFREQ_GOV_START)
+		    || __cpufreq_governor(policy, CPUFREQ_GOV_LIMITS))
+			pr_err("%s: Failed to start governor for policy: %p\n",
+				__func__, policy);
+		else if (cpufreq_driver->resume
+		    && cpufreq_driver->resume(policy))
+			pr_err("%s: Failed to resume driver: %p\n", __func__,
+				policy);
+
+		/*
+		 * schedule call cpufreq_update_policy() for boot CPU, i.e. last
+		 * policy in list. It will verify that the current freq is in
+		 * sync with what we believe it to be.
+		 */
+		if (cpu == 0)
+			schedule_work(&policy->update);
+	}
+}
 
 /**
  *	cpufreq_get_current_driver - return current driver's name
@@ -1742,12 +1805,9 @@ static int __cpufreq_governor(struct cpufreq_policy *policy,
 	struct cpufreq_governor *gov = NULL;
 #endif
 
-	/*
-	 * Governor might not be initiated here if ACPI _PPC changed
-	 * notification happened, so check it.
-	 */
-	if (!policy->governor)
-		return -EINVAL;
+	/* Don't start any governor operations if we are entering suspend */
+	if (cpufreq_suspended)
+		return 0;
 
 	if (policy->governor->max_transition_latency &&
 	    policy->cpuinfo.transition_latency >
@@ -2215,7 +2275,6 @@ static int __init cpufreq_core_init(void)
 
 	cpufreq_global_kobject = kobject_create();
 	BUG_ON(!cpufreq_global_kobject);
-	register_syscore_ops(&cpufreq_syscore_ops);
 
 	return 0;
 }
diff --git a/drivers/cpufreq/cpufreq_stats.c b/drivers/cpufreq/cpufreq_stats.c
index 9e85f2a..b10e43a 100644
--- a/drivers/cpufreq/cpufreq_stats.c
+++ b/drivers/cpufreq/cpufreq_stats.c
@@ -14,6 +14,9 @@
 #include <linux/module.h>
 #include <linux/slab.h>
 #include <asm/cputime.h>
+#ifdef CONFIG_BL_SWITCHER
+#include <asm/bL_switcher.h>
+#endif
 
 static spinlock_t cpufreq_stats_lock;
 
@@ -342,6 +345,27 @@ static int cpufreq_stat_notifier_trans(struct notifier_block *nb,
 	return 0;
 }
 
+static int cpufreq_stats_create_table_cpu(unsigned int cpu)
+{
+	struct cpufreq_policy *policy;
+	struct cpufreq_frequency_table *table;
+	int ret = -ENODEV;
+
+	policy = cpufreq_cpu_get(cpu);
+	if (!policy)
+		return -ENODEV;
+
+	table = cpufreq_frequency_get_table(cpu);
+	if (!table)
+		goto out;
+
+	ret = cpufreq_stats_create_table(policy, table);
+
+out:
+	cpufreq_cpu_put(policy);
+	return ret;
+}
+
 static int __cpuinit cpufreq_stat_cpu_callback(struct notifier_block *nfb,
 					       unsigned long action,
 					       void *hcpu)
@@ -355,6 +379,10 @@ static int __cpuinit cpufreq_stat_cpu_callback(struct notifier_block *nfb,
 	case CPU_DEAD:
 		cpufreq_stats_free_table(cpu);
 		break;
+	case CPU_DOWN_FAILED:
+	case CPU_DOWN_FAILED_FROZEN:
+		cpufreq_stats_create_table_cpu(cpu);
+		break;
 	}
 	return NOTIFY_OK;
 }
@@ -373,7 +401,7 @@ static struct notifier_block notifier_trans_block = {
 	.notifier_call = cpufreq_stat_notifier_trans
 };
 
-static int __init cpufreq_stats_init(void)
+static int cpufreq_stats_setup(void)
 {
 	int ret;
 	unsigned int cpu;
@@ -399,7 +427,8 @@ static int __init cpufreq_stats_init(void)
 
 	return 0;
 }
-static void __exit cpufreq_stats_exit(void)
+
+static void cpufreq_stats_cleanup(void)
 {
 	unsigned int cpu;
 
@@ -414,6 +443,54 @@ static void __exit cpufreq_stats_exit(void)
 	}
 }
 
+#ifdef CONFIG_BL_SWITCHER
+static int cpufreq_stats_switcher_notifier(struct notifier_block *nfb,
+					unsigned long action, void *_arg)
+{
+	switch (action) {
+	case BL_NOTIFY_PRE_ENABLE:
+	case BL_NOTIFY_PRE_DISABLE:
+		cpufreq_stats_cleanup();
+		break;
+
+	case BL_NOTIFY_POST_ENABLE:
+	case BL_NOTIFY_POST_DISABLE:
+		cpufreq_stats_setup();
+		break;
+
+	default:
+		return NOTIFY_DONE;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block switcher_notifier = {
+	.notifier_call = cpufreq_stats_switcher_notifier,
+};
+#endif
+
+static int __init cpufreq_stats_init(void)
+{
+	int ret;
+	spin_lock_init(&cpufreq_stats_lock);
+
+	ret = cpufreq_stats_setup();
+#ifdef CONFIG_BL_SWITCHER
+	if (!ret)
+		bL_switcher_register_notifier(&switcher_notifier);
+#endif
+	return ret;
+}
+
+static void __exit cpufreq_stats_exit(void)
+{
+#ifdef CONFIG_BL_SWITCHER
+	bL_switcher_unregister_notifier(&switcher_notifier);
+#endif
+	cpufreq_stats_cleanup();
+}
+
 MODULE_AUTHOR("Zou Nan hai <nanhai.zou@intel.com>");
 MODULE_DESCRIPTION("'cpufreq_stats' - A driver to export cpufreq stats "
 				"through sysfs filesystem");
diff --git a/drivers/cpufreq/highbank-cpufreq.c b/drivers/cpufreq/highbank-cpufreq.c
index 8c4771d..0b2c228 100644
--- a/drivers/cpufreq/highbank-cpufreq.c
+++ b/drivers/cpufreq/highbank-cpufreq.c
@@ -19,7 +19,7 @@
 #include <linux/cpu.h>
 #include <linux/err.h>
 #include <linux/of.h>
-#include <linux/mailbox.h>
+#include <linux/pl320-ipc.h>
 #include <linux/platform_device.h>
 
 #define HB_CPUFREQ_CHANGE_NOTE	0x80000001
@@ -29,8 +29,26 @@
 static int hb_voltage_change(unsigned int freq)
 {
 	u32 msg[HB_CPUFREQ_IPC_LEN] = {HB_CPUFREQ_CHANGE_NOTE, freq / 1000000};
+	struct ipc_client cl;
+	int ret = -ETIMEDOUT;
+	void *chan;
 
-	return pl320_ipc_transmit(msg);
+	cl.rxcb = NULL;
+	cl.txcb = NULL;
+	cl.tx_block = true;
+	cl.tx_tout = 1000; /* 1 sec */
+	cl.cntlr_data = NULL;
+	cl.knows_txdone = false;
+	cl.chan_name = "pl320:A9_to_M3";
+
+	chan = ipc_request_channel(&cl);
+
+	if (ipc_send_message(chan, (void *)msg))
+		ret = msg[1]; /* PL320 updates buffer with FIFO after ACK */
+
+	ipc_free_channel(chan);
+
+	return ret;
 }
 
 static int hb_cpufreq_clk_notify(struct notifier_block *nb,
diff --git a/drivers/cpufreq/mb86s70-cpufreq.c b/drivers/cpufreq/mb86s70-cpufreq.c
new file mode 100644
index 0000000..834a111
--- /dev/null
+++ b/drivers/cpufreq/mb86s70-cpufreq.c
@@ -0,0 +1,213 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+#include <linux/module.h>
+#include <linux/types.h>
+#include <linux/kernel.h>
+#include <linux/cpufreq.h>
+#include <linux/sched.h>
+#include <linux/cpu.h>
+#include <linux/smp.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+
+#define CLSTR_CA15	0
+#define CLSTR_CA7	1
+
+static unsigned int cur_freq[4] = {1600000, 1600000, 800000, 800000};
+
+static struct cpufreq_frequency_table freq_table_ca15[] = {
+	{ 0, 1200000 },
+	{ 1, 1600000 },
+	{ 2, CPUFREQ_TABLE_END },
+};
+
+static struct cpufreq_frequency_table freq_table_ca7[] = {
+	{ 0, 400000 },
+	{ 1, 800000 },
+	{ 2, CPUFREQ_TABLE_END },
+};
+
+extern void mhu_cluster_rate(int cluster, unsigned long *rate, int get);
+
+static int s70_set_target(struct cpufreq_policy *policy,
+		unsigned int target_freq, unsigned int relation)
+{
+	int cluster, ca15 = 0, cpu = policy->cpu;
+	struct cpufreq_frequency_table *table;
+	struct cpufreq_freqs freqs;
+	cpumask_t cpus_allowed;
+	unsigned long rate;
+	unsigned int idx;
+
+	if (cpu < 2) {
+		table = &freq_table_ca15[0];
+		cluster = CLSTR_CA15;
+	} else {
+		table = &freq_table_ca7[0];
+		cluster = CLSTR_CA7;
+	}
+
+	freqs.old = cur_freq[cpu];
+	cpufreq_frequency_table_target(policy, table, target_freq,
+					relation, &idx);
+	freqs.new = table[idx].frequency;
+
+	/* Return if nothing to do */
+	if (freqs.old == freqs.new)
+		return 0;
+
+	/* Save the CPUs allowed mask */
+	cpus_allowed = current->cpus_allowed;
+
+	/* Make sure we are running on CA7 if we want to change CA15 */
+	if (cluster == CLSTR_CA15) {
+		cpumask_t cpus_ca7;
+		cpumask_clear(&cpus_ca7);
+		cpumask_set_cpu(2, &cpus_ca7);
+		cpumask_set_cpu(3, &cpus_ca7);
+
+		/* Migrate to CA7 */
+		set_cpus_allowed_ptr(current, &cpus_ca7);
+		BUG_ON(smp_processor_id() < 2); /* Still on CA15?! */
+
+		/* Put CA15 down */
+		if (cpu_online(1)) {
+			ca15 |= (1 << 1);
+			cpu_down(1);
+		}
+		if (cpu_online(0)) {
+			ca15 |= (1 << 0);
+			cpu_down(0);
+		}
+	}
+
+	cpufreq_notify_transition(policy, &freqs, CPUFREQ_PRECHANGE);
+
+	rate = freqs.new * 1000;
+	mhu_cluster_rate(cluster, &rate, 0);
+
+	pr_err("%s:%d Cluster=%d Rate %lu/%u\n",
+		__func__, __LINE__, cluster, rate, freqs.new * 1000);
+
+	if (cluster == CLSTR_CA15) {
+		cur_freq[0] = rate;
+		cur_freq[1] = rate;
+	} else {
+		cur_freq[2] = rate;
+		cur_freq[3] = rate;
+	}
+
+	cpufreq_notify_transition(policy, &freqs, CPUFREQ_POSTCHANGE);
+
+	/* Put CA15 down */
+	if (ca15 & (1 << 0))
+		cpu_up(0);
+	if (ca15 & (1 << 1))
+		cpu_up(1);
+
+	/* Restore the CPUs allowed mask */
+	set_cpus_allowed_ptr(current, &cpus_allowed);
+
+	return 0;
+}
+
+static unsigned int s70_get(unsigned int cpu)
+{
+	unsigned long rate;
+	int cluster;
+
+	if (cpu < 2)
+		cluster = CLSTR_CA15;
+	else
+		cluster = CLSTR_CA7;
+
+	mhu_cluster_rate(cluster, &rate, 1);
+
+	pr_err("%s:%d cpu=%d freq=%lu\n",
+			__func__, __LINE__, cpu, rate);
+	return rate / 1000;
+}
+
+static int s70_cpufreq_init(struct cpufreq_policy *policy)
+{
+	struct cpufreq_frequency_table *table;
+	unsigned int cpu = policy->cpu;
+
+	pr_err("%s:%d cpu=%d\n", __func__, __LINE__, cpu);
+
+	if (cpu < 2) {
+		table = &freq_table_ca15[0];
+		cpumask_set_cpu(0, policy->cpus);
+		cpumask_set_cpu(1, policy->cpus);
+	} else {
+		table = &freq_table_ca7[0];
+		cpumask_set_cpu(2, policy->cpus);
+		cpumask_set_cpu(3, policy->cpus);
+	}
+
+	cpufreq_frequency_table_cpuinfo(policy, table);
+	cpufreq_frequency_table_get_attr(table, policy->cpu);
+	policy->cpuinfo.transition_latency = 500000;
+
+	return 0;
+}
+
+static int s70_verify_speed(struct cpufreq_policy *policy)
+{
+	struct cpufreq_frequency_table *table;
+	unsigned int cpu = policy->cpu;
+
+	if (cpu < 2)
+		table = &freq_table_ca15[0];
+	else
+		table = &freq_table_ca7[0];
+
+	return cpufreq_frequency_table_verify(policy, table);
+}
+
+static struct freq_attr *s70_cpufreq_attr[] = {
+	&cpufreq_freq_attr_scaling_available_freqs,
+	NULL,
+};
+
+static struct cpufreq_driver s70_driver = {
+	.verify		= s70_verify_speed,
+	.target		= s70_set_target,
+	.get		= s70_get,
+	.init		= s70_cpufreq_init,
+	.name		= "mb86s70-cpufreq",
+	.attr		= s70_cpufreq_attr,
+};
+
+static int s70_cpufreq_probe(struct platform_device *pdev)
+{
+	return cpufreq_register_driver(&s70_driver);
+}
+
+static int s70_cpufreq_remove(struct platform_device *pdev)
+{
+	cpufreq_unregister_driver(&s70_driver);
+
+	return 0;
+}
+
+static struct platform_driver s70_cpufreq_driver = {
+	.driver = {
+		.name = "s70-cpufreq",
+		.owner = THIS_MODULE,
+	},
+	.probe = s70_cpufreq_probe,
+	.remove = s70_cpufreq_remove,
+};
+module_platform_driver(s70_cpufreq_driver);
+
+MODULE_AUTHOR("Jassi");
+MODULE_DESCRIPTION("cpufreq driver for Fujitsu S70");
+MODULE_LICENSE("GPL");
diff --git a/drivers/cpuidle/Kconfig b/drivers/cpuidle/Kconfig
index 8272a08..47849c5 100644
--- a/drivers/cpuidle/Kconfig
+++ b/drivers/cpuidle/Kconfig
@@ -2,6 +2,12 @@
 config CPU_IDLE
 	bool "CPU idle PM support"
 	default y if ACPI || PPC_PSERIES
+
+config ARM_BIG_LITTLE_CPUIDLE
+	bool "Support for ARM big.LITTLE processors"
+	depends on ARCH_VEXPRESS_TC2_PM || ARCH_MB86S70
+	select ARM_CPU_SUSPEND
+	select CPU_IDLE_MULTIPLE_DRIVERS
 	help
 	  CPU idle is a generic framework for supporting software-controlled
 	  idle processor power management.  It includes modular cross-platform
@@ -31,6 +37,15 @@ config CPU_IDLE_GOV_MENU
 config ARCH_NEEDS_CPU_IDLE_COUPLED
 	def_bool n
 
+config OF_IDLE_STATES
+        bool "Idle states DT support"
+	depends on ARM || ARM64
+	default n
+	help
+	 Allows the CPU idle framework to initialize CPU idle drivers
+	 state data by using DT provided nodes compliant with idle states
+	 device tree bindings.
+
 if CPU_IDLE
 
 config CPU_IDLE_CALXEDA
diff --git a/drivers/cpuidle/Makefile b/drivers/cpuidle/Makefile
index 8767a7b..e3d3f94 100644
--- a/drivers/cpuidle/Makefile
+++ b/drivers/cpuidle/Makefile
@@ -4,7 +4,13 @@
 
 obj-y += cpuidle.o driver.o governor.o sysfs.o governors/
 obj-$(CONFIG_ARCH_NEEDS_CPU_IDLE_COUPLED) += coupled.o
+obj-$(CONFIG_BIG_LITTLE) += arm_big_little.o
+obj-$(CONFIG_OF_IDLE_STATES)		  += of_idle_states.o
 
 obj-$(CONFIG_CPU_IDLE_CALXEDA) += cpuidle-calxeda.o
 obj-$(CONFIG_ARCH_KIRKWOOD) += cpuidle-kirkwood.o
 obj-$(CONFIG_CPU_IDLE_ZYNQ) += cpuidle-zynq.o
+
+###############################################################################
+# ARM64 drivers
+obj-$(CONFIG_ARM64_CPUIDLE)		+= cpuidle-arm64.o
diff --git a/drivers/cpuidle/arm_big_little.c b/drivers/cpuidle/arm_big_little.c
new file mode 100644
index 0000000..c58fa73
--- /dev/null
+++ b/drivers/cpuidle/arm_big_little.c
@@ -0,0 +1,189 @@
+/*
+ * big.LITTLE CPU idle driver.
+ *
+ * Copyright (C) 2012 ARM Ltd.
+ * Author: Lorenzo Pieralisi <lorenzo.pieralisi@arm.com>
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/arm-cci.h>
+#include <linux/bitmap.h>
+#include <linux/cpuidle.h>
+#include <linux/cpu_pm.h>
+#include <linux/clockchips.h>
+#include <linux/debugfs.h>
+#include <linux/hrtimer.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/tick.h>
+#include <linux/vexpress.h>
+#include <asm/mcpm.h>
+#include <asm/cpuidle.h>
+#include <asm/cputype.h>
+#include <asm/idmap.h>
+#include <asm/proc-fns.h>
+#include <asm/suspend.h>
+#include <linux/of.h>
+
+static int bl_cpuidle_simple_enter(struct cpuidle_device *dev,
+		struct cpuidle_driver *drv, int index)
+{
+	ktime_t time_start, time_end;
+	s64 diff;
+
+	time_start = ktime_get();
+
+	cpu_do_idle();
+
+	time_end = ktime_get();
+
+	local_irq_enable();
+
+	diff = ktime_to_us(ktime_sub(time_end, time_start));
+	if (diff > INT_MAX)
+		diff = INT_MAX;
+
+	dev->last_residency = (int) diff;
+
+	return index;
+}
+
+static int bl_enter_powerdown(struct cpuidle_device *dev,
+				struct cpuidle_driver *drv, int idx);
+
+static struct cpuidle_state bl_cpuidle_set[] __initdata = {
+	[0] = {
+		.enter                  = bl_cpuidle_simple_enter,
+		.exit_latency           = 1,
+		.target_residency       = 1,
+		.power_usage		= UINT_MAX,
+		.flags                  = CPUIDLE_FLAG_TIME_VALID,
+		.name                   = "WFI",
+		.desc                   = "ARM WFI",
+	},
+/*
+	[1] = {
+		.enter			= bl_enter_powerdown,
+		.exit_latency		= 5000,
+		.target_residency	= 10000,
+		.flags			= CPUIDLE_FLAG_TIME_VALID |
+					  CPUIDLE_FLAG_TIMER_STOP,
+		.name			= "C1",
+		.desc			= "ARM power down",
+	},
+*/
+};
+
+struct cpuidle_driver bl_idle_driver = {
+	.name = "bl_idle",
+	.owner = THIS_MODULE,
+	.safe_state_index = 0
+};
+
+static DEFINE_PER_CPU(struct cpuidle_device, bl_idle_dev);
+
+static int notrace bl_powerdown_finisher(unsigned long arg)
+{
+	unsigned int mpidr = read_cpuid_mpidr();
+	unsigned int cluster = MPIDR_AFFINITY_LEVEL(mpidr, 1);
+	unsigned int cpu = MPIDR_AFFINITY_LEVEL(mpidr, 0);
+
+	mcpm_set_entry_vector(cpu, cluster, cpu_resume);
+	mcpm_cpu_suspend(0);  /* 0 should be replaced with better value here */
+	return 1;
+}
+
+/*
+ * bl_enter_powerdown - Programs CPU to enter the specified state
+ * @dev: cpuidle device
+ * @drv: The target state to be programmed
+ * @idx: state index
+ *
+ * Called from the CPUidle framework to program the device to the
+ * specified target state selected by the governor.
+ */
+static int __maybe_unused bl_enter_powerdown(struct cpuidle_device *dev,
+				struct cpuidle_driver *drv, int idx)
+{
+	struct timespec ts_preidle, ts_postidle, ts_idle;
+	int ret;
+
+	/* Used to keep track of the total time in idle */
+	getnstimeofday(&ts_preidle);
+
+	BUG_ON(!irqs_disabled());
+
+	cpu_pm_enter();
+
+	clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_ENTER, &dev->cpu);
+
+	ret = cpu_suspend((unsigned long) dev, bl_powerdown_finisher);
+	if (ret)
+		BUG();
+
+	mcpm_cpu_powered_up();
+
+	clockevents_notify(CLOCK_EVT_NOTIFY_BROADCAST_EXIT, &dev->cpu);
+
+	cpu_pm_exit();
+
+	getnstimeofday(&ts_postidle);
+	local_irq_enable();
+	ts_idle = timespec_sub(ts_postidle, ts_preidle);
+
+	dev->last_residency = ts_idle.tv_nsec / NSEC_PER_USEC +
+					ts_idle.tv_sec * USEC_PER_SEC;
+	return idx;
+}
+
+/*
+ * bl_idle_init
+ *
+ * Registers the bl specific cpuidle driver with the cpuidle
+ * framework with the valid set of states.
+ */
+int __init bl_idle_init(void)
+{
+	struct cpuidle_device *dev;
+	int i, cpu_id;
+	struct cpuidle_driver *drv = &bl_idle_driver;
+
+	/*
+	 * Initialize the driver just for a compliant set of machines
+	 */
+ 	if (!of_machine_is_compatible("arm,vexpress,v2p-ca15_a7") &&
+				!of_machine_is_compatible("fujitsu,mb86s70") &&
+				!of_machine_is_compatible("fujitsu,mb86s73"))
+		return -ENODEV;
+
+	drv->state_count = (sizeof(bl_cpuidle_set) /
+				       sizeof(struct cpuidle_state));
+
+	for (i = 0; i < drv->state_count; i++) {
+		memcpy(&drv->states[i], &bl_cpuidle_set[i],
+				sizeof(struct cpuidle_state));
+	}
+
+	cpuidle_register_driver(drv);
+
+	for_each_cpu(cpu_id, cpu_online_mask) {
+		pr_info("CPUidle for CPU%d registered\n", cpu_id);
+		dev = &per_cpu(bl_idle_dev, cpu_id);
+		dev->cpu = cpu_id;
+
+		dev->state_count = drv->state_count;
+
+		if (cpuidle_register_device(dev)) {
+			printk(KERN_ERR "%s: Cpuidle register device failed\n",
+			       __func__);
+			return -EIO;
+		}
+	}
+
+	return 0;
+}
+
+device_initcall(bl_idle_init);
diff --git a/drivers/cpuidle/governors/menu.c b/drivers/cpuidle/governors/menu.c
index bc580b6..ed24b1e 100644
--- a/drivers/cpuidle/governors/menu.c
+++ b/drivers/cpuidle/governors/menu.c
@@ -125,7 +125,7 @@ struct menu_device {
 
 #define LOAD_INT(x) ((x) >> FSHIFT)
 #define LOAD_FRAC(x) LOAD_INT(((x) & (FIXED_1-1)) * 100)
-
+#if 0
 static int get_loadavg(void)
 {
 	unsigned long this = this_cpu_load();
@@ -133,7 +133,7 @@ static int get_loadavg(void)
 
 	return LOAD_INT(this) * 10 + LOAD_FRAC(this) / 10;
 }
-
+#endif
 static inline int which_bucket(unsigned int duration)
 {
 	int bucket = 0;
@@ -173,7 +173,12 @@ static inline int performance_multiplier(void)
 
 	/* for higher loadavg, we are more reluctant */
 
-	mult += 2 * get_loadavg();
+	/*
+	 * this doesn't work as intended - it is almost always 0, but can
+	 * sometimes, depending on workload, spike very high into the hundreds
+	 * even when the average cpu load is under 10%.
+	 */
+	/* mult += 2 * get_loadavg(); */
 
 	/* for IO wait tasks (per cpu!) we add 5x each */
 	mult += 10 * nr_iowait_cpu(smp_processor_id());
diff --git a/drivers/dma/Kconfig b/drivers/dma/Kconfig
index a7687b2..2bff73e 100644
--- a/drivers/dma/Kconfig
+++ b/drivers/dma/Kconfig
@@ -323,6 +323,26 @@ config KEYSTONE_UDMA
 	  Simple user mode DMA client with zero copy semantics.  Say N unless
 	  you really know what you're doing.
 
+config MB8AC0300_HDMAC
+	bool "MB8AC0300 HDMAC support"
+	depends on ARCH_MB8AC0300 || ARCH_MB86S70
+	default y
+	help
+	  Say yes here to support the Fujitsu MB8AC0300
+	  HDMAC device
+	  It provides DMA service to some of the on-chip peripherals
+	  on the SoC
+
+config MB8AC0300_XDMAC
+	bool "MB8AC0300 XDMAC support"
+	depends on ARCH_MB8AC0300 || ARCH_MB86S70
+	default y
+	help
+	  Say yes here to support the Fujitsu MB8AC0300
+	  XDMAC device
+	  It provides DMA service to some of the on-chip peripherals
+	  on the SoC
+
 config DMA_ENGINE
 	bool
 
diff --git a/drivers/dma/Makefile b/drivers/dma/Makefile
index 5517dcd..f75f355 100644
--- a/drivers/dma/Makefile
+++ b/drivers/dma/Makefile
@@ -8,7 +8,7 @@ obj-$(CONFIG_DMA_OF) += of-dma.o
 
 obj-$(CONFIG_NET_DMA) += iovlock.o
 obj-$(CONFIG_INTEL_MID_DMAC) += intel_mid_dma.o
-obj-$(CONFIG_DMATEST) += dmatest.o
+obj-$(CONFIG_DMATEST) += dmatest.o memset.o
 obj-$(CONFIG_INTEL_IOATDMA) += ioat/
 obj-$(CONFIG_INTEL_IOP_ADMA) += iop-adma.o
 obj-$(CONFIG_FSL_DMA) += fsldma.o
@@ -38,6 +38,8 @@ obj-$(CONFIG_DMA_SA11X0) += sa11x0-dma.o
 obj-$(CONFIG_MMP_TDMA) += mmp_tdma.o
 obj-$(CONFIG_DMA_OMAP) += omap-dma.o
 obj-$(CONFIG_MMP_PDMA) += mmp_pdma.o
+obj-$(CONFIG_MB8AC0300_HDMAC) += mb8ac0300-hdmac.o
+obj-$(CONFIG_MB8AC0300_XDMAC) += mb8ac0300-xdmac.o
 obj-$(CONFIG_KEYSTONE_DMA) += keystone-pktdma.o
 obj-$(CONFIG_KEYSTONE_UDMA) += keystone-udma.o
 obj-$(CONFIG_TI_CPPI41) += cppi41.o
diff --git a/drivers/dma/mb86s7x-dma-test.c b/drivers/dma/mb86s7x-dma-test.c
new file mode 100644
index 0000000..2469713
--- /dev/null
+++ b/drivers/dma/mb86s7x-dma-test.c
@@ -0,0 +1,249 @@
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
+#include <linux/freezer.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+#include <linux/kdev_t.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
+#include <linux/iommu.h>
+#include <linux/sizes.h>
+#include <linux/cache.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/interrupt.h>
+#include <asm/dma-iommu.h>
+
+
+unsigned int channel = 0;
+module_param(channel, uint, S_IRUGO);
+MODULE_PARM_DESC(channel, "number of channels to use (default: 0)");
+
+
+struct mb86s7x_dmatest {
+	size_t test_length;
+	struct completion completion;
+	struct device *dev;
+	struct dma_chan *chan;
+	struct iommu_domain *domain;
+	unsigned int *scr_cpu, *dst_cpu;
+	dma_addr_t src_phy_addr, dst_phy_addr;
+};
+
+struct class class = {
+	.name = "testclass",
+	.owner = THIS_MODULE,
+};
+
+struct mb86s7x_dmatest _priv;
+struct mb86s7x_dmatest *priv = &_priv;
+
+
+static void dmatest_callback(void *dma_async_param)
+{
+	struct mb86s7x_dmatest *priv = dma_async_param;
+
+	complete(&priv->completion);
+}
+
+void show_data(struct device *dev, unsigned int *src, unsigned int *dst)
+{
+	int n = 0;
+
+	for (n = 0; n < 24; n += 4)
+		dev_err(dev, "SRC->%02x %02x %02x %02x\n",
+		     src[n], src[n + 1], src[n + 2], src[n + 3]);
+
+	for (n = 0; n < 24; n += 4)
+		dev_err(dev, "DST->%02x %02x %02x %02x\n",
+		      dst[n], dst[n + 1], dst[n + 2], dst[n + 3]);
+}
+
+
+static int dmatest_do_copy_blocking(struct mb86s7x_dmatest *priv,
+				dma_addr_t src, dma_addr_t dest, size_t len)
+{
+	struct dma_async_tx_descriptor *desc;
+	dma_cookie_t cookie;
+	int ret = 0;
+	enum dma_status status;
+
+	if (!is_dma_copy_aligned(priv->chan->device, src, dest, len))
+		dev_err(priv->dev, "not aligned\n");
+
+	/* prepare the transfer */
+	desc = priv->chan->device->device_prep_dma_memcpy(
+		priv->chan, dest, src, len, DMA_PREP_INTERRUPT);
+	if (!desc) {
+		ret = -EINVAL;
+		goto bail;
+	}
+
+	desc->callback = dmatest_callback;
+	desc->callback_param = priv;
+
+	/* set the transfer going */
+
+	init_completion(&priv->completion);
+	cookie = dmaengine_submit(desc);
+	dma_async_issue_pending(priv->chan);
+
+	/* wait for it to complete */
+
+	ret = wait_for_completion_timeout(&priv->completion,
+							msecs_to_jiffies(500));
+	if (ret <= 0) {
+		dev_err(priv->dev, "timeout waiting completion\n");
+		ret = -ETIME;
+		goto bail;
+	} else
+		ret = 0;
+
+	status = dma_async_is_tx_complete(priv->chan, cookie, NULL, NULL);
+
+	switch (dma_async_is_tx_complete(priv->chan, cookie, NULL, NULL)) {
+	case DMA_SUCCESS:
+		break;
+	case DMA_IN_PROGRESS:
+		dev_err(priv->dev, "Status: DMA_IN_PROGRESS\n");
+		break;
+	case DMA_PAUSED:
+		dev_err(priv->dev, "Status: DMA_PAUSED\n");
+		break;
+	case DMA_ERROR:
+		dev_err(priv->dev, "Status: DMA_ERROR\n");
+		break;
+	}
+
+	dev_err(priv->dev, "mb86s70-dmatest: completed\n");
+
+bail:
+	dev_err(priv->dev, "%s: %llX -> %llX len %llX returned %d\n", __func__,
+		(u64)src, (u64)dest, (u64)len, ret);
+
+	return ret;
+}
+
+
+static bool dma_filter(struct dma_chan *chan, void *param)
+{
+	printk(KERN_ERR "%s dma chan %d\n", __func__, chan->chan_id);
+	if (chan->chan_id == channel)
+		return true;
+	else
+		return false;
+}
+
+
+static int __init dmatest_init(void)
+{
+	dma_cap_mask_t mask;
+	static u64 dma_mask = 0xffffffffffffffff;
+	int ret = 0;
+
+	/* init */
+	priv->test_length = SZ_4K;
+
+	/* fake up a device */
+
+	ret = class_register(&class);
+	if (ret)
+		return -EINVAL;
+
+	priv->dev = device_create(&class, NULL,
+	    MKDEV(99, 99), &priv, "mb86s70-dmatest");
+	if (IS_ERR(priv->dev)) {
+		pr_err("mb86s70-dmatest: failed to create device: %d\n",
+		    (int)priv->dev);
+		ret = (int)priv->dev;
+		goto bail_unreg_class;
+	}
+
+	priv->dev->dma_mask = &dma_mask;
+	priv->dev->coherent_dma_mask = dma_mask;
+
+	/* get a DMA channel that can do a memcpy */
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_MEMCPY, mask);
+
+	priv->chan = dma_request_channel(mask, dma_filter, NULL);
+	if (!priv->chan)
+		goto bail_dev;
+
+	dev_err(priv->dev, "Using DMA device %s\n",
+				dev_name(priv->chan->device->dev));
+
+	/* allocate coherent source and dest buffers */
+	priv->scr_cpu = (unsigned int *)dma_alloc_coherent(priv->chan->device->dev,
+	    priv->test_length, &priv->src_phy_addr, GFP_KERNEL);
+	if (!priv->scr_cpu) {
+		dev_err(priv->dev, "Unable to allocate test buffer\n");
+		ret = -ENOMEM;
+		goto bail_chan;
+	}
+	dev_err(priv->dev, "phys1 -> 0x%lx\n", (unsigned long) priv->src_phy_addr);
+
+	priv->dst_cpu = (unsigned int *)dma_alloc_coherent(priv->chan->device->dev,
+	    priv->test_length, &priv->dst_phy_addr, GFP_KERNEL);
+	if (!priv->dst_cpu) {
+		dev_err(priv->dev, "Unable to allocate test buffer\n");
+		ret = -ENOMEM;
+		goto bail_a1;
+	}
+	dev_err(priv->dev, "phys2 -> 0x%lx\n", (unsigned long) priv->dst_phy_addr);
+
+	/* set phys1 contents VIR_SRC_LOW */
+	/* set phys2 contents VIR_DST_LOW */
+	memset(priv->scr_cpu, 0xcc, priv->test_length);
+	memset(priv->dst_cpu, 0x88, priv->test_length);
+
+	dmatest_do_copy_blocking(priv, priv->src_phy_addr,
+	    priv->dst_phy_addr, priv->test_length);
+
+	show_data(priv->dev, priv->scr_cpu, priv->dst_cpu);
+
+	if (memcmp(priv->scr_cpu, priv->dst_cpu, priv->test_length) == 0)
+		dev_err(priv->dev, "src 0x%llx dst 0x%llx memcmp ok!\n\n\n",
+		    (u64)priv->src_phy_addr, (u64)priv->dst_phy_addr);
+	else
+		dev_err(priv->dev, "src 0x%llx dst 0x%llx memcmp fail!\n\n\n",
+		    (u64)priv->src_phy_addr, (u64)priv->dst_phy_addr);
+
+		return ret;
+
+bail_a1:
+	dma_free_coherent(priv->dev, priv->test_length, priv->scr_cpu, priv->src_phy_addr);
+
+bail_chan:
+	dma_release_channel(priv->chan);
+
+bail_dev:
+	device_destroy(NULL, MKDEV(99, 99));
+bail_unreg_class:
+	class_unregister(&class);
+	return -1;
+}
+/* when compiled-in wait for drivers to load first */
+
+module_init(dmatest_init);
+static void __exit dmatest_exit(void)
+{
+	dev_err(priv->dev, "%s\n", __func__);
+	dma_free_coherent(priv->dev, priv->test_length, priv->dst_cpu, priv->dst_phy_addr);
+	dma_free_coherent(priv->dev, priv->test_length, priv->scr_cpu, priv->src_phy_addr);
+	dma_release_channel(priv->chan);
+	device_destroy(&class, MKDEV(99, 99));
+	class_unregister(&class);
+}
+module_exit(dmatest_exit);
+
+MODULE_AUTHOR("Andy Green <andy.green@linaro.org>");
+MODULE_LICENSE("GPL v2");
+
diff --git a/drivers/dma/mb8ac0300-hdmac.c b/drivers/dma/mb8ac0300-hdmac.c
new file mode 100644
index 0000000..a56f734
--- /dev/null
+++ b/drivers/dma/mb8ac0300-hdmac.c
@@ -0,0 +1,943 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/hdmac.c
+ *
+ *  FASP HDMAC registration and IRQ dispatching
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+/* #define DEBUG */
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/errno.h>
+#include <linux/completion.h>
+#include <linux/sched.h>
+#include <linux/clk.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/platform_data/dma-mb8ac0300-hdmac.h>
+#include <linux/slab.h>
+#include <linux/io.h>
+#include <linux/pm_runtime.h>
+
+#include <asm/system.h>
+
+#define DRIVER_NAME		"mb8ac0300-hdmac"
+#define DRIVER_DESC		"MB8AC0300 HDMA Controller Driver"
+
+#define STOP_TIMEOUT 2 /* 2s */
+
+
+static struct mb8ac0300_hdmac_chip hdmac_chips[HDMAC_MAX_CHIPS];
+
+static inline unsigned long hdmac_readl(struct mb8ac0300_hdmac_chip *chip,
+								       int reg)
+{
+	return readl(chip->base + reg);
+}
+
+static inline void hdmac_writel(struct mb8ac0300_hdmac_chip *chip,
+						int reg, unsigned long val)
+{
+	writel(val, chip->base + reg);
+}
+
+int hdmac_get_channel(u32 channel, u8 autostart_flg)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags;
+	struct device *dev;
+	int ret;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	if ((autostart_flg != HDMAC_AUTOSTART_ENABLE) &&
+	    (autostart_flg != HDMAC_AUTOSTART_DISABLE)) {
+		pr_err("hdmac%d:autostart_flg err... please set 0 or 1.\n"
+			, channel);
+		return -EINVAL;
+	}
+
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels) {
+		pr_err("%s(): CHAN_INDEX[%d] is larger than channels numbers[%d]\n"
+			, __func__, CHAN_INDEX(channel), chip->channels);
+		return -EINVAL;
+	}
+
+	ret = pm_runtime_get_sync(chip->dev);
+	if (ret < 0) {
+		dev_err(chip->dev, "get_sync failed with err %d\n", ret);
+		return -EINVAL;
+	}
+
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+	dev_dbg(dev, "%s(): CHIP_INDEX(channel) is %d\n",
+		__func__, CHIP_INDEX(channel));
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state != HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		/*
+		 * should be nonfatal to caller since he can choose another
+		 * channel
+		 */
+		dev_dbg(dev, "hdmac chip %d, channel %d is busy\n"
+			, CHIP_INDEX(channel), CHAN_INDEX(channel));
+		return -EBUSY;
+	}
+
+	chan->state = HDMAC_IDLE;
+	chan->dmaca = 0;
+	chan->dmacb = 0;
+	chan->autostart_flg = autostart_flg;
+
+	INIT_LIST_HEAD(&chan->list);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_get_channel);
+
+int hdmac_enqueue(u32 channel, struct hdmac_req *hdmac_req)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags;
+	unsigned long software_trigger;
+	unsigned long input_select;
+	unsigned long beat_type;
+	unsigned long mode_select;
+	int ret;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(dev, "hdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+
+	software_trigger = hdmac_req->dmaca & HDMACA_ST_MASK;
+	input_select = hdmac_req->dmaca & HDMACA_IS_MASK;
+	beat_type = hdmac_req->dmaca & HDMACA_BT_MASK;
+	mode_select = hdmac_req->dmacb & HDMACB_MS_MASK;
+
+	/* parameters check */
+	if ((mode_select == HDMACB_MS_DEMAND) && ((beat_type !=
+		HDMACA_BT_NORMAL) && (beat_type != HDMACA_BT_SINGLE))) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return -EINVAL;
+	}
+
+	if ((mode_select == HDMACB_MS_DEMAND) && (software_trigger
+			== HDMACA_ST)) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return -EINVAL;
+	}
+
+	if ((software_trigger == HDMACA_ST) && (input_select !=
+			HDMACA_IS_SW)) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return -EINVAL;
+	}
+
+	list_add_tail(&hdmac_req->node, &chan->list);
+
+	if ((chan->state == HDMAC_IDLE) && (chan->autostart_flg ==
+			HDMAC_AUTOSTART_ENABLE)) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		ret = hdmac_start(channel);
+		if (ret)
+			return ret;
+	} else {
+		spin_unlock_irqrestore(&chan->lock, flags);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_enqueue);
+
+static inline void __hdmac_start(struct hdmac_chan *chan)
+{
+	struct hdmac_req *req;
+
+	/* get DMA request from list */
+	req = list_entry(chan->list.next, struct hdmac_req, node);
+
+	/* config and start the channel going */
+	hdmac_writel(chan->chip, DMACSA(chan->number), req->req_data.src);
+	hdmac_writel(chan->chip, DMACDA(chan->number), req->req_data.dst);
+
+	chan->dmacb = req->dmacb;
+	hdmac_writel(chan->chip, DMACB(chan->number), chan->dmacb);
+
+	chan->dmaca = req->dmaca;
+	hdmac_writel(chan->chip, DMACA(chan->number)
+		, (chan->dmaca | HDMACA_EB));
+}
+
+int hdmac_start(u32 channel)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(dev, "hdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+
+	if (chan->state != HDMAC_IDLE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_dbg(dev, "hdmac%d:channel is running\n", channel);
+		return 0;
+	}
+
+	if (chan->list.next == &chan->list) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_dbg(dev, "hdmac%d:no request to start\n", channel);
+		return 0;
+	}
+
+	__hdmac_start(chan);
+
+	chan->state = HDMAC_RUNNING;
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_start);
+
+int hdmac_getposition(u32 channel, u32 *src, u32 *dst)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(dev, "hdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+
+	if (src)
+		*src = hdmac_readl(chan->chip, DMACSA(chan->number));
+	if (dst)
+		*dst = hdmac_readl(chan->chip, DMACDA(chan->number));
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_getposition);
+
+int hdmac_stop(u32 channel)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags, timeout, tmp, state;
+	struct hdmac_req *req;
+	int ret;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	/*
+	 * stop DMA transfer
+	 * If the DMA transfer mode is BURST mode, even if we set the disable
+	 * bit the change of the register during a DMA transfer is reflected
+	 * after the DMA transfer is completed.
+	 */
+
+	spin_lock_irqsave(&chan->lock, flags);
+	if (chan->state == HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(dev, "hdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	} else if (chan->state == HDMAC_RUNNING) {
+		tmp = hdmac_readl(chan->chip, DMACA(chan->number));
+		if (tmp & HDMACA_EB) {
+			tmp &= ~((unsigned long)HDMACA_EB);
+			hdmac_writel(chan->chip, DMACA(chan->number), tmp);
+		}
+		chan->state = HDMAC_STOP_REQUEST;
+		tmp = hdmac_readl(chan->chip, DMACA(chan->number));
+		if ((tmp & (HDMACA_BC_MASK | HDMACA_TC_MASK)) ==
+			(chan->dmaca & (HDMACA_BC_MASK | HDMACA_TC_MASK))) {
+			/*
+			 * Before the dma transfer started by hardware trigger,
+			 * even if we clear the EB bit, the interrupt will
+			 * never raise. We check the BC & TC bits, if the
+			 * transfer has not begun yet, we do the same pross
+			 * as irq handler.
+			 * We change the state to HDMAC_IDLE at the end of
+			 * process.
+			 * Then hdmac_flush & hdmac_start will not work
+			 * which is in the callback handler.
+			 */
+			/* Stop Status */
+			tmp = hdmac_readl(chan->chip, DMACB(chan->number));
+			state = tmp & HDMACB_SS_MASK;
+			tmp &= ~((unsigned long)HDMACB_SS_MASK);
+			hdmac_writel(chan->chip, DMACB(chan->number), tmp);
+
+			req = list_entry(chan->list.next, struct hdmac_req,
+					node);
+			list_del(chan->list.next);
+			if (req->req_data.callback_fn != NULL) {
+				spin_unlock(&chan->lock);
+				req->req_data.callback_fn(
+					OUTER_CHAN(chan->chip->chip_index
+						, chan->number),
+					req->req_data.irq_data, state);
+				spin_lock(&chan->lock);
+			}
+			chan->state = HDMAC_IDLE;
+			spin_unlock_irqrestore(&chan->lock, flags);
+			return 0;
+		}
+	} else if (chan->state == HDMAC_STOP_REQUEST_NOWAIT) {
+		chan->state = HDMAC_STOP_REQUEST;
+	} else { /* state is IDLE */
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_dbg(dev, "hdmac%d:is already stopped\n", channel);
+		return 0;
+	}
+
+	INIT_COMPLETION(chan->stop_completion);
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	timeout = HZ * STOP_TIMEOUT;
+	ret = wait_for_completion_timeout(&chan->stop_completion, timeout);
+	if (!ret) { /* -ETIME */
+		dev_err(dev, "hdmac%d:err stop time out", channel);
+		return -ETIME;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_stop);
+
+int hdmac_stop_nowait(u32 channel)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long tmp, flags, state;
+	struct hdmac_req *req;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+
+	dev = chip->dev;
+	chan = &chip->chans[CHAN_INDEX(channel)];
+
+	/* stop DMA transfer */
+
+	/*
+	 * If the DMA transfer mode is BURST mode, even if we set the disable
+	 * bit the change of the register during a DMA transfer is reflected
+	 * after the DMA transfer is completed.
+	 */
+	spin_lock_irqsave(&chan->lock, flags);
+	if (chan->state == HDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_err(dev, "hdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	} else if (chan->state == HDMAC_RUNNING) {
+		/* clear EB */
+		tmp = hdmac_readl(chan->chip, DMACA(chan->number));
+		if (tmp & HDMACA_EB) {
+			tmp &= ~((unsigned long)HDMACA_EB);
+			hdmac_writel(chan->chip, DMACA(chan->number), tmp);
+		}
+		chan->state = HDMAC_STOP_REQUEST_NOWAIT;
+		tmp = hdmac_readl(chan->chip, DMACA(chan->number));
+		if ((tmp & (HDMACA_BC_MASK | HDMACA_TC_MASK)) ==
+			(chan->dmaca & (HDMACA_BC_MASK | HDMACA_TC_MASK))) {
+			/*
+			 * Before the dma transfer started by hardware trigger,
+			 * even if we clear the EB bit, the interrupt will
+			 * never raise. We check the BC & TC bits, if the
+			 * transfer has not begun yet, we do the same pross
+			 * as irq handler.
+			 * We change the state to HDMAC_IDLE before call
+			 * the callback handler.
+			 * Then hdmac_flush & hdmac_start can work which is in
+			 * the callback handler.
+			 */
+			chan->state = HDMAC_IDLE;
+			/* Stop Status */
+			tmp = hdmac_readl(chan->chip, DMACB(chan->number));
+			state = tmp & HDMACB_SS_MASK;
+			tmp &= ~((unsigned long)HDMACB_SS_MASK);
+			hdmac_writel(chan->chip, DMACB(chan->number), tmp);
+
+			req = list_entry(chan->list.next, struct hdmac_req,
+					 node);
+			list_del(chan->list.next);
+
+			if (req->req_data.callback_fn != NULL) {
+				spin_unlock(&chan->lock);
+				req->req_data.callback_fn(
+					OUTER_CHAN(chan->chip->chip_index
+						, chan->number),
+					req->req_data.irq_data, state);
+				spin_lock(&chan->lock);
+			}
+
+			spin_unlock_irqrestore(&chan->lock, flags);
+			return 0;
+		}
+
+	} /* else state is IDLE & STOP_REQUEST */
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+	/* do not wait for channel stop */
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_stop_nowait);
+
+static inline void __hdmac_flush(struct hdmac_chan *chan)
+{
+	struct hdmac_req *req;
+
+	while (chan->list.next != &chan->list) {
+		req = list_entry(chan->list.next, struct hdmac_req, node);
+		list_del(chan->list.next);
+		if (!req->req_data.callback_fn)
+			continue;
+		req->req_data.callback_fn(
+			OUTER_CHAN(chan->chip->chip_index, chan->number),
+				req->req_data.irq_data, HDMAC_REQ_DATA_FLUSHED);
+	}
+}
+
+int hdmac_flush(u32 channel)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	unsigned long flags;
+	int ret = 0;
+	struct device *dev;
+
+	if (channel >= HDMAC_MAX_CHANNELS) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == HDMAC_PREPARE) {
+		dev_err(dev, "hdmac%d: Flush free channel.\n", channel);
+		ret = -EINVAL;
+		goto bail;
+	}
+	if (chan->state != HDMAC_IDLE) {
+		dev_err(dev, "hdmac%d: Flush busy channel.\n", channel);
+		ret = -EBUSY;
+		goto bail;
+	}
+
+	__hdmac_flush(chan);
+
+bail:
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(hdmac_flush);
+
+int hdmac_free(u32 channel)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	int ret = 0;
+	unsigned long flags;
+	struct device *dev;
+
+	if (channel > HDMAC_MAX_CHANNELS - 1) {
+		pr_err("hdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chip = &hdmac_chips[CHIP_INDEX(channel)];
+	if (CHAN_INDEX(channel) > chip->channels)
+		return -EINVAL;
+	chan = &chip->chans[CHAN_INDEX(channel)];
+	dev = chip->dev;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == HDMAC_PREPARE) {
+		dev_err(dev, "hdmac%d:freeing free channel.\n", channel);
+		ret = -EINVAL;
+		goto bail;
+	}
+	if (chan->state == HDMAC_RUNNING) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		dev_dbg(dev, "hdmac%d:stop the channel\n", channel);
+		ret = hdmac_stop(channel);
+		if (ret)
+			return ret;
+		spin_lock_irqsave(&chan->lock, flags);
+	}
+
+	if (chan->state == HDMAC_IDLE) {
+		__hdmac_flush(chan);
+		chan->state = HDMAC_PREPARE;
+		pm_runtime_put(chip->dev);
+		goto bail;
+	} else {
+		dev_err(dev, "hdmac%d:free channel fault.\n", channel);
+		ret = -EINVAL;
+	}
+bail:
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return ret;
+}
+EXPORT_SYMBOL(hdmac_free);
+
+static irqreturn_t hdmac_irq(int irq, void *devpw)
+{
+	struct hdmac_chan *chan = (struct hdmac_chan *)devpw;
+	struct hdmac_req *req;
+	unsigned long state, tmp, pre_state = 0;
+	struct device *dev = chan->chip->dev;
+
+	/* Stop Status */
+	tmp = hdmac_readl(chan->chip, DMACB(chan->number));
+	state = tmp & HDMACB_SS_MASK;
+	tmp &= ~((unsigned long)HDMACB_SS_MASK);
+	hdmac_writel(chan->chip, DMACB(chan->number), tmp);
+
+	spin_lock(&chan->lock);
+	if (chan->state == HDMAC_RUNNING) {
+		chan->state = HDMAC_IDLE;
+	} else if (chan->state == HDMAC_STOP_REQUEST_NOWAIT) {
+		chan->state = HDMAC_IDLE;
+		pre_state = HDMAC_STOP_REQUEST_NOWAIT;
+	}
+
+	if (chan->state == HDMAC_PREPARE) {
+		dev_err(dev, "hdmac%d: IRQ in invalid state %d\n",
+			OUTER_CHAN(chan->chip->chip_index, chan->number),
+								chan->state);
+		goto bail;
+	}
+
+	if (chan->list.next == &chan->list) {
+		dev_err(dev, "hdmac%d: No DMA request\n",
+			OUTER_CHAN(chan->chip->chip_index, chan->number));
+		goto bail;
+	}
+
+	req = list_entry(chan->list.next, struct hdmac_req, node);
+
+	list_del(chan->list.next);
+
+	if (req->req_data.callback_fn != NULL) {
+		spin_unlock(&chan->lock);
+		req->req_data.callback_fn(
+			OUTER_CHAN(chan->chip->chip_index, chan->number),
+						req->req_data.irq_data, state);
+		spin_lock(&chan->lock);
+	}
+
+	if (chan->state == HDMAC_STOP_REQUEST)
+		complete_all(&chan->stop_completion);
+
+	if ((chan->state == HDMAC_IDLE) && (chan->list.next !=
+		&chan->list) && (pre_state != HDMAC_STOP_REQUEST_NOWAIT)) {
+		__hdmac_start(chan);
+		chan->state = HDMAC_RUNNING;
+	} else if (chan->state == HDMAC_STOP_REQUEST) {
+		chan->state = HDMAC_IDLE;
+	}
+	/*
+	 * else...
+	 * HDMAC_RUNNING (hdmac started in callback_fn)
+	 * HDMAC_IDLE & list is NULL
+	 * HDMAC_IDLE & pre_state == HDMAC_STOP_REQUEST_NOWAIT
+	 */
+bail:
+	spin_unlock(&chan->lock);
+
+	return IRQ_HANDLED;
+}
+
+/* return 0 means successful */
+static int mb8x_clk_control(struct device *dev, bool on)
+{
+	int ret, i;
+	struct clk *clk;
+
+	dev_dbg(dev, "%s() is started (on:%d).\n", __func__, on);
+
+	if (!on)
+		goto clock_off;
+
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		ret = clk_prepare_enable(clk);
+		if (ret) {
+			dev_err(dev, "failed to enable clock[%d]\n", i);
+			goto clock_off;
+		}
+		dev_info(dev, "enabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+
+clock_off:
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		clk_disable_unprepare(clk);
+		dev_info(dev, "disabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return on;
+}
+static int mb8ac0300_hdmac_probe(struct platform_device *pdev)
+{
+	struct mb8ac0300_hdmac_chip *chip;
+	struct hdmac_chan *chan;
+	long channel = 0;
+	int ret;
+	int chip_index;
+	int channels;
+	int rotation = 0;
+	const int *p;
+	struct resource *res;
+	struct device *dev = &pdev->dev;
+
+	if (!pdev->dev.of_node) {
+		dev_err(dev, "Requires DT node\n");
+		return -ENODEV;
+	}
+
+	p = of_get_property(pdev->dev.of_node, "chip", NULL);
+	if (!p) {
+		dev_err(dev, "Requires DT \"chip\" property\n");
+		return -ENODEV;
+	}
+	chip_index = be32_to_cpu(*p);
+
+	p = of_get_property(pdev->dev.of_node, "rotation", NULL);
+	if (!p) {
+		dev_err(dev, "Requires DT \"rotation\" property\n");
+		return -ENODEV;
+	}
+	rotation = be32_to_cpu(*p);
+
+	p = of_get_property(pdev->dev.of_node, "channels", NULL);
+	if (!p) {
+		dev_err(dev, "Requires DT \"channels\" property\n");
+		return -ENODEV;
+	}
+	channels = be32_to_cpu(*p);
+
+	if (chip_index >= HDMAC_MAX_CHIPS) {
+		dev_err(dev, "Invalid chip num %d.\n", chip_index);
+		return -ENODEV;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -EINVAL;
+
+	chip = &hdmac_chips[chip_index];
+	chip->chip_index = chip_index;
+	chip->channels = channels;
+	chip->dev = dev;
+	dev_set_drvdata(&pdev->dev, chip);
+	/* set base address */
+	chip->base = ioremap(res->start, res->end - res->start + 1);
+	if (!chip->base) {
+		dev_err(dev, "unable to map mem region\n");
+		return -EBUSY;
+	}
+
+	/* init chan information */
+	for (channel = 0; channel < chip->channels; channel++) {
+		chan = &chip->chans[channel];
+		memset(chan, 0, sizeof(struct hdmac_chan));
+		chan->chip   = chip;
+		chan->number = channel;
+		chan->state  = HDMAC_PREPARE;
+
+		spin_lock_init(&chan->lock);
+		init_completion(&chan->stop_completion);
+
+		res = platform_get_resource(pdev, IORESOURCE_IRQ, channel);
+		if (!res) {
+			ret = -EINVAL;
+			goto bail2;
+		}
+		chan->irq = res->start;
+
+		sprintf(chan->name, "hdmac-%d-%d", chip_index, (int)channel);
+		ret = request_irq(chan->irq, hdmac_irq, IRQF_TRIGGER_HIGH,
+						chan->name, (void *)chan);
+		if (ret) {
+			dev_err(dev, "ret = %d", ret);
+			dev_err(dev, "hdmac%d:cannot get IRQ %d\n",
+				chan->number, chan->irq);
+			goto bail2;
+		}
+		disable_irq(chan->irq);
+	}
+
+	pm_runtime_enable(&pdev->dev);
+
+	dev_info(dev, "HDMAC %d started: channels: %d rotation: %d\n"
+		, chip_index, channels, rotation);
+	return 0;
+
+bail2:
+	while (--channel >= 0) {
+		chan = &chip->chans[channel];
+		if (chan->irq)
+			free_irq(chan->irq, hdmac_irq);
+	}
+
+	iounmap(chip->base);
+
+	return ret;
+}
+
+static int mb8ac0300_hdmac_remove(struct platform_device *pdev)
+{
+	struct mb8ac0300_hdmac_chip *chip = dev_get_drvdata(&pdev->dev);
+	int channel;
+	struct resource *res;
+
+	for (channel = 0; channel < chip->channels; channel++) {
+		res = platform_get_resource(pdev, IORESOURCE_IRQ, channel);
+		if (res)
+			free_irq(res->start, &chip->chans[channel]);
+	}
+
+	iounmap(chip->base);
+
+	pm_runtime_disable(&pdev->dev);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+#ifdef CONFIG_PM_RUNTIME
+static int mb8x_hdmac_runtime_suspend(struct device *dev)
+{
+	struct mb8ac0300_hdmac_chip *chip = dev_get_drvdata(dev);
+	int channel;
+	struct hdmac_chan *chan;
+
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	/* disable irq */
+	for (channel = 0; channel < chip->channels; channel++) {
+		chan = &chip->chans[channel];
+		disable_irq(chan->irq);
+	}
+
+	mb8x_clk_control(dev, false);
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+
+static int mb8x_hdmac_runtime_resume(struct device *dev)
+{
+	struct mb8ac0300_hdmac_chip *chip = dev_get_drvdata(dev);
+	int channel, ret;
+	struct hdmac_chan *chan;
+	const int *p;
+	int rotation = 0;
+
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	ret = mb8x_clk_control(dev, true);
+	if (ret) {
+		dev_err(dev, "%s failed to enable clock source\n", __func__);
+		return -EINVAL;
+	}
+
+	/* re-configure and re-start dma controller for cold resume */
+	p = of_get_property(dev->of_node, "rotation", NULL);
+	if (!p) {
+		dev_err(dev, "Requires DT \"rotation\" property\n");
+		return -ENODEV;
+	}
+	rotation = be32_to_cpu(*p);
+
+	if (rotation)
+		hdmac_writel(chip, DMACR, (HDMACR_DE | HDMACR_PR));
+	else
+		hdmac_writel(chip, DMACR, HDMACR_DE);
+
+	/* enable irq */
+	for (channel = 0; channel < chip->channels; channel++) {
+		chan = &chip->chans[channel];
+		enable_irq(chan->irq);
+	}
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+#endif
+
+static int mb8x_hdmac_suspend(struct device *dev)
+{
+	dev_dbg(dev, "%s() is executed.\n", __func__);
+
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	return mb8x_hdmac_runtime_suspend(dev);
+}
+
+static int mb8x_hdmac_resume(struct device *dev)
+{
+	dev_dbg(dev, "%s() is executed.\n", __func__);
+
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	return mb8x_hdmac_runtime_resume(dev);
+}
+
+static const struct dev_pm_ops mb8x_hdmac_ops = {
+	.suspend = mb8x_hdmac_suspend,
+	.resume = mb8x_hdmac_resume,
+	SET_RUNTIME_PM_OPS(mb8x_hdmac_runtime_suspend
+		, mb8x_hdmac_runtime_resume, NULL)
+};
+
+#define _mb8x_hdmac_ops (&mb8x_hdmac_ops)
+#else
+#define _mb8x_hdmac_ops NULL
+#endif /* CONFIG_PM */
+
+#ifdef CONFIG_OF
+static const struct of_device_id mb8ac0300_hdmac_dt_ids[] = {
+	{ .compatible = "fujitsu,mb8ac0300-hdmac" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, mb8ac0300_hdmac_dt_ids);
+#else
+#define mb8ac0300_hdmac_dt_ids NULL
+#endif
+
+static struct platform_driver mb8ac0300_hdmac_driver = {
+	.probe     = mb8ac0300_hdmac_probe,
+	.remove    = mb8ac0300_hdmac_remove,
+	.driver    = {
+		.name  = DRIVER_NAME,
+		.owner = THIS_MODULE,
+		.pm = _mb8x_hdmac_ops,
+		.of_match_table = mb8ac0300_hdmac_dt_ids,
+	},
+};
+
+static int __init mb8ac0300_hdmac_driver_init(void)
+{
+	return platform_driver_register(&mb8ac0300_hdmac_driver);
+}
+subsys_initcall(mb8ac0300_hdmac_driver_init);
+
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_ALIAS("platform:" DRIVER_NAME);
+MODULE_LICENSE("GPL");
+
diff --git a/drivers/dma/mb8ac0300-xdmac.c b/drivers/dma/mb8ac0300-xdmac.c
new file mode 100644
index 0000000..ae0b274
--- /dev/null
+++ b/drivers/dma/mb8ac0300-xdmac.c
@@ -0,0 +1,748 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/xdmac.c
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/errno.h>
+#include <linux/completion.h>
+#include <linux/sched.h>
+
+#include <linux/clk.h>
+
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/platform_data/dma-mb8ac0300-xdmac.h>
+
+#include <asm/system.h>
+#include <linux/io.h>
+
+#define DRIVER_NAME		"mb8ac0300-xdmac"
+#define DRIVER_DESC		"MB8AC0300 XDMA Controller Driver"
+
+#define STOP_TIMEOUT 2 /* 2s */
+
+/* XDMAC Virtual Base address */
+static void *xdmac_base;
+static struct xdmac_chan xdmac_chans[MAX_XDMAC_CHANNELS];
+
+static int channels_in_use;
+
+#ifdef CONFIG_PM
+static u32 xdmac_xdacs;
+#endif
+static struct clk *xdmac_clk;
+
+static inline unsigned long xdmac_readl(int reg)
+{
+	return readl(xdmac_base + reg);
+}
+
+static inline void xdmac_writel(unsigned long val, int reg)
+{
+	writel(val, xdmac_base + reg);
+}
+
+/* xdmac_get_channel - get control of a xdmac channel
+ * @channel: channel number of xdmac.
+ * @autostart_flg: set channel auto start data transfer.
+ *
+ * return -EINVAL    Invalid channel number,
+ *        -BUSY      Channel used by other one,
+ *        0          Otherwise.
+ */
+
+int xdmac_get_channel(u32 channel, u8 autostart_flg)
+{
+	struct xdmac_chan *chan;
+	unsigned long flags;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	if (autostart_flg && (autostart_flg != 1)) {
+		pr_err("xdmac:autostart_flg err.please set 0 or 1.\n");
+		return -EINVAL;
+	}
+
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state != XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_info("xdmac%d:channel is busy\n", channel);
+		return -EBUSY;
+	}
+
+	chan->state = XDMAC_IDLE;
+	chan->autostart_flg = autostart_flg;
+	chan->xdsac = 0;
+	chan->xddac = 0;
+	chan->xddcc = 0;
+	chan->xddes = 0;
+	chan->xddpc = 0;
+
+	INIT_LIST_HEAD(&chan->list);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_get_channel);
+
+/* xdmac_enqueue - place the given request onto the queue
+ *                 of operations for the channel.
+ * @channel: channel number of xdmac.
+ * @xdmac_req: request data of xdmac.
+ *
+ * retrun -EINVAL    Invalid channel number,
+ *                   Invalid state,
+ *                   Invalid parameters,
+ *        -ENOMEM    Out of memory when alloc request,
+ *        0          Otherwise.
+ */
+
+int xdmac_enqueue(u32 channel, struct xdmac_req *xdmac_req)
+{
+	struct xdmac_chan *chan;
+	unsigned long flags;
+	int ret;
+	unsigned long software_enable;
+	unsigned long transfer_factor;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+
+	software_enable = xdmac_req->xddes & XDDES_SE_MASK;
+	transfer_factor = xdmac_req->xddes & XDDES_TF_MASK;
+	/* parameters check */
+	if ((software_enable == XDDES_SE) &&
+			(transfer_factor != XDDES_TF_SOFTWEARE)) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return -EINVAL;
+	}
+
+	list_add_tail(&xdmac_req->node, &chan->list);
+
+	if ((chan->state == XDMAC_IDLE) && (chan->autostart_flg) == 1) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		ret = xdmac_start(channel);
+		if (ret)
+			return ret;
+	} else
+		spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_enqueue);
+
+static inline void __xdmac_start(struct xdmac_chan *chan)
+{
+	struct xdmac_req *req;
+
+	/* get DMA request from list */
+	req = list_entry(chan->list.next, struct xdmac_req, node);
+
+	chan->xdsac = req->xdsac;
+	chan->xddac = req->xddac;
+	chan->xddcc = req->xddcc;
+	chan->xddes = req->xddes;
+	chan->xddpc = req->xddpc;
+
+	/* config and start the channel going */
+	xdmac_writel((req->req_data.size - 1), XDTBC(chan->number));
+	xdmac_writel(req->req_data.src, XDSSA(chan->number));
+	xdmac_writel(req->req_data.dst, XDDSA(chan->number));
+	xdmac_writel(chan->xdsac, XDSAC(chan->number));
+	xdmac_writel(chan->xddac, XDDAC(chan->number));
+	xdmac_writel(chan->xddcc, XDDCC(chan->number));
+	xdmac_writel(chan->xddpc, XDDPC(chan->number));
+	xdmac_writel(chan->xddes | XDDES_CE, XDDES(chan->number));
+}
+
+/* xdmac_start - start a xdma channel going
+ * @channel: channel number of xdmac.
+ *
+ * retrun -EINVAL    Invalid channel number,
+ *                   Invalid state,
+ *        0          Otherwise.
+ */
+
+int xdmac_start(u32 channel)
+{
+	unsigned long flags;
+	struct xdmac_chan *chan;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	} else if (chan->state != XDMAC_IDLE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_debug("xdmac%d:channel is running\n", channel);
+		return 0;
+	}
+
+	if (chan->list.next == &chan->list) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_debug("xdmac%d:no request to start\n", channel);
+		return 0;
+	}
+
+	__xdmac_start(chan);
+
+	chan->state = XDMAC_RUNNING;
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_start);
+
+/* xdmac_getposition -Returns the current transfer points for
+ *                    the dma source and destination
+ * @channel: channel number of xdmac.
+ * @src:     source address buffer pointer.
+ * @dst:     destination address buffer pointer.
+ *
+ * retrun -EINVAL    Invalid channel number
+ *        0          Otherwise.
+ */
+
+int xdmac_getposition(u32 channel, u32 *src, u32 *dst)
+{
+	struct xdmac_chan *chan;
+	unsigned long flags;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdma%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+
+	if (src != NULL)
+		*src = xdmac_readl(XDSSA(channel));
+	if (dst != NULL)
+		*dst = xdmac_readl(XDDSA(channel));
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_getposition);
+
+
+/* xdmac_stop(synchronous) -Stop a dma channel transfer.
+ * @channel: channel number of xdmac.
+ *
+ * retrun -EINVAL    Invalid channel number,
+ *        0          Otherwise.
+ */
+
+int xdmac_stop(u32 channel)
+{
+	struct xdmac_chan *chan;
+	unsigned long tmp, flags, timeout;
+	int ret;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	/* stop DMA transfer */
+
+	/* If the DMA transfer mode is BURST mode, even if we set the disable
+	 * bit the change of the register during a DMA transfer is reflected
+	 * after the DMA transfer is completed.
+	 */
+	spin_lock_irqsave(&chan->lock, flags);
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+	if (chan->state == XDMAC_RUNNING) {
+		/* clear CE */
+		tmp = xdmac_readl(XDDES(channel));
+		if (tmp & XDDES_CE) {
+			tmp &= ~((unsigned long)XDDES_CE);
+			xdmac_writel(tmp, XDDES(channel));
+		}
+		chan->state = XDMAC_STOP_REQUEST;
+	} else if (chan->state == XDMAC_STOP_REQUEST_NOWAIT) {
+		chan->state = XDMAC_STOP_REQUEST;
+	} else { /* state is IDLE */
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return 0;
+	}
+
+	INIT_COMPLETION(chan->stop_completion);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	timeout = HZ * STOP_TIMEOUT;
+	ret = wait_for_completion_timeout(&chan->stop_completion, timeout);
+	if (!ret) { /* -ETIME */
+		pr_err("xdmac%d:err stop time out", channel);
+		return -ETIME;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_stop);
+
+int xdmac_stop_nowait(u32 channel)
+{
+	struct xdmac_chan *chan;
+	unsigned long tmp, flags;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	/* stop DMA transfer */
+
+	/* If the DMA transfer mode is BURST mode, even if we set the disable
+	 * bit the change of the register during a DMA transfer is reflected
+	 * after the DMA transfer is completed.
+	 */
+	spin_lock_irqsave(&chan->lock, flags);
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:channel is free.\n", channel);
+		return -EINVAL;
+	}
+	if (chan->state == XDMAC_RUNNING) {
+		/* clear CE */
+		tmp = xdmac_readl(XDDES(channel));
+		if (tmp & XDDES_CE) {
+			tmp &= ~((unsigned long)XDDES_CE);
+			xdmac_writel(tmp, XDDES(channel));
+		}
+		chan->state = XDMAC_STOP_REQUEST_NOWAIT;
+
+	} /* else state is IDLE & STOP_REQUEST */
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+	/* do not wait for channel stop */
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_stop_nowait);
+
+static void __xdmac_flush(struct xdmac_chan *chan)
+{
+	struct xdmac_req *req;
+
+	while (chan->list.next != &chan->list) {
+		req = list_entry(chan->list.next, struct xdmac_req, node);
+		list_del(chan->list.next);
+		if (req->req_data.callback_fn != NULL)
+			req->req_data.callback_fn(chan->number,
+			       req->req_data.irq_data, XDMAC_REQ_DATA_FLUSHED);
+	}
+}
+
+int xdmac_flush(u32 channel)
+{
+	struct xdmac_chan *chan;
+	unsigned long flags;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_debug("xdmac%d:Flush free channel.\n", channel);
+		return -EINVAL;
+	} else if (chan->state != XDMAC_IDLE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:Flush busy channel.\n", channel);
+		return -EBUSY;
+	}
+	__xdmac_flush(chan);
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_flush);
+
+/* xdmac_free - free the xdma channel
+ *              (will also abort any outstanding operations)
+ * @channel: channel number of xdmac.
+ *
+ * retrun -EINVAL    Invalid channel number,
+ *        0          Otherwise.
+ */
+int xdmac_free(u32 channel)
+{
+	struct xdmac_chan *chan;
+	int ret;
+	unsigned long flags;
+
+	if (channel > MAX_XDMAC_CHANNELS - 1) {
+		pr_err("xdmac:Invalid channel num %d.\n", channel);
+		return -EINVAL;
+	}
+	chan = &xdmac_chans[channel];
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_debug("xdmac%d:freeing free channel.\n", channel);
+		return -EINVAL;
+	} else if (chan->state == XDMAC_RUNNING) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_debug("xdmac%d:stop the channel.\n", channel);
+		ret = xdmac_stop(channel);
+		if (ret)
+			return ret;
+		spin_lock_irqsave(&chan->lock, flags);
+	}
+
+	if (chan->state == XDMAC_IDLE) {
+		__xdmac_flush(chan);
+		chan->state = XDMAC_PREPARE;
+		spin_unlock_irqrestore(&chan->lock, flags);
+	} else {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		pr_err("xdmac%d:free channel fault.\n", channel);
+		return -EINVAL;
+	}
+	return 0;
+}
+EXPORT_SYMBOL(xdmac_free);
+
+/* irq handler */
+
+static irqreturn_t xdmac_irq(int irq, void *devpw)
+{
+	struct xdmac_chan *chan = (struct xdmac_chan *)devpw;
+	struct xdmac_req *req;
+	unsigned long state, tmp, pre_state = 0;
+
+	/* get stop state */
+	tmp = xdmac_readl(XDDSD(chan->number));
+	state = tmp & XDDSD_IS_MASK;
+	tmp &= ~((unsigned long)XDDSD_IS_MASK);
+	xdmac_writel(tmp, XDDSD(chan->number));
+
+	spin_lock(&chan->lock);
+	if (chan->state == XDMAC_RUNNING)
+		chan->state = XDMAC_IDLE;
+	else if (chan->state == XDMAC_STOP_REQUEST_NOWAIT) {
+		chan->state = XDMAC_IDLE;
+		pre_state = XDMAC_STOP_REQUEST_NOWAIT;
+	}
+
+	/* modify the channel state */
+	if (chan->state == XDMAC_PREPARE) {
+		spin_unlock(&chan->lock);
+		pr_err("xdmac%d: IRQ in invalid state %d\n",
+				chan->number, chan->state);
+		return IRQ_HANDLED;
+	}
+
+	if (chan->list.next == &chan->list) {
+		spin_unlock(&chan->lock);
+		pr_err("xdmac%d: No DMA request\n", chan->number);
+		return IRQ_HANDLED;
+	}
+
+	req = list_entry(chan->list.next, struct xdmac_req, node);
+
+	list_del(chan->list.next);
+
+	if (req->req_data.callback_fn != NULL) {
+		spin_unlock(&chan->lock);
+		req->req_data.callback_fn(chan->number,
+					req->req_data.irq_data, state);
+		spin_lock(&chan->lock);
+	}
+
+	if (chan->state == XDMAC_STOP_REQUEST)
+		complete_all(&chan->stop_completion);
+
+	if ((chan->state == XDMAC_IDLE) && (chan->list.next != &chan->list)
+				&& (pre_state != XDMAC_STOP_REQUEST_NOWAIT)) {
+		__xdmac_start(chan);
+		chan->state = XDMAC_RUNNING;
+	} else if (chan->state == XDMAC_STOP_REQUEST)
+		chan->state = XDMAC_IDLE;
+
+		/*
+		 * else...
+		 * XDMAC_RUNNING (xdmac started in callback_fn)
+		 * XDMAC_IDLE & list is NULL
+		 * XDMAC_IDLE & pre_state == XDMAC_STOP_REQUEST_NOWAIT
+		 */
+
+
+	spin_unlock(&chan->lock);
+
+	return IRQ_HANDLED;
+}
+
+#ifdef CONFIG_PM
+
+static int xdmac_suspend_noirq(struct device *dev)
+{
+	struct xdmac_chan *chan;
+	u32 channel;
+	unsigned long tmp;
+
+	for (channel = 0; channel < MAX_XDMAC_CHANNELS; channel++) {
+		chan = &xdmac_chans[channel];
+		if (chan->state == XDMAC_RUNNING) {
+			dev_err(dev, "%s Channel%d is running\n",
+							__func__, channel);
+			return -EBUSY;
+		}
+	}
+
+	tmp = xdmac_readl(XDACS);
+	xdmac_xdacs = tmp;
+	tmp &= ~((unsigned long)XDACS_XE);
+	xdmac_writel(tmp, XDACS);
+
+	clk_disable_unprepare(xdmac_clk);
+
+	return 0;
+}
+
+static int xdmac_resume_noirq(struct device *dev)
+{
+	int ret;
+
+	/* clk_enable */
+	ret = clk_prepare_enable(xdmac_clk);
+	if (ret) {
+		dev_err(dev, "%s failed to enable clock source\n", __func__);
+		return ret;
+	}
+	xdmac_writel(xdmac_xdacs, XDACS);
+
+	return 0;
+}
+#else
+	#define xdmac_suspend_noirq NULL
+	#define xdmac_resume_noirq NULL
+#endif
+
+static const struct dev_pm_ops mb8ac0300_xdmac_dev_pm_ops = {
+	.suspend_noirq = xdmac_suspend_noirq,
+	.resume_noirq = xdmac_resume_noirq,
+};
+
+static int mb8ac0300_xdmac_probe(struct platform_device *pdev)
+{
+	struct xdmac_chan *chan;
+	unsigned long channel;
+	unsigned long tmp = 0;
+	int ret;
+	int low_power;
+	int rotation = 0;
+	const int *p;
+	struct resource *res;
+
+	if (!pdev->dev.of_node) {
+		dev_err(&pdev->dev, "Requires DT node\n");
+		return -ENODEV;
+	}
+
+	p = of_get_property(pdev->dev.of_node, "low_power", NULL);
+	if (!p) {
+		dev_err(&pdev->dev, "Requires DT \"low_power\" property\n");
+		return -ENODEV;
+	}
+	low_power = be32_to_cpu(*p);
+	p = of_get_property(pdev->dev.of_node, "rotation", NULL);
+	if (!p) {
+		dev_err(&pdev->dev, "Requires DT \"rotation\" property\n");
+		return -ENODEV;
+	}
+	rotation = be32_to_cpu(*p);
+	p = of_get_property(pdev->dev.of_node, "channels", NULL);
+	if (!p) {
+		dev_err(&pdev->dev, "Requires DT \"channels\" property\n");
+		return -ENODEV;
+	}
+	channels_in_use = be32_to_cpu(*p);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -EINVAL;
+
+	/* set base address */
+	xdmac_base = ioremap(res->start, res->end - res->start + 1);
+	if (!xdmac_base) {
+		dev_err(&pdev->dev, "unable to map mem region\n");
+		return -EBUSY;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	if (!res) {
+		ret = -EINVAL;
+		goto bail1;
+	}
+
+	for (channel = 0; channel < channels_in_use; channel++) {
+		chan = &xdmac_chans[channel];
+		memset(chan, 0, sizeof(struct xdmac_chan));
+
+		chan->number = channel;
+
+		chan->irq    = res->start + channel;
+		chan->state  = XDMAC_PREPARE;
+		spin_lock_init(&chan->lock);
+		init_completion(&chan->stop_completion);
+
+		ret = request_irq(chan->irq, xdmac_irq, IRQF_DISABLED,
+					"xdmac", (void *)chan);
+		if (ret) {
+			dev_err(&pdev->dev, "xdmac%d:cannot get IRQ %d\n",
+						       chan->irq, chan->number);
+			goto bail2;
+		}
+	}
+
+	xdmac_clk = of_clk_get(pdev->dev.of_node, 0);
+	if (IS_ERR(xdmac_clk)) {
+		dev_err(&pdev->dev, "Unable to get clock\n");
+		ret = -EINVAL;
+		goto bail2;
+	}
+
+	ret = clk_prepare_enable(xdmac_clk);
+	if (ret) {
+		dev_err(&pdev->dev, "failed to enable clock source\n");
+		goto bail3;
+	}
+
+	if (rotation)
+		tmp |= XDACS_CP;
+	if (low_power)
+		tmp |= XDACS_LP;
+
+	tmp |= XDACS_XE;
+	xdmac_writel(tmp, XDACS);
+
+	return 0;
+
+bail3:
+	clk_put(xdmac_clk);
+
+bail2:
+	while (--channel >= 0) {
+		chan = &xdmac_chans[channel];
+		free_irq(chan->irq, xdmac_irq);
+	}
+bail1:
+	iounmap(xdmac_base);
+
+	return ret;
+}
+
+static int mb8ac0300_xdmac_remove(struct platform_device *pdev)
+{
+	int channel;
+
+	for (channel = 0; channel < channels_in_use; channel++)
+		free_irq(xdmac_chans[channel].irq,
+					&xdmac_chans[channel]);
+
+	iounmap(xdmac_base);
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id mb8ac0300_xdmac_dt_ids[] = {
+	{ .compatible = "fujitsu,mb8ac0300-xdmac" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, mb8ac0300_xdmac_dt_ids);
+#else
+#define mb8ac0300_xdmac_dt_ids NULL
+#endif
+
+static struct platform_driver mb8ac0300_xdmac_driver = {
+	.probe     = mb8ac0300_xdmac_probe,
+	.remove    = mb8ac0300_xdmac_remove,
+	.driver    = {
+		.name  = DRIVER_NAME,
+		.owner = THIS_MODULE,
+		.of_match_table = mb8ac0300_xdmac_dt_ids,
+		.pm = &mb8ac0300_xdmac_dev_pm_ops,
+	},
+};
+
+static int __init mb8ac0300_xdmac_driver_init(void)
+{
+	return platform_driver_register(&mb8ac0300_xdmac_driver);
+}
+subsys_initcall(mb8ac0300_xdmac_driver_init);
+
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_ALIAS("platform:" DRIVER_NAME);
+MODULE_LICENSE("GPL");
+
+
diff --git a/drivers/dma/memset.c b/drivers/dma/memset.c
new file mode 100644
index 0000000..c2e801f
--- /dev/null
+++ b/drivers/dma/memset.c
@@ -0,0 +1,135 @@
+
+#include <linux/delay.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
+#include <linux/freezer.h>
+#include <linux/init.h>
+#include <linux/kthread.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/random.h>
+#include <linux/slab.h>
+#include <linux/wait.h>
+
+static unsigned int pattern = 0xdeadbabe;
+module_param(pattern, uint, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(pattern,"Pattern to set (default: 0xdeadbabe)");
+
+static unsigned int bufsize = 32768;
+module_param(bufsize, uint, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(bufsize, "Size of buffer to set (default: 32768)");
+
+#define SRC_BUF	128
+
+static DECLARE_COMPLETION(memset_done);
+static struct dma_interleaved_template *xt;
+static struct dma_chan *chan;
+static void *buf;
+
+static void dma_callback(void *data)
+{
+	complete(&memset_done);
+}
+
+static int __init memsettest_init(void)
+{
+	struct dma_async_tx_descriptor *tx;
+	struct dma_device *dmadev;
+	dma_cookie_t cookie;
+	dma_cap_mask_t mask;
+	int ret;
+
+	dma_cap_zero(mask);
+	dma_cap_set(DMA_INTERLEAVE, mask);
+
+	chan = dma_request_channel(mask, NULL, NULL);
+	if (!chan) {
+		printk("%s:%d dma_request_channel!\n", __func__, __LINE__);
+		return -EINVAL;
+	}
+	dmadev = chan->device;
+
+	if (!dma_has_cap(DMA_INTERLEAVE, dmadev->cap_mask)) {
+		printk("%s:%d DMA_INTERLEAVE!\n", __func__, __LINE__);
+		goto err1;
+	}
+
+	bufsize += SRC_BUF;
+	buf = kmalloc(bufsize, GFP_KERNEL);
+	if (!buf) {
+		printk("%s:%d DMA_INTERLEAVE!\n", __func__, __LINE__);
+		goto err1;
+	}
+	*(u32 *)buf = pattern;
+
+	xt = kzalloc(sizeof(struct dma_interleaved_template) +
+			sizeof(struct data_chunk), GFP_KERNEL);
+	xt->src_inc = 0;
+	xt->dst_inc = 1;
+	xt->src_sgl = false;
+	xt->dst_sgl = true;
+	xt->frame_size = 1;
+	xt->dir = DMA_MEM_TO_MEM;
+	xt->sgl[0].size = 4;
+	xt->sgl[0].icg = 0;
+	xt->numf = (bufsize - SRC_BUF) / xt->sgl[0].size;
+	xt->src_start = dma_map_single(dmadev->dev, buf,
+				   bufsize, DMA_BIDIRECTIONAL);
+	ret = dma_mapping_error(dmadev->dev, xt->src_start);
+	if (ret) {
+		printk("%s:%d dma_mapping_error!\n", __func__, __LINE__);
+		goto err2;
+	}
+	xt->dst_start = xt->src_start + SRC_BUF;
+
+	tx = dmadev->device_prep_interleaved_dma(chan, xt, 0);
+	if (tx == NULL) {
+		printk("%s:%d Error!\n", __func__, __LINE__);
+		goto err3;
+	}
+
+	tx->callback = dma_callback;
+	tx->callback_param = NULL;
+	cookie = dmaengine_submit(tx);
+	if (dma_submit_error(cookie)) {
+		printk("%s:%d Error!\n", __func__, __LINE__);
+		goto err3;
+	}
+
+	dma_async_issue_pending(chan);
+	return 0;
+err3:
+	dma_unmap_single(dmadev->dev, xt->src_start,
+				bufsize, DMA_BIDIRECTIONAL);
+err2:
+	kfree(xt);
+	kfree(buf);
+err1:
+	dma_release_channel(chan);
+	return -EINVAL;
+}
+late_initcall(memsettest_init);
+
+static void __exit memsettest_exit(void)
+{
+	struct dma_device *dmadev = chan->device;
+	int i, len = bufsize - SRC_BUF;
+	u32 *dst = buf + SRC_BUF;
+
+	wait_for_completion(&memset_done);
+
+	printk("Destination pattern (%d bytes)\n", len);
+	for (i = 0; i < len / 4; i++)
+		printk("%x", *dst++);
+	printk("\n");
+
+	dma_unmap_single(dmadev->dev, xt->src_start,
+				bufsize, DMA_BIDIRECTIONAL);
+	kfree(xt);
+	kfree(buf);
+	dma_release_channel(chan);
+}
+module_exit(memsettest_exit);
+
+MODULE_AUTHOR("Jassi");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/gpio/Kconfig b/drivers/gpio/Kconfig
index 64b9e7f..322445d 100644
--- a/drivers/gpio/Kconfig
+++ b/drivers/gpio/Kconfig
@@ -118,6 +118,12 @@ config GPIO_GENERIC_PLATFORM
 	help
 	  Say yes here to support basic platform_device memory-mapped GPIO controllers.
 
+config GPIO_MB86S7X
+	bool "GPIO support for Fujitsu MB86S7x Platforms"
+	depends on ARCH_MB86S70
+	help
+	  Say yes here to support the GPIO controller in Fujitsu MB86S70 SoCs.
+
 config GPIO_IT8761E
 	tristate "IT8761E GPIO support"
 	depends on X86  # unconditional access to IO space.
@@ -325,6 +331,15 @@ config GPIO_GRGPIO
 	  Select this to support Aeroflex Gaisler GRGPIO cores from the GRLIB
 	  VHDL IP core library.
 
+config GPIO_MB8AC0300
+	bool "MB8AC0300 GPIO support"
+	depends on ARCH_MB8AC0300
+	help
+	  Say yes here to support the MB8AC0300
+	  GPIO device
+	  The GPIO interrupts are handled by a separate but related
+	  EXIU IP unit.
+
 comment "I2C GPIO expanders:"
 
 config GPIO_ARIZONA
diff --git a/drivers/gpio/Makefile b/drivers/gpio/Makefile
index 71c5611..6d3151b 100644
--- a/drivers/gpio/Makefile
+++ b/drivers/gpio/Makefile
@@ -37,6 +37,8 @@ obj-$(CONFIG_GPIO_MAX730X)	+= gpio-max730x.o
 obj-$(CONFIG_GPIO_MAX7300)	+= gpio-max7300.o
 obj-$(CONFIG_GPIO_MAX7301)	+= gpio-max7301.o
 obj-$(CONFIG_GPIO_MAX732X)	+= gpio-max732x.o
+obj-$(CONFIG_GPIO_MB86S7X)	+= gpio-mb86s7x.o
+obj-$(CONFIG_GPIO_MB8AC0300)	+= gpio-mb8ac0300.o
 obj-$(CONFIG_GPIO_MC33880)	+= gpio-mc33880.o
 obj-$(CONFIG_GPIO_MC9S08DZ60)	+= gpio-mc9s08dz60.o
 obj-$(CONFIG_GPIO_MCP23S08)	+= gpio-mcp23s08.o
diff --git a/drivers/gpio/gpio-mb86s7x.c b/drivers/gpio/gpio-mb86s7x.c
new file mode 100644
index 0000000..f4e1204
--- /dev/null
+++ b/drivers/gpio/gpio-mb86s7x.c
@@ -0,0 +1,241 @@
+/*
+ *  linux/drivers/gpio/gpio-mb86s7x.c
+ *
+ *  Copyright (C) 2015 Fujitsu Semiconductor Limited
+ *  Copyright (C) 2015 Linaro Ltd.
+ *
+ *  This program is free software: you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation, version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ */
+
+#include <linux/io.h>
+#include <linux/init.h>
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/ioport.h>
+#include <linux/of_device.h>
+#include <linux/gpio.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/slab.h>
+
+/*
+ * Only first 8bits of a register correspond to each pin,
+ * so there are 4 registers for 32 pins.
+ */
+#define PDR(x)	(0x0 + x / 8 * 4)
+#define DDR(x)	(0x10 + x / 8 * 4)
+#define PFR(x)	(0x20 + x / 8 * 4)
+
+#define OFFSET(x)	BIT((x) % 8)
+
+struct mb86s70_gpio_chip {
+	struct gpio_chip gc;
+	void __iomem *base;
+	struct clk *clk;
+	spinlock_t lock;
+};
+
+static inline struct mb86s70_gpio_chip *chip_to_mb86s70(struct gpio_chip *gc)
+{
+	return container_of(gc, struct mb86s70_gpio_chip, gc);
+}
+
+static int mb86s70_gpio_request(struct gpio_chip *gc, unsigned gpio)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+	unsigned long flags;
+	u32 val;
+
+	spin_lock_irqsave(&gchip->lock, flags);
+
+	val = readl(gchip->base + PFR(gpio));
+	val &= ~OFFSET(gpio);
+	writel(val, gchip->base + PFR(gpio));
+
+	spin_unlock_irqrestore(&gchip->lock, flags);
+
+	return 0;
+}
+
+static void mb86s70_gpio_free(struct gpio_chip *gc, unsigned gpio)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+	unsigned long flags;
+	u32 val;
+
+	spin_lock_irqsave(&gchip->lock, flags);
+
+	val = readl(gchip->base + PFR(gpio));
+	val |= OFFSET(gpio);
+	writel(val, gchip->base + PFR(gpio));
+
+	spin_unlock_irqrestore(&gchip->lock, flags);
+}
+
+static int mb86s70_gpio_direction_input(struct gpio_chip *gc, unsigned gpio)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+	unsigned long flags;
+	unsigned char val;
+
+	spin_lock_irqsave(&gchip->lock, flags);
+
+	val = readl(gchip->base + DDR(gpio));
+	val &= ~OFFSET(gpio);
+	writel(val, gchip->base + DDR(gpio));
+
+	spin_unlock_irqrestore(&gchip->lock, flags);
+
+	return 0;
+}
+
+static int mb86s70_gpio_direction_output(struct gpio_chip *gc,
+					 unsigned gpio, int value)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+	unsigned long flags;
+	unsigned char val;
+
+	spin_lock_irqsave(&gchip->lock, flags);
+
+	val = readl(gchip->base + PDR(gpio));
+	if (value)
+		val |= OFFSET(gpio);
+	else
+		val &= ~OFFSET(gpio);
+	writel(val, gchip->base + PDR(gpio));
+
+	val = readl(gchip->base + DDR(gpio));
+	val |= OFFSET(gpio);
+	writel(val, gchip->base + DDR(gpio));
+
+	spin_unlock_irqrestore(&gchip->lock, flags);
+
+	return 0;
+}
+
+static int mb86s70_gpio_get(struct gpio_chip *gc, unsigned gpio)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+
+	return !!(readl(gchip->base + PDR(gpio)) & OFFSET(gpio));
+}
+
+static void mb86s70_gpio_set(struct gpio_chip *gc, unsigned gpio, int value)
+{
+	struct mb86s70_gpio_chip *gchip = chip_to_mb86s70(gc);
+	unsigned long flags;
+	unsigned char val;
+
+	spin_lock_irqsave(&gchip->lock, flags);
+
+	val = readl(gchip->base + PDR(gpio));
+	if (value)
+		val |= OFFSET(gpio);
+	else
+		val &= ~OFFSET(gpio);
+	writel(val, gchip->base + PDR(gpio));
+
+	spin_unlock_irqrestore(&gchip->lock, flags);
+}
+
+static int mb86s70_gpio_probe(struct platform_device *pdev)
+{
+	struct mb86s70_gpio_chip *gchip;
+	struct resource *res;
+	int ret, base;
+
+	gchip = devm_kzalloc(&pdev->dev, sizeof(*gchip), GFP_KERNEL);
+	if (gchip == NULL)
+		return -ENOMEM;
+
+	platform_set_drvdata(pdev, gchip);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	gchip->base = devm_ioremap_resource(&pdev->dev, res);
+	if (IS_ERR(gchip->base))
+		return PTR_ERR(gchip->base);
+
+	gchip->clk = devm_clk_get(&pdev->dev, NULL);
+	if (IS_ERR(gchip->clk))
+		return PTR_ERR(gchip->clk);
+
+	clk_prepare_enable(gchip->clk);
+
+	spin_lock_init(&gchip->lock);
+
+	ret = of_property_read_u32_index(pdev->dev.of_node, "base", 0, &base);
+	if (ret) {
+		dev_info(&pdev->dev, "missing base in dt\n");
+		base = -1;
+	}
+
+	gchip->gc.direction_output = mb86s70_gpio_direction_output;
+	gchip->gc.direction_input = mb86s70_gpio_direction_input;
+	gchip->gc.request = mb86s70_gpio_request;
+	gchip->gc.free = mb86s70_gpio_free;
+	gchip->gc.get = mb86s70_gpio_get;
+	gchip->gc.set = mb86s70_gpio_set;
+	gchip->gc.label = dev_name(&pdev->dev);
+	gchip->gc.ngpio = 32;
+	gchip->gc.owner = THIS_MODULE;
+	gchip->gc.dev = &pdev->dev;
+	gchip->gc.base = base;
+
+	platform_set_drvdata(pdev, gchip);
+
+	ret = gpiochip_add(&gchip->gc);
+	if (ret) {
+		dev_err(&pdev->dev, "couldn't register gpio driver\n");
+		clk_disable_unprepare(gchip->clk);
+	}
+
+	return ret;
+}
+
+static int mb86s70_gpio_remove(struct platform_device *pdev)
+{
+	struct mb86s70_gpio_chip *gchip = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = gpiochip_remove(&gchip->gc);
+	if (ret)
+		return ret;
+	clk_disable_unprepare(gchip->clk);
+
+	return 0;
+}
+
+static const struct of_device_id mb86s70_gpio_dt_ids[] = {
+	{ .compatible = "fujitsu,mb86s70-gpio" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, mb86s70_gpio_dt_ids);
+
+static struct platform_driver mb86s70_gpio_driver = {
+	.driver = {
+		.name = "mb86s70-gpio",
+		.of_match_table = mb86s70_gpio_dt_ids,
+	},
+	.probe = mb86s70_gpio_probe,
+	.remove = mb86s70_gpio_remove,
+};
+
+static int __init mb86s70_gpio_init(void)
+{
+	return platform_driver_register(&mb86s70_gpio_driver);
+}
+subsys_initcall(mb86s70_gpio_init);
+
+MODULE_DESCRIPTION("MB86S7x GPIO Driver");
+MODULE_ALIAS("platform:mb86s70-gpio");
+MODULE_LICENSE("GPL");
diff --git a/drivers/gpio/gpio-mb8ac0300.c b/drivers/gpio/gpio-mb8ac0300.c
new file mode 100644
index 0000000..185203f
--- /dev/null
+++ b/drivers/gpio/gpio-mb8ac0300.c
@@ -0,0 +1,586 @@
+/*
+ *  linux/drivers/gpio/mb8ac0300_gpio.c
+ *
+ *  Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ *  This program is free software: you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation, version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+#include <linux/slab.h>
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/gpio.h>
+#include <linux/init.h>
+#include <linux/ioport.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/io.h>
+#include <linux/platform_data/mb8ac0300-gpio.h>
+
+#define DRIVER_NAME		"gpio"
+#define DRIVER_DESC		"MB8AC0300 GPIO Driver"
+
+static int mb8ac0300_gpio_is_valid(struct gpio_chip *gc, unsigned offset)
+{
+#if 0
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned char func;
+	unsigned char dir;
+#endif
+	/* check gpio pin offset */
+	if (offset >= gc->ngpio) {
+		dev_dbg(gc->dev, "offset is more than gpio pin numbers.\n");
+		return 0;
+	}
+#if 0
+	/* get gpio pin functions value */
+	func = chip->functions[offset / MB8AC0300_GPIO_NR_PER_REG];
+	func = (func >> (offset % MB8AC0300_GPIO_NR_PER_REG)) &
+						MB8AC0300_GPIO_IS_PERIPHERAL;
+
+	/* the gpio pin is used as peripheral port */
+	if (func == MB8AC0300_GPIO_IS_PERIPHERAL) {
+		/* get gpio pin directions value */
+		dir = chip->directions[offset/MB8AC0300_GPIO_NR_PER_REG];
+		dir = (dir >> (offset % MB8AC0300_GPIO_NR_PER_REG)) &
+						MB8AC0300_GPIO_IS_OUTPUT;
+
+		/* it is invalid if not used as external interrupt functions */
+		if ((dir == MB8AC0300_GPIO_IS_OUTPUT) ||
+			((offset < chip->pdata->irq_gpio_min) ||
+			(offset > chip->pdata->irq_gpio_max))) {
+			dev_dbg(gc->dev, "the gpio pin with no EXTINT\n");
+			return 0;
+		}
+	}
+#endif
+	return 1;
+}
+
+/**
+ * mb8ac0300_gpio_request - gpio pin request
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ *
+ * Returns 0 if no error, -EINVAL or other negative errno on failure
+ */
+static int mb8ac0300_gpio_request(struct gpio_chip *gc, unsigned offset)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned long flags;
+	unsigned char val;
+
+	/* check whether gpio pin is valid */
+	if (!mb8ac0300_gpio_is_valid(gc, offset)) {
+		dev_dbg(gc->dev, "gpio pin is invalid.\n");
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&chip->lock, flags);
+
+	/* force it to not be in function mode then */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_PFR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	val &= ~(1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	writel(val, chip->base + MB8AC0300_GPIO_REG_PFR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+
+	spin_unlock_irqrestore(&chip->lock, flags);
+
+	return 0;
+}
+
+/**
+ * mb8ac0300_gpio_direction_input - set gpio pin to input
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ *
+ * Returns 0
+ */
+static int mb8ac0300_gpio_direction_input(struct gpio_chip *gc, unsigned offset)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned long flags;
+	unsigned char val;
+
+	/* get lock */
+	spin_lock_irqsave(&chip->lock, flags);
+
+	/* set gpio pin direction */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_DDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	val &= ~(1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	writel(val, chip->base + MB8AC0300_GPIO_REG_DDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+
+	/* release lock */
+	spin_unlock_irqrestore(&chip->lock, flags);
+
+	return 0;
+}
+
+/**
+ * mb8ac0300_gpio_direction_output - set gpio pin to output
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ * @value:	output value of gpio pin
+ *
+ * Returns 0
+ */
+static int mb8ac0300_gpio_direction_output(struct gpio_chip *gc,
+	 unsigned offset, int value)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned long flags;
+	unsigned char val;
+
+	/* get lock */
+	spin_lock_irqsave(&chip->lock, flags);
+
+	/* set gpio pin value */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_PDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	if (value)
+		val |= (1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	else
+		val &= ~(1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	writel(val, chip->base + MB8AC0300_GPIO_REG_PDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+
+	/* set gpio pin direction */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_DDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	val |= (1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	writel(val, chip->base + MB8AC0300_GPIO_REG_DDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+
+	/* release lock */
+	spin_unlock_irqrestore(&chip->lock, flags);
+
+	return 0;
+}
+
+/**
+ * mb8ac0300_gpio_get - get gpio pin value
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ *
+ * Returns 0 if low level,Returns 1 if high level
+ */
+static int mb8ac0300_gpio_get(struct gpio_chip *gc, unsigned offset)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned char val;
+
+	/* get gpio pin value */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_PDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	val &= (1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+
+	return val ? 1 : 0;
+}
+
+/**
+ * mb8ac0300_gpio_set - set gpio pin value
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ * @value:	setup value
+ *
+ * no return value
+ */
+static void mb8ac0300_gpio_set(struct gpio_chip *gc, unsigned offset, int value)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	unsigned long flags;
+	unsigned char val;
+
+	/* get lock */
+	spin_lock_irqsave(&chip->lock, flags);
+
+	/* set gpio pin value */
+	val = readl(chip->base + MB8AC0300_GPIO_REG_PDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+	if (value)
+		val |= (1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	else
+		val &= ~(1 << (offset % MB8AC0300_GPIO_NR_PER_REG));
+	writel(val, chip->base + MB8AC0300_GPIO_REG_PDR0 +
+					MB8AC0300_GPIO_OFFSET_TO_REG(offset));
+
+	/* release lock */
+	spin_unlock_irqrestore(&chip->lock, flags);
+
+	return;
+}
+
+/**
+ * mb8ac0300_gpio_to_irq - Convert the gpio pin number to irq number
+ * @gc:		gpio chip
+ * @offset:	offset of gpio pin number
+ *
+ * Returns the IRQ corresponding to a GPIO, -EINVAL or other negative errno on
+ * failure
+ */
+static int mb8ac0300_gpio_to_irq(struct gpio_chip *gc, unsigned offset)
+{
+	struct mb8ac0300_gpio_chip *chip =
+			container_of(gc, struct mb8ac0300_gpio_chip, gc);
+	int num = offset + gc->base;
+
+	/* check external interrupt gpio pin range */
+	if (num < chip->pdata->irq_gpio_min ||
+					num > chip->pdata->irq_gpio_max) {
+		dev_dbg(gc->dev, "offset is over external interrupt range");
+		return -EINVAL;
+	}
+
+	return chip->pdata->irq_base + (num - chip->pdata->irq_gpio_min);
+}
+
+/**
+ * mb8ac0300_gpio_probe - Probes the gpio device
+ * @pdev:	platform device
+ *
+ * Perform basic init : allocates memory, initialize
+ *
+ * Returns 0 if no error, -ENODEV -ENOMEM -EIO or other negative errno on
+ * failure
+ */
+static int mb8ac0300_gpio_probe(struct platform_device *pdev)
+{
+	struct mb8ac0300_gpio_platform_data *pdata;
+	struct mb8ac0300_gpio_chip *chip;
+	struct resource *res;
+	int counter;
+	int ret = 0;
+#ifdef CONFIG_OF
+	const int *p;
+	int len;
+	int n;
+
+	/* reserve memory space for gpio chip */
+	chip = kzalloc(sizeof(*chip), GFP_KERNEL);
+	if (chip == NULL) {
+		dev_err(&pdev->dev, "can't allocate gpio chip data.\n");
+		ret = -ENOMEM;
+		goto done;
+	}
+
+	if (pdev->dev.of_node) {
+		pdata = kzalloc(sizeof(*pdata), GFP_KERNEL);
+		if (pdata == NULL) {
+			dev_err(&pdev->dev, "Out of memory\n");
+			ret = -ENOMEM;
+			goto done;
+		}
+
+		p = of_get_property(pdev->dev.of_node, "gpio_base", NULL);
+		if (p)
+			pdata->gpio_base = be32_to_cpu(*p);
+		p = of_get_property(pdev->dev.of_node, "irq_base", NULL);
+		if (p)
+			pdata->irq_base = be32_to_cpu(*p);
+		p = of_get_property(pdev->dev.of_node, "irq_gpio_min", NULL);
+		if (p)
+			pdata->irq_gpio_min = be32_to_cpu(*p);
+		p = of_get_property(pdev->dev.of_node, "irq_gpio_max", NULL);
+		if (p)
+			pdata->irq_gpio_max = be32_to_cpu(*p);
+		p = of_get_property(pdev->dev.of_node, "directions", &len);
+		if (p && len == (4 * sizeof(u32)))
+			for (n = 0; n < 4; n++)
+				chip->directions[n] = be32_to_cpu(*p++);
+		p = of_get_property(pdev->dev.of_node, "values", &len);
+		if (p && len == (4 * sizeof(u32)))
+			for (n = 0; n < 4; n++)
+				chip->values[n] = be32_to_cpu(*p++);
+		p = of_get_property(pdev->dev.of_node, "functions", &len);
+		if (p && len == (4 * sizeof(u32)))
+			for (n = 0; n < 4; n++)
+				chip->functions[n] = be32_to_cpu(*p++);
+
+	} else
+#endif
+	{
+		pdata = pdev->dev.platform_data;
+		if (pdata == NULL) {
+			dev_err(&pdev->dev, "No platform data supplied\n");
+			ret = -ENOENT;
+			goto done;
+		}
+	}
+
+	/* gpio chip store in platform private data */
+	platform_set_drvdata(pdev, chip);
+
+	/* get IO resource */
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+
+	/* remap IO resource base address */
+	chip->base = ioremap(res->start, res->end - res->start + 1);
+	if (chip->base == NULL) {
+		dev_err(&pdev->dev, "mapping IO resource is failed.\n");
+		ret = -ENOMEM;
+		goto free_mem;
+	}
+
+	/* get clock for gpio driver */
+	chip->clk = clk_get(&pdev->dev, NULL);
+	if (IS_ERR(chip->clk)) {
+		dev_err(&pdev->dev, "clock not found.\n");
+		ret = PTR_ERR(chip->clk);
+		goto iounmap;
+	}
+
+	/* enable clock */
+	clk_prepare_enable(chip->clk);
+
+	/* initiate lock */
+	spin_lock_init(&chip->lock);
+
+	/* initiate gpio chip structure */
+	chip->gc.request = mb8ac0300_gpio_request;
+	chip->gc.direction_input = mb8ac0300_gpio_direction_input;
+	chip->gc.direction_output = mb8ac0300_gpio_direction_output;
+	chip->gc.get = mb8ac0300_gpio_get;
+	chip->gc.set = mb8ac0300_gpio_set;
+	chip->gc.to_irq = mb8ac0300_gpio_to_irq;
+	chip->gc.base = pdata->gpio_base;
+	chip->gc.ngpio = MB8AC0300_GPIO_NR;
+	chip->gc.label = dev_name(&pdev->dev);
+	chip->gc.dev = &pdev->dev;
+	chip->gc.owner = THIS_MODULE;
+	chip->pdata = pdata;
+
+	/* add gpio chip to gpiolib */
+	ret = gpiochip_add(&chip->gc);
+	if (ret) {
+		dev_err(&pdev->dev, "gpiochip_add error %d\n", ret);
+		goto disable_clk;
+	}
+
+	/* initiate gpio controller register */
+	for (counter = 0; counter < MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG;
+		 counter++) {
+		/* initiate gpio function register */
+		writel(chip->functions[counter],
+			chip->base + MB8AC0300_GPIO_REG_PFR0 + counter * 4);
+
+		/* initiate gpio value register */
+		writel(chip->values[counter],
+			chip->base + MB8AC0300_GPIO_REG_PDR0 + counter * 4);
+
+		/* initiate gpio direction register */
+		writel(chip->directions[counter],
+			chip->base + MB8AC0300_GPIO_REG_DDR0 + counter * 4);
+	}
+
+	goto done;
+
+disable_clk:
+	/* disable clock */
+	clk_disable_unprepare(chip->clk);
+	clk_put(chip->clk);
+iounmap:
+	/* unmap gpio chip base address */
+	iounmap(chip->base);
+free_mem:
+	/* free gpio chip memory space */
+	kfree(chip);
+done:
+	if (ret && pdev->dev.of_node)
+		kfree(pdata);
+	return ret;
+}
+
+/**
+ * mb8ac0300_gpio_remove - Removes the gpio device driver
+ * @pdev:	platform device
+ *
+ * Returns 0
+ */
+static int mb8ac0300_gpio_remove(struct platform_device *pdev)
+{
+	struct mb8ac0300_gpio_chip *chip;
+
+	/* get gpio chip from platform private data */
+	chip = (struct mb8ac0300_gpio_chip *)platform_get_drvdata(pdev);
+
+	if (chip) {
+		/* unmap gpio chip base address */
+		iounmap(chip->base);
+
+		/* disable clock */
+		clk_disable_unprepare(chip->clk);
+		clk_put(chip->clk);
+
+		if (pdev->dev.of_node)
+			kfree(chip->pdata);
+
+		/* free gpio chip memory space */
+		kfree(chip);
+	}
+
+	return 0;
+}
+
+#ifdef CONFIG_OF
+static const struct of_device_id mb8ac0300_gpio_dt_ids[] = {
+	{ .compatible = "fujitsu,mb8ac0300-gpio" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, mb8ac0300_gpio_dt_ids);
+#else
+#define mb8ac0300_gpio_dt_ids NULL
+#endif
+
+#ifdef CONFIG_PM
+static inline void save_gpio_setting(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct mb8ac0300_gpio_chip *chip = platform_get_drvdata(pdev);
+	int n;
+
+	for (n = 0; n < MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG; n++) {
+		chip->directions[n] = (unsigned char)(readl(
+			chip->base + MB8AC0300_GPIO_REG_DDR0 + n * 4));
+
+		chip->values[n] = (unsigned char)(readb(
+			chip->base + MB8AC0300_GPIO_REG_PDR0 + n * 4));
+	}
+}
+
+static inline void restore_gpio_setting(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct mb8ac0300_gpio_chip *chip = platform_get_drvdata(pdev);
+	int n;
+
+	for (n = 0; n < MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG; n++) {
+		/* restore gpio direction register */
+		writel((unsigned long)chip->directions[n],
+			chip->base + MB8AC0300_GPIO_REG_DDR0 + n * 4);
+
+		/* restore  gpio value register */
+		writel(chip->values[n],
+			chip->base + MB8AC0300_GPIO_REG_PDR0 + n * 4);
+	}
+}
+
+static int mb8ac0300_gpio_suspend(struct device *dev)
+{
+	save_gpio_setting(dev);
+	return 0;
+}
+
+static int mb8ac0300_gpio_resume(struct device *dev)
+{
+	restore_gpio_setting(dev);
+	return 0;
+}
+
+static int mb8ac0300_gpio_freeze(struct device *dev)
+{
+	save_gpio_setting(dev);
+	return 0;
+}
+
+static int mb8ac0300_gpio_thaw(struct device *dev)
+{
+	restore_gpio_setting(dev);
+	return 0;
+}
+
+static int mb8ac0300_gpio_restore(struct device *dev)
+{
+	restore_gpio_setting(dev);
+	return 0;
+}
+
+static int mb8ac0300_gpio_suspend_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct mb8ac0300_gpio_chip *chip = platform_get_drvdata(pdev);
+
+	clk_disable_unprepare(chip->clk);
+
+	return 0;
+}
+
+static int mb8ac0300_gpio_resume_noirq(struct device *dev)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+	struct mb8ac0300_gpio_chip *chip = platform_get_drvdata(pdev);
+	int ret;
+
+	ret = clk_prepare_enable(chip->clk);
+	if (ret) {
+		dev_err(dev, "%s: clk_enable failed\n", __func__);
+		return ret;
+	}
+
+	return 0;
+}
+
+
+static const struct dev_pm_ops mb8ac0300_gpio_dev_pm_ops = {
+	.suspend_noirq = mb8ac0300_gpio_suspend_noirq,
+	.resume_noirq = mb8ac0300_gpio_resume_noirq,
+	.suspend	= mb8ac0300_gpio_suspend,
+	.resume		= mb8ac0300_gpio_resume,
+	.freeze		= mb8ac0300_gpio_freeze,
+	.thaw		= mb8ac0300_gpio_thaw,
+	.restore	= mb8ac0300_gpio_restore,
+};
+
+#endif
+
+
+/* GPIO device driver structure */
+static struct platform_driver mb8ac0300_gpio_driver = {
+	.probe     = mb8ac0300_gpio_probe,
+	.remove    = mb8ac0300_gpio_remove,
+	.driver    = {
+		.name  = DRIVER_NAME,
+		.owner = THIS_MODULE,
+		.of_match_table = mb8ac0300_gpio_dt_ids,
+#ifdef CONFIG_PM
+		.pm	= &mb8ac0300_gpio_dev_pm_ops,
+#endif
+	},
+};
+
+/**
+ * mb8ac0300_gpio_init - initialize module
+ *
+ * Returns 0 if no error, negative errno on failure
+ */
+static int __init mb8ac0300_gpio_init(void)
+{
+	return platform_driver_register(&mb8ac0300_gpio_driver);
+}
+subsys_initcall(mb8ac0300_gpio_init);
+
+/* GPIO device module definition */
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_ALIAS("platform:" DRIVER_NAME);
+MODULE_LICENSE("GPL");
diff --git a/drivers/gpio/gpiolib-of.c b/drivers/gpio/gpiolib-of.c
index 48cda3c..a8b450b 100644
--- a/drivers/gpio/gpiolib-of.c
+++ b/drivers/gpio/gpiolib-of.c
@@ -12,6 +12,7 @@
  */
 
 #include <linux/device.h>
+#include <linux/err.h>
 #include <linux/errno.h>
 #include <linux/module.h>
 #include <linux/io.h>
@@ -42,8 +43,14 @@ static int of_gpiochip_find_and_xlate(struct gpio_chip *gc, void *data)
 		return false;
 
 	ret = gc->of_xlate(gc, &gg_data->gpiospec, gg_data->flags);
-	if (ret < 0)
-		return false;
+	if (ret < 0) {
+		/* We've found the gpio chip, but the translation failed.
+		 * Return true to stop looking and return the translation
+		 * error via out_gpio
+		 */
+		gg_data->out_gpio = ret;
+		return true;
+	 }
 
 	gg_data->out_gpio = ret + gc->base;
 	return true;
diff --git a/drivers/gpu/Makefile b/drivers/gpu/Makefile
index d8a22c2..3818ecf 100644
--- a/drivers/gpu/Makefile
+++ b/drivers/gpu/Makefile
@@ -1,2 +1,3 @@
+obj-y			+= drm/ vga/ kds/ mali-t6xx/
 obj-y			+= drm/ vga/
 obj-$(CONFIG_TEGRA_HOST1X)	+= host1x/
diff --git a/drivers/gpu/kds/Kconfig b/drivers/gpu/kds/Kconfig
new file mode 100644
index 0000000..5138d13
--- /dev/null
+++ b/drivers/gpu/kds/Kconfig
@@ -0,0 +1,6 @@
+config KDS
+	tristate "Arm Mali Kernel Dependency System"
+	default n
+	help
+	  Mali Kernel Dependency System
+
diff --git a/drivers/gpu/kds/Makefile b/drivers/gpu/kds/Makefile
new file mode 100644
index 0000000..d275785
--- /dev/null
+++ b/drivers/gpu/kds/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_KDS) += kds.o
diff --git a/drivers/gpu/kds/kds.c b/drivers/gpu/kds/kds.c
new file mode 100644
index 0000000..9b7175a
--- /dev/null
+++ b/drivers/gpu/kds/kds.c
@@ -0,0 +1,529 @@
+/*
+ *
+ * (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the GNU General Public License version 2
+ * as published by the Free Software Foundation, and any use by you of this program is subject to the terms of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained from Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/slab.h>
+#include <linux/list.h>
+#include <linux/spinlock.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/workqueue.h>
+#include <linux/kds.h>
+
+
+#define KDS_LINK_TRIGGERED (1u << 0)
+#define KDS_LINK_EXCLUSIVE (1u << 1)
+
+#define KDS_INVALID (void *)-2
+#define KDS_RESOURCE (void *)-1
+
+struct kds_resource_set
+{
+	unsigned long         num_resources;
+	unsigned long         pending;
+	struct kds_callback  *cb;
+	void                 *callback_parameter;
+	void                 *callback_extra_parameter;
+	struct list_head      callback_link;
+	struct work_struct    callback_work;
+
+	/* This is only initted when kds_waitall() is called. */
+	wait_queue_head_t     wake;
+
+	struct kds_link       resources[0];
+};
+
+static DEFINE_SPINLOCK(kds_lock);
+
+int kds_callback_init(struct kds_callback *cb, int direct, kds_callback_fn user_cb)
+{
+	int ret = 0;
+
+	cb->direct = direct;
+	cb->user_cb = user_cb;
+
+	if (!direct)
+	{
+		cb->wq = alloc_workqueue("kds", WQ_UNBOUND | WQ_HIGHPRI, WQ_UNBOUND_MAX_ACTIVE);
+		if (!cb->wq)
+			ret = -ENOMEM;
+	}
+	else
+	{
+		cb->wq = NULL;
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL(kds_callback_init);
+
+void kds_callback_term(struct kds_callback *cb)
+{
+	if (!cb->direct)
+	{
+		BUG_ON(!cb->wq);
+		destroy_workqueue(cb->wq);
+	}
+	else
+	{
+		BUG_ON(cb->wq);
+	}
+}
+
+EXPORT_SYMBOL(kds_callback_term);
+
+static void kds_do_user_callback(struct kds_resource_set *rset)
+{
+	rset->cb->user_cb(rset->callback_parameter, rset->callback_extra_parameter);
+}
+
+static void kds_queued_callback(struct work_struct *work)
+{
+	struct kds_resource_set *rset;
+	rset = container_of(work, struct kds_resource_set, callback_work);
+
+	kds_do_user_callback(rset);
+}
+
+static void kds_callback_perform(struct kds_resource_set *rset)
+{
+	if (rset->cb->direct)
+		kds_do_user_callback(rset);
+	else
+	{
+		int result;
+		result = queue_work(rset->cb->wq, &rset->callback_work);
+		/* if we got a 0 return it means we've triggered the same rset twice! */
+		WARN_ON(!result);
+	}
+}
+
+void kds_resource_init(struct kds_resource * const res)
+{
+	BUG_ON(!res);
+	INIT_LIST_HEAD(&res->waiters.link);
+	res->waiters.parent = KDS_RESOURCE;
+}
+EXPORT_SYMBOL(kds_resource_init);
+
+int kds_resource_term(struct kds_resource *res)
+{
+	unsigned long lflags;
+	BUG_ON(!res);
+	spin_lock_irqsave(&kds_lock, lflags);
+	if (!list_empty(&res->waiters.link))
+	{
+		spin_unlock_irqrestore(&kds_lock, lflags);
+		printk(KERN_ERR "ERROR: KDS resource is still in use\n");
+		return -EBUSY;
+	}
+	res->waiters.parent = KDS_INVALID;
+	spin_unlock_irqrestore(&kds_lock, lflags);
+	return 0;
+}
+EXPORT_SYMBOL(kds_resource_term);
+
+int kds_async_waitall(
+		struct kds_resource_set ** const pprset,
+		struct kds_callback      *cb,
+		void                     *callback_parameter,
+		void                     *callback_extra_parameter,
+		int                       number_resources,
+		unsigned long            *exclusive_access_bitmap,
+		struct kds_resource     **resource_list)
+{
+	struct kds_resource_set *rset = NULL;
+	unsigned long lflags;
+	int i;
+	int triggered;
+	int err = -EFAULT;
+
+	BUG_ON(!pprset);
+	BUG_ON(!resource_list);
+	BUG_ON(!cb);
+
+	WARN_ONCE(number_resources > 10, "Waiting on a high number of resources may increase latency, see documentation.");
+
+	rset = kmalloc(sizeof(*rset) + number_resources * sizeof(struct kds_link), GFP_KERNEL);
+	if (!rset)
+	{
+		return -ENOMEM;
+	}
+
+	rset->num_resources = number_resources;
+	rset->pending = number_resources;
+	rset->cb = cb;
+	rset->callback_parameter = callback_parameter;
+	rset->callback_extra_parameter = callback_extra_parameter;
+	INIT_LIST_HEAD(&rset->callback_link);
+	INIT_WORK(&rset->callback_work, kds_queued_callback);
+
+	for (i = 0; i < number_resources; i++)
+	{
+		INIT_LIST_HEAD(&rset->resources[i].link);
+		rset->resources[i].parent = rset;
+	}
+
+	spin_lock_irqsave(&kds_lock, lflags);
+
+	for (i = 0; i < number_resources; i++)
+	{
+		unsigned long link_state = 0;
+
+		if (test_bit(i, exclusive_access_bitmap))
+		{
+			link_state |= KDS_LINK_EXCLUSIVE;
+		}
+
+		/* no-one else waiting? */
+		if (list_empty(&resource_list[i]->waiters.link))
+		{
+			link_state |= KDS_LINK_TRIGGERED;
+			rset->pending--;
+		}
+		/* Adding a non-exclusive and the current tail is a triggered non-exclusive? */
+		else if (((link_state & KDS_LINK_EXCLUSIVE) == 0) &&
+				(((list_entry(resource_list[i]->waiters.link.prev, struct kds_link, link)->state & (KDS_LINK_EXCLUSIVE | KDS_LINK_TRIGGERED)) == KDS_LINK_TRIGGERED)))
+		{
+			link_state |= KDS_LINK_TRIGGERED;
+			rset->pending--;
+		}
+		rset->resources[i].state = link_state;
+
+		/* avoid double wait (hang) */
+		if (!list_empty(&resource_list[i]->waiters.link))
+		{
+			/* adding same rset again? */
+			if (list_entry(resource_list[i]->waiters.link.prev, struct kds_link, link)->parent == rset)
+			{
+				goto roll_back;
+			}
+		}
+		list_add_tail(&rset->resources[i].link, &resource_list[i]->waiters.link);
+	}
+
+	triggered = (rset->pending == 0);
+
+	/* set the pointer before the callback is called so it sees it */
+	*pprset = rset;
+
+	spin_unlock_irqrestore(&kds_lock, lflags);
+
+	if (triggered)
+	{
+		/* all resources obtained, trigger callback */
+		kds_callback_perform(rset);
+	}
+
+	return 0;
+
+roll_back:
+	/* roll back */
+	while (i-- > 0)
+	{
+		list_del(&rset->resources[i].link);
+	}
+	err = -EINVAL;
+
+	spin_unlock_irqrestore(&kds_lock, lflags);
+	kfree(rset);
+	return err;
+}
+EXPORT_SYMBOL(kds_async_waitall);
+
+static void wake_up_sync_call(void *callback_parameter, void *callback_extra_parameter)
+{
+	wait_queue_head_t *wait = (wait_queue_head_t *)callback_parameter;
+	wake_up(wait);
+}
+
+static struct kds_callback sync_cb =
+{
+	wake_up_sync_call,
+	1,
+	NULL,
+};
+
+struct kds_resource_set *kds_waitall(
+		int                   number_resources,
+		unsigned long        *exclusive_access_bitmap,
+		struct kds_resource **resource_list,
+		unsigned long         jiffies_timeout)
+{
+	struct kds_resource_set *rset;
+	unsigned long lflags;
+	int i;
+	int triggered = 0;
+	DECLARE_WAIT_QUEUE_HEAD_ONSTACK(wake);
+
+	rset = kmalloc(sizeof(*rset) + number_resources * sizeof(struct kds_link), GFP_KERNEL);
+	if (!rset)
+		return rset;
+
+	rset->num_resources = number_resources;
+	rset->pending = number_resources;
+	init_waitqueue_head(&rset->wake);
+	INIT_LIST_HEAD(&rset->callback_link);
+	INIT_WORK(&rset->callback_work, kds_queued_callback);
+
+	spin_lock_irqsave(&kds_lock, lflags);
+
+	for (i = 0; i < number_resources; i++)
+	{
+		unsigned long link_state = 0;
+
+		if (test_bit(i, exclusive_access_bitmap))
+		{
+			link_state |= KDS_LINK_EXCLUSIVE;
+		}
+
+		if (list_empty(&resource_list[i]->waiters.link))
+		{
+			link_state |= KDS_LINK_TRIGGERED;
+			rset->pending--;
+		}
+		/* Adding a non-exclusive and the current tail is a triggered non-exclusive? */
+		else if (((link_state & KDS_LINK_EXCLUSIVE) == 0) &&
+				(((list_entry(resource_list[i]->waiters.link.prev, struct kds_link, link)->state & (KDS_LINK_EXCLUSIVE | KDS_LINK_TRIGGERED)) == KDS_LINK_TRIGGERED)))
+		{
+			link_state |= KDS_LINK_TRIGGERED;
+			rset->pending--;
+		}
+
+		INIT_LIST_HEAD(&rset->resources[i].link);
+		rset->resources[i].parent = rset;
+		rset->resources[i].state = link_state;
+
+		/* avoid double wait (hang) */
+		if (!list_empty(&resource_list[i]->waiters.link))
+		{
+			/* adding same rset again? */
+			if (list_entry(resource_list[i]->waiters.link.prev, struct kds_link, link)->parent == rset)
+			{
+				goto roll_back;
+			}
+		}
+
+		list_add_tail(&rset->resources[i].link, &resource_list[i]->waiters.link);
+	}
+
+	if (rset->pending == 0)
+		triggered = 1;
+	else
+	{
+		rset->cb = &sync_cb;
+		rset->callback_parameter = &rset->wake;
+		rset->callback_extra_parameter = NULL;
+	}
+
+	spin_unlock_irqrestore(&kds_lock, lflags);
+
+	if (!triggered)
+	{
+		long wait_res = 0;
+		long timeout = (jiffies_timeout == KDS_WAIT_BLOCKING) ?
+				MAX_SCHEDULE_TIMEOUT : jiffies_timeout;
+
+		if (timeout)
+		{
+			wait_res = wait_event_interruptible_timeout(rset->wake,
+					rset->pending == 0, timeout);
+		}
+
+		if ((wait_res == -ERESTARTSYS) || (wait_res == 0))
+		{
+			/* use \a kds_resource_set_release to roll back */
+			kds_resource_set_release(&rset);
+			return ERR_PTR(wait_res);
+		}
+	}
+	return rset;
+
+roll_back:
+	/* roll back */
+	while (i-- > 0)
+	{
+		list_del(&rset->resources[i].link);
+	}
+
+	spin_unlock_irqrestore(&kds_lock, lflags);
+	kfree(rset);
+	return ERR_PTR(-EINVAL);
+}
+EXPORT_SYMBOL(kds_waitall);
+
+static void __kds_resource_set_release_common(struct kds_resource_set *rset)
+{
+	struct list_head triggered = LIST_HEAD_INIT(triggered);
+	struct kds_resource_set *it;
+	unsigned long lflags;
+	int i;
+
+	spin_lock_irqsave(&kds_lock, lflags);
+
+	for (i = 0; i < rset->num_resources; i++)
+	{
+		struct kds_resource *resource;
+		struct kds_link *it = NULL;
+
+		/* fetch the previous entry on the linked list */
+		it = list_entry(rset->resources[i].link.prev, struct kds_link, link);
+		/* unlink ourself */
+		list_del(&rset->resources[i].link);
+
+		/* any waiters? */
+		if (list_empty(&it->link))
+			continue;
+
+		/* were we the head of the list? (head if prev is a resource) */
+		if (it->parent != KDS_RESOURCE)
+		{
+			if ((it->state & KDS_LINK_TRIGGERED) && !(it->state & KDS_LINK_EXCLUSIVE) )
+			{
+				/*
+				 * previous was triggered and not exclusive, so we
+				 * trigger non-exclusive until end-of-list or first
+				 * exclusive
+				 */
+
+				struct kds_link *it_waiting = it;
+
+				list_for_each_entry(it, &it_waiting->link, link)
+				{
+					/* exclusive found, stop triggering */
+					if (it->state & KDS_LINK_EXCLUSIVE)
+						break;
+
+					it->state |= KDS_LINK_TRIGGERED;
+					/* a parent to update? */
+					if ( it->parent != KDS_RESOURCE )
+					{
+						if (0 == --it->parent->pending)
+						{
+							/* new owner now triggered, track for callback later */
+							list_add(&it->parent->callback_link, &triggered);
+						}
+					}
+				}
+			}
+			continue;
+		}
+
+		/* we were the head, find the kds_resource */
+		resource = container_of(it, struct kds_resource, waiters);
+
+		/* we know there is someone waiting from the any-waiters test above */
+
+		/* find the head of the waiting list */
+		it = list_first_entry(&resource->waiters.link, struct kds_link, link);
+
+		/* new exclusive owner? */
+		if (it->state & KDS_LINK_EXCLUSIVE)
+		{
+			/* link now triggered */
+			it->state |= KDS_LINK_TRIGGERED;
+			/* a parent to update? */
+			if (0 == --it->parent->pending)
+			{
+				/* new owner now triggered, track for callback later */
+				list_add(&it->parent->callback_link, &triggered);
+			}
+		}
+		/* exclusive releasing ? */
+		else if (rset->resources[i].state & KDS_LINK_EXCLUSIVE)
+		{
+			/* trigger non-exclusive until end-of-list or first exclusive */
+			list_for_each_entry(it, &resource->waiters.link, link)
+			{
+				/* exclusive found, stop triggering */
+				if (it->state & KDS_LINK_EXCLUSIVE)
+					break;
+
+				it->state |= KDS_LINK_TRIGGERED;
+				/* a parent to update? */
+				if (0 == --it->parent->pending)
+				{
+					/* new owner now triggered, track for callback later */
+					list_add(&it->parent->callback_link, &triggered);
+				}
+			}
+		}
+	}
+
+	spin_unlock_irqrestore(&kds_lock, lflags);
+
+	while (!list_empty(&triggered))
+	{
+		it = list_first_entry(&triggered, struct kds_resource_set, callback_link);
+		list_del(&it->callback_link);
+		kds_callback_perform(it);
+	}
+}
+
+void kds_resource_set_release(struct kds_resource_set **pprset)
+{
+	struct kds_resource_set *rset;
+
+	rset = cmpxchg(pprset,*pprset,NULL);
+
+	if (!rset)
+	{
+		/* caught a race between a cancelation
+		 * and a completion, nothing to do */
+		return;
+	}
+
+	__kds_resource_set_release_common(rset);
+
+	/*
+	 * Caller is responsible for guaranteeing that callback work is not
+	 * pending (i.e. its running or completed) prior to calling release.
+	 */
+	BUG_ON(work_pending(&rset->callback_work));
+
+	/* free the resource set */
+	kfree(rset);
+}
+EXPORT_SYMBOL(kds_resource_set_release);
+
+void kds_resource_set_release_sync(struct kds_resource_set **pprset)
+{
+	struct kds_resource_set *rset;
+
+	rset = cmpxchg(pprset,*pprset,NULL);
+	if (!rset)
+	{
+		/* caught a race between a cancelation
+		 * and a completion, nothing to do */
+		return;
+	}
+
+	__kds_resource_set_release_common(rset);
+
+	/*
+	 * In the case of a kds async wait cancellation ensure the deferred
+	 * call back does not get scheduled if a trigger fired at the same time
+	 * to release the wait.
+	 */
+	cancel_work_sync(&rset->callback_work);
+
+	/* free the resource set */
+	kfree(rset);
+}
+EXPORT_SYMBOL(kds_resource_set_release_sync);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("ARM Ltd.");
+MODULE_VERSION("1.0");
diff --git a/drivers/gpu/mali-t6xx/Kconfig b/drivers/gpu/mali-t6xx/Kconfig
new file mode 100644
index 0000000..459351e
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/Kconfig
@@ -0,0 +1,32 @@
+menuconfig MALI_T6XX_VERSION_R4P1
+	bool "Mali-T6XX R4P1 version"
+		default n
+	help
+	  Enable building Mali T6XX R4P1 version
+	if MALI_T6XX_VERSION_R4P1
+		source "drivers/gpu/mali-t6xx/r4p1/Kconfig"
+	endif
+
+menu "Mali Platform configuration"
+if MALI_T6XX_VERSION_R3P0 || MALI_T6XX_VERSION_R4P1
+choice
+	prompt "Platform configuration"
+	default MALI_PLATFORM_THIRDPARTY
+	depends on MALI_EXPERT
+	help
+	  Select the SOC platform that contains a Mali-T6XX
+
+config MALI_PLATFORM_VEXPRESS
+	depends on ARCH_VEXPRESS && (ARCH_VEXPRESS_CA9X4 || ARCH_VEXPRESS_CA15X4)
+	bool "Versatile Express"
+config MALI_PLATFORM_GOLDFISH
+	depends on ARCH_GOLDFISH
+	bool "Android Goldfish virtual CPU"
+config MALI_PLATFORM_PBX
+	depends on ARCH_REALVIEW && REALVIEW_EB_A9MP && MACH_REALVIEW_PBX
+	bool "Realview PBX-A9"
+config MALI_PLATFORM_THIRDPARTY
+	bool "Third Party Platform"
+endchoice
+endif # MALI_T6XX_VERSION_R3P0 || MALI_T6XX_VERSION_R4P1
+endmenu
diff --git a/drivers/gpu/mali-t6xx/Makefile b/drivers/gpu/mali-t6xx/Makefile
new file mode 100644
index 0000000..51a5178
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/Makefile
@@ -0,0 +1,2 @@
+obj-$(CONFIG_MALI_T6XX_VERSION_R3P0)                   += r3p0/
+obj-$(CONFIG_MALI_T6XX_VERSION_R4P1)                   += r4p1/
diff --git a/drivers/gpu/mali-t6xx/r4p1/Kbuild b/drivers/gpu/mali-t6xx/r4p1/Kbuild
new file mode 100644
index 0000000..3a62792
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/Kbuild
@@ -0,0 +1,230 @@
+#
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+# Driver version string which is returned to userspace via an ioctl
+MALI_RELEASE_NAME ?= "r4p1-00rel0"
+
+# Paths required for build
+KBASE_PATH = $(src)
+KBASE_PLATFORM_PATH = $(KBASE_PATH)/platform_dummy
+UMP_PATH = $(src)/../../../base
+
+ifeq ($(CONFIG_MALI_ERROR_INJECTION),y)
+MALI_ERROR_INJECT_ON = 1
+endif
+
+# Set up defaults if not defined by build system
+MALI_CUSTOMER_RELEASE ?= 1
+MALI_UNIT_TEST ?= 0
+MALI_KERNEL_TEST_API ?= 0
+MALI_ERROR_INJECT_ON ?= 0
+MALI_MOCK_TEST ?= 0
+MALI_COVERAGE ?= 0
+MALI_INSTRUMENTATION_LEVEL ?= 0
+# This workaround is for what seems to be a compiler bug we observed in
+# GCC 4.7 on AOSP 4.3.  The bug caused an intermittent failure compiling
+# the "_Pragma" syntax, where an error message is returned:
+#
+# "internal compiler error: unspellable token PRAGMA"
+#
+# This regression has thus far only been seen on the GCC 4.7 compiler bundled
+# with AOSP 4.3.0.  So this makefile, intended for in-tree kernel builds
+# which are not known to be used with AOSP, is hardcoded to disable the
+# workaround, i.e. set the define to 0.
+MALI_GCC_WORKAROUND_MIDCOM_4598 ?= 0
+
+# Set up our defines, which will be passed to gcc
+DEFINES = \
+	-DMALI_CUSTOMER_RELEASE=$(MALI_CUSTOMER_RELEASE) \
+	-DMALI_KERNEL_TEST_API=$(MALI_KERNEL_TEST_API) \
+	-DMALI_UNIT_TEST=$(MALI_UNIT_TEST) \
+	-DMALI_ERROR_INJECT_ON=$(MALI_ERROR_INJECT_ON) \
+	-DMALI_MOCK_TEST=$(MALI_MOCK_TEST) \
+	-DMALI_COVERAGE=$(MALI_COVERAGE) \
+	-DMALI_INSTRUMENTATION_LEVEL=$(MALI_INSTRUMENTATION_LEVEL) \
+	-DMALI_RELEASE_NAME=\"$(MALI_RELEASE_NAME)\" \
+	-DMALI_GCC_WORKAROUND_MIDCOM_4598=$(MALI_GCC_WORKAROUND_MIDCOM_4598)
+
+ifeq ($(KBUILD_EXTMOD),)
+# in-tree
+DEFINES +=-DMALI_KBASE_THIRDPARTY_PATH=../../$(src)/platform/$(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME)
+else
+# out-of-tree
+DEFINES +=-DMALI_KBASE_THIRDPARTY_PATH=$(src)/platform/$(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME)
+endif
+
+DEFINES += -I$(srctree)/drivers/staging/android
+
+# Use our defines when compiling
+ccflags-y += $(DEFINES) -I$(KBASE_PATH)   -I$(KBASE_PLATFORM_PATH) -I$(UMP_PATH) -I$(srctree)/include/linux
+subdir-ccflags-y += $(DEFINES) -I$(KBASE_PATH)   -I$(KBASE_PLATFORM_PATH) -I$(OSK_PATH) -I$(UMP_PATH) -I$(srctree)/include/linux
+
+SRC := \
+	mali_kbase_device.c \
+	mali_kbase_cache_policy.c \
+	mali_kbase_mem.c \
+	mali_kbase_mmu.c \
+	mali_kbase_jd.c \
+	mali_kbase_jm.c \
+	mali_kbase_cpuprops.c \
+	mali_kbase_gpuprops.c \
+	mali_kbase_js.c \
+	mali_kbase_js_affinity.c \
+	mali_kbase_js_ctx_attr.c \
+	mali_kbase_event.c \
+	mali_kbase_context.c \
+	mali_kbase_pm.c \
+	mali_kbase_pm_driver.c \
+	mali_kbase_pm_metrics.c \
+	mali_kbase_pm_ca.c \
+	mali_kbase_pm_ca_fixed.c \
+	mali_kbase_pm_always_on.c \
+	mali_kbase_pm_coarse_demand.c \
+	mali_kbase_pm_demand.c \
+	mali_kbase_pm_policy.c \
+	mali_kbase_config.c \
+	mali_kbase_security.c \
+	mali_kbase_instr.c \
+	mali_kbase_softjobs.c \
+	mali_kbase_10969_workaround.c \
+	mali_kbase_hw.c \
+	mali_kbase_utility.c \
+	mali_kbase_mem_lowlevel.c \
+	mali_kbase_debug.c \
+	mali_kbase_trace_timeline.c \
+	mali_kbase_gpu_memory_debugfs.c \
+	mali_kbase_mem_linux.c \
+	mali_kbase_core_linux.c \
+	mali_kbase_sync.c \
+	mali_kbase_sync_user.c \
+	mali_kbase_replay.c \
+
+ifeq ($(MALI_CUSTOMER_RELEASE),0)
+SRC += \
+     mali_kbase_pm_ca_random.c \
+     mali_kbase_pm_demand_always_powered.c \
+     mali_kbase_pm_fast_start.c
+endif
+
+# Job Scheduler Policy: Completely Fair Scheduler
+SRC += mali_kbase_js_policy_cfs.c
+
+ifeq ($(CONFIG_MACH_MANTA),y)
+	SRC += mali_kbase_mem_alloc_carveout.c
+else
+	SRC += mali_kbase_mem_alloc.c
+endif
+
+# ensure GPL version of malisw gets pulled in
+ccflags-y += -I$(KBASE_PATH)
+
+ifeq ($(CONFIG_MALI_NO_MALI),y)
+	# Dummy model
+	SRC += mali_kbase_model_dummy.c
+	SRC += mali_kbase_model_linux.c
+	# HW error simulation
+	SRC += mali_kbase_model_error_generator.c
+endif
+
+ifeq ($(MALI_MOCK_TEST),1)
+	# Test functionality
+	SRC += tests/internal/src/mock/mali_kbase_pm_driver_mock.c
+endif
+
+# in-tree/out-of-tree logic needs to be slightly different to determine if a file is present
+ifeq ($(KBUILD_EXTMOD),)
+# in-tree
+MALI_METRICS_PATH = $(srctree)/drivers/gpu/arm/midgard
+else
+# out-of-tree
+MALI_METRICS_PATH = $(KBUILD_EXTMOD)
+endif
+
+# Use vsync metrics example using PL111 driver, if available
+ifeq ($(wildcard $(MALI_METRICS_PATH)/mali_kbase_pm_metrics_linux.c),)
+	SRC += mali_kbase_pm_metrics_dummy.c
+else
+	SRC += mali_kbase_pm_metrics_linux.c
+endif
+
+ifeq ($(CONFIG_MALI_PLATFORM_FAKE),y)
+	SRC += mali_kbase_platform_fake.c
+
+	ifeq ($(CONFIG_MALI_PLATFORM_VEXPRESS),y)
+		SRC += platform/vexpress/mali_kbase_config_vexpress.c \
+		platform/vexpress/mali_kbase_cpu_vexpress.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_RTSM_VE),y)
+		SRC += platform/rtsm_ve/mali_kbase_config_vexpress.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_JUNO),y)
+		SRC += platform/juno/mali_kbase_config_vexpress.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_VEXPRESS_VIRTEX7_40MHZ),y)
+		SRC += platform/vexpress_virtex7_40mhz/mali_kbase_config_vexpress.c \
+		platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_VEXPRESS_6XVIRTEX7_10MHZ),y)
+		SRC += platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_vexpress.c \
+		platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_GOLDFISH),y)
+		SRC += platform/goldfish/mali_kbase_config_goldfish.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_PBX),y)
+		SRC += platform/pbx/mali_kbase_config_pbx.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_PANDA),y)
+		SRC += platform/panda/mali_kbase_config_panda.c
+	endif
+
+	ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
+	ifeq ($(CONFIG_MALI_MIDGARD),m)
+	# remove begin and end quotes from the Kconfig string type
+	platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+	MALI_PLATFORM_THIRDPARTY_DIR := platform/$(platform_name)
+	include  $(src)/platform/$(platform_name)/Kbuild
+	else ifeq ($(CONFIG_MALI_MIDGARD),y)
+	obj-$(CONFIG_MALI_MIDGARD) += platform/
+	endif
+	endif
+endif # CONFIG_MALI_PLATFORM_FAKE=y
+
+ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
+ifeq ($(CONFIG_MALI_MIDGARD),m)
+# remove begin and end quotes from the Kconfig string type
+platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+MALI_PLATFORM_THIRDPARTY_DIR := platform/$(platform_name)
+include  $(src)/platform/$(platform_name)/Kbuild
+else ifeq ($(CONFIG_MALI_MIDGARD),y)
+obj-$(CONFIG_MALI_MIDGARD) += platform/
+endif
+endif
+
+# Tell the Linux build system from which .o file to create the kernel module
+obj-$(CONFIG_MALI_MIDGARD) += mali_kbase.o
+
+# Tell the Linux build system to enable building of our .c files
+mali_kbase-y := $(SRC:.c=.o)
+
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/Kconfig b/drivers/gpu/mali-t6xx/r4p1/Kconfig
new file mode 100644
index 0000000..05ede51
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/Kconfig
@@ -0,0 +1,159 @@
+#
+# (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+config MALI_MIDGARD
+	tristate "Mali Midgard series support"
+	default n
+        depends on MALI_T6XX_VERSION_R4P1
+	help
+	  Enable this option to build support for a ARM Mali Midgard GPU.
+
+	  To compile this driver as a module, choose M here:
+	  this will generate a single module, called mali_kbase.
+
+if MALI_MIDGARD
+config MALI_GATOR_SUPPORT
+	bool "Streamline Debug support"
+	depends on MALI_MIDGARD
+	default n
+	help
+	  Adds diagnostic support for use with the ARM Streamline Performance Analyzer.
+	  You will need the Gator device driver already loaded before loading this driver when enabling
+	  Streamline debug support.
+
+config MALI_MIDGARD_DVFS
+	bool "Enable DVFS"
+	depends on MALI_MIDGARD
+	default n
+	help
+	  Choose this option to enable DVFS in the Mali Midgard DDK.
+
+config MALI_MIDGARD_RT_PM
+	bool "Enable Runtime power management"
+	depends on MALI_MIDGARD
+	depends on PM_RUNTIME
+	default n
+	help
+	  Choose this option to enable runtime power management in the Mali Midgard DDK.
+
+config MALI_MIDGARD_ENABLE_TRACE
+	bool "Enable kbase tracing"
+	depends on MALI_MIDGARD
+	default n
+	help
+	  Enables tracing in kbase.  Trace log available through
+	  the "mali_trace" debugfs file, when the CONFIG_DEBUG_FS is enabled
+
+config MALI_MIDGARD_DEBUG_SYS
+	bool "Enable sysfs for the Mali Midgard DDK "
+	depends on MALI_MIDGARD && SYSFS
+	default n
+	help
+	  Enables sysfs for the Mali Midgard DDK. Set/Monitor the Mali Midgard DDK
+
+# MALI_EXPERT configuration options
+
+menuconfig MALI_EXPERT
+	depends on MALI_MIDGARD
+	bool "Enable Expert Settings"
+	default n
+	help
+	  Enabling this option and modifying the default settings may produce a driver with performance or
+	  other limitations.
+
+if MALI_EXPERT
+config MALI_DEBUG_SHADER_SPLIT_FS
+	bool "Allow mapping of shader cores via sysfs"
+	depends on MALI_MIDGARD && MALI_MIDGARD_DEBUG_SYS && MALI_EXPERT
+	default n
+	help
+	  Select this option to provide a sysfs entry for runtime configuration of shader
+	  core affinity masks.
+
+config MALI_PLATFORM_FAKE
+	bool "Enable fake platform device support"
+	depends on MALI_MIDGARD && MALI_EXPERT
+	default n
+	help
+	  When you start to work with the Mali Midgard series device driver the platform-specific code of
+	  the Linux kernel for your platform may not be complete. In this situation the kernel device driver
+	  supports creating the platform device outside of the Linux platform-specific code.
+	  Enable this option if would like to use a platform device configuration from within the device driver.
+
+
+config MALI_PLATFORM_THIRDPARTY_NAME
+	depends on MALI_MIDGARD && MALI_PLATFORM_THIRDPARTY && MALI_EXPERT
+	string "Third party platform name"
+	help
+	  Enter the name of a third party platform that is supported. The third part configuration
+	  file must be in midgard/config/tpip/mali_kbase_config_xxx.c where xxx is the name
+	  specified here.
+
+config MALI_DEBUG
+	bool "Debug build"
+	depends on MALI_MIDGARD && MALI_EXPERT
+	default n
+	help
+	  Select this option for increased checking and reporting of errors.
+
+config MALI_NO_MALI
+	bool "No Mali"
+	depends on MALI_MIDGARD && MALI_EXPERT
+	default n
+	help
+	  This can be used to test the driver in a simulated environment
+	  whereby the hardware is not physically present. If the hardware is physically
+	  present it will not be used. This can be used to test the majority of the
+	  driver without needing actual hardware or for software benchmarking.
+	  All calls to the simulated hardware will complete immediately as if the hardware
+	  completed the task.
+
+config MALI_ERROR_INJECT
+	bool "Error injection"
+	depends on MALI_MIDGARD && MALI_EXPERT && MALI_NO_MALI
+	default n
+	help
+	  Enables insertion of errors to test module failure and recovery mechanisms.
+
+config MALI_TRACE_TIMELINE
+	bool "Timeline tracing"
+	depends on MALI_MIDGARD && MALI_EXPERT
+	default n
+	help
+	  Enables timeline tracing through the kernel tracepoint system.
+
+config MALI_SYSTEM_TRACE
+	bool "Enable system event tracing support"
+	depends on MALI_MIDGARD && MALI_EXPERT
+	default n
+	help
+	  Choose this option to enable system trace events for each
+	  kbase event.	This is typically used for debugging but has
+	  minimal overhead when not in use. Enable only if you know what
+	  you are doing.
+
+endif #MALI_EXPERT
+
+config MALI_GPU_TRACEPOINTS
+	bool "Enable GPU tracepoints"
+	depends on MALI_MIDGARD && ANDROID
+	select GPU_TRACEPOINTS
+	help
+	  Enables GPU tracepoints using Android trace event definitions.
+
+source "drivers/gpu/mali-t6xx/r4p1/platform/Kconfig"
+
+endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/Makefile b/drivers/gpu/mali-t6xx/r4p1/Makefile
new file mode 100644
index 0000000..9820be2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/Makefile
@@ -0,0 +1,36 @@
+#
+# (C) COPYRIGHT 2010-2013 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+KDIR ?= /lib/modules/$(shell uname -r)/build
+
+UMP_PATH_RELATIVE = $(CURDIR)/../../../base/ump
+KBASE_PATH_RELATIVE = $(CURDIR)
+KDS_PATH_RELATIVE = $(CURDIR)/../../../..
+EXTRA_SYMBOLS = $(UMP_PATH_RELATIVE)/src/Module.symvers
+
+ifeq ($(MALI_UNIT_TEST), 1)
+	EXTRA_SYMBOLS += $(KBASE_PATH_RELATIVE)/tests/internal/src/kernel_assert_module/linux/Module.symvers
+endif
+
+# GPL driver supports KDS
+EXTRA_SYMBOLS += $(KDS_PATH_RELATIVE)/drivers/base/kds/Module.symvers
+
+# we get the symbols from modules using KBUILD_EXTRA_SYMBOLS to prevent warnings about unknown functions
+all:
+	$(MAKE) -C $(KDIR) M=$(CURDIR) EXTRA_CFLAGS="-I$(CURDIR)/../../../../include $(SCONS_CFLAGS)" $(SCONS_CONFIGS) KBUILD_EXTRA_SYMBOLS="$(EXTRA_SYMBOLS)" modules
+
+clean:
+	$(MAKE) -C $(KDIR) M=$(CURDIR) clean
diff --git a/drivers/gpu/mali-t6xx/r4p1/Makefile.kbase b/drivers/gpu/mali-t6xx/r4p1/Makefile.kbase
new file mode 100644
index 0000000..2bef9c2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/Makefile.kbase
@@ -0,0 +1,17 @@
+#
+# (C) COPYRIGHT 2010 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+EXTRA_CFLAGS += -I$(ROOT) -I$(KBASE_PATH) -I$(OSK_PATH)/src/linux/include -I$(KBASE_PATH)/platform_$(PLATFORM)
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_base_hwconfig.h b/drivers/gpu/mali-t6xx/r4p1/mali_base_hwconfig.h
new file mode 100644
index 0000000..5f5c945
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_base_hwconfig.h
@@ -0,0 +1,711 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Software workarounds configuration for Hardware issues.
+ */
+
+#ifndef _BASE_HWCONFIG_H_
+#define _BASE_HWCONFIG_H_
+
+#include <malisw/mali_malisw.h>
+
+/**
+ * List of all hw features.
+ *
+ */
+typedef enum base_hw_feature {
+	/* Allow soft/hard stopping of job depending on job chain flag */
+	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
+
+	/* Allow writes to SHADER_PWRON and TILER_PWRON registers while these cores are currently transitioning to OFF power state */
+	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
+
+	/* The BASE_HW_FEATURE_END value must be the last feature listed in this enumeration
+	 * and must be the last value in each array that contains the list of features
+	 * for a particular HW version.
+	 */
+	BASE_HW_FEATURE_END
+} base_hw_feature;
+
+static const base_hw_feature base_hw_features_generic[] = {
+	BASE_HW_FEATURE_END
+}; 
+
+static const base_hw_feature base_hw_features_t76x[] = {
+	BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION,
+	BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS,
+	BASE_HW_FEATURE_END
+};
+
+
+/**
+ * List of all workarounds.
+ *
+ */
+
+typedef enum base_hw_issue {
+
+	/* The current version of the model doesn't support Soft-Stop */
+	BASE_HW_ISSUE_5736,
+
+	/* Need way to guarantee that all previously-translated memory accesses are commited */
+	BASE_HW_ISSUE_6367,
+
+	/* Result swizzling doesn't work for GRDESC/GRDESC_DER */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_6398,
+
+	/* Unaligned load stores crossing 128 bit boundaries will fail */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_6402,
+
+	/* On job complete with non-done the cache is not flushed */
+	BASE_HW_ISSUE_6787,
+
+	/* WLS allocation does not respect the Instances field in the Thread Storage Descriptor */
+	BASE_HW_ISSUE_7027,
+
+	/* The clamp integer coordinate flag bit of the sampler descriptor is reserved */
+	BASE_HW_ISSUE_7144,
+
+	/* TEX_INDEX LOD is always use converted */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_8073,
+
+	/* Write of PRFCNT_CONFIG_MODE_MANUAL to PRFCNT_CONFIG causes a instrumentation dump if
+	   PRFCNT_TILER_EN is enabled */
+	BASE_HW_ISSUE_8186,
+
+	/* Do not set .skip flag on the GRDESC, GRDESC_DER, DELTA, MOV, and NOP texturing instructions */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_8215,
+
+	/* TIB: Reports faults from a vtile which has not yet been allocated */
+	BASE_HW_ISSUE_8245,
+
+	/* WLMA memory goes wrong when run on shader cores other than core 0. */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_8250,
+
+	/* Hierz doesn't work when stenciling is enabled */
+	BASE_HW_ISSUE_8260,
+
+	/* Livelock in L0 icache */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_8280,
+
+	/* uTLB deadlock could occur when writing to an invalid page at the same time as
+	 * access to a valid page in the same uTLB cache line ( == 4 PTEs == 16K block of mapping) */
+	BASE_HW_ISSUE_8316,
+
+	/* TLS base address mismatch, must stay below 1MB TLS */
+	BASE_HW_ISSUE_8381,
+
+	/* HT: TERMINATE for RUN command ignored if previous LOAD_DESCRIPTOR is still executing */
+	BASE_HW_ISSUE_8394,
+
+	/* CSE : Sends a TERMINATED response for a task that should not be terminated */
+	/* (Note that PRLAM-8379 also uses this workaround) */
+	BASE_HW_ISSUE_8401,
+
+	/* Repeatedly Soft-stopping a job chain consisting of (Vertex Shader, Cache Flush, Tiler)
+	 * jobs causes 0x58 error on tiler job. */
+	BASE_HW_ISSUE_8408,
+
+	/* Disable the Pause Buffer in the LS pipe. */
+	BASE_HW_ISSUE_8443,
+
+	/* Stencil test enable 1->0 sticks */
+	BASE_HW_ISSUE_8456,
+
+	/* Tiler heap issue using FBOs or multiple processes using the tiler simultaneously */
+	/* (Note that PRLAM-9049 also uses this work-around) */
+	BASE_HW_ISSUE_8564,
+
+	/* Fragments are clamped instead of discarded when fragment depth bound op is discard and depth datum source is shader. */
+	BASE_HW_ISSUE_8634,
+
+	/* Livelock issue using atomic instructions (particularly when using atomic_cmpxchg as a spinlock) */
+	BASE_HW_ISSUE_8791,
+
+	/* Fused jobs are not supported (for various reasons) */
+	/* Jobs with relaxed dependencies do not support soft-stop */
+	/* (Note that PRLAM-8803, PRLAM-8393, PRLAM-8559, PRLAM-8601 & PRLAM-8607 all use this work-around) */
+	BASE_HW_ISSUE_8803,
+
+	/* Blend shader output is wrong for certain formats */
+	BASE_HW_ISSUE_8833,
+
+	/* Occlusion queries can create false 0 result in boolean and counter modes */
+	BASE_HW_ISSUE_8879,
+
+	/* Output has half intensity with blend shaders enabled on 8xMSAA. */
+	BASE_HW_ISSUE_8896,
+
+	/* 8xMSAA does not work with CRC */
+	BASE_HW_ISSUE_8975,
+
+	/* Boolean occlusion queries don't work properly due to sdc issue. */
+	BASE_HW_ISSUE_8986,
+
+	/* Change in RMUs in use causes problems related with the core's SDC */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_8987,
+
+	/* Occlusion query result is not updated if color writes are disabled. */
+	BASE_HW_ISSUE_9010,
+
+	/* Problem with number of work registers in the RSD if set to 0 */
+	BASE_HW_ISSUE_9275,
+
+	/* Translate load/store moves into decode instruction */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_9418,
+
+	/* Incorrect coverage mask for 8xMSAA */
+	BASE_HW_ISSUE_9423,
+
+	/* Compute endpoint has a 4-deep queue of tasks, meaning a soft stop won't complete until all 4 tasks have completed */
+	BASE_HW_ISSUE_9435,
+
+	/* HT: Tiler returns TERMINATED for command that hasn't been terminated */
+	BASE_HW_ISSUE_9510,
+
+	/* Livelock issue using atomic_cmpxchg */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_9566,
+
+	/* Occasionally the GPU will issue multiple page faults for the same address before the MMU page table has been read by the GPU */
+	BASE_HW_ISSUE_9630,
+
+	/* Must clear the 64 byte private state of the tiler information */
+	BASE_HW_ISSUE_10127,
+
+	/* RA DCD load request to SDC returns invalid load ignore causing colour buffer mismatch */
+	BASE_HW_ISSUE_10327,
+
+	/* Occlusion query result may be updated prematurely when fragment shader alters coverage */
+	BASE_HW_ISSUE_10410,
+
+	/* TEXGRD doesn't honor Sampler Descriptor LOD clamps nor bias */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_10471,
+
+	/* MAG / MIN filter selection happens after image descriptor clamps were applied */
+	BASE_HW_ISSUE_10472,
+
+	/* GPU interprets sampler and image descriptor pointer array sizes as one bigger than they are defined in midg structures */
+	BASE_HW_ISSUE_10487,
+
+	/* ld_special 0x1n applies SRGB conversion */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_10607,
+
+	/* LD_SPECIAL instruction reads incorrect RAW tile buffer value when internal tib format is R10G10B10A2 */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_10632,
+
+	/* MMU TLB invalidation hazards */
+	BASE_HW_ISSUE_10649,
+
+	/* Missing cache flush in multi core-group configuration */
+	BASE_HW_ISSUE_10676,
+
+	/* Indexed format 95 cannot be used with a component swizzle of "set to 1" when sampled as integer texture */
+	BASE_HW_ISSUE_10682,
+
+	/* sometimes HW doesn't invalidate cached VPDs when it has to */
+	BASE_HW_ISSUE_10684,
+
+	/* Chicken bit on t72x to work for a HW workaround in compiler */
+	BASE_HW_ISSUE_10797,
+
+	/* Soft-stopping fragment jobs might fail with TILE_RANGE_FAULT */
+	BASE_HW_ISSUE_10817,
+
+	/* Fragment frontend heuristic bias to force early-z required */
+	BASE_HW_ISSUE_10821,
+
+	/* Intermittent missing interrupt on job completion */
+	BASE_HW_ISSUE_10883,
+
+	/* Depth bounds incorrectly normalized in hierz depth bounds test */
+	BASE_HW_ISSUE_10931,
+
+	/* Incorrect cubemap sampling */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_10946,
+
+	/* Soft-stopping fragment jobs might fail with TILE_RANGE_ERROR (similar to issue 10817) and can use BASE_HW_ISSUE_10817 workaround  */
+	BASE_HW_ISSUE_10959,
+
+	/* Soft-stopped fragment shader job can restart with out-of-bound restart index  */
+	BASE_HW_ISSUE_10969,
+
+	/* Instanced arrays conformance fail, workaround by unrolling */
+	BASE_HW_ISSUE_10984,
+
+	/* TEX_INDEX lod selection (immediate , register) not working with 8.8 encoding for levels > 1 */
+	/* NOTE: compiler workaround: keep in sync with _essl_hwrev_needs_workaround() */
+	BASE_HW_ISSUE_10995,
+
+	/* LD_SPECIAL instruction reads incorrect RAW tile buffer value when internal tib format is RGB565 or RGBA5551 */
+	BASE_HW_ISSUE_11012,
+
+	/* Race condition can cause tile list corruption */
+	BASE_HW_ISSUE_11020,
+
+	/* Write buffer can cause tile list corruption */
+	BASE_HW_ISSUE_11024,
+
+	/* Pause buffer can cause a fragment job hang */
+	BASE_HW_ISSUE_11035,
+
+	/* T76X hw issues */
+
+	/* Partial 16xMSAA support */
+	BASE_HW_ISSUE_T76X_26,
+
+	/* RTD doesn't specify the row stride for AFBC surfaces. */
+	BASE_HW_ISSUE_T76X_3086,
+
+	/* Clear encoder state for a hard stopped fragment job which is AFBC
+	 * encoded by soft resetting the GPU. Only for T76X r0p0, r0p1 and r0p1_50rel0
+	 */
+	BASE_HW_ISSUE_T76X_3542,
+
+	/* Do not use 8xMSAA with 16x8 pixel tile size or 16xMSAA with 8x8 pixel
+	 * tile size.
+	 */
+	BASE_HW_ISSUE_T76X_3556,
+
+	/* T76X cannot disable uses_discard even if depth and stencil are read-only. */
+	BASE_HW_ISSUE_T76X_3700,
+
+	/* Preload ignores any size or bounding box restrictions of the output image. */
+	BASE_HW_ISSUE_T76X_3793,
+
+	/* The BASE_HW_ISSUE_END value must be the last issue listed in this enumeration
+	 * and must be the last value in each array that contains the list of workarounds
+	 * for a particular HW version.
+	 */
+	BASE_HW_ISSUE_END
+} base_hw_issue;
+
+/**
+ * Workarounds configuration for each HW revision
+ */
+/* Mali T60x r0p0-15dev0 - 2011-W39-stable-9 */
+static const base_hw_issue base_hw_issues_t60x_r0p0_15dev0[] = {
+	BASE_HW_ISSUE_6367,
+	BASE_HW_ISSUE_6398,
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_6787,
+	BASE_HW_ISSUE_7027,
+	BASE_HW_ISSUE_7144,
+	BASE_HW_ISSUE_8073,
+	BASE_HW_ISSUE_8186,
+	BASE_HW_ISSUE_8215,
+	BASE_HW_ISSUE_8245,
+	BASE_HW_ISSUE_8250,
+	BASE_HW_ISSUE_8260,
+	BASE_HW_ISSUE_8280,
+	BASE_HW_ISSUE_8316,
+	BASE_HW_ISSUE_8381,
+	BASE_HW_ISSUE_8394,
+	BASE_HW_ISSUE_8401,
+	BASE_HW_ISSUE_8408,
+	BASE_HW_ISSUE_8443,
+	BASE_HW_ISSUE_8456,
+	BASE_HW_ISSUE_8564,
+	BASE_HW_ISSUE_8634,
+	BASE_HW_ISSUE_8791,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_8833,
+	BASE_HW_ISSUE_8896,
+	BASE_HW_ISSUE_8975,
+	BASE_HW_ISSUE_8986,
+	BASE_HW_ISSUE_8987,
+	BASE_HW_ISSUE_9010,
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9418,
+	BASE_HW_ISSUE_9423,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_9510,
+	BASE_HW_ISSUE_9566,
+	BASE_HW_ISSUE_9630,
+	BASE_HW_ISSUE_10410,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10487,
+	BASE_HW_ISSUE_10607,
+	BASE_HW_ISSUE_10632,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10676,
+	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_10969,
+	BASE_HW_ISSUE_10984,
+	BASE_HW_ISSUE_10995,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11035,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T60x r0p0-00rel0 - 2011-W46-stable-13c */
+static const base_hw_issue base_hw_issues_t60x_r0p0_eac[] = {
+	BASE_HW_ISSUE_6367,
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_6787,
+	BASE_HW_ISSUE_7027,
+	BASE_HW_ISSUE_8408,
+	BASE_HW_ISSUE_8564,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_8975,
+	BASE_HW_ISSUE_9010,
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9418,
+	BASE_HW_ISSUE_9423,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_9510,
+	BASE_HW_ISSUE_10410,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10487,
+	BASE_HW_ISSUE_10607,
+	BASE_HW_ISSUE_10632,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10676,
+	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_10969,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11035,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T60x r0p1 */
+static const base_hw_issue base_hw_issues_t60x_r0p1[] = {
+	BASE_HW_ISSUE_6367,
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_6787,
+	BASE_HW_ISSUE_7027,
+	BASE_HW_ISSUE_8408,
+	BASE_HW_ISSUE_8564,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_8975,
+	BASE_HW_ISSUE_9010,
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_9510,
+	BASE_HW_ISSUE_10410,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10487,
+	BASE_HW_ISSUE_10607,
+	BASE_HW_ISSUE_10632,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10676,
+	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11035,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T62x r0p1 */
+static const base_hw_issue base_hw_issues_t62x_r0p1[] = {
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10127,
+	BASE_HW_ISSUE_10327,
+	BASE_HW_ISSUE_10410,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10487,
+	BASE_HW_ISSUE_10607,
+	BASE_HW_ISSUE_10632,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10676,
+	BASE_HW_ISSUE_10682,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10817,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_10959,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	BASE_HW_ISSUE_11035,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T62x r1p0 */
+static const base_hw_issue base_hw_issues_t62x_r1p0[] = {
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_10959,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T62x r1p1 */
+static const base_hw_issue base_hw_issues_t62x_r1p1[] =
+{
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_10959,
+	BASE_HW_ISSUE_11012,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r0p0 */
+static const base_hw_issue base_hw_issues_t76x_r0p0[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	BASE_HW_ISSUE_T76X_26,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3542,
+	BASE_HW_ISSUE_T76X_3556,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r0p1 */
+static const base_hw_issue base_hw_issues_t76x_r0p1[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	BASE_HW_ISSUE_T76X_26,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3542,
+	BASE_HW_ISSUE_T76X_3556,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r0p1_50rel0 */
+static const base_hw_issue base_hw_issues_t76x_r0p1_50rel0[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_T76X_26,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3542,
+	BASE_HW_ISSUE_T76X_3556,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r0p2 */
+static const base_hw_issue base_hw_issues_t76x_r0p2[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	BASE_HW_ISSUE_T76X_26,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3542,
+	BASE_HW_ISSUE_T76X_3556,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r0p3 */
+static const base_hw_issue base_hw_issues_t76x_r0p3[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_T76X_26,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3542,
+	BASE_HW_ISSUE_T76X_3556,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T76x r1p0 */
+static const base_hw_issue base_hw_issues_t76x_r1p0[] = {
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10946,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+
+/* Mali T72x r0p0 */
+static const base_hw_issue base_hw_issues_t72x_r0p0[] = {
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10797,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Mali T72x r1p0 */
+static const base_hw_issue base_hw_issues_t72x_r1p0[] = {
+	BASE_HW_ISSUE_6402,
+	BASE_HW_ISSUE_8803,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10649,
+	BASE_HW_ISSUE_10684,
+	BASE_HW_ISSUE_10797,
+	BASE_HW_ISSUE_10821,
+	BASE_HW_ISSUE_10883,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_10946,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+/* Model configuration
+ */
+static const base_hw_issue base_hw_issues_model_t72x[] =
+{
+	BASE_HW_ISSUE_5736,
+	BASE_HW_ISSUE_6402, /* NOTE: Fix is present in model r125162 but is not enabled until RTL is fixed */
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10471,
+	BASE_HW_ISSUE_10797,
+	BASE_HW_ISSUE_10931,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+static const base_hw_issue base_hw_issues_model_t7xx[] =
+{
+	BASE_HW_ISSUE_5736,
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	BASE_HW_ISSUE_T76X_3086,
+	BASE_HW_ISSUE_T76X_3700,
+	BASE_HW_ISSUE_T76X_3793,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+static const base_hw_issue base_hw_issues_model_t6xx[] =
+{
+	BASE_HW_ISSUE_5736,
+	BASE_HW_ISSUE_6402, /* NOTE: Fix is present in model r125162 but is not enabled until RTL is fixed */
+	BASE_HW_ISSUE_9275,
+	BASE_HW_ISSUE_9435,
+	BASE_HW_ISSUE_10472,
+	BASE_HW_ISSUE_10931,
+	BASE_HW_ISSUE_11012,
+	BASE_HW_ISSUE_11020,
+	BASE_HW_ISSUE_11024,
+	/* List of hardware issues must end with BASE_HW_ISSUE_END */
+	BASE_HW_ISSUE_END
+};
+
+#endif				/* _BASE_HWCONFIG_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel.h b/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel.h
new file mode 100644
index 0000000..53e6431
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel.h
@@ -0,0 +1,1803 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Base structures shared with the kernel.
+ */
+
+#ifndef _BASE_KERNEL_H_
+#define _BASE_KERNEL_H_
+
+/* For now we support the legacy API as well as the new API */
+#define BASE_LEGACY_JD_API 1
+
+typedef mali_addr64 base_mem_handle;
+
+#include "mali_base_mem_priv.h"
+#include "mali_kbase_profiling_gator_api.h"
+
+/*
+ * Dependency stuff, keep it private for now. May want to expose it if
+ * we decide to make the number of semaphores a configurable
+ * option.
+ */
+#define BASE_JD_ATOM_COUNT              256
+
+#define BASEP_JD_SEM_PER_WORD_LOG2      5
+#define BASEP_JD_SEM_PER_WORD           (1 << BASEP_JD_SEM_PER_WORD_LOG2)
+#define BASEP_JD_SEM_WORD_NR(x)         ((x) >> BASEP_JD_SEM_PER_WORD_LOG2)
+#define BASEP_JD_SEM_MASK_IN_WORD(x)    (1 << ((x) & (BASEP_JD_SEM_PER_WORD - 1)))
+#define BASEP_JD_SEM_ARRAY_SIZE         BASEP_JD_SEM_WORD_NR(BASE_JD_ATOM_COUNT)
+
+#if BASE_LEGACY_JD_API
+/* Size of the ring buffer */
+#define BASEP_JCTX_RB_NRPAGES           4
+#endif				/* BASE_LEGACY_JD_API */
+
+#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 3
+
+#define BASE_MAX_COHERENT_GROUPS 16
+
+#if defined CDBG_ASSERT
+#define LOCAL_ASSERT CDBG_ASSERT
+#elif defined KBASE_DEBUG_ASSERT
+#define LOCAL_ASSERT KBASE_DEBUG_ASSERT
+#else
+#error assert macro not defined!
+#endif
+
+#if defined PAGE_MASK
+#define LOCAL_PAGE_LSB ~PAGE_MASK
+#else
+#include <osu/mali_osu.h>
+
+#if defined OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+#define LOCAL_PAGE_LSB ((1ul << OSU_CONFIG_CPU_PAGE_SIZE_LOG2) - 1)
+#else
+#error Failed to find page size
+#endif
+#endif
+
+/** 32/64-bit neutral way to represent pointers */
+typedef union kbase_pointer {
+	void *value;	  /**< client should store their pointers here */
+	u32 compat_value; /**< 64-bit kernels should fetch value here when handling 32-bit clients */
+	u64 sizer;	  /**< Force 64-bit storage for all clients regardless */
+} kbase_pointer;
+
+/**
+ * @addtogroup base_user_api User-side Base APIs
+ * @{
+ */
+
+/**
+ * @addtogroup base_user_api_memory User-side Base Memory APIs
+ * @{
+ */
+
+/**
+ * @brief Memory allocation, access/hint flags
+ *
+ * A combination of MEM_PROT/MEM_HINT flags must be passed to each allocator
+ * in order to determine the best cache policy. Some combinations are
+ * of course invalid (eg @c MEM_PROT_CPU_WR | @c MEM_HINT_CPU_RD),
+ * which defines a @a write-only region on the CPU side, which is
+ * heavily read by the CPU...
+ * Other flags are only meaningful to a particular allocator.
+ * More flags can be added to this list, as long as they don't clash
+ * (see ::BASE_MEM_FLAGS_NR_BITS for the number of the first free bit).
+ */
+typedef u32 base_mem_alloc_flags;
+
+/**
+ * @brief Memory allocation, access/hint flags
+ *
+ * See ::base_mem_alloc_flags.
+ *
+ */
+enum {
+	BASE_MEM_PROT_CPU_RD = (1U << 0),      /**< Read access CPU side */
+	BASE_MEM_PROT_CPU_WR = (1U << 1),      /**< Write access CPU side */
+	BASE_MEM_PROT_GPU_RD = (1U << 2),      /**< Read access GPU side */
+	BASE_MEM_PROT_GPU_WR = (1U << 3),      /**< Write access GPU side */
+	BASE_MEM_PROT_GPU_EX = (1U << 4),      /**< Execute allowed on the GPU side */
+
+	/* Note that the HINT flags are obsolete now. If you want the memory
+	 * to be cached on the CPU please use the BASE_MEM_CACHED_CPU flag
+	 */
+	BASE_MEM_HINT_CPU_RD = (1U << 5),      /**< Heavily read CPU side - OBSOLETE */
+	BASE_MEM_HINT_CPU_WR = (1U << 6),      /**< Heavily written CPU side - OBSOLETE */
+	BASE_MEM_HINT_GPU_RD = (1U << 7),      /**< Heavily read GPU side  - OBSOLETE */
+	BASE_MEM_HINT_GPU_WR = (1U << 8),      /**< Heavily written GPU side - OBSOLETE */
+
+	BASE_MEM_GROW_ON_GPF = (1U << 9),      /**< Grow backing store on GPU Page Fault */
+
+	BASE_MEM_COHERENT_SYSTEM = (1U << 10), /**< Page coherence Outer shareable */
+	BASE_MEM_COHERENT_LOCAL = (1U << 11),  /**< Page coherence Inner shareable */
+	BASE_MEM_CACHED_CPU = (1U << 12),      /**< Should be cached on the CPU */
+
+	BASE_MEM_SAME_VA = (1U << 13) /**< Must have same VA on both the GPU and the CPU */
+};
+
+/**
+ * @brief Memory types supported by @a base_mem_import
+ *
+ * Each type defines what the supported handle type is.
+ *
+ * If any new type is added here ARM must be contacted
+ * to allocate a numeric value for it.
+ * Do not just add a new type without synchronizing with ARM
+ * as future releases from ARM might include other new types
+ * which could clash with your custom types.
+ */
+typedef enum base_mem_import_type {
+	BASE_MEM_IMPORT_TYPE_INVALID = 0,
+	/** UMP import. Handle type is ump_secure_id. */
+	BASE_MEM_IMPORT_TYPE_UMP = 1,
+	/** UMM import. Handle type is a file descriptor (int) */
+	BASE_MEM_IMPORT_TYPE_UMM = 2
+} base_mem_import_type;
+
+/* legacy API wrappers */
+#define base_tmem_import_type          base_mem_import_type
+#define BASE_TMEM_IMPORT_TYPE_INVALID  BASE_MEM_IMPORT_TYPE_INVALID
+#define BASE_TMEM_IMPORT_TYPE_UMP      BASE_MEM_IMPORT_TYPE_UMP
+#define BASE_TMEM_IMPORT_TYPE_UMM      BASE_MEM_IMPORT_TYPE_UMM
+
+/**
+ * @brief Invalid memory handle type.
+ * Return value from functions returning @a base_mem_handle on error.
+ */
+#define BASE_MEM_INVALID_HANDLE                (0ull  << 12)
+#define BASE_MEM_MMU_DUMP_HANDLE               (1ull  << 12)
+#define BASE_MEM_TRACE_BUFFER_HANDLE           (2ull  << 12)
+#define BASE_MEM_MAP_TRACKING_HANDLE           (3ull  << 12)
+#define BASE_MEM_WRITE_ALLOC_PAGES_HANDLE      (4ull  << 12)
+/* reserved handles ..-64<<PAGE_SHIFT> for future special handles */
+#define BASE_MEM_COOKIE_BASE                   (64ul  << 12)
+#define BASE_MEM_FIRST_FREE_ADDRESS            ((BITS_PER_LONG << 12) + \
+						BASE_MEM_COOKIE_BASE)
+
+/* Bit mask of cookies used for for memory allocation setup */
+#define KBASE_COOKIE_MASK  ~1UL /* bit 0 is reserved */
+
+/**
+ * @brief Number of bits used as flags for base memory management
+ *
+ * Must be kept in sync with the ::base_mem_alloc_flags flags
+ */
+#define BASE_MEM_FLAGS_NR_BITS  14
+
+/**
+ * @brief Result codes of changing the size of the backing store allocated to a tmem region
+ */
+typedef enum base_backing_threshold_status {
+	BASE_BACKING_THRESHOLD_OK = 0,			    /**< Resize successful */
+	BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE = -1,	    /**< Not a growable tmem object */
+	BASE_BACKING_THRESHOLD_ERROR_OOM = -2,		    /**< Increase failed due to an out-of-memory condition */
+	BASE_BACKING_THRESHOLD_ERROR_MAPPED = -3,	    /**< Resize attempted on buffer while it was mapped, which is not permitted */
+	BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS = -4 /**< Invalid arguments (not tmem, illegal size request, etc.) */
+} base_backing_threshold_status;
+
+/**
+ * @addtogroup base_user_api_memory_defered User-side Base Defered Memory Coherency APIs
+ * @{
+ */
+
+/**
+ * @brief a basic memory operation (sync-set).
+ *
+ * The content of this structure is private, and should only be used
+ * by the accessors.
+ */
+typedef struct base_syncset {
+	basep_syncset basep_sset;
+} base_syncset;
+
+/** @} end group base_user_api_memory_defered */
+
+/**
+ * Handle to represent imported memory object.
+ * Simple opague handle to imported memory, can't be used
+ * with anything but base_external_resource_init to bind to an atom.
+ */
+typedef struct base_import_handle {
+	struct {
+		mali_addr64 handle;
+	} basep;
+} base_import_handle;
+
+/** @} end group base_user_api_memory */
+
+/**
+ * @addtogroup base_user_api_job_dispatch User-side Base Job Dispatcher APIs
+ * @{
+ */
+
+typedef int platform_fence_type;
+#define INVALID_PLATFORM_FENCE ((platform_fence_type)-1)
+
+/**
+ * Base stream handle.
+ *
+ * References an underlying base stream object.
+ */
+typedef struct base_stream {
+	struct {
+		int fd;
+	} basep;
+} base_stream;
+
+/**
+ * Base fence handle.
+ *
+ * References an underlying base fence object.
+ */
+typedef struct base_fence {
+	struct {
+		int fd;
+		int stream_fd;
+	} basep;
+} base_fence;
+
+#if BASE_LEGACY_JD_API
+/**
+ * @brief A pre- or post- dual dependency.
+ *
+ * This structure is used to express either
+ * @li a single or dual pre-dependency (a job depending on one or two
+ * other jobs),
+ * @li a single or dual post-dependency (a job resolving a dependency
+ * for one or two other jobs).
+ *
+ * The dependency itself is specified as a u8, where 0 indicates no
+ * dependency. A single dependency is expressed by having one of the
+ * dependencies set to 0.
+ */
+typedef struct base_jd_dep {
+	u8 dep[2];	/**< pre/post dependencies */
+} base_jd_dep;
+#endif				/* BASE_LEGACY_JD_API */
+
+/**
+ * @brief Per-job data
+ *
+ * This structure is used to store per-job data, and is completly unused
+ * by the Base driver. It can be used to store things such as callback
+ * function pointer, data to handle job completion. It is guaranteed to be
+ * untouched by the Base driver.
+ */
+typedef struct base_jd_udata {
+	u64 blob[2];	 /**< per-job data array */
+} base_jd_udata;
+
+/**
+ * @brief Memory aliasing info
+ *
+ * Describes a memory handle to be aliased.
+ * A subset of the handle can be chosen for aliasing, given an offset and a
+ * length.
+ * A special handle BASE_MEM_WRITE_ALLOC_PAGES_HANDLE is used to represent a
+ * region where a special page is mapped with a write-alloc cache setup,
+ * typically used when the write result of the GPU isn't needed, but the GPU
+ * must write anyway.
+ *
+ * Offset and length are specified in pages.
+ * Offset must be within the size of the handle.
+ * Offset+length must not overrun the size of the handle.
+ *
+ * @handle Handle to alias, can be BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ * @offset Offset within the handle to start aliasing from, in pages.
+ *         Not used with BASE_MEM_WRITE_ALLOC_PAGES_HANDLE.
+ * @length Length to alias, in pages. For BASE_MEM_WRITE_ALLOC_PAGES_HANDLE
+ *         specifies the number of times the special page is needed.
+ */
+struct base_mem_aliasing_info {
+	base_mem_handle handle;
+	u64 offset;
+	u64 length;
+};
+
+/**
+ * @brief Job dependency type.
+ *
+ * A flags field will be inserted into the atom structure to specify whether a dependency is a data or 
+ * ordering dependency (by putting it before/after 'core_req' in the structure it should be possible to add without 
+ * changing the structure size).
+ * When the flag is set for a particular dependency to signal that it is an ordering only dependency then 
+ * errors will not be propagated.
+ */
+typedef u8 base_jd_dep_type;
+
+
+#define BASE_JD_DEP_TYPE_INVALID  (0) 	/**< Invalid dependency */
+#define BASE_JD_DEP_TYPE_DATA     (1U << 0) 	/**< Data dependency */
+#define BASE_JD_DEP_TYPE_ORDER    (1U << 1) 	/**< Order dependency */
+
+/**
+ * @brief Job chain hardware requirements.
+ *
+ * A job chain must specify what GPU features it needs to allow the
+ * driver to schedule the job correctly.  By not specifying the
+ * correct settings can/will cause an early job termination.  Multiple
+ * values can be ORed together to specify multiple requirements.
+ * Special case is ::BASE_JD_REQ_DEP, which is used to express complex
+ * dependencies, and that doesn't execute anything on the hardware.
+ */
+typedef u16 base_jd_core_req;
+
+/* Requirements that come from the HW */
+#define BASE_JD_REQ_DEP 0	    /**< No requirement, dependency only */
+#define BASE_JD_REQ_FS  (1U << 0)   /**< Requires fragment shaders */
+/**
+ * Requires compute shaders
+ * This covers any of the following Midgard Job types:
+ * - Vertex Shader Job
+ * - Geometry Shader Job
+ * - An actual Compute Shader Job
+ *
+ * Compare this with @ref BASE_JD_REQ_ONLY_COMPUTE, which specifies that the
+ * job is specifically just the "Compute Shader" job type, and not the "Vertex
+ * Shader" nor the "Geometry Shader" job type.
+ */
+#define BASE_JD_REQ_CS  (1U << 1)
+#define BASE_JD_REQ_T   (1U << 2)   /**< Requires tiling */
+#define BASE_JD_REQ_CF  (1U << 3)   /**< Requires cache flushes */
+#define BASE_JD_REQ_V   (1U << 4)   /**< Requires value writeback */
+
+/* SW-only requirements - the HW does not expose these as part of the job slot capabilities */
+
+/* Requires fragment job with AFBC encoding */
+#define BASE_JD_REQ_FS_AFBC  (1U << 13)
+
+/**
+ * SW Only requirement: the job chain requires a coherent core group. We don't
+ * mind which coherent core group is used.
+ */
+#define BASE_JD_REQ_COHERENT_GROUP  (1U << 6)
+
+/**
+ * SW Only requirement: The performance counters should be enabled only when
+ * they are needed, to reduce power consumption.
+ */
+
+#define BASE_JD_REQ_PERMON               (1U << 7)
+
+/**
+ * SW Only requirement: External resources are referenced by this atom.
+ * When external resources are referenced no syncsets can be bundled with the atom
+ * but should instead be part of a NULL jobs inserted into the dependency tree.
+ * The first pre_dep object must be configured for the external resouces to use,
+ * the second pre_dep object can be used to create other dependencies.
+ */
+#define BASE_JD_REQ_EXTERNAL_RESOURCES   (1U << 8)
+
+/**
+ * SW Only requirement: Software defined job. Jobs with this bit set will not be submitted
+ * to the hardware but will cause some action to happen within the driver
+ */
+#define BASE_JD_REQ_SOFT_JOB        (1U << 9)
+
+#define BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME      (BASE_JD_REQ_SOFT_JOB | 0x1)
+#define BASE_JD_REQ_SOFT_FENCE_TRIGGER          (BASE_JD_REQ_SOFT_JOB | 0x2)
+#define BASE_JD_REQ_SOFT_FENCE_WAIT             (BASE_JD_REQ_SOFT_JOB | 0x3)
+
+/**
+ * SW Only requirement : Replay job.
+ *
+ * If the preceeding job fails, the replay job will cause the jobs specified in
+ * the list of base_jd_replay_payload pointed to by the jc pointer to be
+ * replayed.
+ *
+ * A replay job will only cause jobs to be replayed up to BASEP_JD_REPLAY_LIMIT
+ * times. If a job fails more than BASEP_JD_REPLAY_LIMIT times then the replay
+ * job is failed, as well as any following dependencies.
+ *
+ * The replayed jobs will require a number of atom IDs. If there are not enough
+ * free atom IDs then the replay job will fail.
+ *
+ * If the preceeding job does not fail, then the replay job is returned as
+ * completed.
+ *
+ * The replayed jobs will never be returned to userspace. The preceeding failed
+ * job will be returned to userspace as failed; the status of this job should
+ * be ignored. Completion should be determined by the status of the replay soft
+ * job.
+ *
+ * In order for the jobs to be replayed, the job headers will have to be
+ * modified. The Status field will be reset to NOT_STARTED. If the Job Type
+ * field indicates a Vertex Shader Job then it will be changed to Null Job.
+ *
+ * The replayed jobs have the following assumptions :
+ *
+ * - No external resources. Any required external resources will be held by the
+ *   replay atom.
+ * - Pre-dependencies are created based on job order.
+ * - Atom numbers are automatically assigned.
+ * - device_nr is set to 0. This is not relevant as
+ *   BASE_JD_REQ_SPECIFIC_COHERENT_GROUP should not be set.
+ * - Priority is inherited from the replay job.
+ */
+#define BASE_JD_REQ_SOFT_REPLAY                 (BASE_JD_REQ_SOFT_JOB | 0x4)
+
+/**
+ * HW Requirement: Requires Compute shaders (but not Vertex or Geometry Shaders)
+ *
+ * This indicates that the Job Chain contains Midgard Jobs of the 'Compute Shaders' type.
+ *
+ * In contrast to @ref BASE_JD_REQ_CS, this does \b not indicate that the Job
+ * Chain contains 'Geometry Shader' or 'Vertex Shader' jobs.
+ *
+ * @note This is a more flexible variant of the @ref BASE_CONTEXT_HINT_ONLY_COMPUTE flag,
+ * allowing specific jobs to be marked as 'Only Compute' instead of the entire context
+ */
+#define BASE_JD_REQ_ONLY_COMPUTE    (1U << 10)
+
+/**
+ * HW Requirement: Use the base_jd_atom::device_nr field to specify a
+ * particular core group
+ *
+ * If both BASE_JD_REQ_COHERENT_GROUP and this flag are set, this flag takes priority
+ *
+ * This is only guaranteed to work for BASE_JD_REQ_ONLY_COMPUTE atoms.
+ *
+ * If the core availability policy is keeping the required core group turned off, then 
+ * the job will fail with a BASE_JD_EVENT_PM_EVENT error code.
+ */
+#define BASE_JD_REQ_SPECIFIC_COHERENT_GROUP (1U << 11)
+
+/**
+ * SW Flag: If this bit is set then the successful completion of this atom
+ * will not cause an event to be sent to userspace
+ */
+#define BASE_JD_REQ_EVENT_ONLY_ON_FAILURE   (1U << 12)
+
+/**
+ * SW Flag: If this bit is set then completion of this atom will not cause an
+ * event to be sent to userspace, whether successful or not.
+ */
+#define BASEP_JD_REQ_EVENT_NEVER (1U << 14)
+
+/**
+* These requirement bits are currently unused in base_jd_core_req (currently a u16)
+*/
+
+#define BASEP_JD_REQ_RESERVED_BIT5 (1U << 5)
+#define BASEP_JD_REQ_RESERVED_BIT15 (1U << 15)
+
+/**
+* Mask of all the currently unused requirement bits in base_jd_core_req.
+*/
+
+#define BASEP_JD_REQ_RESERVED (BASEP_JD_REQ_RESERVED_BIT5 | \
+				BASEP_JD_REQ_RESERVED_BIT15)
+
+/**
+ * Mask of all bits in base_jd_core_req that control the type of the atom.
+ *
+ * This allows dependency only atoms to have flags set
+ */
+#define BASEP_JD_REQ_ATOM_TYPE (~(BASEP_JD_REQ_RESERVED | BASE_JD_REQ_EVENT_ONLY_ON_FAILURE |\
+				BASE_JD_REQ_EXTERNAL_RESOURCES | BASEP_JD_REQ_EVENT_NEVER))
+
+#if BASE_LEGACY_JD_API
+/**
+ * @brief A single job chain, with pre/post dependendencies and mem ops
+ *
+ * This structure is used to describe a single job-chain to be submitted
+ * as part of a bag.
+ * It contains all the necessary information for Base to take care of this
+ * job-chain, including core requirements, priority, syncsets and
+ * dependencies.
+ */
+typedef struct base_jd_atom {
+	mali_addr64 jc;			    /**< job-chain GPU address */
+	base_jd_udata udata;		    /**< user data */
+	base_jd_dep pre_dep;		    /**< pre-dependencies */
+	base_jd_dep post_dep;		    /**< post-dependencies */
+	base_jd_core_req core_req;	    /**< core requirements */
+	u16 nr_syncsets;		    /**< nr of syncsets following the atom */
+	u16 nr_extres;			    /**< nr of external resources following the atom */
+
+	/** @brief Relative priority.
+	 *
+	 * A positive value requests a lower priority, whilst a negative value
+	 * requests a higher priority. Only privileged processes may request a
+	 * higher priority. For unprivileged processes, a negative priority will
+	 * be interpreted as zero.
+	 */
+	s8 prio;
+
+	/**
+	 * @brief Device number to use, depending on @ref base_jd_core_req flags set.
+	 *
+	 * When BASE_JD_REQ_SPECIFIC_COHERENT_GROUP is set, a 'device' is one of
+	 * the coherent core groups, and so this targets a particular coherent
+	 * core-group. They are numbered from 0 to (mali_base_gpu_coherent_group_info::num_groups - 1),
+	 * and the cores targeted by this device_nr will usually be those specified by
+	 * (mali_base_gpu_coherent_group_info::group[device_nr].core_mask).
+	 * Further, two atoms from different processes using the same \a device_nr
+	 * at the same time will always target the same coherent core-group.
+	 *
+	 * There are exceptions to when the device_nr is ignored:
+	 * - when any process in the system uses a BASE_JD_REQ_CS or
+	 * BASE_JD_REQ_ONLY_COMPUTE atom that can run on all cores across all
+	 * coherency groups (i.e. also does \b not have the
+	 * BASE_JD_REQ_COHERENT_GROUP or BASE_JD_REQ_SPECIFIC_COHERENT_GROUP flags
+	 * set). In this case, such atoms would block device_nr==1 being used due
+	 * to restrictions on affinity, perhaps indefinitely. To ensure progress is
+	 * made, the atoms targeted for device_nr 1 will instead be redirected to
+	 * device_nr 0
+	 * - During certain HW workarounds, such as BASE_HW_ISSUE_8987, where
+	 * BASE_JD_REQ_ONLY_COMPUTE atoms must not use the same cores as other
+	 * atoms. In this case, all atoms are targeted to device_nr == min( num_groups, 1 )
+	 *
+	 * Note that the 'device' number for a coherent coregroup cannot exceed
+	 * (BASE_MAX_COHERENT_GROUPS - 1).
+	 */
+	u8 device_nr;
+} base_jd_atom;
+#endif				/* BASE_LEGACY_JD_API */
+
+typedef u8 base_atom_id; /**< Type big enough to store an atom number in */
+
+struct base_dependency {
+	base_atom_id  atom_id;               /**< An atom number */
+	base_jd_dep_type dependency_type;    /**< Dependency type */
+}; 
+
+typedef struct base_jd_atom_v2 {
+	mali_addr64 jc;			    /**< job-chain GPU address */
+	base_jd_udata udata;		    /**< user data */
+	kbase_pointer extres_list;	    /**< list of external resources */
+	u16 nr_extres;			    /**< nr of external resources */
+	base_jd_core_req core_req;	    /**< core requirements */
+	const struct base_dependency pre_dep[2]; /**< pre-dependencies, one need to use SETTER function to assign this field,
+	this is done in order to reduce possibility of improper assigment of a dependency field */
+	base_atom_id atom_number;	    /**< unique number to identify the atom */
+	s8 prio;			    /**< priority - smaller is higher priority */
+	u8 device_nr;			    /**< coregroup when BASE_JD_REQ_SPECIFIC_COHERENT_GROUP specified */
+	u8 padding[5];
+} base_jd_atom_v2;
+
+#if BASE_LEGACY_JD_API
+/* Structure definition works around the fact that C89 doesn't allow arrays of size 0 */
+typedef struct basep_jd_atom_ss {
+	base_jd_atom atom;
+	base_syncset syncsets[1];
+} basep_jd_atom_ss;
+#endif				/* BASE_LEGACY_JD_API */
+
+typedef enum base_external_resource_access {
+	BASE_EXT_RES_ACCESS_SHARED,
+	BASE_EXT_RES_ACCESS_EXCLUSIVE
+} base_external_resource_access;
+
+typedef struct base_external_resource {
+	u64 ext_resource;
+} base_external_resource;
+
+#if BASE_LEGACY_JD_API
+/* Structure definition works around the fact that C89 doesn't allow arrays of size 0 */
+typedef struct basep_jd_atom_ext_res {
+	base_jd_atom atom;
+	base_external_resource resources[1];
+} basep_jd_atom_ext_res;
+
+static INLINE size_t base_jd_atom_size_ex(u32 syncset_count, u32 external_res_count)
+{
+	int size;
+
+	LOCAL_ASSERT(0 == syncset_count || 0 == external_res_count);
+
+	size = syncset_count ? offsetof(basep_jd_atom_ss, syncsets[0]) + (sizeof(base_syncset) * syncset_count) : external_res_count ? offsetof(basep_jd_atom_ext_res, resources[0]) + (sizeof(base_external_resource) * external_res_count) : sizeof(base_jd_atom);
+
+	/* Atom minimum size set to 64 bytes to ensure that the maximum
+	 * number of atoms in the ring buffer is limited to 256 */
+	return MAX(64, size);
+}
+
+/**
+ * @brief Atom size evaluator
+ *
+ * This function returns the size in bytes of a ::base_jd_atom
+ * containing @a n syncsets. It must be used to compute the size of a
+ * bag before allocation.
+ *
+ * @param nr the number of syncsets for this atom
+ * @return the atom size in bytes
+ */
+static INLINE size_t base_jd_atom_size(u32 nr)
+{
+	return base_jd_atom_size_ex(nr, 0);
+}
+
+/**
+ * @brief Atom syncset accessor
+ *
+ * This function returns a pointer to the nth syncset allocated
+ * together with an atom.
+ *
+ * @param[in] atom The allocated atom
+ * @param     n    The number of the syncset to be returned
+ * @return a pointer to the nth syncset.
+ */
+static INLINE base_syncset *base_jd_get_atom_syncset(base_jd_atom *atom, u16 n)
+{
+	LOCAL_ASSERT(atom != NULL);
+	LOCAL_ASSERT(0 == (atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES));
+	LOCAL_ASSERT(n <= atom->nr_syncsets);
+	return &((basep_jd_atom_ss *) atom)->syncsets[n];
+}
+#endif				/* BASE_LEGACY_JD_API */
+
+
+/**
+ * @brief Setter for a dependency structure
+ *
+ * @param[in] dep          The kbase jd atom dependency to be initialized.
+ * @param     id           The atom_id to be assigned.
+ * @param     dep_type     The dep_type to be assigned.
+ *
+ */
+static INLINE void base_jd_atom_dep_set(const struct base_dependency* const_dep, base_atom_id id, base_jd_dep_type dep_type)
+{
+	struct base_dependency* dep;
+	
+	LOCAL_ASSERT(const_dep != NULL);
+	/* make sure we don't set not allowed combinations of atom_id/dependency_type */
+	LOCAL_ASSERT( ( id == 0 && dep_type == BASE_JD_DEP_TYPE_INVALID) || 
+				(id > 0 && dep_type != BASE_JD_DEP_TYPE_INVALID) );
+
+	dep = REINTERPRET_CAST(struct base_dependency*)const_dep;
+
+	dep->atom_id = id;
+	dep->dependency_type = dep_type;
+}
+
+/**
+ * @brief Make a copy of a dependency structure
+ *
+ * @param[in,out] dep          The kbase jd atom dependency to be written.
+ * @param[in]     from         The dependency to make a copy from.
+ *
+ */
+static INLINE void base_jd_atom_dep_copy(const struct base_dependency* const_dep, const struct base_dependency* from)
+{
+	LOCAL_ASSERT(const_dep != NULL);
+
+	base_jd_atom_dep_set(const_dep, from->atom_id, from->dependency_type);
+}
+
+/**
+ * @brief Soft-atom fence trigger setup.
+ *
+ * Sets up an atom to be a SW-only atom signaling a fence
+ * when it reaches the run state.
+ *
+ * Using the existing base dependency system the fence can
+ * be set to trigger when a GPU job has finished.
+ *
+ * The base fence object must not be terminated until the atom
+ * has been submitted to @a base_jd_submit_bag and @a base_jd_submit_bag has returned.
+ *
+ * @a fence must be a valid fence set up with @a base_fence_init.
+ * Calling this function with a uninitialized fence results in undefined behavior.
+ *
+ * @param[out] atom A pre-allocated atom to configure as a fence trigger SW atom
+ * @param[in] fence The base fence object to trigger.
+ */
+static INLINE void base_jd_fence_trigger_setup(base_jd_atom * const atom, base_fence *fence)
+{
+	LOCAL_ASSERT(atom);
+	LOCAL_ASSERT(fence);
+	LOCAL_ASSERT(fence->basep.fd == INVALID_PLATFORM_FENCE);
+	LOCAL_ASSERT(fence->basep.stream_fd >= 0);
+	atom->jc = (uintptr_t) fence;
+	atom->core_req = BASE_JD_REQ_SOFT_FENCE_TRIGGER;
+}
+
+static INLINE void base_jd_fence_trigger_setup_v2(base_jd_atom_v2 *atom, base_fence *fence)
+{
+	LOCAL_ASSERT(atom);
+	LOCAL_ASSERT(fence);
+	LOCAL_ASSERT(fence->basep.fd == INVALID_PLATFORM_FENCE);
+	LOCAL_ASSERT(fence->basep.stream_fd >= 0);
+	atom->jc = (uintptr_t) fence;
+	atom->core_req = BASE_JD_REQ_SOFT_FENCE_TRIGGER;
+}
+
+/**
+ * @brief Soft-atom fence wait setup.
+ *
+ * Sets up an atom to be a SW-only atom waiting on a fence.
+ * When the fence becomes triggered the atom becomes runnable
+ * and completes immediately.
+ *
+ * Using the existing base dependency system the fence can
+ * be set to block a GPU job until it has been triggered.
+ *
+ * The base fence object must not be terminated until the atom
+ * has been submitted to @a base_jd_submit_bag and @a base_jd_submit_bag has returned.
+ *
+ * @a fence must be a valid fence set up with @a base_fence_init or @a base_fence_import.
+ * Calling this function with a uninitialized fence results in undefined behavior.
+ *
+ * @param[out] atom A pre-allocated atom to configure as a fence wait SW atom
+ * @param[in] fence The base fence object to wait on
+ */
+static INLINE void base_jd_fence_wait_setup(base_jd_atom * const atom, base_fence *fence)
+{
+	LOCAL_ASSERT(atom);
+	LOCAL_ASSERT(fence);
+	LOCAL_ASSERT(fence->basep.fd >= 0);
+	atom->jc = (uintptr_t) fence;
+	atom->core_req = BASE_JD_REQ_SOFT_FENCE_WAIT;
+}
+
+static INLINE void base_jd_fence_wait_setup_v2(base_jd_atom_v2 *atom, base_fence *fence)
+{
+	LOCAL_ASSERT(atom);
+	LOCAL_ASSERT(fence);
+	LOCAL_ASSERT(fence->basep.fd >= 0);
+	atom->jc = (uintptr_t) fence;
+	atom->core_req = BASE_JD_REQ_SOFT_FENCE_WAIT;
+}
+
+#if BASE_LEGACY_JD_API
+/**
+ * @brief Atom external resource accessor
+ *
+ * This functions returns a pointer to the nth external resource tracked by the atom.
+ *
+ * @param[in] atom The allocated atom
+ * @param     n    The number of the external resource to return a pointer to
+ * @return a pointer to the nth external resource
+ */
+static INLINE base_external_resource * base_jd_get_external_resource(base_jd_atom *atom, u16 n)
+{
+	LOCAL_ASSERT(atom != NULL);
+	LOCAL_ASSERT(BASE_JD_REQ_EXTERNAL_RESOURCES == (atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES));
+	LOCAL_ASSERT(n <= atom->nr_extres);
+	return &((basep_jd_atom_ext_res *) atom)->resources[n];
+}
+#endif				/* BASE_LEGACY_JD_API */
+
+/**
+ * @brief External resource info initialization.
+ *
+ * Sets up a external resource object to reference
+ * a memory allocation and the type of access requested.
+ *
+ * @param[in] res     The resource object to initialize
+ * @param     handle  The handle to the imported memory object
+ * @param     access  The type of access requested
+ */
+static INLINE void base_external_resource_init(base_external_resource * res, base_import_handle handle, base_external_resource_access access)
+{
+	mali_addr64 address;
+	address = handle.basep.handle;
+
+	LOCAL_ASSERT(res != NULL);
+	LOCAL_ASSERT(0 == (address & LOCAL_PAGE_LSB));
+	LOCAL_ASSERT(access == BASE_EXT_RES_ACCESS_SHARED || access == BASE_EXT_RES_ACCESS_EXCLUSIVE);
+
+	res->ext_resource = address | (access & LOCAL_PAGE_LSB);
+}
+
+#if BASE_LEGACY_JD_API
+/**
+ * @brief Next atom accessor
+ *
+ * This function returns a pointer to the next allocated atom. It
+ * relies on the fact that the current atom has been correctly
+ * initialized (relies on the base_jd_atom::nr_syncsets field).
+ *
+ * @param[in] atom The allocated atom
+ * @return a pointer to the next atom.
+ */
+static INLINE base_jd_atom *base_jd_get_next_atom(base_jd_atom *atom)
+{
+	LOCAL_ASSERT(atom != NULL);
+	return (atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES) ? (base_jd_atom *) base_jd_get_external_resource(atom, atom->nr_extres) : (base_jd_atom *) base_jd_get_atom_syncset(atom, atom->nr_syncsets);
+}
+#endif				/* BASE_LEGACY_JD_API */
+
+/**
+ * @brief Job chain event code bits
+ * Defines the bits used to create ::base_jd_event_code
+ */
+enum {
+	BASE_JD_SW_EVENT_KERNEL = (1u << 15), /**< Kernel side event */
+	BASE_JD_SW_EVENT = (1u << 14), /**< SW defined event */
+	BASE_JD_SW_EVENT_SUCCESS = (1u << 13), /**< Event idicates success (SW events only) */
+	BASE_JD_SW_EVENT_JOB = (0u << 11), /**< Job related event */
+	BASE_JD_SW_EVENT_BAG = (1u << 11), /**< Bag related event */
+	BASE_JD_SW_EVENT_INFO = (2u << 11), /**< Misc/info event */
+	BASE_JD_SW_EVENT_RESERVED = (3u << 11),	/**< Reserved event type */
+	BASE_JD_SW_EVENT_TYPE_MASK = (3u << 11)	    /**< Mask to extract the type from an event code */
+};
+
+/**
+ * @brief Job chain event codes
+ *
+ * HW and low-level SW events are represented by event codes.
+ * The status of jobs which succeeded are also represented by
+ * an event code (see ::BASE_JD_EVENT_DONE).
+ * Events are usually reported as part of a ::base_jd_event.
+ *
+ * The event codes are encoded in the following way:
+ * @li 10:0  - subtype
+ * @li 12:11 - type
+ * @li 13    - SW success (only valid if the SW bit is set)
+ * @li 14    - SW event (HW event if not set)
+ * @li 15    - Kernel event (should never be seen in userspace)
+ *
+ * Events are split up into ranges as follows:
+ * - BASE_JD_EVENT_RANGE_\<description\>_START
+ * - BASE_JD_EVENT_RANGE_\<description\>_END
+ *
+ * \a code is in \<description\>'s range when:
+ * - <tt>BASE_JD_EVENT_RANGE_\<description\>_START <= code < BASE_JD_EVENT_RANGE_\<description\>_END </tt>
+ *
+ * Ranges can be asserted for adjacency by testing that the END of the previous
+ * is equal to the START of the next. This is useful for optimizing some tests
+ * for range.
+ *
+ * A limitation is that the last member of this enum must explicitly be handled
+ * (with an assert-unreachable statement) in switch statements that use
+ * variables of this type. Otherwise, the compiler warns that we have not
+ * handled that enum value.
+ */
+typedef enum base_jd_event_code {
+	/* HW defined exceptions */
+
+	/** Start of HW Non-fault status codes
+	 *
+	 * @note Obscurely, BASE_JD_EVENT_TERMINATED indicates a real fault,
+	 * because the job was hard-stopped
+	 */
+	BASE_JD_EVENT_RANGE_HW_NONFAULT_START = 0,
+
+	/* non-fatal exceptions */
+	BASE_JD_EVENT_NOT_STARTED = 0x00, /**< Can't be seen by userspace, treated as 'previous job done' */
+	BASE_JD_EVENT_DONE = 0x01,
+	BASE_JD_EVENT_STOPPED = 0x03,	  /**< Can't be seen by userspace, becomes TERMINATED, DONE or JOB_CANCELLED */
+	BASE_JD_EVENT_TERMINATED = 0x04,  /**< This is actually a fault status code - the job was hard stopped */
+	BASE_JD_EVENT_ACTIVE = 0x08,	  /**< Can't be seen by userspace, jobs only returned on complete/fail/cancel */
+
+	/** End of HW Non-fault status codes
+	 *
+	 * @note Obscurely, BASE_JD_EVENT_TERMINATED indicates a real fault,
+	 * because the job was hard-stopped
+	 */
+	BASE_JD_EVENT_RANGE_HW_NONFAULT_END = 0x40,
+
+	/** Start of HW fault and SW Error status codes */
+	BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_START = 0x40,
+
+	/* job exceptions */
+	BASE_JD_EVENT_JOB_CONFIG_FAULT = 0x40,
+	BASE_JD_EVENT_JOB_POWER_FAULT = 0x41,
+	BASE_JD_EVENT_JOB_READ_FAULT = 0x42,
+	BASE_JD_EVENT_JOB_WRITE_FAULT = 0x43,
+	BASE_JD_EVENT_JOB_AFFINITY_FAULT = 0x44,
+	BASE_JD_EVENT_JOB_BUS_FAULT = 0x48,
+	BASE_JD_EVENT_INSTR_INVALID_PC = 0x50,
+	BASE_JD_EVENT_INSTR_INVALID_ENC = 0x51,
+	BASE_JD_EVENT_INSTR_TYPE_MISMATCH = 0x52,
+	BASE_JD_EVENT_INSTR_OPERAND_FAULT = 0x53,
+	BASE_JD_EVENT_INSTR_TLS_FAULT = 0x54,
+	BASE_JD_EVENT_INSTR_BARRIER_FAULT = 0x55,
+	BASE_JD_EVENT_INSTR_ALIGN_FAULT = 0x56,
+	BASE_JD_EVENT_DATA_INVALID_FAULT = 0x58,
+	BASE_JD_EVENT_TILE_RANGE_FAULT = 0x59,
+	BASE_JD_EVENT_STATE_FAULT = 0x5A,
+	BASE_JD_EVENT_OUT_OF_MEMORY = 0x60,
+	BASE_JD_EVENT_UNKNOWN = 0x7F,
+
+	/* GPU exceptions */
+	BASE_JD_EVENT_DELAYED_BUS_FAULT = 0x80,
+	BASE_JD_EVENT_SHAREABILITY_FAULT = 0x88,
+
+	/* MMU exceptions */
+	BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL1 = 0xC1,
+	BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL2 = 0xC2,
+	BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL3 = 0xC3,
+	BASE_JD_EVENT_TRANSLATION_FAULT_LEVEL4 = 0xC4,
+	BASE_JD_EVENT_PERMISSION_FAULT = 0xC8,
+	BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL1 = 0xD1,
+	BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL2 = 0xD2,
+	BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL3 = 0xD3,
+	BASE_JD_EVENT_TRANSTAB_BUS_FAULT_LEVEL4 = 0xD4,
+	BASE_JD_EVENT_ACCESS_FLAG = 0xD8,
+
+	/* SW defined exceptions */
+	BASE_JD_EVENT_MEM_GROWTH_FAILED = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x000,
+	BASE_JD_EVENT_TIMED_OUT = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x001,
+	BASE_JD_EVENT_JOB_CANCELLED = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x002,
+	BASE_JD_EVENT_JOB_INVALID = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x003,
+	BASE_JD_EVENT_PM_EVENT = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_JOB | 0x004,
+
+	BASE_JD_EVENT_BAG_INVALID = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_BAG | 0x003,
+
+	/** End of HW fault and SW Error status codes */
+	BASE_JD_EVENT_RANGE_HW_FAULT_OR_SW_ERROR_END = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_RESERVED | 0x3FF,
+
+	/** Start of SW Success status codes */
+	BASE_JD_EVENT_RANGE_SW_SUCCESS_START = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | 0x000,
+
+	BASE_JD_EVENT_PROGRESS_REPORT = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | BASE_JD_SW_EVENT_JOB | 0x000,
+	BASE_JD_EVENT_BAG_DONE = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | BASE_JD_SW_EVENT_BAG | 0x000,
+	BASE_JD_EVENT_DRV_TERMINATED = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | BASE_JD_SW_EVENT_INFO | 0x000,
+
+	/** End of SW Success status codes */
+	BASE_JD_EVENT_RANGE_SW_SUCCESS_END = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_SUCCESS | BASE_JD_SW_EVENT_RESERVED | 0x3FF,
+
+	/** Start of Kernel-only status codes. Such codes are never returned to user-space */
+	BASE_JD_EVENT_RANGE_KERNEL_ONLY_START = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | 0x000,
+	BASE_JD_EVENT_REMOVED_FROM_NEXT = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | BASE_JD_SW_EVENT_JOB | 0x000,
+
+	/** End of Kernel-only status codes. */
+	BASE_JD_EVENT_RANGE_KERNEL_ONLY_END = BASE_JD_SW_EVENT | BASE_JD_SW_EVENT_KERNEL | BASE_JD_SW_EVENT_RESERVED | 0x3FF
+} base_jd_event_code;
+
+/**
+ * @brief Event reporting structure
+ *
+ * This structure is used by the kernel driver to report information
+ * about GPU events. The can either be HW-specific events or low-level
+ * SW events, such as job-chain completion.
+ *
+ * The event code contains an event type field which can be extracted
+ * by ANDing with ::BASE_JD_SW_EVENT_TYPE_MASK.
+ *
+ * Based on the event type base_jd_event::data holds:
+ * @li ::BASE_JD_SW_EVENT_JOB : the offset in the ring-buffer for the completed
+ * job-chain
+ * @li ::BASE_JD_SW_EVENT_BAG : The address of the ::base_jd_bag that has
+ * been completed (ie all contained job-chains have been completed).
+ * @li ::BASE_JD_SW_EVENT_INFO : base_jd_event::data not used
+ */
+#if BASE_LEGACY_JD_API
+typedef struct base_jd_event {
+	base_jd_event_code event_code;	    /**< event code */
+	void *data;			    /**< event specific data */
+} base_jd_event;
+#endif
+
+typedef struct base_jd_event_v2 {
+	base_jd_event_code event_code;	    /**< event code */
+	base_atom_id atom_number;	    /**< the atom number that has completed */
+	base_jd_udata udata;		    /**< user data */
+} base_jd_event_v2;
+
+/**
+ * Padding required to ensure that the @ref base_dump_cpu_gpu_counters structure fills
+ * a full cache line.
+ */
+
+#define BASE_CPU_GPU_CACHE_LINE_PADDING (36)
+
+
+/**
+ * @brief Structure for BASE_JD_REQ_SOFT_DUMP_CPU_GPU_COUNTERS jobs.
+ *
+ * This structure is stored into the memory pointed to by the @c jc field of @ref base_jd_atom.
+ *
+ * This structure must be padded to ensure that it will occupy whole cache lines. This is to avoid
+ * cases where access to pages containing the structure is shared between cached and un-cached
+ * memory regions, which would cause memory corruption.  Here we set the structure size to be 64 bytes
+ * which is the cache line for ARM A15 processors.
+ */
+
+typedef struct base_dump_cpu_gpu_counters {
+	u64 system_time;
+	u64 cycle_counter;
+	u64 sec;
+	u32 usec;
+	u8 padding[BASE_CPU_GPU_CACHE_LINE_PADDING];
+} base_dump_cpu_gpu_counters;
+
+
+
+/** @} end group base_user_api_job_dispatch */
+
+#ifdef __KERNEL__
+/*
+ * The following typedefs should be removed when a midg types header is added.
+ * See MIDCOM-1657 for details.
+ */
+typedef u32 midg_product_id;
+typedef u32 midg_cache_features;
+typedef u32 midg_tiler_features;
+typedef u32 midg_mem_features;
+typedef u32 midg_mmu_features;
+typedef u32 midg_js_features;
+typedef u32 midg_as_present;
+typedef u32 midg_js_present;
+
+#define MIDG_MAX_JOB_SLOTS 16
+
+#else
+#include <midg/mali_midg.h>
+#endif
+
+/**
+ * @page page_base_user_api_gpuprops User-side Base GPU Property Query API
+ *
+ * The User-side Base GPU Property Query API encapsulates two
+ * sub-modules:
+ *
+ * - @ref base_user_api_gpuprops_dyn "Dynamic GPU Properties"
+ * - @ref base_plat_config_gpuprops "Base Platform Config GPU Properties"
+ *
+ * There is a related third module outside of Base, which is owned by the MIDG
+ * module:
+ * - @ref midg_gpuprops_static "Midgard Compile-time GPU Properties"
+ *
+ * Base only deals with properties that vary between different Midgard
+ * implementations - the Dynamic GPU properties and the Platform Config
+ * properties.
+ *
+ * For properties that are constant for the Midgard Architecture, refer to the
+ * MIDG module. However, we will discuss their relevance here <b>just to
+ * provide background information.</b>
+ *
+ * @section sec_base_user_api_gpuprops_about About the GPU Properties in Base and MIDG modules
+ *
+ * The compile-time properties (Platform Config, Midgard Compile-time
+ * properties) are exposed as pre-processor macros.
+ *
+ * Complementing the compile-time properties are the Dynamic GPU
+ * Properties, which act as a conduit for the Midgard Configuration
+ * Discovery.
+ *
+ * In general, the dynamic properties are present to verify that the platform
+ * has been configured correctly with the right set of Platform Config
+ * Compile-time Properties.
+ *
+ * As a consistant guide across the entire DDK, the choice for dynamic or
+ * compile-time should consider the following, in order:
+ * -# Can the code be written so that it doesn't need to know the
+ * implementation limits at all?
+ * -# If you need the limits, get the information from the Dynamic Property
+ * lookup. This should be done once as you fetch the context, and then cached
+ * as part of the context data structure, so it's cheap to access.
+ * -# If there's a clear and arguable inefficiency in using Dynamic Properties,
+ * then use a Compile-Time Property (Platform Config, or Midgard Compile-time
+ * property). Examples of where this might be sensible follow:
+ *  - Part of a critical inner-loop
+ *  - Frequent re-use throughout the driver, causing significant extra load
+ * instructions or control flow that would be worthwhile optimizing out.
+ *
+ * We cannot provide an exhaustive set of examples, neither can we provide a
+ * rule for every possible situation. Use common sense, and think about: what
+ * the rest of the driver will be doing; how the compiler might represent the
+ * value if it is a compile-time constant; whether an OEM shipping multiple
+ * devices would benefit much more from a single DDK binary, instead of
+ * insignificant micro-optimizations.
+ *
+ * @section sec_base_user_api_gpuprops_dyn Dynamic GPU Properties
+ *
+ * Dynamic GPU properties are presented in two sets:
+ * -# the commonly used properties in @ref base_gpu_props, which have been
+ * unpacked from GPU register bitfields.
+ * -# The full set of raw, unprocessed properties in @ref midg_raw_gpu_props
+ * (also a member of @ref base_gpu_props). All of these are presented in
+ * the packed form, as presented by the GPU  registers themselves.
+ *
+ * @usecase The raw properties in @ref midg_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it <b>does not need to be processed
+ * by the driver</b>. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ * The properties returned extend the Midgard Configuration Discovery
+ * registers. For example, GPU clock speed is not specified in the Midgard
+ * Architecture, but is <b>necessary for OpenCL's clGetDeviceInfo() function</b>.
+ *
+ * The GPU properties are obtained by a call to
+ * _mali_base_get_gpu_props(). This simply returns a pointer to a const
+ * base_gpu_props structure. It is constant for the life of a base
+ * context. Multiple calls to _mali_base_get_gpu_props() to a base context
+ * return the same pointer to a constant structure. This avoids cache pollution
+ * of the common data.
+ *
+ * This pointer must not be freed, because it does not point to the start of a
+ * region allocated by the memory allocator; instead, just close the @ref
+ * base_context.
+ *
+ *
+ * @section sec_base_user_api_gpuprops_config Platform Config Compile-time Properties
+ *
+ * The Platform Config File sets up gpu properties that are specific to a
+ * certain platform. Properties that are 'Implementation Defined' in the
+ * Midgard Architecture spec are placed here.
+ *
+ * @note Reference configurations are provided for Midgard Implementations, such as
+ * the Mali-T600 family. The customer need not repeat this information, and can select one of
+ * these reference configurations. For example, VA_BITS, PA_BITS and the
+ * maximum number of samples per pixel might vary between Midgard Implementations, but
+ * \b not for platforms using the Mali-T604. This information is placed in
+ * the reference configuration files.
+ *
+ * The System Integrator creates the following structure:
+ * - platform_XYZ
+ * - platform_XYZ/plat
+ * - platform_XYZ/plat/plat_config.h
+ *
+ * They then edit plat_config.h, using the example plat_config.h files as a
+ * guide.
+ *
+ * At the very least, the customer must set @ref CONFIG_GPU_CORE_TYPE, and will
+ * receive a helpful \#error message if they do not do this correctly. This
+ * selects the Reference Configuration for the Midgard Implementation. The rationale
+ * behind this decision (against asking the customer to write \#include
+ * <gpus/mali_t600.h> in their plat_config.h) is as follows:
+ * - This mechanism 'looks' like a regular config file (such as Linux's
+ * .config)
+ * - It is difficult to get wrong in a way that will produce strange build
+ * errors:
+ *  - They need not know where the mali_t600.h, other_midg_gpu.h etc. files are stored - and
+ *  so they won't accidentally pick another file with 'mali_t600' in its name
+ *  - When the build doesn't work, the System Integrator may think the DDK is
+ *  doesn't work, and attempt to fix it themselves:
+ *   - For the @ref CONFIG_GPU_CORE_TYPE mechanism, the only way to get past the
+ *   error is to set @ref CONFIG_GPU_CORE_TYPE, and this is what the \#error tells
+ *   you.
+ *   - For a \#include mechanism, checks must still be made elsewhere, which the
+ *   System Integrator may try working around by setting \#defines (such as
+ *   VA_BITS) themselves in their plat_config.h. In the  worst case, they may
+ *   set the prevention-mechanism \#define of
+ *   "A_CORRECT_MIDGARD_CORE_WAS_CHOSEN".
+ *   - In this case, they would believe they are on the right track, because
+ *   the build progresses with their fix, but with errors elsewhere.
+ *
+ * However, there is nothing to prevent the customer using \#include to organize
+ * their own configurations files hierarchically.
+ *
+ * The mechanism for the header file processing is as follows:
+ *
+ * @dot
+   digraph plat_config_mechanism {
+	   rankdir=BT
+	   size="6,6"
+
+       "mali_base.h";
+	   "midg/midg.h";
+
+	   node [ shape=box ];
+	   {
+	       rank = same; ordering = out;
+
+		   "midg/midg_gpu_props.h";
+		   "base/midg_gpus/mali_t600.h";
+		   "base/midg_gpus/other_midg_gpu.h";
+	   }
+	   { rank = same; "plat/plat_config.h"; }
+	   {
+	       rank = same;
+		   "midg/midg.h" [ shape=box ];
+		   gpu_chooser [ label="" style="invisible" width=0 height=0 fixedsize=true ];
+		   select_gpu [ label="Mali-T600 | Other\n(select_gpu.h)" shape=polygon,sides=4,distortion=0.25 width=3.3 height=0.99 fixedsize=true ] ;
+	   }
+	   node [ shape=box ];
+	   { rank = same; "plat/plat_config.h"; }
+	   { rank = same; "mali_base.h"; }
+
+	   "mali_base.h" -> "midg/midg.h" -> "midg/midg_gpu_props.h";
+	   "mali_base.h" -> "plat/plat_config.h" ;
+	   "mali_base.h" -> select_gpu ;
+
+	   "plat/plat_config.h" -> gpu_chooser [style="dotted,bold" dir=none weight=4] ;
+	   gpu_chooser -> select_gpu [style="dotted,bold"] ;
+
+	   select_gpu -> "base/midg_gpus/mali_t600.h" ;
+	   select_gpu -> "base/midg_gpus/other_midg_gpu.h" ;
+   }
+   @enddot
+ *
+ *
+ * @section sec_base_user_api_gpuprops_kernel Kernel Operation
+ *
+ * During Base Context Create time, user-side makes a single kernel call:
+ * - A call to fill user memory with GPU information structures
+ *
+ * The kernel-side will fill the provided the entire processed @ref base_gpu_props
+ * structure, because this information is required in both
+ * user and kernel side; it does not make sense to decode it twice.
+ *
+ * Coherency groups must be derived from the bitmasks, but this can be done
+ * kernel side, and just once at kernel startup: Coherency groups must already
+ * be known kernel-side, to support chains that specify a 'Only Coherent Group'
+ * SW requirement, or 'Only Coherent Group with Tiler' SW requirement.
+ *
+ * @section sec_base_user_api_gpuprops_cocalc Coherency Group calculation
+ * Creation of the coherent group data is done at device-driver startup, and so
+ * is one-time. This will most likely involve a loop with CLZ, shifting, and
+ * bit clearing on the L2_PRESENT or L3_PRESENT masks, depending on whether the
+ * system is L2 or L2+L3 Coherent. The number of shader cores is done by a
+ * population count, since faulty cores may be disabled during production,
+ * producing a non-contiguous mask.
+ *
+ * The memory requirements for this algoirthm can be determined either by a u64
+ * population count on the L2/L3_PRESENT masks (a LUT helper already is
+ * requried for the above), or simple assumption that there can be no more than
+ * 16 coherent groups, since core groups are typically 4 cores.
+ */
+
+/**
+ * @addtogroup base_user_api_gpuprops User-side Base GPU Property Query APIs
+ * @{
+ */
+
+/**
+ * @addtogroup base_user_api_gpuprops_dyn Dynamic HW Properties
+ * @{
+ */
+
+#define BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS 3
+
+#define BASE_MAX_COHERENT_GROUPS 16
+
+struct mali_base_gpu_core_props {
+	/**
+	 * Product specific value.
+	 */
+	midg_product_id product_id;
+
+	/**
+	 * Status of the GPU release.
+     * No defined values, but starts at 0 and increases by one for each release
+     * status (alpha, beta, EAC, etc.).
+     * 4 bit values (0-15).
+	 */
+	u16 version_status;
+
+	/**
+	 * Minor release number of the GPU. "P" part of an "RnPn" release number.
+     * 8 bit values (0-255).
+	 */
+	u16 minor_revision;
+
+	/**
+	 * Major release number of the GPU. "R" part of an "RnPn" release number.
+     * 4 bit values (0-15).
+	 */
+	u16 major_revision;
+
+	u16 padding;
+
+	/**
+	 * @usecase GPU clock speed is not specified in the Midgard Architecture, but is
+	 * <b>necessary for OpenCL's clGetDeviceInfo() function</b>.
+	 */
+	u32 gpu_speed_mhz;
+
+	/**
+	 * @usecase GPU clock max/min speed is required for computing best/worst case
+	 * in tasks as job scheduling ant irq_throttling. (It is not specified in the
+	 *  Midgard Architecture).
+	 */
+	u32 gpu_freq_khz_max;
+	u32 gpu_freq_khz_min;
+
+	/**
+	 * Size of the shader program counter, in bits.
+	 */
+	u32 log2_program_counter_size;
+
+	/**
+	 * TEXTURE_FEATURES_x registers, as exposed by the GPU. This is a
+	 * bitpattern where a set bit indicates that the format is supported.
+	 *
+	 * Before using a texture format, it is recommended that the corresponding
+	 * bit be checked.
+	 */
+	u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+	/**
+	 * Theoretical maximum memory available to the GPU. It is unlikely that a
+	 * client will be able to allocate all of this memory for their own
+	 * purposes, but this at least provides an upper bound on the memory
+	 * available to the GPU.
+	 *
+	 * This is required for OpenCL's clGetDeviceInfo() call when
+	 * CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL GPU devices. The
+	 * client will not be expecting to allocate anywhere near this value.
+	 */
+	u64 gpu_available_memory_size;
+};
+
+/**
+ *
+ * More information is possible - but associativity and bus width are not
+ * required by upper-level apis.
+ */
+struct mali_base_gpu_l2_cache_props {
+	u8 log2_line_size;
+	u8 log2_cache_size;
+	u8 num_l2_slices; /* Number of L2C slices. 1 or higher */
+	u8 padding[5];
+};
+
+struct mali_base_gpu_l3_cache_props {
+	u8 log2_line_size;
+	u8 log2_cache_size;
+	u8 padding[6];
+};
+
+struct mali_base_gpu_tiler_props {
+	u32 bin_size_bytes;	/* Max is 4*2^15 */
+	u32 max_active_levels;	/* Max is 2^15 */
+};
+
+/**
+ * GPU threading system details.  
+ */
+struct mali_base_gpu_thread_props {
+	u32 max_threads;            /* Max. number of threads per core */ 
+	u32 max_workgroup_size;     /* Max. number of threads per workgroup */
+	u32 max_barrier_size;       /* Max. number of threads that can synchronize on a simple barrier */
+	u16 max_registers;			/* Total size [1..65535] of the register file available per core. */
+	u8  max_task_queue;			/* Max. tasks [1..255] which may be sent to a core before it becomes blocked. */
+	u8  max_thread_group_split;	/* Max. allowed value [1..15] of the Thread Group Split field. */
+	u8  impl_tech;		    	/* 0 = Not specified, 1 = Silicon, 2 = FPGA, 3 = SW Model/Emulation */
+	u8  padding[7];
+};
+
+/**
+ * @brief descriptor for a coherent group
+ *
+ * \c core_mask exposes all cores in that coherent group, and \c num_cores
+ * provides a cached population-count for that mask.
+ *
+ * @note Whilst all cores are exposed in the mask, not all may be available to
+ * the application, depending on the Kernel Job Scheduler policy. Therefore,
+ * the application should not further restrict the core mask itself, as it may
+ * result in an empty core mask. However, it can guarentee that there will be
+ * at least one core available for each core group exposed .
+ *
+ * @usecase Chains marked at certain user-side priorities (e.g. the Long-running
+ * (batch) priority ) can be prevented from running on entire core groups by the
+ * Kernel Chain Scheduler policy.
+ *
+ * @note if u64s must be 8-byte aligned, then this structure has 32-bits of wastage.
+ */
+struct mali_base_gpu_coherent_group {
+	u64 core_mask;	       /**< Core restriction mask required for the group */
+	u16 num_cores;	       /**< Number of cores in the group */
+	u16 padding[3];
+};
+
+/**
+ * @brief Coherency group information
+ *
+ * Note that the sizes of the members could be reduced. However, the \c group
+ * member might be 8-byte aligned to ensure the u64 core_mask is 8-byte
+ * aligned, thus leading to wastage if the other members sizes were reduced.
+ *
+ * The groups are sorted by core mask. The core masks are non-repeating and do
+ * not intersect.
+ */
+struct mali_base_gpu_coherent_group_info {
+	u32 num_groups;
+
+	/**
+	 * Number of core groups (coherent or not) in the GPU. Equivalent to the number of L2 Caches.
+	 *
+	 * The GPU Counter dumping writes 2048 bytes per core group, regardless of
+	 * whether the core groups are coherent or not. Hence this member is needed
+	 * to calculate how much memory is required for dumping.
+	 *
+	 * @note Do not use it to work out how many valid elements are in the
+	 * group[] member. Use num_groups instead.
+	 */
+	u32 num_core_groups;
+
+	/**
+	 * Coherency features of the memory, accessed by @ref midg_mem_features
+	 * methods
+	 */
+	midg_mem_features coherency;
+
+	u32 padding;
+
+	/**
+	 * Descriptors of coherent groups
+	 */
+	struct mali_base_gpu_coherent_group group[BASE_MAX_COHERENT_GROUPS];
+};
+
+/**
+ * A complete description of the GPU's Hardware Configuration Discovery
+ * registers.
+ *
+ * The information is presented inefficiently for access. For frequent access,
+ * the values should be better expressed in an unpacked form in the
+ * base_gpu_props structure.
+ *
+ * @usecase The raw properties in @ref midg_raw_gpu_props are necessary to
+ * allow a user of the Mali Tools (e.g. PAT) to determine "Why is this device
+ * behaving differently?". In this case, all information about the
+ * configuration is potentially useful, but it <b>does not need to be processed
+ * by the driver</b>. Instead, the raw registers can be processed by the Mali
+ * Tools software on the host PC.
+ *
+ */
+struct midg_raw_gpu_props {
+	u64 shader_present;
+	u64 tiler_present;
+	u64 l2_present;
+	u64 l3_present;
+
+	midg_cache_features l2_features;
+	midg_cache_features l3_features;
+	midg_mem_features mem_features;
+	midg_mmu_features mmu_features;
+
+	midg_as_present as_present;
+
+	u32 js_present;
+	midg_js_features js_features[MIDG_MAX_JOB_SLOTS];
+	midg_tiler_features tiler_features;
+	u32 texture_features[3];
+
+	u32 gpu_id;
+	
+	u32 thread_max_threads;
+	u32 thread_max_workgroup_size;
+	u32 thread_max_barrier_size;
+	u32 thread_features;
+
+	u32 padding;
+};
+
+/**
+ * Return structure for _mali_base_get_gpu_props().
+ *
+ * NOTE: the raw_props member in this datastructure contains the register
+ * values from which the value of the other members are derived. The derived
+ * members exist to allow for efficient access and/or shielding the details
+ * of the layout of the registers.
+ *
+ */
+typedef struct mali_base_gpu_props {
+	struct mali_base_gpu_core_props core_props;
+	struct mali_base_gpu_l2_cache_props l2_props;
+	struct mali_base_gpu_l3_cache_props l3_props;
+	struct mali_base_gpu_tiler_props tiler_props;
+	struct mali_base_gpu_thread_props thread_props;
+
+	/** This member is large, likely to be 128 bytes */
+	struct midg_raw_gpu_props raw_props;
+
+	/** This must be last member of the structure */
+	struct mali_base_gpu_coherent_group_info coherency_info;
+} base_gpu_props;
+
+/** @} end group base_user_api_gpuprops_dyn */
+
+/** @} end group base_user_api_gpuprops */
+
+/**
+ * @addtogroup base_user_api_core User-side Base core APIs
+ * @{
+ */
+
+/**
+ * \enum base_context_create_flags
+ *
+ * Flags to pass to ::base_context_init.
+ * Flags can be ORed together to enable multiple things.
+ *
+ * These share the same space as @ref basep_context_private_flags, and so must
+ * not collide with them.
+ */
+enum base_context_create_flags {
+	/** No flags set */
+	BASE_CONTEXT_CREATE_FLAG_NONE = 0,
+
+	/** Base context is embedded in a cctx object (flag used for CINSTR software counter macros) */
+	BASE_CONTEXT_CCTX_EMBEDDED = (1u << 0),
+
+	/** Base context is a 'System Monitor' context for Hardware counters.
+	 *
+	 * One important side effect of this is that job submission is disabled. */
+	BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED = (1u << 1),
+
+	/** Base context flag indicating a 'hint' that this context uses Compute
+	 * Jobs only.
+	 *
+	 * Specifially, this means that it only sends atoms that <b>do not</b>
+	 * contain the following @ref base_jd_core_req :
+	 * - BASE_JD_REQ_FS
+	 * - BASE_JD_REQ_T
+	 *
+	 * Violation of these requirements will cause the Job-Chains to be rejected.
+	 *
+	 * In addition, it is inadvisable for the atom's Job-Chains to contain Jobs
+	 * of the following @ref midg_job_type (whilst it may work now, it may not
+	 * work in future) :
+	 * - @ref MIDG_JOB_VERTEX
+	 * - @ref MIDG_JOB_GEOMETRY
+	 *
+	 * @note An alternative to using this is to specify the BASE_JD_REQ_ONLY_COMPUTE
+	 * requirement in atoms.
+	 */
+	BASE_CONTEXT_HINT_ONLY_COMPUTE = (1u << 2)
+};
+
+/**
+ * Bitpattern describing the ::base_context_create_flags that can be passed to base_context_init()
+ */
+#define BASE_CONTEXT_CREATE_ALLOWED_FLAGS \
+	(((u32)BASE_CONTEXT_CCTX_EMBEDDED) | \
+	  ((u32)BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED) | \
+	  ((u32)BASE_CONTEXT_HINT_ONLY_COMPUTE))
+
+/**
+ * Bitpattern describing the ::base_context_create_flags that can be passed to the kernel
+ */
+#define BASE_CONTEXT_CREATE_KERNEL_FLAGS \
+	(((u32)BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED) | \
+	  ((u32)BASE_CONTEXT_HINT_ONLY_COMPUTE))
+
+/**
+ * Private flags used on the base context
+ *
+ * These start at bit 31, and run down to zero.
+ *
+ * They share the same space as @ref base_context_create_flags, and so must
+ * not collide with them.
+ */
+enum basep_context_private_flags {
+	/** Private flag tracking whether job descriptor dumping is disabled */
+	BASEP_CONTEXT_FLAG_JOB_DUMP_DISABLED = (1 << 31)
+};
+
+/** @} end group base_user_api_core */
+
+/** @} end group base_user_api */
+
+/**
+ * @addtogroup base_plat_config_gpuprops Base Platform Config GPU Properties
+ * @{
+ *
+ * C Pre-processor macros are exposed here to do with Platform
+ * Config.
+ *
+ * These include:
+ * - GPU Properties that are constant on a particular Midgard Family
+ * Implementation e.g. Maximum samples per pixel on Mali-T600.
+ * - General platform config for the GPU, such as the GPU major and minor
+ * revison.
+ */
+
+/** @} end group base_plat_config_gpuprops */
+
+/**
+ * @addtogroup base_api Base APIs
+ * @{
+ */
+/**
+ * @addtogroup basecpuprops Base CPU Properties
+ * @{
+ */
+
+/**
+ * @brief CPU Property Flag for base_cpu_props::cpu_flags, indicating a
+ * Little Endian System. If not set in base_cpu_props::cpu_flags, then the
+ * system is Big Endian.
+ *
+ * The compile-time equivalent is @ref OSU_CONFIG_CPU_LITTLE_ENDIAN.
+ */
+#define BASE_CPU_PROPERTY_FLAG_LITTLE_ENDIAN F_BIT_0
+
+
+/**
+ * @brief Platform dynamic CPU ID properties structure
+ */
+typedef struct base_cpu_id_props
+{
+	/**
+	 * CPU ID
+	 */
+	u32 id;
+
+	/**
+	 * CPU Part number
+	 */
+	u16 part;
+
+	/**
+	 * ASCII code of implementer trademark
+	 */
+	u8 implementer;
+
+	/**
+	 * CPU Variant
+	 */
+	u8 variant;
+
+	/**
+	 * CPU Architecture
+	 */
+	u8 arch;
+
+	/**
+	 * CPU revision
+	 */
+	u8 rev;
+	
+	/**
+	Validity of CPU id where 0-invalid and
+	1-valid only if ALL the cpu_id props are valid
+	*/
+	u8 valid;  
+
+	u8 padding[1];
+}base_cpu_id_props;
+
+
+/** @brief Platform Dynamic CPU properties structure */
+typedef struct base_cpu_props {
+	u32 nr_cores;	     /**< Number of CPU cores */
+
+    /**
+     * CPU page size as a Logarithm to Base 2. The compile-time
+     * equivalent is @ref OSU_CONFIG_CPU_PAGE_SIZE_LOG2
+     */
+	u32 cpu_page_size_log2;
+
+    /**
+     * CPU L1 Data cache line size as a Logarithm to Base 2. The compile-time
+     * equivalent is @ref OSU_CONFIG_CPU_L1_DCACHE_LINE_SIZE_LOG2.
+     */
+	u32 cpu_l1_dcache_line_size_log2;
+
+    /**
+     * CPU L1 Data cache size, in bytes. The compile-time equivalient is
+     * @ref OSU_CONFIG_CPU_L1_DCACHE_SIZE.
+     *
+     * This CPU Property is mainly provided to implement OpenCL's
+     * clGetDeviceInfo(), which allows the CL_DEVICE_GLOBAL_MEM_CACHE_SIZE
+     * hint to be queried.
+     */
+	u32 cpu_l1_dcache_size;
+
+    /**
+     * CPU Property Flags bitpattern.
+     *
+     * This is a combination of bits as specified by the macros prefixed with
+     * 'BASE_CPU_PROPERTY_FLAG_'.
+     */
+	u32 cpu_flags;
+
+    /**
+     * Maximum clock speed in MHz.
+     * @usecase 'Maximum' CPU Clock Speed information is required by OpenCL's
+     * clGetDeviceInfo() function for the CL_DEVICE_MAX_CLOCK_FREQUENCY hint.
+     */
+	u32 max_cpu_clock_speed_mhz;
+
+    /**
+     * @brief Total memory, in bytes.
+     *
+     * This is the theoretical maximum memory available to the CPU. It is
+     * unlikely that a client will be able to allocate all of this memory for
+     * their own purposes, but this at least provides an upper bound on the
+     * memory available to the CPU.
+     *
+     * This is required for OpenCL's clGetDeviceInfo() call when
+     * CL_DEVICE_GLOBAL_MEM_SIZE is requested, for OpenCL CPU devices.
+     */
+	u64 available_memory_size;
+
+	/**
+	 * CPU ID detailed info
+	 */
+	base_cpu_id_props cpu_id;
+
+	u32 padding;
+} base_cpu_props;
+/** @} end group basecpuprops */
+
+/**
+ * @brief The payload for a replay job. This must be in GPU memory.
+ */
+typedef struct base_jd_replay_payload {
+	/**
+	 * Pointer to the first entry in the base_jd_replay_jc list.  These
+	 * will be replayed in @b reverse order (so that extra ones can be added
+	 * to the head in future soft jobs without affecting this soft job)
+	 */
+	mali_addr64 tiler_jc_list;
+
+	/**
+	 * Pointer to the fragment job chain.
+	 */
+	mali_addr64 fragment_jc;
+
+	/**
+	 * Pointer to the tiler heap free FBD field to be modified.
+	 */
+	mali_addr64 tiler_heap_free;
+
+	/**
+	 * Hierarchy mask for the replayed fragment jobs. May be zero.
+	 */
+	u16 fragment_hierarchy_mask;
+
+	/**
+	 * Hierarchy mask for the replayed tiler jobs. May be zero.
+	 */
+	u16 tiler_hierarchy_mask;
+
+	/**
+	 * Default weight to be used for hierarchy levels not in the original
+	 * mask.
+	 */
+	u32 hierarchy_default_weight;
+
+	/**
+	 * Core requirements for the tiler job chain
+	 */
+	base_jd_core_req tiler_core_req;
+
+	/**
+	 * Core requirements for the fragment job chain
+	 */
+	base_jd_core_req fragment_core_req;
+
+	u8 padding[4];
+} base_jd_replay_payload;
+
+/**
+ * @brief An entry in the linked list of job chains to be replayed. This must
+ *        be in GPU memory.
+ */
+typedef struct base_jd_replay_jc {
+	/**
+	 * Pointer to next entry in the list. A setting of NULL indicates the
+	 * end of the list.
+	 */
+	mali_addr64 next;
+
+	/**
+	 * Pointer to the job chain.
+	 */
+	mali_addr64 jc;
+
+} base_jd_replay_jc;
+
+/* Maximum number of jobs allowed in a fragment chain in the payload of a
+ * replay job */
+#define BASE_JD_REPLAY_F_CHAIN_JOB_LIMIT 256
+
+/** @} end group base_api */
+
+typedef struct base_profiling_controls {
+	u32 profiling_controls[FBDUMP_CONTROL_MAX];
+} base_profiling_controls;
+
+#endif				/* _BASE_KERNEL_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel_sync.h b/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel_sync.h
new file mode 100644
index 0000000..01a837c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_base_kernel_sync.h
@@ -0,0 +1,47 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Base cross-proccess sync API.
+ */
+
+#ifndef _BASE_KERNEL_SYNC_H_
+#define _BASE_KERNEL_SYNC_H_
+
+#include <linux/ioctl.h>
+
+#define STREAM_IOC_MAGIC '~'
+
+/* Fence insert.
+ *
+ * Inserts a fence on the stream operated on.
+ * Fence can be waited via a base fence wait soft-job
+ * or triggered via a base fence trigger soft-job.
+ *
+ * Fences must be cleaned up with close when no longer needed.
+ *
+ * No input/output arguments.
+ * Returns
+ * >=0 fd
+ * <0  error code
+ */
+#define STREAM_IOC_FENCE_INSERT _IO(STREAM_IOC_MAGIC, 0)
+
+#endif				/* _BASE_KERNEL_SYNC_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_base_mem_priv.h b/drivers/gpu/mali-t6xx/r4p1/mali_base_mem_priv.h
new file mode 100644
index 0000000..4094228
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_base_mem_priv.h
@@ -0,0 +1,52 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _BASE_MEM_PRIV_H_
+#define _BASE_MEM_PRIV_H_
+
+#define BASE_SYNCSET_OP_MSYNC	(1U << 0)
+#define BASE_SYNCSET_OP_CSYNC	(1U << 1)
+
+/*
+ * This structure describe a basic memory coherency operation.
+ * It can either be:
+ * @li a sync from CPU to Memory:
+ *	- type = ::BASE_SYNCSET_OP_MSYNC
+ *	- mem_handle = a handle to the memory object on which the operation
+ *	  is taking place
+ *	- user_addr = the address of the range to be synced
+ *	- size = the amount of data to be synced, in bytes
+ *	- offset is ignored.
+ * @li a sync from Memory to CPU:
+ *	- type = ::BASE_SYNCSET_OP_CSYNC
+ *	- mem_handle = a handle to the memory object on which the operation
+ *	  is taking place
+ *	- user_addr = the address of the range to be synced
+ *	- size = the amount of data to be synced, in bytes.
+ *	- offset is ignored.
+ */
+typedef struct basep_syncset {
+	base_mem_handle mem_handle;
+	u64 user_addr;
+	u64 size;
+	u8 type;
+	u8 padding[7];
+} basep_syncset;
+
+#endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_base_vendor_specific_func.h b/drivers/gpu/mali-t6xx/r4p1/mali_base_vendor_specific_func.h
new file mode 100644
index 0000000..ce02e8b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_base_vendor_specific_func.h
@@ -0,0 +1,26 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+#ifndef _BASE_VENDOR_SPEC_FUNC_H_
+#define _BASE_VENDOR_SPEC_FUNC_H_
+
+#include <malisw/mali_stdtypes.h>
+
+mali_error kbase_get_vendor_specific_cpu_clock_speed(u32 * const);
+
+#endif	/*_BASE_VENDOR_SPEC_FUNC_H_*/
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase.h
new file mode 100644
index 0000000..41e174c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase.h
@@ -0,0 +1,455 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_H_
+#define _KBASE_H_
+
+#include <malisw/mali_malisw.h>
+
+#include <mali_kbase_debug.h>
+
+#include <asm/page.h>
+
+#include <linux/atomic.h>
+#include <linux/highmem.h>
+#include <linux/hrtimer.h>
+#include <linux/ktime.h>
+#include <linux/list.h>
+#include <linux/mm_types.h>
+#include <linux/mutex.h>
+#include <linux/rwsem.h>
+#include <linux/sched.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include <linux/workqueue.h>
+
+#include "mali_base_kernel.h"
+#include <mali_kbase_uku.h>
+#include <mali_kbase_linux.h>
+
+#include "mali_kbase_pm.h"
+#include "mali_kbase_mem_lowlevel.h"
+#include "mali_kbase_defs.h"
+#include "mali_kbase_trace_timeline.h"
+#include "mali_kbase_js.h"
+#include "mali_kbase_mem.h"
+#include "mali_kbase_security.h"
+#include "mali_kbase_utility.h"
+#include <mali_kbase_gpu_memory_debugfs.h>
+#include "mali_kbase_cpuprops.h"
+#include "mali_kbase_gpuprops.h"
+//#ifdef CONFIG_GPU_TRACEPOINTS
+//#include <trace/events/gpu.h>
+//#endif
+/**
+ * @page page_base_kernel_main Kernel-side Base (KBase) APIs
+ *
+ * The Kernel-side Base (KBase) APIs are divided up as follows:
+ * - @subpage page_kbase_js_policy
+ */
+
+/**
+ * @defgroup base_kbase_api Kernel-side Base (KBase) APIs
+ */
+
+kbase_device *kbase_device_alloc(void);
+/*
+* note: configuration attributes member of kbdev needs to have
+* been setup before calling kbase_device_init
+*/
+
+/*
+* API to acquire device list semaphone and return pointer
+* to the device list head
+*/
+const struct list_head *kbase_dev_list_get(void);
+/* API to release the device list semaphore */
+void kbase_dev_list_put(const struct list_head *dev_list);
+
+mali_error kbase_device_init(kbase_device * const kbdev);
+void kbase_device_term(kbase_device *kbdev);
+void kbase_device_free(kbase_device *kbdev);
+int kbase_device_has_feature(kbase_device *kbdev, u32 feature);
+kbase_midgard_type kbase_device_get_type(kbase_device *kbdev);
+struct kbase_device *kbase_find_device(int minor);	/* Only needed for gator integration */
+
+void kbase_set_profiling_control(struct kbase_device *kbdev, u32 control, u32 value);
+
+u32 kbase_get_profiling_control(struct kbase_device *kbdev, u32 control);
+
+/**
+ * Ensure that all IRQ handlers have completed execution
+ *
+ * @param kbdev     The kbase device
+ */
+void kbase_synchronize_irqs(kbase_device *kbdev);
+
+kbase_context *kbase_create_context(kbase_device *kbdev);
+void kbase_destroy_context(kbase_context *kctx);
+mali_error kbase_context_set_create_flags(kbase_context *kctx, u32 flags);
+
+mali_error kbase_instr_hwcnt_setup(kbase_context *kctx, kbase_uk_hwcnt_setup *setup);
+mali_error kbase_instr_hwcnt_enable(kbase_context *kctx, kbase_uk_hwcnt_setup *setup);
+mali_error kbase_instr_hwcnt_disable(kbase_context *kctx);
+mali_error kbase_instr_hwcnt_clear(kbase_context *kctx);
+mali_error kbase_instr_hwcnt_dump(kbase_context *kctx);
+mali_error kbase_instr_hwcnt_dump_irq(kbase_context *kctx);
+mali_bool kbase_instr_hwcnt_dump_complete(kbase_context *kctx, mali_bool * const success);
+void kbase_instr_hwcnt_suspend(kbase_device *kbdev);
+void kbase_instr_hwcnt_resume(kbase_device *kbdev);
+
+void kbasep_cache_clean_worker(struct work_struct *data);
+void kbase_clean_caches_done(kbase_device *kbdev);
+
+/**
+ * The GPU has completed performance count sampling successfully.
+ */
+void kbase_instr_hwcnt_sample_done(kbase_device *kbdev);
+
+mali_error kbase_jd_init(kbase_context *kctx);
+void kbase_jd_exit(kbase_context *kctx);
+mali_error kbase_jd_submit(kbase_context *kctx, const kbase_uk_job_submit *user_bag);
+void kbase_jd_done(kbase_jd_atom *katom, int slot_nr, ktime_t *end_timestamp,
+                   kbasep_js_atom_done_code done_code);
+void kbase_jd_cancel(kbase_device *kbdev, kbase_jd_atom *katom);
+void kbase_jd_zap_context(kbase_context *kctx);
+mali_bool jd_done_nolock(kbase_jd_atom *katom);
+void kbase_jd_free_external_resources(kbase_jd_atom *katom);
+mali_bool jd_submit_atom(kbase_context *kctx,
+			 const base_jd_atom_v2 *user_atom,
+			 kbase_jd_atom *katom);
+
+mali_error kbase_job_slot_init(kbase_device *kbdev);
+void kbase_job_slot_halt(kbase_device *kbdev);
+void kbase_job_slot_term(kbase_device *kbdev);
+void kbase_job_done(kbase_device *kbdev, u32 done);
+void kbase_job_zap_context(kbase_context *kctx);
+
+void kbase_job_slot_softstop(kbase_device *kbdev, int js, kbase_jd_atom *target_katom);
+void kbase_job_slot_hardstop(kbase_context *kctx, int js, kbase_jd_atom *target_katom);
+
+void kbase_event_post(kbase_context *ctx, kbase_jd_atom *event);
+int kbase_event_dequeue(kbase_context *ctx, base_jd_event_v2 *uevent);
+int kbase_event_pending(kbase_context *ctx);
+mali_error kbase_event_init(kbase_context *kctx);
+void kbase_event_close(kbase_context *kctx);
+void kbase_event_cleanup(kbase_context *kctx);
+void kbase_event_wakeup(kbase_context *kctx);
+
+int kbase_process_soft_job(kbase_jd_atom *katom);
+mali_error kbase_prepare_soft_job(kbase_jd_atom *katom);
+void kbase_finish_soft_job(kbase_jd_atom *katom);
+void kbase_cancel_soft_job(kbase_jd_atom *katom);
+void kbase_resume_suspended_soft_jobs(kbase_device *kbdev);
+
+int kbase_replay_process(kbase_jd_atom *katom);
+
+/* api used internally for register access. Contains validation and tracing */
+void kbase_reg_write(kbase_device *kbdev, u16 offset, u32 value, kbase_context *kctx);
+u32 kbase_reg_read(kbase_device *kbdev, u16 offset, kbase_context *kctx);
+void kbase_device_trace_register_access(kbase_context *kctx, kbase_reg_access_type type, u16 reg_offset, u32 reg_value);
+void kbase_device_trace_buffer_install(kbase_context *kctx, u32 *tb, size_t size);
+void kbase_device_trace_buffer_uninstall(kbase_context *kctx);
+
+/* api to be ported per OS, only need to do the raw register access */
+void kbase_os_reg_write(kbase_device *kbdev, u16 offset, u32 value);
+u32 kbase_os_reg_read(kbase_device *kbdev, u16 offset);
+
+/** Report a GPU fault.
+ *
+ * This function is called from the interrupt handler when a GPU fault occurs.
+ * It reports the details of the fault using KBASE_DEBUG_PRINT_WARN.
+ *
+ * @param kbdev     The kbase device that the GPU fault occurred from.
+ * @param multiple  Zero if only GPU_FAULT was raised, non-zero if MULTIPLE_GPU_FAULTS was also set
+ */
+void kbase_report_gpu_fault(kbase_device *kbdev, int multiple);
+
+/** Kill all jobs that are currently running from a context
+ *
+ * This is used in response to a page fault to remove all jobs from the faulting context from the hardware.
+ *
+ * @param kctx      The context to kill jobs from
+ */
+void kbase_job_kill_jobs_from_context(kbase_context *kctx);
+
+/**
+ * GPU interrupt handler
+ *
+ * This function is called from the interrupt handler when a GPU irq is to be handled.
+ *
+ * @param kbdev The kbase device to handle an IRQ for
+ * @param val   The value of the GPU IRQ status register which triggered the call
+ */
+void kbase_gpu_interrupt(kbase_device *kbdev, u32 val);
+
+/**
+ * Prepare for resetting the GPU.
+ * This function just soft-stops all the slots to ensure that as many jobs as possible are saved.
+ *
+ * The function returns a boolean which should be interpreted as follows:
+ * - MALI_TRUE - Prepared for reset, kbase_reset_gpu should be called.
+ * - MALI_FALSE - Another thread is performing a reset, kbase_reset_gpu should not be called.
+ *
+ * @return See description
+ */
+mali_bool kbase_prepare_to_reset_gpu(kbase_device *kbdev);
+
+/**
+ * Pre-locked version of @a kbase_prepare_to_reset_gpu.
+ *
+ * Identical to @a kbase_prepare_to_reset_gpu, except that the
+ * kbasep_js_device_data::runpool_irq::lock is externally locked.
+ *
+ * @see kbase_prepare_to_reset_gpu
+ */
+mali_bool kbase_prepare_to_reset_gpu_locked(kbase_device *kbdev);
+
+/** Reset the GPU
+ *
+ * This function should be called after kbase_prepare_to_reset_gpu iff it returns MALI_TRUE.
+ * It should never be called without a corresponding call to kbase_prepare_to_reset_gpu.
+ *
+ * After this function is called (or not called if kbase_prepare_to_reset_gpu returned MALI_FALSE),
+ * the caller should wait for kbdev->reset_waitq to be signalled to know when the reset has completed.
+ */
+void kbase_reset_gpu(kbase_device *kbdev);
+
+/**
+ * Pre-locked version of @a kbase_reset_gpu.
+ *
+ * Identical to @a kbase_reset_gpu, except that the
+ * kbasep_js_device_data::runpool_irq::lock is externally locked.
+ *
+ * @see kbase_reset_gpu
+ */
+void kbase_reset_gpu_locked(kbase_device *kbdev);
+
+/** Returns the name associated with a Mali exception code
+ *
+ * @param[in] exception_code  exception code
+ * @return name associated with the exception code
+ */
+const char *kbase_exception_name(u32 exception_code);
+
+/**
+ * Check whether a system suspend is in progress, or has already been suspended
+ *
+ * The caller should ensure that either kbdev->pm.active_count_lock is held, or
+ * a dmb was executed recently (to ensure the value is most
+ * up-to-date). However, without a lock the value could change afterwards.
+ *
+ * @return MALI_FALSE if a suspend is not in progress
+ * @return !=MALI_FALSE otherwise
+ */
+static INLINE mali_bool kbase_pm_is_suspending(struct kbase_device *kbdev) {
+	return kbdev->pm.suspending;
+}
+
+/**
+ * Return the atom's ID, as was originally supplied by userspace in
+ * base_jd_atom_v2::atom_number
+ */
+static INLINE int kbase_jd_atom_id(kbase_context *kctx, kbase_jd_atom *katom)
+{
+	int result;
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(katom);
+	KBASE_DEBUG_ASSERT(katom->kctx == kctx);
+
+	result = katom - &kctx->jctx.atoms[0];
+	KBASE_DEBUG_ASSERT(result >= 0 && result <= BASE_JD_ATOM_COUNT);
+	return result;
+}
+
+#if KBASE_TRACE_ENABLE != 0
+/** Add trace values about a job-slot
+ *
+ * @note Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_DEBUG not defined). Therefore, when KBASE_TRACE_ENABLE == 0 any
+ * functions called to get the parameters supplied to this macro must:
+ * - be static or static inline
+ * - must just return 0 and have no other statements present in the body.
+ */
+#define KBASE_TRACE_ADD_SLOT(kbdev, code, ctx, katom, gpu_addr, jobslot) \
+	kbasep_trace_add(kbdev, KBASE_TRACE_CODE(code), ctx, katom, gpu_addr, \
+			KBASE_TRACE_FLAG_JOBSLOT, 0, jobslot, 0)
+
+/** Add trace values about a job-slot, with info
+ *
+ * @note Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_DEBUG not defined). Therefore, when KBASE_TRACE_ENABLE == 0 any
+ * functions called to get the parameters supplied to this macro must:
+ * - be static or static inline
+ * - must just return 0 and have no other statements present in the body.
+ */
+#define KBASE_TRACE_ADD_SLOT_INFO(kbdev, code, ctx, katom, gpu_addr, jobslot, info_val) \
+	kbasep_trace_add(kbdev, KBASE_TRACE_CODE(code), ctx, katom, gpu_addr, \
+			KBASE_TRACE_FLAG_JOBSLOT, 0, jobslot, info_val)
+
+/** Add trace values about a ctx refcount
+ *
+ * @note Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_DEBUG not defined). Therefore, when KBASE_TRACE_ENABLE == 0 any
+ * functions called to get the parameters supplied to this macro must:
+ * - be static or static inline
+ * - must just return 0 and have no other statements present in the body.
+ */
+#define KBASE_TRACE_ADD_REFCOUNT(kbdev, code, ctx, katom, gpu_addr, refcount) \
+	kbasep_trace_add(kbdev, KBASE_TRACE_CODE(code), ctx, katom, gpu_addr, \
+			KBASE_TRACE_FLAG_REFCOUNT, refcount, 0, 0)
+/** Add trace values about a ctx refcount, and info
+ *
+ * @note Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_DEBUG not defined). Therefore, when KBASE_TRACE_ENABLE == 0 any
+ * functions called to get the parameters supplied to this macro must:
+ * - be static or static inline
+ * - must just return 0 and have no other statements present in the body.
+ */
+#define KBASE_TRACE_ADD_REFCOUNT_INFO(kbdev, code, ctx, katom, gpu_addr, refcount, info_val) \
+	kbasep_trace_add(kbdev, KBASE_TRACE_CODE(code), ctx, katom, gpu_addr, \
+			KBASE_TRACE_FLAG_REFCOUNT, refcount, 0, info_val)
+
+/** Add trace values (no slot or refcount)
+ *
+ * @note Any functions called through this macro will still be evaluated in
+ * Release builds (CONFIG_MALI_DEBUG not defined). Therefore, when KBASE_TRACE_ENABLE == 0 any
+ * functions called to get the parameters supplied to this macro must:
+ * - be static or static inline
+ * - must just return 0 and have no other statements present in the body.
+ */
+#define KBASE_TRACE_ADD(kbdev, code, ctx, katom, gpu_addr, info_val)     \
+	kbasep_trace_add(kbdev, KBASE_TRACE_CODE(code), ctx, katom, gpu_addr, \
+			0, 0, 0, info_val)
+
+/** Clear the trace */
+#define KBASE_TRACE_CLEAR(kbdev) \
+	kbasep_trace_clear(kbdev)
+
+/** Dump the slot trace */
+#define KBASE_TRACE_DUMP(kbdev) \
+	kbasep_trace_dump(kbdev)
+
+/** PRIVATE - do not use directly. Use KBASE_TRACE_ADD() instead */
+void kbasep_trace_add(kbase_device *kbdev, kbase_trace_code code, void *ctx, kbase_jd_atom *katom, u64 gpu_addr, u8 flags, int refcount, int jobslot, unsigned long info_val);
+/** PRIVATE - do not use directly. Use KBASE_TRACE_CLEAR() instead */
+void kbasep_trace_clear(kbase_device *kbdev);
+#else
+#ifdef CONFIG_MALI_SYSTEM_TRACE
+/* Dispatch kbase trace events as system trace events */
+#include <mali_linux_kbase_trace.h>
+#define KBASE_TRACE_ADD_SLOT( kbdev, code, ctx, katom, gpu_addr, jobslot )\
+	trace_mali_##code(jobslot, 0)
+
+#define KBASE_TRACE_ADD_SLOT_INFO( kbdev, code, ctx, katom, gpu_addr, jobslot, info_val )\
+	trace_mali_##code(jobslot, info_val)
+
+#define KBASE_TRACE_ADD_REFCOUNT( kbdev, code, ctx, katom, gpu_addr, refcount )\
+	trace_mali_##code(refcount, 0)
+
+#define KBASE_TRACE_ADD_REFCOUNT_INFO( kbdev, code, ctx, katom, gpu_addr, refcount, info_val )\
+	trace_mali_##code(refcount, info_val)
+
+#define KBASE_TRACE_ADD( kbdev, code, ctx, katom, gpu_addr, info_val )\
+	trace_mali_##code(gpu_addr, info_val)
+
+#define KBASE_TRACE_CLEAR( kbdev )\
+	do{\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(0);\
+	}while(0)
+#define KBASE_TRACE_DUMP( kbdev )\
+	do{\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(0);\
+	}while(0)
+
+#else /* CONFIG_MALI_SYSTEM_TRACE */
+#define KBASE_TRACE_ADD_SLOT( kbdev, code, ctx, katom, gpu_addr, jobslot )\
+	do{\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(ctx);\
+		CSTD_UNUSED(katom);\
+		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(jobslot);\
+	} while (0)
+
+#define KBASE_TRACE_ADD_SLOT_INFO(kbdev, code, ctx, katom, gpu_addr, jobslot, info_val)\
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(ctx);\
+		CSTD_UNUSED(katom);\
+		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(jobslot);\
+		CSTD_UNUSED(info_val);\
+		CSTD_NOP(0);\
+	} while (0)
+
+#define KBASE_TRACE_ADD_REFCOUNT(kbdev, code, ctx, katom, gpu_addr, refcount)\
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(ctx);\
+		CSTD_UNUSED(katom);\
+		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(refcount);\
+		CSTD_NOP(0);\
+	} while (0)
+
+#define KBASE_TRACE_ADD_REFCOUNT_INFO(kbdev, code, ctx, katom, gpu_addr, refcount, info_val)\
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(ctx);\
+		CSTD_UNUSED(katom);\
+		CSTD_UNUSED(gpu_addr);\
+		CSTD_UNUSED(info_val);\
+		CSTD_NOP(0);\
+	} while (0)
+
+#define KBASE_TRACE_ADD(kbdev, code, subcode, ctx, katom, val)\
+	do {\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(code);\
+		CSTD_UNUSED(subcode);\
+		CSTD_UNUSED(ctx);\
+		CSTD_UNUSED(katom);\
+		CSTD_UNUSED(val);\
+		CSTD_NOP(0);\
+	}while(0)
+
+#define KBASE_TRACE_CLEAR( kbdev )\
+	do{\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(0);\
+	}while(0)
+#define KBASE_TRACE_DUMP( kbdev )\
+	do{\
+		CSTD_UNUSED(kbdev);\
+		CSTD_NOP(0);\
+	}while(0)
+#endif /* CONFIG_MALI_SYSTEM_TRACE */
+#endif
+/** PRIVATE - do not use directly. Use KBASE_TRACE_DUMP() instead */
+void kbasep_trace_dump(kbase_device *kbdev);
+#endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.c
new file mode 100644
index 0000000..6d52dcf
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.c
@@ -0,0 +1,180 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <mali_kbase.h>
+
+/* This function is used to solve an HW issue with single iterator GPUs.
+ * If a fragment job is soft-stopped on the edge of its bounding box, can happen that the
+ * restart index is out of bounds and the rerun causes a tile range fault. If this happens
+ * we try to clamp the restart index to a correct value and rerun the job.
+ */
+/* Mask of X and Y coordinates for the coordinates words in the descriptors*/
+#define X_COORDINATE_MASK 0x00000FFF
+#define Y_COORDINATE_MASK 0x0FFF0000
+/* Max number of words needed from the fragment shader job descriptor */
+#define JOB_HEADER_SIZE_IN_WORDS 10
+#define JOB_HEADER_SIZE (JOB_HEADER_SIZE_IN_WORDS*sizeof(u32))
+
+/* Word 0: Status Word */
+#define JOB_DESC_STATUS_WORD 0
+/* Word 1: Restart Index */
+#define JOB_DESC_RESTART_INDEX_WORD 1
+/* Word 2: Fault address low word */
+#define JOB_DESC_FAULT_ADDR_LOW_WORD 2
+/* Word 8: Minimum Tile Coordinates */
+#define FRAG_JOB_DESC_MIN_TILE_COORD_WORD 8
+/* Word 9: Maximum Tile Coordinates */
+#define FRAG_JOB_DESC_MAX_TILE_COORD_WORD 9
+
+int kbasep_10969_workaround_clamp_coordinates(kbase_jd_atom *katom)
+{
+	struct device *dev = katom->kctx->kbdev->dev;
+	u32   clamped = 0;
+	dev_warn(dev, "Called TILE_RANGE_FAULT workaround clamping function.\n");
+	if (katom->core_req & BASE_JD_REQ_FS){
+		kbase_va_region *region;
+
+		kbase_gpu_vm_lock(katom->kctx);
+		region = kbase_region_tracker_find_region_enclosing_address(katom->kctx, katom->jc);
+
+		if (region){
+			phys_addr_t * page_array = kbase_get_phy_pages(region);
+
+			if (page_array){
+				u64 page_index = (katom->jc >> PAGE_SHIFT) - region->start_pfn;
+				u32 offset = katom->jc & (~PAGE_MASK);
+				u32 * page_1 = NULL;
+				u32 * page_2 = NULL;
+				u32   job_header[JOB_HEADER_SIZE_IN_WORDS];
+				void* dst = job_header;
+
+				/* we need the first 10 words of the fragment shader job descriptor. We need to check
+				 * that the offset + 10 words is less that the page size otherwise we need to load the next
+				 * page. page_size_overflow will be equal to 0 in case the whole descriptor is within the page
+				 * >0 otherwise.
+				 */
+				u32 copy_size = MIN(PAGE_SIZE - offset, JOB_HEADER_SIZE);
+
+				page_1 = kmap_atomic(pfn_to_page(PFN_DOWN(page_array[page_index])));
+
+				/* page_1 is a u32 pointer, offset is expressed in bytes */
+				page_1 += offset>>2;
+				kbase_sync_to_cpu(page_array[page_index] + offset, page_1, copy_size);
+				memcpy(dst, page_1, copy_size);
+
+				/* The data needed overflows page the dimension, need to map the subsequent page */
+				if (copy_size < JOB_HEADER_SIZE){
+					page_2 = kmap_atomic(pfn_to_page(PFN_DOWN(page_array[page_index + 1])));
+
+					kbase_sync_to_cpu(page_array[page_index + 1], page_2, JOB_HEADER_SIZE - copy_size);
+					memcpy(dst + copy_size, page_2, JOB_HEADER_SIZE - copy_size);
+				}
+
+				/* We managed to correctly map one or two pages (in case of overflow ) */
+				{
+					u32 minX,minY,maxX,maxY;
+					u32 restartX,restartY;
+
+					/* Get Bounding Box data and restart index from fault address low word*/
+					minX     = job_header[FRAG_JOB_DESC_MIN_TILE_COORD_WORD] & X_COORDINATE_MASK;
+					minY     = job_header[FRAG_JOB_DESC_MIN_TILE_COORD_WORD] & Y_COORDINATE_MASK;
+					maxX     = job_header[FRAG_JOB_DESC_MAX_TILE_COORD_WORD] & X_COORDINATE_MASK;
+					maxY     = job_header[FRAG_JOB_DESC_MAX_TILE_COORD_WORD] & Y_COORDINATE_MASK;
+					restartX = job_header[JOB_DESC_FAULT_ADDR_LOW_WORD] & X_COORDINATE_MASK;
+					restartY = job_header[JOB_DESC_FAULT_ADDR_LOW_WORD] & Y_COORDINATE_MASK;
+
+					dev_warn(dev, "Before Clamping: \n" \
+					              "Jobstatus: %08x  \n" \
+					              "restartIdx: %08x  \n" \
+					              "Fault_addr_low: %08x \n" \
+					              "minCoordsX: %08x minCoordsY: %08x \n" \
+					              "maxCoordsX: %08x maxCoordsY: %08x \n", 
+					              job_header[JOB_DESC_STATUS_WORD],
+					              job_header[JOB_DESC_RESTART_INDEX_WORD],
+					              job_header[JOB_DESC_FAULT_ADDR_LOW_WORD],
+					              minX,minY,
+					              maxX,maxY );
+
+					/* Set the restart index to the one which generated the fault*/
+					job_header[JOB_DESC_RESTART_INDEX_WORD] = job_header[JOB_DESC_FAULT_ADDR_LOW_WORD];
+
+					if (restartX < minX){
+						job_header[JOB_DESC_RESTART_INDEX_WORD] = (minX) | restartY;
+						dev_warn(dev,
+						         "Clamping restart X index to minimum. %08x clamped to %08x \n",
+						         restartX, minX );
+						clamped =  1;
+					}
+					if (restartY < minY){
+						job_header[JOB_DESC_RESTART_INDEX_WORD] = (minY) | restartX;
+						dev_warn(dev,
+						         "Clamping restart Y index to minimum. %08x clamped to %08x \n",
+						         restartY, minY );
+						clamped =  1;
+					}
+					if (restartX > maxX){
+						job_header[JOB_DESC_RESTART_INDEX_WORD] = (maxX) | restartY;
+						dev_warn(dev,
+						         "Clamping restart X index to maximum. %08x clamped to %08x \n",
+						         restartX, maxX );
+						clamped =  1;
+					}
+					if (restartY > maxY){
+						job_header[JOB_DESC_RESTART_INDEX_WORD] = (maxY) | restartX;
+						dev_warn(dev,
+						         "Clamping restart Y index to maximum. %08x clamped to %08x \n",
+						         restartY, maxY );
+						clamped =  1;
+					}
+
+					if (clamped){
+						/* Reset the fault address low word and set the job status to STOPPED */
+						job_header[JOB_DESC_FAULT_ADDR_LOW_WORD] = 0x0;
+						job_header[JOB_DESC_STATUS_WORD] = BASE_JD_EVENT_STOPPED;
+						dev_warn(dev, "After Clamping: \n"                   \
+						              "Jobstatus: %08x  \n"                  \
+						              "restartIdx: %08x  \n"                 \
+						              "Fault_addr_low: %08x \n"              \
+						              "minCoordsX: %08x minCoordsY: %08x \n" \
+						              "maxCoordsX: %08x maxCoordsY: %08x \n", 
+						              job_header[JOB_DESC_STATUS_WORD],
+						              job_header[JOB_DESC_RESTART_INDEX_WORD],
+						              job_header[JOB_DESC_FAULT_ADDR_LOW_WORD],
+						              minX,minY,
+						              maxX,maxY );
+
+						/* Flush CPU cache to update memory for future GPU reads*/
+						memcpy(page_1, dst, copy_size);
+						kbase_sync_to_memory(page_array[page_index] + offset, page_1, copy_size);
+
+						if (copy_size < JOB_HEADER_SIZE){
+							 memcpy(page_2, dst + copy_size, JOB_HEADER_SIZE - copy_size);
+							 kbase_sync_to_memory(page_array[page_index + 1], page_2, JOB_HEADER_SIZE - copy_size);
+						}
+
+					}
+				}
+				if (copy_size < JOB_HEADER_SIZE) 
+					kunmap_atomic(page_2);
+
+				kunmap_atomic(page_1);
+			}
+		}
+		kbase_gpu_vm_unlock(katom->kctx);
+	}
+	return clamped;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.h
new file mode 100644
index 0000000..85184c9
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_10969_workaround.h
@@ -0,0 +1,23 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifndef _KBASE_10969_WORKAROUND_
+#define _KBASE_10969_WORKAROUND_
+
+int kbasep_10969_workaround_clamp_coordinates( kbase_jd_atom * katom );
+
+#endif /* _KBASE_10969_WORKAROUND_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.c
new file mode 100644
index 0000000..a1c3aa8
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.c
@@ -0,0 +1,41 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_cache_policy.h
+ * Cache Policy API.
+ */
+
+#include "mali_kbase_cache_policy.h"
+
+/*
+ * The output flags should be a combination of the following values:
+ * KBASE_REG_CPU_CACHED: CPU cache should be enabled
+ */
+u32 kbase_cache_enabled(u32 flags, u32 nr_pages)
+{
+	u32 cache_flags = 0;
+
+	CSTD_UNUSED(nr_pages);
+
+	if (flags & BASE_MEM_CACHED_CPU)
+		cache_flags |= KBASE_REG_CPU_CACHED;
+
+	return cache_flags;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.h
new file mode 100644
index 0000000..70f8a4c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cache_policy.h
@@ -0,0 +1,47 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_cache_policy.h
+ * Cache Policy API.
+ */
+
+#ifndef _KBASE_CACHE_POLICY_H_
+#define _KBASE_CACHE_POLICY_H_
+
+#include <malisw/mali_malisw.h>
+#include "mali_kbase.h"
+#include "mali_base_kernel.h"
+
+/**
+ * @brief Choose the cache policy for a specific region
+ *
+ * Tells whether the CPU and GPU caches should be enabled or not for a specific region.
+ * This function can be modified to customize the cache policy depending on the flags
+ * and size of the region.
+ *
+ * @param[in] flags     flags describing attributes of the region
+ * @param[in] nr_pages  total number of pages (backed or not) for the region
+ *
+ * @return a combination of KBASE_REG_CPU_CACHED and KBASE_REG_GPU_CACHED depending
+ * on the cache policy
+ */
+u32 kbase_cache_enabled(u32 flags, u32 nr_pages);
+
+#endif				/* _KBASE_CACHE_POLICY_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.c
new file mode 100644
index 0000000..fe9f027
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.c
@@ -0,0 +1,358 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_cpuprops.h>
+#include <mali_kbase_config_defaults.h>
+
+/* Specifies how many attributes are permitted in the config (excluding terminating attribute).
+ * This is used in validation function so we can detect if configuration is properly terminated. This value can be
+ * changed if we need to introduce more attributes or many memory regions need to be defined */
+#define ATTRIBUTE_COUNT_MAX 32
+
+/* Limits for gpu frequency configuration parameters. These will use for config validation. */
+#define MAX_GPU_ALLOWED_FREQ_KHZ 1000000
+#define MIN_GPU_ALLOWED_FREQ_KHZ 1
+
+int kbasep_get_config_attribute_count(const kbase_attribute *attributes)
+{
+	int count = 1;
+
+	if (!attributes)
+		return -EINVAL;
+
+	while (attributes->id != KBASE_CONFIG_ATTR_END) {
+		attributes++;
+		count++;
+	}
+
+	return count;
+}
+
+const kbase_attribute *kbasep_get_next_attribute(const kbase_attribute *attributes, int attribute_id)
+{
+	KBASE_DEBUG_ASSERT(attributes != NULL);
+
+	while (attributes->id != KBASE_CONFIG_ATTR_END) {
+		if (attributes->id == attribute_id)
+			return attributes;
+
+		attributes++;
+	}
+	return NULL;
+}
+
+KBASE_EXPORT_TEST_API(kbasep_get_next_attribute)
+
+uintptr_t kbasep_get_config_value(struct kbase_device *kbdev, const kbase_attribute *attributes, int attribute_id)
+{
+	const kbase_attribute *attr;
+
+	KBASE_DEBUG_ASSERT(attributes != NULL);
+
+	attr = kbasep_get_next_attribute(attributes, attribute_id);
+	if (attr != NULL)
+		return attr->data;
+
+	/* default values */
+	switch (attribute_id) {
+	case KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US:
+		return DEFAULT_IRQ_THROTTLE_TIME_US;
+		/* Begin scheduling defaults */
+	case KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS:
+		return DEFAULT_JS_SCHEDULING_TICK_NS;
+	case KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS:
+		return DEFAULT_JS_SOFT_STOP_TICKS;
+	case KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL:
+		return DEFAULT_JS_SOFT_STOP_TICKS_CL;
+	case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS:
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8408))
+			return DEFAULT_JS_HARD_STOP_TICKS_SS_HW_ISSUE_8408;
+		else
+			return DEFAULT_JS_HARD_STOP_TICKS_SS;
+	case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL:
+		return DEFAULT_JS_HARD_STOP_TICKS_CL;
+	case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS:
+		return DEFAULT_JS_HARD_STOP_TICKS_NSS;
+	case KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS:
+		return DEFAULT_JS_CTX_TIMESLICE_NS;
+	case KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES:
+		return DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES;
+	case KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES:
+		return DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES;
+	case KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS:
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8408))
+			return DEFAULT_JS_RESET_TICKS_SS_HW_ISSUE_8408;
+		else
+			return DEFAULT_JS_RESET_TICKS_SS;
+	case KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL:
+		return DEFAULT_JS_RESET_TICKS_CL;
+	case KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS:
+		return DEFAULT_JS_RESET_TICKS_NSS;
+	case KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS:
+		return DEFAULT_JS_RESET_TIMEOUT_MS;
+		/* End scheduling defaults */
+	case KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS:
+		return 0;
+	case KBASE_CONFIG_ATTR_PLATFORM_FUNCS:
+		return 0;
+	case KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE:
+		return DEFAULT_SECURE_BUT_LOSS_OF_PERFORMANCE;
+	case KBASE_CONFIG_ATTR_CPU_SPEED_FUNC:
+		return DEFAULT_CPU_SPEED_FUNC;
+	case KBASE_CONFIG_ATTR_GPU_SPEED_FUNC:
+		return 0;
+	case KBASE_CONFIG_ATTR_ARID_LIMIT:
+		return DEFAULT_ARID_LIMIT;
+	case KBASE_CONFIG_ATTR_AWID_LIMIT:
+		return DEFAULT_AWID_LIMIT;
+	case KBASE_CONFIG_ATTR_POWER_MANAGEMENT_DVFS_FREQ:
+		return DEFAULT_PM_DVFS_FREQ;
+	case KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS:
+		return DEFAULT_PM_GPU_POWEROFF_TICK_NS;
+	case KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_SHADER:
+		return DEFAULT_PM_POWEROFF_TICK_SHADER;
+	case KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_GPU:
+		return DEFAULT_PM_POWEROFF_TICK_GPU;
+
+	default:
+		dev_err(kbdev->dev, "kbasep_get_config_value. Cannot get value of attribute with id=%d and no default value defined", attribute_id);
+		return 0;
+	}
+}
+
+KBASE_EXPORT_TEST_API(kbasep_get_config_value)
+
+mali_bool kbasep_platform_device_init(kbase_device *kbdev)
+{
+	kbase_platform_funcs_conf *platform_funcs;
+
+	platform_funcs = (kbase_platform_funcs_conf *) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_PLATFORM_FUNCS);
+	if (platform_funcs) {
+		if (platform_funcs->platform_init_func)
+			return platform_funcs->platform_init_func(kbdev);
+	}
+	return MALI_TRUE;
+}
+
+void kbasep_platform_device_term(kbase_device *kbdev)
+{
+	kbase_platform_funcs_conf *platform_funcs;
+
+	platform_funcs = (kbase_platform_funcs_conf *) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_PLATFORM_FUNCS);
+	if (platform_funcs) {
+		if (platform_funcs->platform_term_func)
+			platform_funcs->platform_term_func(kbdev);
+	}
+}
+
+static mali_bool kbasep_validate_gpu_clock_freq(kbase_device *kbdev, const kbase_attribute *attributes)
+{
+	uintptr_t freq_min = kbasep_get_config_value(kbdev, attributes, KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN);
+	uintptr_t freq_max = kbasep_get_config_value(kbdev, attributes, KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX);
+
+	if ((freq_min > MAX_GPU_ALLOWED_FREQ_KHZ) || (freq_min < MIN_GPU_ALLOWED_FREQ_KHZ) || (freq_max > MAX_GPU_ALLOWED_FREQ_KHZ) || (freq_max < MIN_GPU_ALLOWED_FREQ_KHZ) || (freq_min > freq_max)) {
+		dev_warn(kbdev->dev, "Invalid GPU frequencies found in configuration: min=%ldkHz, max=%ldkHz.", freq_min, freq_max);
+		return MALI_FALSE;
+	}
+
+	return MALI_TRUE;
+}
+
+static mali_bool kbasep_validate_pm_callback(const kbase_pm_callback_conf *callbacks, const kbase_device * kbdev )
+{
+	if (callbacks == NULL) {
+		/* Having no callbacks is valid */
+		return MALI_TRUE;
+	}
+
+	if ((callbacks->power_off_callback != NULL && callbacks->power_on_callback == NULL) || (callbacks->power_off_callback == NULL && callbacks->power_on_callback != NULL)) {
+		dev_warn(kbdev->dev, "Invalid power management callbacks: Only one of power_off_callback and power_on_callback was specified");
+		return MALI_FALSE;
+	}
+	return MALI_TRUE;
+}
+
+static mali_bool kbasep_validate_cpu_speed_func(kbase_cpuprops_clock_speed_function fcn)
+{
+	return fcn != NULL;
+}
+
+mali_bool kbasep_validate_configuration_attributes(kbase_device *kbdev, const kbase_attribute *attributes)
+{
+	int i;
+	mali_bool had_gpu_freq_min = MALI_FALSE, had_gpu_freq_max = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(attributes);
+
+	for (i = 0; attributes[i].id != KBASE_CONFIG_ATTR_END; i++) {
+		if (i >= ATTRIBUTE_COUNT_MAX) {
+			dev_warn(kbdev->dev, "More than ATTRIBUTE_COUNT_MAX=%d configuration attributes defined. Is attribute list properly terminated?", ATTRIBUTE_COUNT_MAX);
+			return MALI_FALSE;
+		}
+
+		switch (attributes[i].id) {
+		case KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN:
+			had_gpu_freq_min = MALI_TRUE;
+			if (MALI_FALSE == kbasep_validate_gpu_clock_freq(kbdev, attributes)) {
+				/* Warning message handled by kbasep_validate_gpu_clock_freq() */
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX:
+			had_gpu_freq_max = MALI_TRUE;
+			if (MALI_FALSE == kbasep_validate_gpu_clock_freq(kbdev, attributes)) {
+				/* Warning message handled by kbasep_validate_gpu_clock_freq() */
+				return MALI_FALSE;
+			}
+			break;
+
+			/* Only non-zero unsigned 32-bit values accepted */
+		case KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS:
+#if CSTD_CPU_64BIT
+			if (attributes[i].data == 0u || (u64) attributes[i].data > (u64) U32_MAX)
+#else
+			if (attributes[i].data == 0u)
+#endif
+			{
+				dev_warn(kbdev->dev, "Invalid Job Scheduling Configuration attribute for " "KBASE_CONFIG_ATTR_JS_SCHEDULING_TICKS_NS: %d", (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+			break;
+
+			/* All these Job Scheduling attributes are FALLTHROUGH: only unsigned 32-bit values accepted */
+		case KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS:
+		case KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL:
+		case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS:
+		case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL:
+		case KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS:
+		case KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS:
+		case KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL:
+		case KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS:
+		case KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS:
+		case KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS:
+		case KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES:
+		case KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES:
+#if CSTD_CPU_64BIT
+			if ((u64) attributes[i].data > (u64) U32_MAX) {
+				dev_warn(kbdev->dev, "Job Scheduling Configuration attribute exceeds 32-bits: " "id==%d val==%d", attributes[i].id, (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+#endif
+			break;
+
+		case KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US:
+#if CSTD_CPU_64BIT
+			if ((u64) attributes[i].data > (u64) U32_MAX) {
+				dev_warn(kbdev->dev, "IRQ throttle time attribute exceeds 32-bits: " "id==%d val==%d", attributes[i].id, (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+#endif
+			break;
+
+		case KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS:
+			if (MALI_FALSE == kbasep_validate_pm_callback((kbase_pm_callback_conf *) attributes[i].data, kbdev)) {
+				/* Warning message handled by kbasep_validate_pm_callback() */
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE:
+			if (attributes[i].data != MALI_TRUE && attributes[i].data != MALI_FALSE) {
+				dev_warn(kbdev->dev, "Value for KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE was not " "MALI_TRUE or MALI_FALSE: %u", (unsigned int)attributes[i].data);
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_CPU_SPEED_FUNC:
+			if (MALI_FALSE == kbasep_validate_cpu_speed_func((kbase_cpuprops_clock_speed_function) attributes[i].data)) {
+				dev_warn(kbdev->dev, "Invalid function pointer in KBASE_CONFIG_ATTR_CPU_SPEED_FUNC");
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_GPU_SPEED_FUNC:
+			if (0 == attributes[i].data) {
+				dev_warn(kbdev->dev, "Invalid function pointer in KBASE_CONFIG_ATTR_GPU_SPEED_FUNC");
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_PLATFORM_FUNCS:
+			/* any value is allowed */
+			break;
+
+		case KBASE_CONFIG_ATTR_AWID_LIMIT:
+		case KBASE_CONFIG_ATTR_ARID_LIMIT:
+			if ((u32) attributes[i].data > 0x3) {
+				dev_warn(kbdev->dev, "Invalid AWID or ARID limit");
+				return MALI_FALSE;
+			}
+			break;
+
+		case KBASE_CONFIG_ATTR_POWER_MANAGEMENT_DVFS_FREQ:
+#if CSTD_CPU_64BIT
+			if ((u64) attributes[i].data > (u64) U32_MAX) {
+				dev_warn(kbdev->dev, "PM DVFS interval exceeds 32-bits: " "id==%d val==%d", attributes[i].id, (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+#endif
+			break;
+
+		case KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS:
+#if CSTD_CPU_64BIT
+			if (attributes[i].data == 0u || (u64) attributes[i].data > (u64) U32_MAX) {
+#else
+			if (attributes[i].data == 0u) {
+#endif
+				dev_warn(kbdev->dev, "Invalid Power Manager Configuration attribute for " "KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS: %d", (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+			break;
+
+	case KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_SHADER:
+	case KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_GPU:
+#if CSTD_CPU_64BIT
+			if ((u64) attributes[i].data > (u64) U32_MAX) {
+				dev_warn(kbdev->dev, "Power Manager Configuration attribute exceeds 32-bits: " "id==%d val==%d", attributes[i].id, (int)attributes[i].data);
+				return MALI_FALSE;
+			}
+#endif
+			break;
+
+		default:
+			dev_warn(kbdev->dev, "Invalid attribute found in configuration: %d", attributes[i].id);
+			return MALI_FALSE;
+		}
+	}
+
+	if (!had_gpu_freq_min) {
+		dev_warn(kbdev->dev, "Configuration does not include mandatory attribute KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN");
+		return MALI_FALSE;
+	}
+
+	if (!had_gpu_freq_max) {
+		dev_warn(kbdev->dev, "Configuration does not include mandatory attribute KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX");
+		return MALI_FALSE;
+	}
+
+	return MALI_TRUE;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.h
new file mode 100644
index 0000000..52d9eda
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config.h
@@ -0,0 +1,843 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_config.h
+ * Configuration API and Attributes for KBase
+ */
+
+#ifndef _KBASE_CONFIG_H_
+#define _KBASE_CONFIG_H_
+
+#include <malisw/mali_stdtypes.h>
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_config Configuration API and Attributes
+ * @{
+ */
+
+#if MALI_CUSTOMER_RELEASE == 0
+/* This flag is set for internal builds so we can run tests without credentials. */
+#define KBASE_HWCNT_DUMP_BYPASS_ROOT 1
+#else
+#define KBASE_HWCNT_DUMP_BYPASS_ROOT 0
+#endif
+
+#include <linux/rbtree.h>
+
+/**
+ * Device wide configuration
+ */
+enum {
+	/**
+	 * Invalid attribute ID (reserve 0).
+	 *
+	 * Attached value: Ignored
+	 * Default value: NA
+	 * */
+	KBASE_CONFIG_ATTR_INVALID,
+
+	/**
+	 * Maximum frequency GPU will be clocked at. Given in kHz.
+	 * This must be specified as there is no default value.
+	 *
+	 * Attached value: number in kHz
+	 * Default value: NA
+	 */
+	KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+
+	/**
+	 * Minimum frequency GPU will be clocked at. Given in kHz.
+	 * This must be specified as there is no default value.
+	 *
+	 * Attached value: number in kHz
+	 * Default value: NA
+	 */
+	KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+
+	/**
+	 * Irq throttle. It is the minimum desired time in between two
+	 * consecutive gpu interrupts (given in 'us'). The irq throttle
+	 * gpu register will be configured after this, taking into
+	 * account the configured max frequency.
+	 *
+	 * Attached value: number in micro seconds
+	 * Default value: see DEFAULT_IRQ_THROTTLE_TIME_US
+	 */
+	KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US,
+
+	/*** Begin Job Scheduling Configs ***/
+	/**
+	 * Job Scheduler scheduling tick granuality. This is in nanoseconds to
+	 * allow HR timer support.
+	 *
+	 * On each scheduling tick, the scheduler may decide to:
+	 * -# soft stop a job (the job will be re-run later, and other jobs will
+	 * be able to run on the GPU now). This effectively controls the
+	 * 'timeslice' given to a job.
+	 * -# hard stop a job (to kill a job if it has spent too long on the GPU
+	 * and didn't soft-stop).
+	 *
+	 * The numbers of ticks for these events are controlled by:
+	 * - @ref KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS
+	 * - @ref KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS
+	 * - @ref KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS
+	 *
+	 * A soft-stopped job will later be resumed, allowing it to use more GPU
+	 * time <em>in total</em> than that defined by any of the above. However,
+	 * the scheduling policy attempts to limit the amount of \em uninterrupted
+	 * time spent on the GPU using the above values (that is, the 'timeslice'
+	 * of a job)
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::scheduling_tick_ns.
+	 * The value might be rounded down to lower precision. Must be non-zero
+	 * after rounding.<br>
+	 * Default value: @ref DEFAULT_JS_SCHEDULING_TICK_NS
+	 *
+	 * @note this value is allowed to be greater than
+	 * @ref KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS. This allows jobs to run on (much)
+	 * longer than the job-timeslice, but once this happens, the context gets
+	 * scheduled in (much) less frequently than others that stay within the
+	 * ctx-timeslice.
+	 */
+	KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before non-CL jobs
+	 * are soft-stopped.
+	 *
+	 * This defines the amount of time a job is allowed to stay on the GPU,
+	 * before it is soft-stopped to allow other jobs to run.
+	 *
+	 * That is, this defines the 'timeslice' of the job. It is separate from the
+	 * timeslice of the context that contains the job (see
+	 * @ref KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS).
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::soft_stop_ticks<br>
+	 * Default value: @ref DEFAULT_JS_SOFT_STOP_TICKS
+	 *
+	 * @note a value of zero means "the quickest time to soft-stop a job",
+	 * which is somewhere between instant and one tick later.
+	 *
+	 * @note this value is allowed to be greater than
+	 * @ref KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS or
+	 * @ref KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS. This effectively disables
+	 * soft-stop, and just uses hard-stop instead. In this case, this value
+	 * should be much greater than any of the hard stop values (to avoid
+	 * soft-stop-after-hard-stop)
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before CL jobs
+	 * are soft-stopped.
+	 *
+	 * This defines the amount of time a job is allowed to stay on the GPU,
+	 * before it is soft-stopped to allow other jobs to run.
+	 *
+	 * That is, this defines the 'timeslice' of the job. It is separate
+	 * from the timeslice of the context that contains the job (see
+	 * @ref KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS).
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit
+	 *                         kbasep_js_device_data::soft_stop_ticks_cl<br>
+	 * Default value: @ref DEFAULT_JS_SOFT_STOP_TICKS_CL
+	 *
+	 * @note a value of zero means "the quickest time to soft-stop a job",
+	 * which is somewhere between instant and one tick later.
+	 *
+	 * @note this value is allowed to be greater than
+	 * @ref KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL. This effectively
+	 * disables soft-stop, and just uses hard-stop instead. In this case,
+	 * this value should be much greater than any of the hard stop values
+	 * (to avoid soft-stop-after-hard-stop)
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before non-CL jobs
+	 * are hard-stopped.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it
+	 * is killed. Such jobs won't be resumed if killed.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::hard_stop_ticks_ss<br>
+	 * Default value: @ref DEFAULT_JS_HARD_STOP_TICKS_SS
+	 *
+	 * @note a value of zero means "the quickest time to hard-stop a job",
+	 * which is somewhere between instant and one tick later.
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before CL jobs are hard-stopped.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it
+	 * is killed. Such jobs won't be resumed if killed.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::hard_stop_ticks_cl<br>
+	 * Default value: @ref DEFAULT_JS_HARD_STOP_TICKS_CL
+	 *
+	 * @note a value of zero means "the quickest time to hard-stop a job",
+	 * which is somewhere between instant and one tick later.
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before jobs are hard-stopped
+	 * when dumping.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it
+	 * is killed. Such jobs won't be resumed if killed.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::hard_stop_ticks_nss<br>
+	 * Default value: @ref DEFAULT_JS_HARD_STOP_TICKS_NSS
+	 *
+	 * @note a value of zero means "the quickest time to hard-stop a job",
+	 * which is somewhere between instant and one tick later.
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+
+	/**
+	 * Job Scheduler timeslice that a context is scheduled in for, in nanoseconds.
+	 *
+	 * When a context has used up this amount of time across its jobs, it is
+	 * scheduled out to let another run.
+	 *
+	 * @note the resolution is nanoseconds (ns) here, because that's the format
+	 * often used by the OS.
+	 *
+	 * This value controls affects the actual time defined by the following
+	 * config values:
+	 * - @ref KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES
+	 * - @ref KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::ctx_timeslice_ns.
+	 * The value might be rounded down to lower precision.<br>
+	 * Default value: @ref DEFAULT_JS_CTX_TIMESLICE_NS
+	 *
+	 * @note a value of zero models a "Round Robin" scheduling policy, and
+	 * disables @ref KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES
+	 * (initially causing LIFO scheduling) and
+	 * @ref KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES (allowing
+	 * not-run-often contexts to get scheduled in quickly, but to only use
+	 * a single timeslice when they get scheduled in).
+	 */
+	KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+
+	/**
+	 * Job Scheduler initial runtime of a context for the CFS Policy, in time-slices.
+	 *
+	 * This value is relative to that of the least-run context, and defines
+	 * where in the CFS queue a new context is added. A value of 1 means 'after
+	 * the least-run context has used its timeslice'. Therefore, when all
+	 * contexts consistently use the same amount of time, a value of 1 models a
+	 * FIFO. A value of 0 would model a LIFO.
+	 *
+	 * The value is represented in "numbers of time slices". Multiply this
+	 * value by that defined in @ref KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS to get
+	 * the time value for this in nanoseconds.
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::cfs_ctx_runtime_init_slices<br>
+	 * Default value: @ref DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES
+	 */
+	KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES,
+
+	/**
+	 * Job Scheduler minimum runtime value of a context for CFS, in time_slices
+	 * relative to that of the least-run context.
+	 *
+	 * This is a measure of how much preferrential treatment is given to a
+	 * context that is not run very often.
+	 *
+	 * Specficially, this value defines how many timeslices such a context is
+	 * (initially) allowed to use at once. Such contexts (e.g. 'interactive'
+	 * processes) will appear near the front of the CFS queue, and can initially
+	 * use more time than contexts that run continuously (e.g. 'batch'
+	 * processes).
+	 *
+	 * This limit \b prevents a "stored-up timeslices" DoS attack, where a ctx
+	 * not run for a long time attacks the system by using a very large initial
+	 * number of timeslices when it finally does run.
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::cfs_ctx_runtime_min_slices<br>
+	 * Default value: @ref DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES
+	 *
+	 * @note A value of zero allows not-run-often contexts to get scheduled in
+	 * quickly, but to only use a single timeslice when they get scheduled in.
+	 */
+	KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before non-CL jobs
+	 * cause the GPU to be reset.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it
+	 * is assumed that the GPU has hung and needs to be reset. The assumes that the job
+	 * has been hard-stopped already and so the presence of a job that has remained on
+	 * the GPU for so long indicates that the GPU has in some way hung.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::gpu_reset_ticks_nss<br>
+	 * Default value: @ref DEFAULT_JS_RESET_TICKS_SS
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before CL jobs
+	 * cause the GPU to be reset.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it 
+	 * is assumed that the GPU has hung and needs to be reset. The assumes that the job
+	 * has been hard-stopped already and so the presence of a job that has remained on
+	 * the GPU for so long indicates that the GPU has in some way hung.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::gpu_reset_ticks_cl<br>
+	 * Default value: @ref DEFAULT_JS_RESET_TICKS_CL
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+
+	/**
+	 * Job Scheduler minimum number of scheduling ticks before jobs cause the GPU to be
+	 * reset when dumping.
+	 *
+	 * This defines the amount of time a job is allowed to spend on the GPU before it
+	 * is assumed that the GPU has hung and needs to be reset. The assumes that the job
+	 * has been hard-stopped already and so the presence of a job that has remained on
+	 * the GPU for so long indicates that the GPU has in some way hung.
+	 *
+	 * This value is supported by the following scheduling policies:
+	 * - The Completely Fair Share (CFS) policy
+	 *
+	 * Attached value: unsigned 32-bit kbasep_js_device_data::gpu_reset_ticks_nss<br>
+	 * Default value: @ref DEFAULT_JS_RESET_TICKS_NSS
+	 *
+	 * @see KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+
+	/**
+	 * Number of milliseconds given for other jobs on the GPU to be
+	 * soft-stopped when the GPU needs to be reset.
+	 *
+	 * Attached value: number in milliseconds
+	 * Default value: @ref DEFAULT_JS_RESET_TIMEOUT_MS
+	 */
+	KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	/*** End Job Scheduling Configs ***/
+
+	/** Power management configuration
+	 *
+	 * Attached value: pointer to @ref kbase_pm_callback_conf
+	 * Default value: See @ref kbase_pm_callback_conf
+	 */
+	KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+
+	/**
+	 * Boolean indicating whether the driver is configured to be secure at
+	 * a potential loss of performance.
+	 *
+	 * This currently affects only r0p0-15dev0 HW and earlier.
+	 *
+	 * On r0p0-15dev0 HW and earlier, there are tradeoffs between security and
+	 * performance:
+	 *
+	 * - When this is set to MALI_TRUE, the driver remains fully secure,
+	 * but potentially loses performance compared with setting this to
+	 * MALI_FALSE.
+	 * - When set to MALI_FALSE, the driver is open to certain security
+	 * attacks.
+	 *
+	 * From r0p0-00rel0 and onwards, there is no security loss by setting
+	 * this to MALI_FALSE, and no performance loss by setting it to
+	 * MALI_TRUE.
+	 *
+	 * Attached value: mali_bool value
+	 * Default value: @ref DEFAULT_SECURE_BUT_LOSS_OF_PERFORMANCE
+	 */
+	KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+
+	/**
+	 * A pointer to a function that calculates the CPU clock
+	 * speed of the platform in MHz - see
+	 * @ref kbase_cpuprops_clock_speed_function for the function
+	 * prototype.
+	 *
+	 * Attached value: A @ref kbase_cpuprops_clock_speed_function.
+	 * Default Value:  Pointer to @ref DEFAULT_CPU_SPEED_FUNC -
+	 *                 returns a clock speed of 100 MHz.
+	 */
+	KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+
+	/**
+	 * A pointer to a function that calculates the GPU clock
+	 * speed of the platform in MHz - see
+	 * @ref kbase_gpuprops_clock_speed_function for the function
+	 * prototype.
+	 *
+	 * Attached value: A @ref kbase_gpuprops_clock_speed_function.
+	 * Default Value:  NULL (in which case the driver assumes a current
+	 *                 GPU frequency specified by KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX)
+	 */
+	KBASE_CONFIG_ATTR_GPU_SPEED_FUNC,
+
+	/**
+	 * Platform specific configuration functions
+	 *
+	 * Attached value: pointer to @ref kbase_platform_funcs_conf
+	 * Default value: See @ref kbase_platform_funcs_conf
+	 */
+	KBASE_CONFIG_ATTR_PLATFORM_FUNCS,
+
+	/**
+	 * Limit ARID width on the AXI bus.
+	 *
+	 * Attached value: u32 register value
+	 *    KBASE_AID_32 - use the full 32 IDs (5 ID bits)
+	 *    KBASE_AID_16 - use 16 IDs (4 ID bits)
+	 *    KBASE_AID_8  - use 8 IDs (3 ID bits)
+	 *    KBASE_AID_4  - use 4 IDs (2 ID bits)
+	 * Default value: KBASE_AID_32 (no limit). Note hardware implementation
+	 * may limit to a lower value.
+	 */
+	KBASE_CONFIG_ATTR_ARID_LIMIT,
+
+	/**
+	 * Limit AWID width on the AXI bus.
+	 *
+	 * Attached value: u32 register value
+	 *    KBASE_AID_32 - use the full 32 IDs (5 ID bits)
+	 *    KBASE_AID_16 - use 16 IDs (4 ID bits)
+	 *    KBASE_AID_8  - use 8 IDs (3 ID bits)
+	 *    KBASE_AID_4  - use 4 IDs (2 ID bits)
+	 * Default value: KBASE_AID_32 (no limit). Note hardware implementation
+	 * may limit to a lower value.
+	 */
+	KBASE_CONFIG_ATTR_AWID_LIMIT,
+
+	/**
+	 * Rate at which dvfs data should be collected.
+	 *
+	 * Attached value: u32 value
+	 * Default value: 500 Milliseconds
+	 */
+	KBASE_CONFIG_ATTR_POWER_MANAGEMENT_DVFS_FREQ,
+
+	/**
+	 * Power Management poweroff tick granuality. This is in nanoseconds to
+	 * allow HR timer support.
+	 *
+	 * On each scheduling tick, the power manager core may decide to:
+	 * -# Power off one or more shader cores
+	 * -# Power off the entire GPU
+	 *
+	 * Attached value: number in nanoseconds
+	 * Default value: @ref DEFAULT_PM_GPU_POWEROFF_TICK_NS,
+	 */
+	KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS,
+
+	/**
+	 * Power Manager number of ticks before shader cores are powered off
+	 *
+	 * Attached value: unsigned 32-bit kbasep_pm_device_data::poweroff_shader_ticks<br>
+	 * Default value: @ref DEFAULT_PM_POWEROFF_TICK_SHADER
+	 *
+	 * @see KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_SHADER,
+
+	/**
+	 * Power Manager number of ticks before GPU is powered off
+	 *
+	 * Attached value: unsigned 32-bit kbasep_pm_device_data::poweroff_gpu_ticks<br>
+	 * Default value: @ref DEFAULT_PM_POWEROFF_TICK_GPU
+	 *
+	 * @see KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS
+	 */
+	KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_GPU,
+
+	/**
+	 * End of attribute list indicator.
+	 * The configuration loader will stop processing any more elements
+	 * when it encounters this attribute.
+	 *
+	 * Default value: NA
+	 */
+	KBASE_CONFIG_ATTR_END = 0x1FFFUL
+};
+
+enum {
+	/**
+	 * Use unrestricted Address ID width on the AXI bus.
+	 */
+	KBASE_AID_32 = 0x0,
+
+	/**
+	 * Restrict GPU to a half of maximum Address ID count.
+	 * This will reduce performance, but reduce bus load due to GPU.
+	 */
+	KBASE_AID_16 = 0x3,
+
+	/**
+	 * Restrict GPU to a quarter of maximum Address ID count.
+	 * This will reduce performance, but reduce bus load due to GPU.
+	 */
+	KBASE_AID_8  = 0x2,
+
+	/**
+	 * Restrict GPU to an eighth of maximum Address ID count.
+	 * This will reduce performance, but reduce bus load due to GPU.
+	 */
+	KBASE_AID_4  = 0x1
+};
+
+/*
+ * @brief specifies a single attribute
+ *
+ * Attribute is identified by attr field. Data is either integer or a pointer to attribute-specific structure.
+ */
+typedef struct kbase_attribute {
+	int id;
+	uintptr_t data;
+} kbase_attribute;
+
+/* Forward declaration of kbase_device */
+struct kbase_device;
+
+/*
+ * @brief Specifies the functions for platform specific initialization and termination
+ *
+ * By default no functions are required. No additional platform specific control is necessary.
+ */
+typedef struct kbase_platform_funcs_conf {
+	/**
+	 * Function pointer for platform specific initialization or NULL if no initialization function is required.
+	 * This function will be called \em before any other callbacks listed in the kbase_attribute struct (such as
+	 * Power Management callbacks).
+	 * The platform specific private pointer kbase_device::platform_context can be accessed (and possibly initialized) in here.
+	 */
+	mali_bool(*platform_init_func) (struct kbase_device *kbdev);
+	/**
+	 * Function pointer for platform specific termination or NULL if no termination function is required.
+	 * This function will be called \em after any other callbacks listed in the kbase_attribute struct (such as
+	 * Power Management callbacks).
+	 * The platform specific private pointer kbase_device::platform_context can be accessed (and possibly terminated) in here.
+	 */
+	void (*platform_term_func) (struct kbase_device *kbdev);
+
+} kbase_platform_funcs_conf;
+
+/*
+ * @brief Specifies the callbacks for power management
+ *
+ * By default no callbacks will be made and the GPU must not be powered off.
+ */
+typedef struct kbase_pm_callback_conf {
+	/** Callback for when the GPU is idle and the power to it can be switched off.
+	 *
+	 * The system integrator can decide whether to either do nothing, just switch off
+	 * the clocks to the GPU, or to completely power down the GPU.
+	 * The platform specific private pointer kbase_device::platform_context can be accessed and modified in here. It is the
+	 * platform \em callbacks responsiblity to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
+	 */
+	void (*power_off_callback) (struct kbase_device *kbdev);
+
+	/** Callback for when the GPU is about to become active and power must be supplied.
+	 *
+	 * This function must not return until the GPU is powered and clocked sufficiently for register access to
+	 * succeed.  The return value specifies whether the GPU was powered down since the call to power_off_callback.
+	 * If the GPU state has been lost then this function must return 1, otherwise it should return 0.
+	 * The platform specific private pointer kbase_device::platform_context can be accessed and modified in here. It is the
+	 * platform \em callbacks responsiblity to initialize and terminate this pointer if used (see @ref kbase_platform_funcs_conf).
+	 *
+	 * The return value of the first call to this function is ignored.
+	 *
+	 * @return 1 if the GPU state may have been lost, 0 otherwise.
+	 */
+	int (*power_on_callback) (struct kbase_device *kbdev);
+
+	/** Callback for when the system is requesting a suspend and GPU power
+	 * must be switched off.
+	 *
+	 * Note that if this callback is present, then this may be called
+	 * without a preceding call to power_off_callback. Therefore this
+	 * callback must be able to take any action that might otherwise happen
+	 * in power_off_callback.
+	 *
+	 * The platform specific private pointer kbase_device::platform_context
+	 * can be accessed and modified in here. It is the platform \em
+	 * callbacks responsibility to initialize and terminate this pointer if
+	 * used (see @ref kbase_platform_funcs_conf).
+	 */
+	void (*power_suspend_callback) (struct kbase_device *kbdev);
+
+	/** Callback for when the system is resuming from a suspend and GPU
+	 * power must be switched on.
+	 *
+	 * Note that if this callback is present, then this may be called
+	 * without a following call to power_on_callback. Therefore this
+	 * callback must be able to take any action that might otherwise happen
+	 * in power_on_callback.
+	 *
+	 * The platform specific private pointer kbase_device::platform_context
+	 * can be accessed and modified in here. It is the platform \em
+	 * callbacks responsibility to initialize and terminate this pointer if
+	 * used (see @ref kbase_platform_funcs_conf).
+	 */
+	void (*power_resume_callback) (struct kbase_device *kbdev);
+
+	/** Callback for handling runtime power management initialization.
+	 *
+	 * The runtime power management callbacks @ref power_runtime_off_callback and @ref power_runtime_on_callback
+	 * will become active from calls made to the OS from within this function.
+	 * The runtime calls can be triggered by calls from @ref power_off_callback and @ref power_on_callback.
+	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
+	 *
+	 * @return MALI_ERROR_NONE on success, else mali_error erro code.
+	 */
+	 mali_error(*power_runtime_init_callback) (struct kbase_device *kbdev);
+
+	/** Callback for handling runtime power management termination.
+	 *
+	 * The runtime power management callbacks @ref power_runtime_off_callback and @ref power_runtime_on_callback
+	 * should no longer be called by the OS on completion of this function.
+	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
+	 */
+	void (*power_runtime_term_callback) (struct kbase_device *kbdev);
+
+	/** Callback for runtime power-off power management callback
+	 *
+	 * For linux this callback will be called by the kernel runtime_suspend callback.
+	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
+	 *
+	 * @return 0 on success, else OS error code.
+	 */
+	void (*power_runtime_off_callback) (struct kbase_device *kbdev);
+
+	/** Callback for runtime power-on power management callback
+	 *
+	 * For linux this callback will be called by the kernel runtime_resume callback.
+	 * Note: for linux the kernel must have CONFIG_PM_RUNTIME enabled to use this feature.
+	 */
+	int (*power_runtime_on_callback) (struct kbase_device *kbdev);
+
+} kbase_pm_callback_conf;
+
+/**
+ * Type of the function pointer for KBASE_CONFIG_ATTR_CPU_SPEED_FUNC.
+ *
+ * @param clock_speed [out] Once called this will contain the current CPU clock speed in MHz.
+ *                          This is mainly used to implement OpenCL's clGetDeviceInfo().
+ *
+ * @return 0 on success, 1 on error.
+ */
+typedef int (*kbase_cpuprops_clock_speed_function) (u32 *clock_speed);
+
+/**
+ * Type of the function pointer for KBASE_CONFIG_ATTR_GPU_SPEED_FUNC.
+ *
+ * @param clock_speed [out] Once called this will contain the current GPU clock speed in MHz.
+ *                          If the system timer is not available then this function is required
+ *                          for the OpenCL queue profiling to return correct timing information.
+ *
+ * @return 0 on success, 1 on error. When an error is returned the caller assumes a current
+ * GPU speed as specified by KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX.
+ */
+typedef int (*kbase_gpuprops_clock_speed_function) (u32 *clock_speed);
+
+#ifdef CONFIG_OF
+typedef struct kbase_platform_config {
+	const kbase_attribute *attributes;
+	u32 midgard_type;
+} kbase_platform_config;
+#else
+
+/*
+ * @brief Specifies start and end of I/O memory region.
+ */
+typedef struct kbase_io_memory_region {
+	u64 start;
+	u64 end;
+} kbase_io_memory_region;
+
+/*
+ * @brief Specifies I/O related resources like IRQs and memory region for I/O operations.
+ */
+typedef struct kbase_io_resources {
+	u32 job_irq_number;
+	u32 mmu_irq_number;
+	u32 gpu_irq_number;
+	kbase_io_memory_region io_memory_region;
+} kbase_io_resources;
+
+typedef struct kbase_platform_config {
+	const kbase_attribute *attributes;
+	const kbase_io_resources *io_resources;
+	u32 midgard_type;
+} kbase_platform_config;
+
+#endif /* CONFIG_OF */
+
+/**
+ * @brief Return character string associated with the given midgard type.
+ *
+ * @param[in]  midgard_type - ID of midgard type
+  *
+ * @return  Pointer to NULL-terminated character array associated with the given midgard type
+ */
+const char *kbasep_midgard_type_to_string(u32 midgard_type);
+
+/**
+ * @brief Gets the next config attribute with the specified ID from the array of attributes.
+ *
+ * Function gets the next attribute with specified attribute id within specified array. If no such attribute is found,
+ * NULL is returned.
+ *
+ * @param[in]  attributes     Array of attributes in which lookup is performed
+ * @param[in]  attribute_id   ID of attribute
+ *
+ * @return  Pointer to the first attribute matching id or NULL if none is found.
+ */
+const kbase_attribute *kbasep_get_next_attribute(const kbase_attribute *attributes, int attribute_id);
+
+/**
+ * @brief Gets the value of a single config attribute.
+ *
+ * Function gets the value of attribute specified as parameter. If no such attribute is found in the array of
+ * attributes, default value is used.
+ *
+ * @param[in]  kbdev          Kbase device pointer
+ * @param[in]  attributes     Array of attributes in which lookup is performed
+ * @param[in]  attribute_id   ID of attribute
+ *
+ * @return Value of attribute with the given id
+ */
+uintptr_t kbasep_get_config_value(struct kbase_device *kbdev, const kbase_attribute *attributes, int attribute_id);
+
+/**
+ * @brief Validates configuration attributes
+ *
+ * Function checks validity of given configuration attributes. It will fail on any attribute with unknown id, attribute
+ * with invalid value or attribute list that is not correctly terminated. It will also fail if
+ * KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN or KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX are not specified.
+ *
+ * @param[in]  kbdev       Kbase device pointer
+ * @param[in]  attributes  Array of attributes to validate
+ *
+ * @return   MALI_TRUE if no errors have been found in the config. MALI_FALSE otherwise.
+ */
+mali_bool kbasep_validate_configuration_attributes(struct kbase_device *kbdev, const kbase_attribute *attributes);
+
+/**
+ * @brief Gets the pointer to platform config.
+ *
+ * @return Pointer to the platform config
+ */
+kbase_platform_config *kbase_get_platform_config(void);
+
+/**
+ * @brief Gets the count of attributes in array
+ *
+ * Function gets the count of attributes in array. Note that end of list indicator is also included.
+ *
+ * @param[in]  attributes     Array of attributes
+ *
+ * @return  Number of attributes in the array including end of list indicator.
+ */
+int kbasep_get_config_attribute_count(const kbase_attribute *attributes);
+
+/**
+ * @brief Platform specific call to initialize hardware
+ *
+ * Function calls a platform defined routine if specified in the configuration attributes.
+ * The routine can initialize any hardware and context state that is required for the GPU block to function.
+ *
+ * @param[in]  kbdev       Kbase device pointer
+ *
+ * @return   MALI_TRUE if no errors have been found in the config. MALI_FALSE otherwise.
+ */
+mali_bool kbasep_platform_device_init(struct kbase_device *kbdev);
+
+/**
+ * @brief Platform specific call to terminate hardware
+ *
+ * Function calls a platform defined routine if specified in the configuration attributes.
+ * The routine can destroy any platform specific context state and shut down any hardware functionality that are
+ * outside of the Power Management callbacks.
+ *
+ * @param[in]  kbdev       Kbase device pointer
+ *
+ */
+void kbasep_platform_device_term(struct kbase_device *kbdev);
+
+	  /** @} *//* end group kbase_config */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_CONFIG_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config_defaults.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config_defaults.h
new file mode 100644
index 0000000..7c73581
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_config_defaults.h
@@ -0,0 +1,202 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_config_defaults.h
+ *
+ * Default values for configuration settings
+ *
+ */
+
+#ifndef _KBASE_CONFIG_DEFAULTS_H_
+#define _KBASE_CONFIG_DEFAULTS_H_
+
+/* Default irq throttle time. This is the default desired minimum time in
+ * between two consecutive interrupts from the gpu. The irq throttle gpu
+ * register is set after this value. */
+#define DEFAULT_IRQ_THROTTLE_TIME_US 20
+
+/*** Begin Scheduling defaults ***/
+
+/**
+ * Default scheduling tick granuality, in nanoseconds
+ */
+/* 50ms */
+#define DEFAULT_JS_SCHEDULING_TICK_NS 50000000u
+
+/**
+ * Default minimum number of scheduling ticks before jobs are soft-stopped.
+ *
+ * This defines the time-slice for a job (which may be different from that of
+ * a context)
+ */
+/* Between 0.1 and 0.15s before soft-stop */
+#define DEFAULT_JS_SOFT_STOP_TICKS 2
+
+/**
+ * Default minimum number of scheduling ticks before CL jobs are soft-stopped.
+ */
+/* Between 0.05 and 0.1s before soft-stop */
+#define DEFAULT_JS_SOFT_STOP_TICKS_CL 1
+
+/**
+ * Default minimum number of scheduling ticks before jobs are hard-stopped
+ */
+/* 1.2s before hard-stop, for a certain GLES2 test at 128x128 (bound by
+ * combined vertex+tiler job)
+ */
+#define DEFAULT_JS_HARD_STOP_TICKS_SS_HW_ISSUE_8408 24
+/* Between 0.2 and 0.25s before hard-stop */
+#define DEFAULT_JS_HARD_STOP_TICKS_SS 4
+
+/**
+ * Default minimum number of scheduling ticks before CL jobs are hard-stopped.
+ */
+/* Between 0.1 and 0.15s before hard-stop */
+#define DEFAULT_JS_HARD_STOP_TICKS_CL 2
+
+/**
+ * Default minimum number of scheduling ticks before jobs are hard-stopped
+ * during dumping
+ */
+/* 60s @ 50ms tick */
+#define DEFAULT_JS_HARD_STOP_TICKS_NSS 1200
+
+/**
+ * Default minimum number of scheduling ticks before the GPU is reset
+ * to clear a "stuck" job
+ */
+/* 1.8s before resetting GPU, for a certain GLES2 test at 128x128 (bound by
+ * combined vertex+tiler job)
+ */
+#define DEFAULT_JS_RESET_TICKS_SS_HW_ISSUE_8408 36
+/* 0.3-0.35s before GPU is reset */
+#define DEFAULT_JS_RESET_TICKS_SS 6
+
+/**
+ * Default minimum number of scheduling ticks before the GPU is reset
+ * to clear a "stuck" CL job.
+ */
+/* 0.2-0.25s before GPU is reset */
+#define DEFAULT_JS_RESET_TICKS_CL 4
+
+/**
+ * Default minimum number of scheduling ticks before the GPU is reset
+ * to clear a "stuck" job during dumping.
+ */
+/* 60.1s @ 100ms tick */
+#define DEFAULT_JS_RESET_TICKS_NSS 1202
+
+/**
+ * Number of milliseconds given for other jobs on the GPU to be
+ * soft-stopped when the GPU needs to be reset.
+ */
+#define DEFAULT_JS_RESET_TIMEOUT_MS 3000
+
+/**
+ * Default timeslice that a context is scheduled in for, in nanoseconds.
+ *
+ * When a context has used up this amount of time across its jobs, it is
+ * scheduled out to let another run.
+ *
+ * @note the resolution is nanoseconds (ns) here, because that's the format
+ * often used by the OS.
+ */
+/* 0.05s - at 20fps a ctx does at least 1 frame before being scheduled out.
+ * At 40fps, 2 frames, etc
+ */
+#define DEFAULT_JS_CTX_TIMESLICE_NS 50000000
+
+/**
+ * Default initial runtime of a context for CFS, in ticks.
+ *
+ * This value is relative to that of the least-run context, and defines where
+ * in the CFS queue a new context is added.
+ */
+#define DEFAULT_JS_CFS_CTX_RUNTIME_INIT_SLICES 1
+
+/**
+ * Default minimum runtime value of a context for CFS, in ticks.
+ *
+ * This value is relative to that of the least-run context. This prevents
+ * "stored-up timeslices" DoS attacks.
+ */
+#define DEFAULT_JS_CFS_CTX_RUNTIME_MIN_SLICES 2
+
+/**
+ * Default setting for whether to prefer security or performance.
+ *
+ * Currently affects only r0p0-15dev0 HW and earlier.
+ */
+#define DEFAULT_SECURE_BUT_LOSS_OF_PERFORMANCE MALI_FALSE
+
+/**
+ * Default setting for read Address ID limiting on AXI.
+ */
+#define DEFAULT_ARID_LIMIT KBASE_AID_32
+
+/**
+ * Default setting for write Address ID limiting on AXI.
+ */
+#define DEFAULT_AWID_LIMIT KBASE_AID_32
+
+/**
+ * Default setting for using alternative hardware counters.
+ */
+#define DEFAULT_ALTERNATIVE_HWC MALI_FALSE
+
+/*** End Scheduling defaults ***/
+
+/*** Begin Power Manager defaults */
+
+/* Milliseconds */
+#define DEFAULT_PM_DVFS_FREQ 500
+
+/**
+ * Default poweroff tick granuality, in nanoseconds
+ */
+/* 400us */
+#define DEFAULT_PM_GPU_POWEROFF_TICK_NS 400000
+
+/**
+ * Default number of poweroff ticks before shader cores are powered off
+ */
+/* 400-800us */
+#define DEFAULT_PM_POWEROFF_TICK_SHADER 2
+
+/**
+ * Default number of poweroff ticks before GPU is powered off
+ */
+#define DEFAULT_PM_POWEROFF_TICK_GPU 2         /* 400-800us */
+
+/*** End Power Manager defaults ***/
+
+/**
+ * Default UMP device mapping. A UMP_DEVICE_<device>_SHIFT value which
+ * defines which UMP device this GPU should be mapped to.
+ */
+#define DEFAULT_UMP_GPU_DEVICE_SHIFT UMP_DEVICE_Z_SHIFT
+
+/**
+ * Default value for KBASE_CONFIG_ATTR_CPU_SPEED_FUNC.
+ * Points to @ref kbase_cpuprops_get_default_clock_speed.
+ */
+#define DEFAULT_CPU_SPEED_FUNC \
+	((uintptr_t)kbase_cpuprops_get_default_clock_speed)
+
+#endif /* _KBASE_CONFIG_DEFAULTS_H_ */
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_context.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_context.c
new file mode 100644
index 0000000..3320e84
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_context.c
@@ -0,0 +1,257 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_context.c
+ * Base kernel context APIs
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+
+#define MEMPOOL_PAGES 16384
+
+
+/**
+ * @brief Create a kernel base context.
+ *
+ * Allocate and init a kernel base context.
+ */
+kbase_context *kbase_create_context(kbase_device *kbdev)
+{
+	kbase_context *kctx;
+	mali_error mali_err;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	/* zero-inited as lot of code assume it's zero'ed out on create */
+	kctx = vzalloc(sizeof(*kctx));
+
+	if (!kctx)
+		goto out;
+
+	kctx->kbdev = kbdev;
+	kctx->as_nr = KBASEP_AS_NR_INVALID;
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	kctx->timeline.owner_tgid = task_tgid_nr(current);
+#endif
+	atomic_set(&kctx->setup_complete, 0);
+	atomic_set(&kctx->setup_in_progress, 0);
+	kctx->keep_gpu_powered = MALI_FALSE;
+	spin_lock_init(&kctx->mm_update_lock);
+	kctx->process_mm = NULL;
+	atomic_set(&kctx->nonmapped_pages, 0);
+
+	if (MALI_ERROR_NONE != kbase_mem_allocator_init(&kctx->osalloc, MEMPOOL_PAGES))
+		goto free_kctx;
+
+	kctx->pgd_allocator = &kctx->osalloc;
+	atomic_set(&kctx->used_pages, 0);
+
+	if (kbase_jd_init(kctx))
+		goto free_allocator;
+
+	mali_err = kbasep_js_kctx_init(kctx);
+	if (MALI_ERROR_NONE != mali_err)
+		goto free_jd;	/* safe to call kbasep_js_kctx_term  in this case */
+
+	mali_err = kbase_event_init(kctx);
+	if (MALI_ERROR_NONE != mali_err)
+		goto free_jd;
+
+	mutex_init(&kctx->reg_lock);
+
+	INIT_LIST_HEAD(&kctx->waiting_soft_jobs);
+#ifdef CONFIG_KDS
+	INIT_LIST_HEAD(&kctx->waiting_kds_resource);
+#endif
+
+	mali_err = kbase_mmu_init(kctx);
+	if (MALI_ERROR_NONE != mali_err)
+		goto free_event;
+
+	kctx->pgd = kbase_mmu_alloc_pgd(kctx);
+	if (!kctx->pgd)
+		goto free_mmu;
+
+	if (MALI_ERROR_NONE != kbase_mem_allocator_alloc(&kctx->osalloc, 1, &kctx->aliasing_sink_page))
+		goto no_sink_page;
+
+	kctx->tgid = current->tgid;
+	kctx->pid = current->pid; 
+	init_waitqueue_head(&kctx->event_queue);
+
+	kctx->cookies = KBASE_COOKIE_MASK;
+
+	/* Make sure page 0 is not used... */
+	if (kbase_region_tracker_init(kctx))
+		goto no_region_tracker;
+#ifdef CONFIG_GPU_TRACEPOINTS
+	atomic_set(&kctx->jctx.work_id, 0);
+#endif
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	atomic_set(&kctx->timeline.jd_atoms_in_flight, 0);
+#endif
+
+	return kctx;
+
+no_region_tracker:
+no_sink_page:
+	kbase_mem_allocator_free(&kctx->osalloc, 1, &kctx->aliasing_sink_page, 0);
+	kbase_mmu_free_pgd(kctx);
+free_mmu:
+	kbase_mmu_term(kctx);
+free_event:
+	kbase_event_cleanup(kctx);
+free_jd:
+	/* Safe to call this one even when didn't initialize (assuming kctx was sufficiently zeroed) */
+	kbasep_js_kctx_term(kctx);
+	kbase_jd_exit(kctx);
+free_allocator:
+	kbase_mem_allocator_term(&kctx->osalloc);
+free_kctx:
+	vfree(kctx);
+out:
+	return NULL;
+
+}
+KBASE_EXPORT_SYMBOL(kbase_create_context)
+
+static void kbase_reg_pending_dtor(struct kbase_va_region *reg)
+{
+	dev_dbg(reg->kctx->kbdev->dev, "Freeing pending unmapped region\n");
+	kbase_mem_phy_alloc_put(reg->alloc);
+	kfree(reg);
+}
+
+/**
+ * @brief Destroy a kernel base context.
+ *
+ * Destroy a kernel base context. Calls kbase_destroy_os_context() to
+ * free OS specific structures. Will release all outstanding regions.
+ */
+void kbase_destroy_context(kbase_context *kctx)
+{
+	kbase_device *kbdev;
+	int pages;
+	unsigned long pending_regions_to_clean;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	KBASE_TRACE_ADD(kbdev, CORE_CTX_DESTROY, kctx, NULL, 0u, 0u);
+
+	/* Ensure the core is powered up for the destroy process */
+	/* A suspend won't happen here, because we're in a syscall from a userspace
+	 * thread. */
+	kbase_pm_context_active(kbdev);
+
+	if (kbdev->hwcnt.kctx == kctx) {
+		/* disable the use of the hw counters if the app didn't use the API correctly or crashed */
+		KBASE_TRACE_ADD(kbdev, CORE_CTX_HWINSTR_TERM, kctx, NULL, 0u, 0u);
+		dev_warn(kbdev->dev, "The privileged process asking for instrumentation forgot to disable it " "before exiting. Will end instrumentation for them");
+		kbase_instr_hwcnt_disable(kctx);
+	}
+
+	kbase_jd_zap_context(kctx);
+	kbase_event_cleanup(kctx);
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* MMU is disabled as part of scheduling out the context */
+	kbase_mmu_free_pgd(kctx);
+
+	/* drop the aliasing sink page now that it can't be mapped anymore */
+	kbase_mem_allocator_free(&kctx->osalloc, 1, &kctx->aliasing_sink_page, 0);
+
+	/* free pending region setups */
+	pending_regions_to_clean = (~kctx->cookies) & KBASE_COOKIE_MASK;
+	while (pending_regions_to_clean) {
+		unsigned int cookie = __ffs(pending_regions_to_clean);
+		BUG_ON(!kctx->pending_regions[cookie]);
+
+		kbase_reg_pending_dtor(kctx->pending_regions[cookie]);
+
+		kctx->pending_regions[cookie] = NULL;
+		pending_regions_to_clean &= ~(1UL << cookie);
+	}
+
+	kbase_region_tracker_term(kctx);
+	kbase_gpu_vm_unlock(kctx);
+
+	/* Safe to call this one even when didn't initialize (assuming kctx was sufficiently zeroed) */
+	kbasep_js_kctx_term(kctx);
+
+	kbase_jd_exit(kctx);
+
+	kbase_pm_context_idle(kbdev);
+
+	kbase_mmu_term(kctx);
+
+	pages = atomic_read(&kctx->used_pages);
+	if (pages != 0)
+		dev_warn(kbdev->dev, "%s: %d pages in use!\n", __func__, pages);
+
+	if (kctx->keep_gpu_powered) {
+		atomic_dec(&kbdev->keep_gpu_powered_count);
+		kbase_pm_context_idle(kbdev);
+	}
+
+	kbase_mem_allocator_term(&kctx->osalloc);
+	WARN_ON(atomic_read(&kctx->nonmapped_pages) != 0);
+	vfree(kctx);
+}
+KBASE_EXPORT_SYMBOL(kbase_destroy_context)
+
+/**
+ * Set creation flags on a context
+ */
+mali_error kbase_context_set_create_flags(kbase_context *kctx, u32 flags)
+{
+	mali_error err = MALI_ERROR_NONE;
+	kbasep_js_kctx_info *js_kctx_info;
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	/* Validate flags */
+	if (flags != (flags & BASE_CONTEXT_CREATE_KERNEL_FLAGS)) {
+		err = MALI_ERROR_FUNCTION_FAILED;
+		goto out;
+	}
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+
+	/* Translate the flags */
+	if ((flags & BASE_CONTEXT_SYSTEM_MONITOR_SUBMIT_DISABLED) == 0)
+		js_kctx_info->ctx.flags &= ~((u32) KBASE_CTX_FLAG_SUBMIT_DISABLED);
+
+	if ((flags & BASE_CONTEXT_HINT_ONLY_COMPUTE) != 0)
+		js_kctx_info->ctx.flags |= (u32) KBASE_CTX_FLAG_HINT_ONLY_COMPUTE;
+
+	/* Latch the initial attributes into the Job Scheduler */
+	kbasep_js_ctx_attr_set_initial_attrs(kctx->kbdev, kctx);
+
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+ out:
+	return err;
+}
+KBASE_EXPORT_SYMBOL(kbase_context_set_create_flags)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_core_linux.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_core_linux.c
new file mode 100644
index 0000000..167dcbb
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_core_linux.c
@@ -0,0 +1,3060 @@
+
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_core_linux.c
+ * Base kernel driver init.
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_uku.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_mem_linux.h>
+#ifdef CONFIG_MALI_NO_MALI
+#include "mali_kbase_model_linux.h"
+#endif /* CONFIG_MALI_NO_MALI */
+
+#ifdef CONFIG_KDS
+#include <linux/kds.h>
+#include <linux/anon_inodes.h>
+#include <linux/syscalls.h>
+#endif /* CONFIG_KDS */
+
+#include <linux/module.h>
+#include <linux/init.h>
+#include <linux/poll.h>
+#include <linux/kernel.h>
+#include <linux/errno.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/list.h>
+#include <linux/semaphore.h>
+#include <linux/fs.h>
+#include <linux/uaccess.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/compat.h>	/* is_compat_task */
+#include <mali_kbase_hw.h>
+#include <platform/mali_kbase_platform_common.h>
+#ifdef CONFIG_SYNC
+#include <mali_kbase_sync.h>
+#endif /* CONFIG_SYNC */
+
+/*
+ * This file is included since when we support device tree we don't
+ * use the platform fake code for registering the kbase config attributes.
+ */
+#ifdef CONFIG_OF
+#include <mali_kbase_config.h>
+#endif
+
+#ifdef CONFIG_MACH_MANTA
+#include <plat/devs.h>
+#endif
+
+#define	JOB_IRQ_TAG	0
+#define MMU_IRQ_TAG	1
+#define GPU_IRQ_TAG	2
+
+struct kbase_irq_table {
+	u32 tag;
+	irq_handler_t handler;
+};
+#if MALI_UNIT_TEST
+kbase_exported_test_data shared_kernel_test_data;
+EXPORT_SYMBOL(shared_kernel_test_data);
+#endif /* MALI_UNIT_TEST */
+
+#define KBASE_DRV_NAME "mali"
+
+static const char kbase_drv_name[] = KBASE_DRV_NAME;
+
+static int kbase_dev_nr;
+
+static DEFINE_SEMAPHORE(kbase_dev_list_lock);
+static LIST_HEAD(kbase_dev_list);
+
+KBASE_EXPORT_TEST_API(kbase_dev_list_lock)
+KBASE_EXPORT_TEST_API(kbase_dev_list)
+#define KERNEL_SIDE_DDK_VERSION_STRING "K:" MALI_RELEASE_NAME "(GPL)"
+static INLINE void __compile_time_asserts(void)
+{
+	CSTD_COMPILE_TIME_ASSERT(sizeof(KERNEL_SIDE_DDK_VERSION_STRING) <= KBASE_GET_VERSION_BUFFER_SIZE);
+}
+
+#ifdef CONFIG_KDS
+
+typedef struct kbasep_kds_resource_set_file_data {
+	struct kds_resource_set *lock;
+} kbasep_kds_resource_set_file_data;
+
+static int kds_resource_release(struct inode *inode, struct file *file);
+
+static const struct file_operations kds_resource_fops = {
+	.release = kds_resource_release
+};
+
+typedef struct kbase_kds_resource_list_data {
+	struct kds_resource **kds_resources;
+	unsigned long *kds_access_bitmap;
+	int num_elems;
+} kbase_kds_resource_list_data;
+
+static int kds_resource_release(struct inode *inode, struct file *file)
+{
+	struct kbasep_kds_resource_set_file_data *data;
+
+	data = (struct kbasep_kds_resource_set_file_data *)file->private_data;
+	if (NULL != data) {
+		if (NULL != data->lock)
+			kds_resource_set_release(&data->lock);
+
+		kfree(data);
+	}
+	return 0;
+}
+
+mali_error kbasep_kds_allocate_resource_list_data(kbase_context *kctx, base_external_resource *ext_res, int num_elems, kbase_kds_resource_list_data *resources_list)
+{
+	base_external_resource *res = ext_res;
+	int res_id;
+
+	/* assume we have to wait for all */
+
+	KBASE_DEBUG_ASSERT(0 != num_elems);
+	resources_list->kds_resources = kmalloc(sizeof(struct kds_resource *) * num_elems, GFP_KERNEL);
+
+	if (NULL == resources_list->kds_resources)
+		return MALI_ERROR_OUT_OF_MEMORY;
+
+	KBASE_DEBUG_ASSERT(0 != num_elems);
+	resources_list->kds_access_bitmap = kzalloc(sizeof(unsigned long) * ((num_elems + BITS_PER_LONG - 1) / BITS_PER_LONG), GFP_KERNEL);
+
+	if (NULL == resources_list->kds_access_bitmap) {
+		kfree(resources_list->kds_access_bitmap);
+		return MALI_ERROR_OUT_OF_MEMORY;
+	}
+
+	kbase_gpu_vm_lock(kctx);
+	for (res_id = 0; res_id < num_elems; res_id++, res++) {
+		int exclusive;
+		kbase_va_region *reg;
+		struct kds_resource *kds_res = NULL;
+
+		exclusive = res->ext_resource & BASE_EXT_RES_ACCESS_EXCLUSIVE;
+		reg = kbase_region_tracker_find_region_enclosing_address(kctx, res->ext_resource & ~BASE_EXT_RES_ACCESS_EXCLUSIVE);
+
+		/* did we find a matching region object? */
+		if (NULL == reg)
+			break;
+
+		/* no need to check reg->alloc as only regions with an alloc has
+		 * a size, and kbase_region_tracker_find_region_enclosing_address
+		 * only returns regions with size > 0 */
+		switch (reg->alloc->type) {
+#if defined(CONFIG_UMP) && defined(CONFIG_KDS)
+		case KBASE_MEM_TYPE_IMPORTED_UMP:
+			kds_res = ump_dd_kds_resource_get(reg->alloc->imported.ump_handle);
+			break;
+#endif /* defined(CONFIG_UMP) && defined(CONFIG_KDS) */
+		default:
+			break;
+		}
+
+		/* no kds resource for the region ? */
+		if (!kds_res)
+			break;
+
+		resources_list->kds_resources[res_id] = kds_res;
+
+		if (exclusive)
+			set_bit(res_id, resources_list->kds_access_bitmap);
+	}
+	kbase_gpu_vm_unlock(kctx);
+
+	/* did the loop run to completion? */
+	if (res_id == num_elems)
+		return MALI_ERROR_NONE;
+
+	/* Clean up as the resource list is not valid. */
+	kfree(resources_list->kds_resources);
+	kfree(resources_list->kds_access_bitmap);
+
+	return MALI_ERROR_FUNCTION_FAILED;
+}
+
+mali_bool kbasep_validate_kbase_pointer(kbase_pointer *p)
+{
+#ifdef CONFIG_COMPAT
+	if (is_compat_task()) {
+		if (p->compat_value == 0)
+			return MALI_FALSE;
+	} else {
+#endif /* CONFIG_COMPAT */
+		if (NULL == p->value)
+			return MALI_FALSE;
+#ifdef CONFIG_COMPAT
+	}
+#endif /* CONFIG_COMPAT */
+	return MALI_TRUE;
+}
+
+mali_error kbase_external_buffer_lock(kbase_context *kctx, kbase_uk_ext_buff_kds_data *args, u32 args_size)
+{
+	base_external_resource *ext_res_copy;
+	size_t ext_resource_size;
+	mali_error return_error = MALI_ERROR_FUNCTION_FAILED;
+	int fd;
+
+	if (args_size != sizeof(kbase_uk_ext_buff_kds_data))
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	/* Check user space has provided valid data */
+	if (!kbasep_validate_kbase_pointer(&args->external_resource) || !kbasep_validate_kbase_pointer(&args->file_descriptor) || (0 == args->num_res) || (args->num_res > KBASE_MAXIMUM_EXT_RESOURCES))
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	ext_resource_size = sizeof(base_external_resource) * args->num_res;
+
+	KBASE_DEBUG_ASSERT(0 != ext_resource_size);
+	ext_res_copy = kmalloc(ext_resource_size, GFP_KERNEL);
+
+	if (NULL != ext_res_copy) {
+		base_external_resource *__user ext_res_user;
+		int *__user file_descriptor_user;
+#ifdef CONFIG_COMPAT
+		if (is_compat_task()) {
+			ext_res_user = compat_ptr(args->external_resource.compat_value);
+			file_descriptor_user = compat_ptr(args->file_descriptor.compat_value);
+		} else {
+#endif /* CONFIG_COMPAT */
+			ext_res_user = args->external_resource.value;
+			file_descriptor_user = args->file_descriptor.value;
+#ifdef CONFIG_COMPAT
+		}
+#endif /* CONFIG_COMPAT */
+
+		/* Copy the external resources to lock from user space */
+		if (0 == copy_from_user(ext_res_copy, ext_res_user, ext_resource_size)) {
+			kbasep_kds_resource_set_file_data *fdata;
+
+			/* Allocate data to be stored in the file */
+			fdata = kmalloc(sizeof(kbasep_kds_resource_set_file_data), GFP_KERNEL);
+
+			if (NULL != fdata) {
+				kbase_kds_resource_list_data resource_list_data;
+				/* Parse given elements and create resource and access lists */
+				return_error = kbasep_kds_allocate_resource_list_data(kctx, ext_res_copy, args->num_res, &resource_list_data);
+				if (MALI_ERROR_NONE == return_error) {
+					long err;
+
+					fdata->lock = NULL;
+
+					fd = anon_inode_getfd("kds_ext", &kds_resource_fops, fdata, 0);
+
+					err = copy_to_user(file_descriptor_user, &fd, sizeof(fd));
+
+					/* If the file descriptor was valid and we successfully copied it to user space, then we
+					 * can try and lock the requested kds resources.
+					 */
+					if ((fd >= 0) && (0 == err)) {
+						struct kds_resource_set *lock;
+
+						lock = kds_waitall(args->num_res, resource_list_data.kds_access_bitmap, resource_list_data.kds_resources, KDS_WAIT_BLOCKING);
+
+						if (IS_ERR_OR_NULL(lock)) {
+							return_error = MALI_ERROR_FUNCTION_FAILED;
+						} else {
+							return_error = MALI_ERROR_NONE;
+							fdata->lock = lock;
+						}
+					} else {
+						return_error = MALI_ERROR_FUNCTION_FAILED;
+					}
+
+					kfree(resource_list_data.kds_resources);
+					kfree(resource_list_data.kds_access_bitmap);
+				}
+
+				if (MALI_ERROR_NONE != return_error) {
+					/* If the file was opened successfully then close it which will clean up
+					 * the file data, otherwise we clean up the file data ourself. */
+					if (fd >= 0)
+						sys_close(fd);
+					else
+						kfree(fdata);
+				}
+			} else {
+				return_error = MALI_ERROR_OUT_OF_MEMORY;
+			}
+		}
+		kfree(ext_res_copy);
+	}
+	return return_error;
+}
+#endif /* CONFIG_KDS */
+
+static mali_error kbase_dispatch(kbase_context *kctx, void * const args, u32 args_size)
+{
+	struct kbase_device *kbdev;
+	uk_header *ukh = args;
+	u32 id;
+
+	KBASE_DEBUG_ASSERT(ukh != NULL);
+
+	kbdev = kctx->kbdev;
+	id = ukh->id;
+	ukh->ret = MALI_ERROR_NONE;	/* Be optimistic */
+
+	if (UKP_FUNC_ID_CHECK_VERSION == id) {
+		if (args_size == sizeof(uku_version_check_args)) {
+			uku_version_check_args *version_check = (uku_version_check_args *)args;
+
+			version_check->major = BASE_UK_VERSION_MAJOR;
+			version_check->minor = BASE_UK_VERSION_MINOR;
+
+			ukh->ret = MALI_ERROR_NONE;
+		} else {
+			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+		}
+		return MALI_ERROR_NONE;
+	}
+
+
+	if (!atomic_read(&kctx->setup_complete)) {
+		/* setup pending, try to signal that we'll do the setup */
+		if (atomic_cmpxchg(&kctx->setup_in_progress, 0, 1)) {
+			/* setup was already in progress, err this call */
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		/* we're the one doing setup */
+
+		/* is it the only call we accept? */
+		if (id == KBASE_FUNC_SET_FLAGS) {
+			kbase_uk_set_flags *kbase_set_flags = (kbase_uk_set_flags *) args;
+
+			if (sizeof(*kbase_set_flags) != args_size) {
+				/* not matching the expected call, stay stuck in setup mode */
+				goto bad_size;
+			}
+
+			if (MALI_ERROR_NONE != kbase_context_set_create_flags(kctx, kbase_set_flags->create_flags)) {
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				/* bad flags, will stay stuck in setup mode */
+				return MALI_ERROR_NONE;
+			} else {
+				/* we've done the setup, all OK */
+				atomic_set(&kctx->setup_complete, 1);
+				return MALI_ERROR_NONE;
+			}
+		} else {
+			/* unexpected call, will stay stuck in setup mode */
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+	}
+
+	/* setup complete, perform normal operation */
+	switch (id) {
+	case KBASE_FUNC_MEM_ALLOC:
+		{
+			kbase_uk_mem_alloc *mem = args;
+			struct kbase_va_region *reg;
+
+			if (sizeof(*mem) != args_size)
+				goto bad_size;
+
+			reg = kbase_mem_alloc(kctx, mem->va_pages, mem->commit_pages, mem->extent, &mem->flags, &mem->gpu_va, &mem->va_alignment);
+			if (!reg)
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+	case KBASE_FUNC_MEM_IMPORT:
+		{
+			kbase_uk_mem_import *mem_import = args;
+			int *__user phandle;
+			int handle;
+
+			if (sizeof(*mem_import) != args_size)
+				goto bad_size;
+#ifdef CONFIG_64BIT
+			if (is_compat_task())
+				phandle = compat_ptr(mem_import->phandle.compat_value);
+			else
+#endif
+				phandle = mem_import->phandle.value;
+
+			switch (mem_import->type) {
+			case BASE_MEM_IMPORT_TYPE_UMP:
+				get_user(handle, phandle);
+				break;
+			case BASE_MEM_IMPORT_TYPE_UMM:
+				get_user(handle, phandle);
+				break;
+			default:
+				goto bad_type;
+				break;
+			}
+
+			if (kbase_mem_import(kctx, mem_import->type, handle, &mem_import->gpu_va, &mem_import->va_pages, &mem_import->flags)) {
+bad_type:
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			}
+			break;
+		}
+	case KBASE_FUNC_MEM_ALIAS: {
+			kbase_uk_mem_alias *alias = args;
+			struct base_mem_aliasing_info *__user user_ai;
+			struct base_mem_aliasing_info *ai;
+
+			if (sizeof(*alias) != args_size)
+				goto bad_size;
+
+			if (alias->nents > 4) {
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+#ifdef CONFIG_64BIT
+			if (is_compat_task())
+				user_ai = compat_ptr(alias->ai.compat_value);
+			else
+#endif
+				user_ai = alias->ai.value;
+
+			ai = kmalloc(GFP_KERNEL, sizeof(*ai) * alias->nents);
+			if (!ai) {
+				ukh->ret = MALI_ERROR_OUT_OF_MEMORY;
+				break;
+			}
+
+			if (copy_from_user(ai, user_ai,
+					   sizeof(*ai) * alias->nents)) {
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				goto copy_failed;
+			}
+
+			alias->gpu_va = kbase_mem_alias(kctx, &alias->flags,
+							alias->stride,
+							alias->nents, ai,
+							&alias->va_pages);
+			if (!alias->gpu_va) {
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				goto no_alias;
+			}
+no_alias:
+copy_failed:
+			kfree(ai);
+			break;
+		}
+	case KBASE_FUNC_MEM_COMMIT:
+		{
+			kbase_uk_mem_commit *commit = args;
+
+			if (sizeof(*commit) != args_size)
+				goto bad_size;
+
+			if (commit->gpu_addr & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_COMMIT: commit->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			if (kbase_mem_commit(kctx, commit->gpu_addr, commit->pages, (base_backing_threshold_status*)&commit->result_subcode))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_MEM_QUERY:
+		{
+			kbase_uk_mem_query *query = args;
+			if (sizeof(*query) != args_size)
+				goto bad_size;
+
+			if (query->gpu_addr & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_QUERY: query->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+			if (query->query != KBASE_MEM_QUERY_COMMIT_SIZE &&
+			    query->query != KBASE_MEM_QUERY_VA_SIZE &&
+				query->query != KBASE_MEM_QUERY_FLAGS) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_QUERY: query->query = %lld unknown", (unsigned long long)query->query);
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			ukh->ret = kbase_mem_query(kctx, query->gpu_addr, query->query, &query->value);
+			break;
+		}
+		break;
+
+	case KBASE_FUNC_MEM_FLAGS_CHANGE:
+		{
+			kbase_uk_mem_flags_change * fc = args;
+			if (sizeof(*fc) != args_size)
+				goto bad_size;
+
+			if ((fc->gpu_va & ~PAGE_MASK) && (fc->gpu_va >= PAGE_SIZE)) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_FLAGS_CHANGE: mem->gpu_va: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			if (kbase_mem_flags_change(kctx, fc->gpu_va, fc->flags, fc->mask))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+
+			break;
+		}
+	case KBASE_FUNC_MEM_FREE:
+		{
+			kbase_uk_mem_free *mem = args;
+
+			if (sizeof(*mem) != args_size)
+				goto bad_size;
+
+			if ((mem->gpu_addr & ~PAGE_MASK) && (mem->gpu_addr >= PAGE_SIZE)) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_MEM_FREE: mem->gpu_addr: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			if (kbase_mem_free(kctx, mem->gpu_addr))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_JOB_SUBMIT:
+		{
+			kbase_uk_job_submit *job = args;
+
+			if (sizeof(*job) != args_size)
+				goto bad_size;
+
+			if (MALI_ERROR_NONE != kbase_jd_submit(kctx, job))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_SYNC:
+		{
+			kbase_uk_sync_now *sn = args;
+
+			if (sizeof(*sn) != args_size)
+				goto bad_size;
+
+			if (sn->sset.basep_sset.mem_handle & ~PAGE_MASK) {
+				dev_warn(kbdev->dev, "kbase_dispatch case KBASE_FUNC_SYNC: sn->sset.basep_sset.mem_handle: passed parameter is invalid");
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			if (MALI_ERROR_NONE != kbase_sync_now(kctx, &sn->sset))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_POST_TERM:
+		{
+			kbase_event_close(kctx);
+			break;
+		}
+
+	case KBASE_FUNC_HWCNT_SETUP:
+		{
+			kbase_uk_hwcnt_setup *setup = args;
+
+			if (sizeof(*setup) != args_size)
+				goto bad_size;
+
+			if (MALI_ERROR_NONE != kbase_instr_hwcnt_setup(kctx, setup))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_HWCNT_DUMP:
+		{
+			/* args ignored */
+			if (MALI_ERROR_NONE != kbase_instr_hwcnt_dump(kctx))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_HWCNT_CLEAR:
+		{
+			/* args ignored */
+			if (MALI_ERROR_NONE != kbase_instr_hwcnt_clear(kctx))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_CPU_PROPS_REG_DUMP:
+		{
+			kbase_uk_cpuprops *setup = args;
+
+			if (sizeof(*setup) != args_size)
+				goto bad_size;
+
+			if (MALI_ERROR_NONE != kbase_cpuprops_uk_get_props(kctx, setup))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+
+	case KBASE_FUNC_GPU_PROPS_REG_DUMP:
+		{
+			kbase_uk_gpuprops *setup = args;
+
+			if (sizeof(*setup) != args_size)
+				goto bad_size;
+
+			if (MALI_ERROR_NONE != kbase_gpuprops_uk_get_props(kctx, setup))
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			break;
+		}
+	case KBASE_FUNC_FIND_CPU_OFFSET:
+		{
+			kbase_uk_find_cpu_offset *find = args;
+
+			if (sizeof(*find) != args_size)
+				goto bad_size;
+
+			if (find->gpu_addr & ~PAGE_MASK) {
+				dev_warn(kbdev->dev,	
+					"kbase_dispatch case KBASE_FUNC_FIND_CPU_OFFSET:"
+					"find->gpu_addr: passed parameter is invalid");
+				goto out_bad;
+			}
+
+			if (find->size > SIZE_MAX || find->cpu_addr > ULONG_MAX)
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			else {
+				mali_error err;
+
+				err = kbasep_find_enclosing_cpu_mapping_offset(
+						kctx,
+						find->gpu_addr,
+						(uintptr_t) find->cpu_addr,
+						(size_t) find->size,
+						&find->offset);
+
+				if (err != MALI_ERROR_NONE)
+					ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+			}
+			break;
+		}
+	case KBASE_FUNC_GET_VERSION:
+		{
+			kbase_uk_get_ddk_version *get_version = (kbase_uk_get_ddk_version *) args;
+
+			if (sizeof(*get_version) != args_size)
+				goto bad_size;
+
+			/* version buffer size check is made in compile time assert */
+			memcpy(get_version->version_buffer, KERNEL_SIDE_DDK_VERSION_STRING, sizeof(KERNEL_SIDE_DDK_VERSION_STRING));
+			get_version->version_string_size = sizeof(KERNEL_SIDE_DDK_VERSION_STRING);
+			break;
+		}
+
+	case KBASE_FUNC_STREAM_CREATE:
+		{
+#ifdef CONFIG_SYNC
+			kbase_uk_stream_create *screate = (kbase_uk_stream_create *) args;
+
+			if (sizeof(*screate) != args_size)
+				goto bad_size;
+
+			if (strnlen(screate->name, sizeof(screate->name)) >= sizeof(screate->name)) {
+				/* not NULL terminated */
+				ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+				break;
+			}
+
+			ukh->ret = kbase_stream_create(screate->name, &screate->fd);
+#else /* CONFIG_SYNC */
+			ukh->ret = MALI_ERROR_FUNCTION_FAILED;
+#endif /* CONFIG_SYNC */
+			break;
+		}
+	case KBASE_FUNC_FENCE_VALIDATE:
+		{
+#ifdef CONFIG_SYNC
+			kbase_uk_fence_validate *fence_validate = (kbase_uk_fence_validate *) args;
+			if (sizeof(*fence_validate) != args_size)
+				goto bad_size;
+
+			ukh->ret = kbase_fence_validate(fence_validate->fd);
+#endif /* CONFIG_SYNC */
+			break;
+		}
+
+	case KBASE_FUNC_EXT_BUFFER_LOCK:
+		{
+#ifdef CONFIG_KDS
+			ukh->ret = kbase_external_buffer_lock(kctx, (kbase_uk_ext_buff_kds_data *) args, args_size);
+#endif /* CONFIG_KDS */
+			break;
+		}
+
+	case KBASE_FUNC_SET_TEST_DATA:
+		{
+#if MALI_UNIT_TEST
+			kbase_uk_set_test_data *set_data = args;
+
+			shared_kernel_test_data = set_data->test_data;
+			shared_kernel_test_data.kctx.value = kctx;
+			shared_kernel_test_data.mm.value = (void *)current->mm;
+			ukh->ret = MALI_ERROR_NONE;
+#endif /* MALI_UNIT_TEST */
+			break;
+		}
+
+	case KBASE_FUNC_INJECT_ERROR:
+		{
+#ifdef CONFIG_MALI_ERROR_INJECT
+			unsigned long flags;
+			kbase_error_params params = ((kbase_uk_error_params *) args)->params;
+			/*mutex lock */
+			spin_lock_irqsave(&kbdev->reg_op_lock, flags);
+			ukh->ret = job_atom_inject_error(&params);
+			spin_unlock_irqrestore(&kbdev->reg_op_lock, flags);
+			/*mutex unlock */
+#endif /* CONFIG_MALI_ERROR_INJECT */
+			break;
+		}
+
+	case KBASE_FUNC_MODEL_CONTROL:
+		{
+#ifdef CONFIG_MALI_NO_MALI
+			unsigned long flags;
+			kbase_model_control_params params = ((kbase_uk_model_control_params *) args)->params;
+			/*mutex lock */
+			spin_lock_irqsave(&kbdev->reg_op_lock, flags);
+			ukh->ret = midg_model_control(kbdev->model, &params);
+			spin_unlock_irqrestore(&kbdev->reg_op_lock, flags);
+			/*mutex unlock */
+#endif /* CONFIG_MALI_NO_MALI */
+			break;
+		}
+
+	case KBASE_FUNC_KEEP_GPU_POWERED:
+		{
+			kbase_uk_keep_gpu_powered *kgp = (kbase_uk_keep_gpu_powered *) args;
+			/* A suspend won't happen here, because we're in a syscall from a
+			 * userspace thread.
+			 *
+			 * Nevertheless, we'd get the wrong pm_context_active/idle counting
+			 * here if a suspend did happen, so let's assert it won't: */
+			KBASE_DEBUG_ASSERT(!kbase_pm_is_suspending(kbdev));
+
+			if (kgp->enabled && !kctx->keep_gpu_powered) {
+				kbase_pm_context_active(kbdev);
+				atomic_inc(&kbdev->keep_gpu_powered_count);
+				kctx->keep_gpu_powered = MALI_TRUE;
+			} else if (!kgp->enabled && kctx->keep_gpu_powered) {
+				atomic_dec(&kbdev->keep_gpu_powered_count);
+				kbase_pm_context_idle(kbdev);
+				kctx->keep_gpu_powered = MALI_FALSE;
+			}
+
+			break;
+		}
+
+	case KBASE_FUNC_GET_PROFILING_CONTROLS :
+		{
+			struct kbase_uk_profiling_controls *controls = \
+					(struct kbase_uk_profiling_controls *)args;
+			u32 i;
+
+			if (sizeof(*controls) != args_size)
+				goto bad_size;
+
+			for (i = FBDUMP_CONTROL_MIN; i < FBDUMP_CONTROL_MAX; i++) {
+				controls->profiling_controls[i] = kbase_get_profiling_control(kbdev, i);
+			}
+
+			break;
+		}
+
+	/* used only for testing purposes; these controls are to be set by gator through gator API */
+	case KBASE_FUNC_SET_PROFILING_CONTROLS :
+		{
+			struct kbase_uk_profiling_controls *controls = \
+					(struct kbase_uk_profiling_controls *)args;
+			u32 i;
+
+			if (sizeof(*controls) != args_size)
+				goto bad_size;
+
+			for (i = FBDUMP_CONTROL_MIN; i < FBDUMP_CONTROL_MAX; i++)
+			{
+				_mali_profiling_control(i, controls->profiling_controls[i]);
+			}
+
+			break;
+		}
+
+	default:
+		dev_err(kbdev->dev, "unknown ioctl %u", id);
+		goto out_bad;
+	}
+
+	return MALI_ERROR_NONE;
+
+ bad_size:
+	dev_err(kbdev->dev, "Wrong syscall size (%d) for %08x\n", args_size, id);
+ out_bad:
+	return MALI_ERROR_FUNCTION_FAILED;
+}
+
+static struct kbase_device *to_kbase_device(struct device *dev)
+{
+	return dev_get_drvdata(dev);
+}
+
+/*
+ * API to acquire device list semaphore and
+ * return pointer to the device list head
+ */
+const struct list_head *kbase_dev_list_get(void)
+{
+	down(&kbase_dev_list_lock);
+	return &kbase_dev_list;
+}
+
+/* API to release the device list semaphore */
+void kbase_dev_list_put(const struct list_head *dev_list)
+{
+	up(&kbase_dev_list_lock);
+}
+
+/* Find a particular kbase device (as specified by minor number), or find the "first" device if -1 is specified */
+struct kbase_device *kbase_find_device(int minor)
+{
+	struct kbase_device *kbdev = NULL;
+	struct list_head *entry;
+
+	down(&kbase_dev_list_lock);
+	list_for_each(entry, &kbase_dev_list) {
+		struct kbase_device *tmp;
+
+		tmp = list_entry(entry, struct kbase_device, entry);
+		if (tmp->mdev.minor == minor || minor == -1) {
+			kbdev = tmp;
+			get_device(kbdev->dev);
+			break;
+		}
+	}
+	up(&kbase_dev_list_lock);
+
+	return kbdev;
+}
+EXPORT_SYMBOL(kbase_find_device);
+
+void kbase_release_device(struct kbase_device *kbdev)
+{
+	put_device(kbdev->dev);
+}
+EXPORT_SYMBOL(kbase_release_device);
+
+static int kbase_open(struct inode *inode, struct file *filp)
+{
+	struct kbase_device *kbdev = NULL;
+	kbase_context *kctx;
+	int ret = 0;
+
+	kbdev = kbase_find_device(iminor(inode));
+
+	if (!kbdev)
+		return -ENODEV;
+
+	kctx = kbase_create_context(kbdev);
+	if (!kctx) {
+		ret = -ENOMEM;
+		goto out;
+	}
+
+	init_waitqueue_head(&kctx->event_queue);
+	filp->private_data = kctx;
+
+	dev_dbg(kbdev->dev, "created base context\n");
+
+	{
+		kbasep_kctx_list_element *element;
+
+		element = kzalloc(sizeof(kbasep_kctx_list_element), GFP_KERNEL);
+		if (element) {
+			mutex_lock(&kbdev->kctx_list_lock);
+			element->kctx = kctx;
+			list_add(&element->link, &kbdev->kctx_list);
+			mutex_unlock(&kbdev->kctx_list_lock);
+		} else {
+			/* we don't treat this as a fail - just warn about it */
+			dev_warn(kbdev->dev, "couldn't add kctx to kctx_list\n");
+		}
+	}
+	return 0;
+
+ out:
+	kbase_release_device(kbdev);
+	return ret;
+}
+
+static int kbase_release(struct inode *inode, struct file *filp)
+{
+	kbase_context *kctx = filp->private_data;
+	struct kbase_device *kbdev = kctx->kbdev;
+	kbasep_kctx_list_element *element, *tmp;
+	mali_bool found_element = MALI_FALSE;
+
+	mutex_lock(&kbdev->kctx_list_lock);
+	list_for_each_entry_safe(element, tmp, &kbdev->kctx_list, link) {
+		if (element->kctx == kctx) {
+			list_del(&element->link);
+			kfree(element);
+			found_element = MALI_TRUE;
+		}
+	}
+	mutex_unlock(&kbdev->kctx_list_lock);
+	if (!found_element)
+		dev_warn(kbdev->dev, "kctx not in kctx_list\n");
+
+	filp->private_data = NULL;
+	kbase_destroy_context(kctx);
+
+	dev_dbg(kbdev->dev, "deleted base context\n");
+	kbase_release_device(kbdev);
+	return 0;
+}
+
+#define CALL_MAX_SIZE 536
+
+static long kbase_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	u64 msg[(CALL_MAX_SIZE + 7) >> 3] = { 0xdeadbeefdeadbeefull };	/* alignment fixup */
+	u32 size = _IOC_SIZE(cmd);
+	kbase_context *kctx = filp->private_data;
+
+	if (size > CALL_MAX_SIZE)
+		return -ENOTTY;
+
+	if (0 != copy_from_user(&msg, (void *)arg, size)) {
+		dev_err(kctx->kbdev->dev, "failed to copy ioctl argument into kernel space\n");
+		return -EFAULT;
+	}
+
+	if (MALI_ERROR_NONE != kbase_dispatch(kctx, &msg, size))
+		return -EFAULT;
+
+	if (0 != copy_to_user((void *)arg, &msg, size)) {
+		dev_err(kctx->kbdev->dev, "failed to copy results of UK call back to user space\n");
+		return -EFAULT;
+	}
+	return 0;
+}
+
+static ssize_t kbase_read(struct file *filp, char __user *buf, size_t count, loff_t *f_pos)
+{
+	kbase_context *kctx = filp->private_data;
+	base_jd_event_v2 uevent;
+	int out_count = 0;
+
+	if (count < sizeof(uevent))
+		return -ENOBUFS;
+
+	do {
+		while (kbase_event_dequeue(kctx, &uevent)) {
+			if (out_count > 0)
+				goto out;
+
+			if (filp->f_flags & O_NONBLOCK)
+				return -EAGAIN;
+
+			if (wait_event_interruptible(kctx->event_queue, kbase_event_pending(kctx)))
+				return -ERESTARTSYS;
+		}
+		if (uevent.event_code == BASE_JD_EVENT_DRV_TERMINATED) {
+			if (out_count == 0)
+				return -EPIPE;
+			goto out;
+		}
+
+		if (copy_to_user(buf, &uevent, sizeof(uevent)))
+			return -EFAULT;
+
+		buf += sizeof(uevent);
+		out_count++;
+		count -= sizeof(uevent);
+	} while (count >= sizeof(uevent));
+
+ out:
+	return out_count * sizeof(uevent);
+}
+
+static unsigned int kbase_poll(struct file *filp, poll_table *wait)
+{
+	kbase_context *kctx = filp->private_data;
+
+	poll_wait(filp, &kctx->event_queue, wait);
+	if (kbase_event_pending(kctx))
+		return POLLIN | POLLRDNORM;
+
+	return 0;
+}
+
+void kbase_event_wakeup(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx);
+
+	wake_up_interruptible(&kctx->event_queue);
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_wakeup)
+
+int kbase_check_flags(int flags)
+{
+	/* Enforce that the driver keeps the O_CLOEXEC flag so that execve() always
+	 * closes the file descriptor in a child process.
+	 */
+	if (0 == (flags & O_CLOEXEC))
+		return -EINVAL;
+
+	return 0;
+}
+
+static const struct file_operations kbase_fops = {
+	.owner = THIS_MODULE,
+	.open = kbase_open,
+	.release = kbase_release,
+	.read = kbase_read,
+	.poll = kbase_poll,
+	.unlocked_ioctl = kbase_ioctl,
+	.compat_ioctl = kbase_ioctl,
+	.mmap = kbase_mmap,
+	.check_flags = kbase_check_flags,
+};
+
+#ifndef CONFIG_MALI_NO_MALI
+void kbase_os_reg_write(kbase_device *kbdev, u16 offset, u32 value)
+{
+	writel(value, kbdev->reg + offset);
+}
+
+u32 kbase_os_reg_read(kbase_device *kbdev, u16 offset)
+{
+	return readl(kbdev->reg + offset);
+}
+
+static void *kbase_tag(void *ptr, u32 tag)
+{
+	return (void *)(((uintptr_t) ptr) | tag);
+}
+
+static void *kbase_untag(void *ptr)
+{
+	return (void *)(((uintptr_t) ptr) & ~3);
+}
+
+static irqreturn_t kbase_job_irq_handler(int irq, void *data)
+{
+	unsigned long flags;
+	struct kbase_device *kbdev = kbase_untag(data);
+	u32 val;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* GPU is turned off - IRQ is not for us */
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+		return IRQ_NONE;
+	}
+
+	val = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS), NULL);
+
+#ifdef CONFIG_MALI_DEBUG
+	if (!kbdev->pm.driver_ready_for_irqs)
+		dev_warn(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n",
+				__func__, irq, val );
+#endif /* CONFIG_MALI_DEBUG */
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!val)
+		return IRQ_NONE;
+
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
+
+	kbase_job_done(kbdev, val);
+
+	return IRQ_HANDLED;
+}
+
+KBASE_EXPORT_TEST_API(kbase_job_irq_handler);
+
+static irqreturn_t kbase_mmu_irq_handler(int irq, void *data)
+{
+	unsigned long flags;
+	struct kbase_device *kbdev = kbase_untag(data);
+	u32 val;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* GPU is turned off - IRQ is not for us */
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+		return IRQ_NONE;
+	}
+
+	val = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_STATUS), NULL);
+
+#ifdef CONFIG_MALI_DEBUG
+	if (!kbdev->pm.driver_ready_for_irqs)
+		dev_warn(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n",
+				__func__, irq, val );
+#endif /* CONFIG_MALI_DEBUG */
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!val)
+		return IRQ_NONE;
+
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
+
+	kbase_mmu_interrupt(kbdev, val);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t kbase_gpu_irq_handler(int irq, void *data)
+{
+	unsigned long flags;
+	struct kbase_device *kbdev = kbase_untag(data);
+	u32 val;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* GPU is turned off - IRQ is not for us */
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+		return IRQ_NONE;
+	}
+
+	val = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_STATUS), NULL);
+
+#ifdef CONFIG_MALI_DEBUG
+	if (!kbdev->pm.driver_ready_for_irqs)
+		dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x before driver is ready\n",
+				__func__, irq, val );
+#endif /* CONFIG_MALI_DEBUG */
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!val)
+		return IRQ_NONE;
+
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
+
+	kbase_gpu_interrupt(kbdev, val);
+
+	return IRQ_HANDLED;
+}
+
+static irq_handler_t kbase_handler_table[] = {
+	[JOB_IRQ_TAG] = kbase_job_irq_handler,
+	[MMU_IRQ_TAG] = kbase_mmu_irq_handler,
+	[GPU_IRQ_TAG] = kbase_gpu_irq_handler,
+};
+
+#ifdef CONFIG_MALI_DEBUG
+#define  JOB_IRQ_HANDLER JOB_IRQ_TAG
+#define  MMU_IRQ_HANDLER MMU_IRQ_TAG
+#define  GPU_IRQ_HANDLER GPU_IRQ_TAG
+
+/**
+ * @brief Registers given interrupt handler for requested interrupt type
+ *        Case irq handler is not specified default handler shall be registered
+ *
+ * @param[in] kbdev           - Device for which the handler is to be registered
+ * @param[in] custom_handler  - Handler to be registered
+ * @param[in] irq_type        - Interrupt type
+ * @return	MALI_ERROR_NONE case success, MALI_ERROR_FUNCTION_FAILED otherwise
+ */
+static mali_error kbase_set_custom_irq_handler(kbase_device *kbdev, irq_handler_t custom_handler, int irq_type)
+{
+	mali_error result = MALI_ERROR_NONE;
+	irq_handler_t requested_irq_handler = NULL;
+	KBASE_DEBUG_ASSERT((JOB_IRQ_HANDLER <= irq_type) && (GPU_IRQ_HANDLER >= irq_type));
+
+	/* Release previous handler */
+	if (kbdev->irqs[irq_type].irq)
+		free_irq(kbdev->irqs[irq_type].irq, kbase_tag(kbdev, irq_type));
+
+	requested_irq_handler = (NULL != custom_handler) ? custom_handler : kbase_handler_table[irq_type];
+
+	if (0 != request_irq(kbdev->irqs[irq_type].irq, requested_irq_handler, kbdev->irqs[irq_type].flags | IRQF_SHARED, dev_name(kbdev->dev), kbase_tag(kbdev, irq_type))) {
+		result = MALI_ERROR_FUNCTION_FAILED;
+		dev_err(kbdev->dev, "Can't request interrupt %d (index %d)\n", kbdev->irqs[irq_type].irq, irq_type);
+#ifdef CONFIG_SPARSE_IRQ
+		dev_err(kbdev->dev, "You have CONFIG_SPARSE_IRQ support enabled - is the interrupt number correct for this configuration?\n");
+#endif /* CONFIG_SPARSE_IRQ */
+	}
+
+	return result;
+}
+
+KBASE_EXPORT_TEST_API(kbase_set_custom_irq_handler)
+
+/* test correct interrupt assigment and reception by cpu */
+typedef struct kbasep_irq_test {
+	struct hrtimer timer;
+	wait_queue_head_t wait;
+	int triggered;
+	u32 timeout;
+} kbasep_irq_test;
+
+static kbasep_irq_test kbasep_irq_test_data;
+
+#define IRQ_TEST_TIMEOUT    500
+
+static irqreturn_t kbase_job_irq_test_handler(int irq, void *data)
+{
+	unsigned long flags;
+	struct kbase_device *kbdev = kbase_untag(data);
+	u32 val;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* GPU is turned off - IRQ is not for us */
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+		return IRQ_NONE;
+	}
+
+	val = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_STATUS), NULL);
+
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!val)
+		return IRQ_NONE;
+
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
+
+	kbasep_irq_test_data.triggered = 1;
+	wake_up(&kbasep_irq_test_data.wait);
+
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), val, NULL);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t kbase_mmu_irq_test_handler(int irq, void *data)
+{
+	unsigned long flags;
+	struct kbase_device *kbdev = kbase_untag(data);
+	u32 val;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* GPU is turned off - IRQ is not for us */
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+		return IRQ_NONE;
+	}
+
+	val = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_STATUS), NULL);
+
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (!val)
+		return IRQ_NONE;
+
+	dev_dbg(kbdev->dev, "%s: irq %d irqstatus 0x%x\n", __func__, irq, val);
+
+	kbasep_irq_test_data.triggered = 1;
+	wake_up(&kbasep_irq_test_data.wait);
+
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), val, NULL);
+
+	return IRQ_HANDLED;
+}
+
+static enum hrtimer_restart kbasep_test_interrupt_timeout(struct hrtimer *timer)
+{
+	kbasep_irq_test *test_data = container_of(timer, kbasep_irq_test, timer);
+
+	test_data->timeout = 1;
+	test_data->triggered = 1;
+	wake_up(&test_data->wait);
+	return HRTIMER_NORESTART;
+}
+
+static mali_error kbasep_common_test_interrupt(kbase_device * const kbdev, u32 tag)
+{
+	mali_error err = MALI_ERROR_NONE;
+	irq_handler_t test_handler;
+
+	u32 old_mask_val;
+	u16 mask_offset;
+	u16 rawstat_offset;
+
+	switch (tag) {
+	case JOB_IRQ_TAG:
+		test_handler = kbase_job_irq_test_handler;
+		rawstat_offset = JOB_CONTROL_REG(JOB_IRQ_RAWSTAT);
+		mask_offset = JOB_CONTROL_REG(JOB_IRQ_MASK);
+		break;
+	case MMU_IRQ_TAG:
+		test_handler = kbase_mmu_irq_test_handler;
+		rawstat_offset = MMU_REG(MMU_IRQ_RAWSTAT);
+		mask_offset = MMU_REG(MMU_IRQ_MASK);
+		break;
+	case GPU_IRQ_TAG:
+		/* already tested by pm_driver - bail out */
+	default:
+		return MALI_ERROR_NONE;
+	}
+
+	/* store old mask */
+	old_mask_val = kbase_reg_read(kbdev, mask_offset, NULL);
+	/* mask interrupts */
+	kbase_reg_write(kbdev, mask_offset, 0x0, NULL);
+
+	if (kbdev->irqs[tag].irq) {
+		/* release original handler and install test handler */
+		if (MALI_ERROR_NONE != kbase_set_custom_irq_handler(kbdev, test_handler, tag)) {
+			err = MALI_ERROR_FUNCTION_FAILED;
+		} else {
+			kbasep_irq_test_data.timeout = 0;
+			hrtimer_init(&kbasep_irq_test_data.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+			kbasep_irq_test_data.timer.function = kbasep_test_interrupt_timeout;
+
+			/* trigger interrupt */
+			kbase_reg_write(kbdev, mask_offset, 0x1, NULL);
+			kbase_reg_write(kbdev, rawstat_offset, 0x1, NULL);
+
+			hrtimer_start(&kbasep_irq_test_data.timer, HR_TIMER_DELAY_MSEC(IRQ_TEST_TIMEOUT), HRTIMER_MODE_REL);
+
+			wait_event(kbasep_irq_test_data.wait, kbasep_irq_test_data.triggered != 0);
+
+			if (kbasep_irq_test_data.timeout != 0) {
+				dev_err(kbdev->dev, "Interrupt %d (index %d) didn't reach CPU.\n", kbdev->irqs[tag].irq, tag);
+				err = MALI_ERROR_FUNCTION_FAILED;
+			} else {
+				dev_dbg(kbdev->dev, "Interrupt %d (index %d) reached CPU.\n", kbdev->irqs[tag].irq, tag);
+			}
+
+			hrtimer_cancel(&kbasep_irq_test_data.timer);
+			kbasep_irq_test_data.triggered = 0;
+
+			/* mask interrupts */
+			kbase_reg_write(kbdev, mask_offset, 0x0, NULL);
+
+			/* release test handler */
+			free_irq(kbdev->irqs[tag].irq, kbase_tag(kbdev, tag));
+		}
+
+		/* restore original interrupt */
+		if (request_irq(kbdev->irqs[tag].irq, kbase_handler_table[tag], kbdev->irqs[tag].flags | IRQF_SHARED, dev_name(kbdev->dev), kbase_tag(kbdev, tag))) {
+			dev_err(kbdev->dev, "Can't restore original interrupt %d (index %d)\n", kbdev->irqs[tag].irq, tag);
+			err = MALI_ERROR_FUNCTION_FAILED;
+		}
+	}
+	/* restore old mask */
+	kbase_reg_write(kbdev, mask_offset, old_mask_val, NULL);
+
+	return err;
+}
+
+static mali_error kbasep_common_test_interrupt_handlers(kbase_device * const kbdev)
+{
+	mali_error err;
+
+	init_waitqueue_head(&kbasep_irq_test_data.wait);
+	kbasep_irq_test_data.triggered = 0;
+
+	/* A suspend won't happen during startup/insmod */
+	kbase_pm_context_active(kbdev);
+
+	err = kbasep_common_test_interrupt(kbdev, JOB_IRQ_TAG);
+	if (MALI_ERROR_NONE != err) {
+		dev_err(kbdev->dev, "Interrupt JOB_IRQ didn't reach CPU. Check interrupt assignments.\n");
+		goto out;
+	}
+
+	err = kbasep_common_test_interrupt(kbdev, MMU_IRQ_TAG);
+	if (MALI_ERROR_NONE != err) {
+		dev_err(kbdev->dev, "Interrupt MMU_IRQ didn't reach CPU. Check interrupt assignments.\n");
+		goto out;
+	}
+
+	dev_err(kbdev->dev, "Interrupts are correctly assigned.\n");
+
+ out:
+	kbase_pm_context_idle(kbdev);
+
+	return err;
+
+}
+#endif /* CONFIG_MALI_DEBUG */
+
+static int kbase_install_interrupts(kbase_device *kbdev)
+{
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
+	int err;
+	u32 i;
+
+	for (i = 0; i < nr; i++) {
+		err = request_irq(kbdev->irqs[i].irq, kbase_handler_table[i], kbdev->irqs[i].flags | IRQF_SHARED, dev_name(kbdev->dev), kbase_tag(kbdev, i));
+		if (err) {
+			dev_err(kbdev->dev, "Can't request interrupt %d (index %d)\n", kbdev->irqs[i].irq, i);
+#ifdef CONFIG_SPARSE_IRQ
+			dev_err(kbdev->dev, "You have CONFIG_SPARSE_IRQ support enabled - is the interrupt number correct for this configuration?\n");
+#endif /* CONFIG_SPARSE_IRQ */
+			goto release;
+		}
+	}
+
+	return 0;
+
+ release:
+	while (i-- > 0)
+		free_irq(kbdev->irqs[i].irq, kbase_tag(kbdev, i));
+
+	return err;
+}
+
+static void kbase_release_interrupts(kbase_device *kbdev)
+{
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
+	u32 i;
+
+	for (i = 0; i < nr; i++) {
+		if (kbdev->irqs[i].irq)
+			free_irq(kbdev->irqs[i].irq, kbase_tag(kbdev, i));
+	}
+}
+
+void kbase_synchronize_irqs(kbase_device *kbdev)
+{
+	u32 nr = ARRAY_SIZE(kbase_handler_table);
+	u32 i;
+
+	for (i = 0; i < nr; i++) {
+		if (kbdev->irqs[i].irq)
+			synchronize_irq(kbdev->irqs[i].irq);
+	}
+}
+
+#endif /* CONFIG_MALI_NO_MALI */
+
+
+/** Show callback for the @c power_policy sysfs file.
+ *
+ * This function is called to get the contents of the @c power_policy sysfs
+ * file. This is a list of the available policies with the currently active one
+ * surrounded by square brackets.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_policy(struct device *dev, struct device_attribute *attr, char *const buf)
+{
+	struct kbase_device *kbdev;
+	const struct kbase_pm_policy *current_policy;
+	const struct kbase_pm_policy *const *policy_list;
+	int policy_count;
+	int i;
+	ssize_t ret = 0;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	current_policy = kbase_pm_get_policy(kbdev);
+
+	policy_count = kbase_pm_list_policies(&policy_list);
+
+	for (i = 0; i < policy_count && ret < PAGE_SIZE; i++) {
+		if (policy_list[i] == current_policy)
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "[%s] ", policy_list[i]->name);
+		else
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s ", policy_list[i]->name);
+	}
+
+	if (ret < PAGE_SIZE - 1) {
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");
+	} else {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/** Store callback for the @c power_policy sysfs file.
+ *
+ * This function is called when the @c power_policy sysfs file is written to.
+ * It matches the requested policy against the available policies and if a
+ * matching policy is found calls @ref kbase_pm_set_policy to change the
+ * policy.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_policy(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	const struct kbase_pm_policy *new_policy = NULL;
+	const struct kbase_pm_policy *const *policy_list;
+	int policy_count;
+	int i;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	policy_count = kbase_pm_list_policies(&policy_list);
+
+	for (i = 0; i < policy_count; i++) {
+		if (sysfs_streq(policy_list[i]->name, buf)) {
+			new_policy = policy_list[i];
+			break;
+		}
+	}
+
+	if (!new_policy) {
+		dev_err(dev, "power_policy: policy not found\n");
+		return -EINVAL;
+	}
+
+	kbase_pm_set_policy(kbdev, new_policy);
+
+	return count;
+}
+
+/** The sysfs file @c power_policy.
+ *
+ * This is used for obtaining information about the available policies,
+ * determining which policy is currently active, and changing the active
+ * policy.
+ */
+DEVICE_ATTR(power_policy, S_IRUGO | S_IWUSR, show_policy, set_policy);
+
+/** Show callback for the @c core_availability_policy sysfs file.
+ *
+ * This function is called to get the contents of the @c core_availability_policy
+ * sysfs file. This is a list of the available policies with the currently
+ * active one surrounded by square brackets.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_ca_policy(struct device *dev, struct device_attribute *attr, char *const buf)
+{
+	struct kbase_device *kbdev;
+	const struct kbase_pm_ca_policy *current_policy;
+	const struct kbase_pm_ca_policy *const *policy_list;
+	int policy_count;
+	int i;
+	ssize_t ret = 0;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	current_policy = kbase_pm_ca_get_policy(kbdev);
+
+	policy_count = kbase_pm_ca_list_policies(&policy_list);
+
+	for (i = 0; i < policy_count && ret < PAGE_SIZE; i++) {
+		if (policy_list[i] == current_policy)
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "[%s] ", policy_list[i]->name);
+		else
+			ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s ", policy_list[i]->name);
+	}
+
+	if (ret < PAGE_SIZE - 1) {
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "\n");
+	} else {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/** Store callback for the @c core_availability_policy sysfs file.
+ *
+ * This function is called when the @c core_availability_policy sysfs file is
+ * written to. It matches the requested policy against the available policies
+ * and if a matching policy is found calls @ref kbase_pm_set_policy to change
+ * the policy.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_ca_policy(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	const struct kbase_pm_ca_policy *new_policy = NULL;
+	const struct kbase_pm_ca_policy *const *policy_list;
+	int policy_count;
+	int i;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	policy_count = kbase_pm_ca_list_policies(&policy_list);
+
+	for (i = 0; i < policy_count; i++) {
+		if (sysfs_streq(policy_list[i]->name, buf)) {
+			new_policy = policy_list[i];
+			break;
+		}
+	}
+
+	if (!new_policy) {
+		dev_err(dev, "core_availability_policy: policy not found\n");
+		return -EINVAL;
+	}
+
+	kbase_pm_ca_set_policy(kbdev, new_policy);
+
+	return count;
+}
+
+/** The sysfs file @c core_availability_policy
+ *
+ * This is used for obtaining information about the available policies,
+ * determining which policy is currently active, and changing the active
+ * policy.
+ */
+DEVICE_ATTR(core_availability_policy, S_IRUGO | S_IWUSR, show_ca_policy, set_ca_policy);
+
+/** Show callback for the @c core_mask sysfs file.
+ *
+ * This function is called to get the contents of the @c core_mask sysfs
+ * file.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_core_mask(struct device *dev, struct device_attribute *attr, char *const buf)
+{
+	struct kbase_device *kbdev;
+	ssize_t ret = 0;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Current core mask : 0x%llX\n", kbdev->pm.debug_core_mask);
+	ret += scnprintf(buf + ret, PAGE_SIZE - ret, "Available core mask : 0x%llX\n", kbdev->shader_present_bitmap);
+
+	return ret;
+}
+
+/** Store callback for the @c core_mask sysfs file.
+ *
+ * This function is called when the @c core_mask sysfs file is written to.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_core_mask(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	u64 new_core_mask;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	new_core_mask = simple_strtoull(buf, NULL, 16);
+
+	if ((new_core_mask & kbdev->shader_present_bitmap) != new_core_mask ||
+	    !(new_core_mask & kbdev->gpu_props.props.coherency_info.group[0].core_mask)) {
+		dev_err(dev, "power_policy: invalid core specification\n");
+		return -EINVAL;
+	}
+
+	if (kbdev->pm.debug_core_mask != new_core_mask) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+		kbdev->pm.debug_core_mask = new_core_mask;
+		kbase_pm_update_cores_state_nolock(kbdev);
+
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+	}
+
+	return count;
+}
+
+/** The sysfs file @c core_mask.
+ *
+ * This is used to restrict shader core availability for debugging purposes.
+ * Reading it will show the current core mask and the mask of cores available.
+ * Writing to it will set the current core mask.
+ */
+DEVICE_ATTR(core_mask, S_IRUGO | S_IWUSR, show_core_mask, set_core_mask);
+
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+/* Import the external affinity mask variables */
+extern u64 mali_js0_affinity_mask;
+extern u64 mali_js1_affinity_mask;
+extern u64 mali_js2_affinity_mask;
+
+/**
+ * Structure containing a single shader affinity split configuration.
+ */
+typedef struct {
+	char const * tag;
+	char const * human_readable;
+	u64          js0_mask;
+	u64          js1_mask;
+	u64          js2_mask;
+} sc_split_config;
+
+/**
+ * Array of available shader affinity split configurations.
+ */
+static sc_split_config const sc_split_configs[] =
+{
+	/* All must be the first config (default). */
+	{
+		"all", "All cores",
+		0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL, 0xFFFFFFFFFFFFFFFFULL
+	},
+	{
+		"mp1", "MP1 shader core",
+		0x1, 0x1, 0x1
+	},
+	{
+		"mp2", "MP2 shader core",
+		0x3, 0x3, 0x3
+	},
+	{
+		"mp4", "MP4 shader core",
+		0xF, 0xF, 0xF
+	},
+	{
+		"mp1_vf", "MP1 vertex + MP1 fragment shader core",
+		0x2, 0x1, 0xFFFFFFFFFFFFFFFFULL
+	},
+	{
+		"mp2_vf", "MP2 vertex + MP2 fragment shader core",
+		0xA, 0x5, 0xFFFFFFFFFFFFFFFFULL
+	},
+	/* This must be the last config. */
+	{
+		NULL, NULL,
+		0x0, 0x0, 0x0
+	},
+};
+
+/* Pointer to the currently active shader split configuration. */
+static sc_split_config const * current_sc_split_config = &sc_split_configs[0];
+
+/** Show callback for the @c sc_split sysfs file
+ *
+ * Returns the current shader core affinity policy.
+ */
+static ssize_t show_split(struct device *dev, struct device_attribute *attr, char * const buf)
+{
+	ssize_t ret;
+	/* We know we are given a buffer which is PAGE_SIZE long. Our strings are all guaranteed
+	 * to be shorter than that at this time so no length check needed. */
+	ret = scnprintf(buf, PAGE_SIZE, "Current sc_split: '%s'\n", current_sc_split_config->tag );
+	return ret;
+}
+
+/** Store callback for the @c sc_split sysfs file.
+ *
+ * This function is called when the @c sc_split sysfs file is written to
+ * It modifies the system shader core affinity configuration to allow
+ * system profiling with different hardware configurations.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_split(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	sc_split_config const * config = &sc_split_configs[0];
+
+	/* Try to match: loop until we hit the last "NULL" entry */
+	while( config->tag )
+	{
+		if (sysfs_streq(config->tag, buf))
+		{
+			current_sc_split_config = config;
+			mali_js0_affinity_mask  = config->js0_mask;
+			mali_js1_affinity_mask  = config->js1_mask;
+			mali_js2_affinity_mask  = config->js2_mask;
+			dev_dbg(dev, "Setting sc_split: '%s'\n", config->tag);
+			return count;
+		}
+		config++;
+	}
+
+	/* No match found in config list */
+	dev_err(dev, "sc_split: invalid value\n");
+	dev_err(dev, "  Possible settings: mp[1|2|4], mp[1|2]_vf\n");
+	return -ENOENT;
+}
+
+/** The sysfs file @c sc_split
+ *
+ * This is used for configuring/querying the current shader core work affinity
+ * configuration.
+ */
+DEVICE_ATTR(sc_split, S_IRUGO|S_IWUSR, show_split, set_split);
+#endif /* CONFIG_MALI_DEBUG_SHADER_SPLIT_FS */
+
+
+#if MALI_CUSTOMER_RELEASE == 0
+/** Store callback for the @c js_timeouts sysfs file.
+ *
+ * This function is called to get the contents of the @c js_timeouts sysfs
+ * file. This file contains five values separated by whitespace. The values
+ * are basically the same as KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+ * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS, KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+ * KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS, BASE_CONFIG_ATTR_JS_RESET_TICKS_NSS
+ * configuration values (in that order), with the difference that the js_timeout
+ * valus are expressed in MILLISECONDS.
+ *
+ * The js_timeouts sysfile file allows the current values in
+ * use by the job scheduler to get override. Note that a value needs to
+ * be other than 0 for it to override the current job scheduler value.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_js_timeouts(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	int items;
+	unsigned long js_soft_stop_ms;
+	unsigned long js_soft_stop_ms_cl;
+	unsigned long js_hard_stop_ms_ss;
+	unsigned long js_hard_stop_ms_cl;
+	unsigned long js_hard_stop_ms_nss;
+	unsigned long js_reset_ms_ss;
+	unsigned long js_reset_ms_cl;
+	unsigned long js_reset_ms_nss;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	items = sscanf(buf, "%lu %lu %lu %lu %lu %lu %lu %lu", &js_soft_stop_ms, &js_soft_stop_ms_cl, &js_hard_stop_ms_ss, &js_hard_stop_ms_cl, &js_hard_stop_ms_nss, &js_reset_ms_ss, &js_reset_ms_cl, &js_reset_ms_nss);
+	if (items == 8) {
+		u64 ticks;
+
+		ticks = js_soft_stop_ms * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_soft_stop_ticks = ticks;
+
+		ticks = js_soft_stop_ms_cl * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_soft_stop_ticks_cl = ticks;
+
+		ticks = js_hard_stop_ms_ss * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_hard_stop_ticks_ss = ticks;
+
+		ticks = js_hard_stop_ms_cl * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_hard_stop_ticks_cl = ticks;
+
+		ticks = js_hard_stop_ms_nss * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_hard_stop_ticks_nss = ticks;
+
+		ticks = js_reset_ms_ss * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_reset_ticks_ss = ticks;
+
+		ticks = js_reset_ms_cl * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_reset_ticks_cl = ticks;
+
+		ticks = js_reset_ms_nss * 1000000ULL;
+		do_div(ticks, kbdev->js_data.scheduling_tick_ns);
+		kbdev->js_reset_ticks_nss = ticks;
+
+		dev_dbg( kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_soft_stop_ticks, js_soft_stop_ms);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_soft_stop_ticks_cl, js_soft_stop_ms_cl);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_hard_stop_ticks_ss, js_hard_stop_ms_ss);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_hard_stop_ticks_cl, js_hard_stop_ms_cl);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_hard_stop_ticks_nss, js_hard_stop_ms_nss);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_reset_ticks_ss, js_reset_ms_ss);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_reset_ticks_cl, js_reset_ms_cl);
+		dev_dbg(kbdev->dev, "Overriding KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS with %lu ticks (%lu ms)\n", (unsigned long)kbdev->js_reset_ticks_nss, js_reset_ms_nss);
+
+		return count;
+	} else {
+		dev_err(kbdev->dev, "Couldn't process js_timeouts write operation.\nUse format " "<soft_stop_ms> <hard_stop_ms_ss> <hard_stop_ms_nss> <reset_ms_ss> <reset_ms_nss>\n");
+		return -EINVAL;
+	}
+}
+
+/** Show callback for the @c js_timeouts sysfs file.
+ *
+ * This function is called to get the contents of the @c js_timeouts sysfs
+ * file. It returns the last set values written to the js_timeouts sysfs file.
+ * If the file didn't get written yet, the values will be 0.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_js_timeouts(struct device *dev, struct device_attribute *attr, char * const buf)
+{
+	struct kbase_device *kbdev;
+	ssize_t ret;
+	u64 ms;
+	unsigned long js_soft_stop_ms;
+	unsigned long js_soft_stop_ms_cl;
+	unsigned long js_hard_stop_ms_ss;
+	unsigned long js_hard_stop_ms_cl;
+	unsigned long js_hard_stop_ms_nss;
+	unsigned long js_reset_ms_ss;
+	unsigned long js_reset_ms_cl;
+	unsigned long js_reset_ms_nss;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	ms = (u64) kbdev->js_soft_stop_ticks * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_soft_stop_ms = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_soft_stop_ticks_cl * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_soft_stop_ms_cl = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_hard_stop_ticks_ss * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_hard_stop_ms_ss = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_hard_stop_ticks_cl * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_hard_stop_ms_cl = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_hard_stop_ticks_nss * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_hard_stop_ms_nss = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_reset_ticks_ss * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_reset_ms_ss = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_reset_ticks_cl * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_reset_ms_cl = (unsigned long)ms;
+
+	ms = (u64) kbdev->js_reset_ticks_nss * kbdev->js_data.scheduling_tick_ns;
+	do_div(ms, 1000000UL);
+	js_reset_ms_nss = (unsigned long)ms;
+
+	ret = scnprintf(buf, PAGE_SIZE, "%lu %lu %lu %lu %lu %lu %lu %lu\n", js_soft_stop_ms, js_soft_stop_ms_cl, js_hard_stop_ms_ss, js_hard_stop_ms_cl, js_hard_stop_ms_nss, js_reset_ms_ss, js_reset_ms_cl, js_reset_ms_nss);
+
+	if (ret >= PAGE_SIZE) {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/** The sysfs file @c js_timeouts.
+ *
+ * This is used to override the current job scheduler values for
+ * KBASE_CONFIG_ATTR_JS_STOP_STOP_TICKS_SS
+ * KBASE_CONFIG_ATTR_JS_STOP_STOP_TICKS_CL
+ * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS
+ * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL
+ * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS
+ * KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS
+ * KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL
+ * KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS.
+ */
+DEVICE_ATTR(js_timeouts, S_IRUGO | S_IWUSR, show_js_timeouts, set_js_timeouts);
+
+
+
+/** Store callback for the @c force_replay sysfs file.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t set_force_replay(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	if (!strncmp("limit=", buf, MIN(6, count))) {
+		int force_replay_limit;
+		int items = sscanf(buf, "limit=%u", &force_replay_limit);
+
+		if (items == 1) {
+			kbdev->force_replay_random = MALI_FALSE;
+			kbdev->force_replay_limit = force_replay_limit;
+			kbdev->force_replay_count = 0;
+
+			return count;
+		}
+	} else if (!strncmp("random_limit", buf, MIN(12, count))) {
+		kbdev->force_replay_random = MALI_TRUE;
+		kbdev->force_replay_count = 0;
+
+		return count;
+	} else if (!strncmp("norandom_limit", buf, MIN(14, count))) {
+		kbdev->force_replay_random = MALI_FALSE;
+		kbdev->force_replay_limit = KBASEP_FORCE_REPLAY_DISABLED;
+		kbdev->force_replay_count = 0;
+
+		return count;
+	} else if (!strncmp("core_req=", buf, MIN(9, count))) {
+		unsigned int core_req;
+		int items = sscanf(buf, "core_req=%x", &core_req);
+
+		if (items == 1) {
+			kbdev->force_replay_core_req = (base_jd_core_req)core_req;
+
+			return count;
+		}
+	}
+	dev_err(kbdev->dev, "Couldn't process force_replay write operation.\nPossible settings: limit=<limit>, random_limit, norandom_limit, core_req=<core_req>\n");
+	return -EINVAL;
+}
+
+/** Show callback for the @c force_replay sysfs file.
+ *
+ * This function is called to get the contents of the @c force_replay sysfs
+ * file. It returns the last set value written to the force_replay sysfs file.
+ * If the file didn't get written yet, the values will be 0.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_force_replay(struct device *dev, struct device_attribute *attr, char * const buf)
+{
+	struct kbase_device *kbdev;
+	ssize_t ret;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	if (kbdev->force_replay_random)
+		ret = scnprintf(buf, PAGE_SIZE,
+				"limit=0\nrandom_limit\ncore_req=%x\n",
+				kbdev->force_replay_core_req);
+	else
+		ret = scnprintf(buf, PAGE_SIZE,
+				"limit=%u\nnorandom_limit\ncore_req=%x\n",
+				kbdev->force_replay_limit,
+				kbdev->force_replay_core_req);
+
+	if (ret >= PAGE_SIZE) {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/** The sysfs file @c force_replay.
+ *
+ */
+DEVICE_ATTR(force_replay, S_IRUGO | S_IWUSR, show_force_replay, set_force_replay);
+#endif /* MALI_CUSTOMER_RELEASE == 0 */
+
+#ifdef CONFIG_MALI_DEBUG
+static ssize_t set_js_softstop_always(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	int items;
+	int softstop_always;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	items = sscanf(buf, "%d", &softstop_always);
+	if ((items == 1) && ((softstop_always == 0) || (softstop_always == 1))) {
+		kbdev->js_data.softstop_always = (mali_bool) softstop_always;
+		dev_dbg(kbdev->dev, "Support for softstop on a single context: %s\n", (kbdev->js_data.softstop_always == MALI_FALSE) ? "Disabled" : "Enabled");
+		return count;
+	} else {
+		dev_err(kbdev->dev, "Couldn't process js_softstop_always write operation.\nUse format " "<soft_stop_always>\n");
+		return -EINVAL;
+	}
+}
+
+static ssize_t show_js_softstop_always(struct device *dev, struct device_attribute *attr, char * const buf)
+{
+	struct kbase_device *kbdev;
+	ssize_t ret;
+
+	kbdev = to_kbase_device(dev);
+	if (!kbdev)
+		return -ENODEV;
+
+	ret = scnprintf(buf, PAGE_SIZE, "%d\n", kbdev->js_data.softstop_always);
+
+	if (ret >= PAGE_SIZE) {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/**
+ * By default, soft-stops are disabled when only a single context is present. The ability to
+ * enable soft-stop when only a single context is present can be used for debug and unit-testing purposes.
+ * (see CL t6xx_stress_1 unit-test as an example whereby this feature is used.)
+ */
+DEVICE_ATTR(js_softstop_always, S_IRUGO | S_IWUSR, show_js_softstop_always, set_js_softstop_always);
+#endif /* CONFIG_MALI_DEBUG */
+
+#ifdef CONFIG_MALI_DEBUG
+typedef void (kbasep_debug_command_func) (kbase_device *);
+
+typedef enum {
+	KBASEP_DEBUG_COMMAND_DUMPTRACE,
+
+	/* This must be the last enum */
+	KBASEP_DEBUG_COMMAND_COUNT
+} kbasep_debug_command_code;
+
+typedef struct kbasep_debug_command {
+	char *str;
+	kbasep_debug_command_func *func;
+} kbasep_debug_command;
+
+/** Debug commands supported by the driver */
+static const kbasep_debug_command debug_commands[] = {
+	{
+	 .str = "dumptrace",
+	 .func = &kbasep_trace_dump,
+	 }
+};
+
+/** Show callback for the @c debug_command sysfs file.
+ *
+ * This function is called to get the contents of the @c debug_command sysfs
+ * file. This is a list of the available debug commands, separated by newlines.
+ *
+ * @param dev	The device this sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The output buffer for the sysfs file contents
+ *
+ * @return The number of bytes output to @c buf.
+ */
+static ssize_t show_debug(struct device *dev, struct device_attribute *attr, char *const buf)
+{
+	struct kbase_device *kbdev;
+	int i;
+	ssize_t ret = 0;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	for (i = 0; i < KBASEP_DEBUG_COMMAND_COUNT && ret < PAGE_SIZE; i++)
+		ret += scnprintf(buf + ret, PAGE_SIZE - ret, "%s\n", debug_commands[i].str);
+
+	if (ret >= PAGE_SIZE) {
+		buf[PAGE_SIZE - 2] = '\n';
+		buf[PAGE_SIZE - 1] = '\0';
+		ret = PAGE_SIZE - 1;
+	}
+
+	return ret;
+}
+
+/** Store callback for the @c debug_command sysfs file.
+ *
+ * This function is called when the @c debug_command sysfs file is written to.
+ * It matches the requested command against the available commands, and if
+ * a matching command is found calls the associated function from
+ * @ref debug_commands to issue the command.
+ *
+ * @param dev	The device with sysfs file is for
+ * @param attr	The attributes of the sysfs file
+ * @param buf	The value written to the sysfs file
+ * @param count	The number of bytes written to the sysfs file
+ *
+ * @return @c count if the function succeeded. An error code on failure.
+ */
+static ssize_t issue_debug(struct device *dev, struct device_attribute *attr, const char *buf, size_t count)
+{
+	struct kbase_device *kbdev;
+	int i;
+
+	kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	for (i = 0; i < KBASEP_DEBUG_COMMAND_COUNT; i++) {
+		if (sysfs_streq(debug_commands[i].str, buf)) {
+			debug_commands[i].func(kbdev);
+			return count;
+		}
+	}
+
+	/* Debug Command not found */
+	dev_err(dev, "debug_command: command not known\n");
+	return -EINVAL;
+}
+
+/** The sysfs file @c debug_command.
+ *
+ * This is used to issue general debug commands to the device driver.
+ * Reading it will produce a list of debug commands, separated by newlines.
+ * Writing to it with one of those commands will issue said command.
+ */
+DEVICE_ATTR(debug_command, S_IRUGO | S_IWUSR, show_debug, issue_debug);
+#endif /* CONFIG_MALI_DEBUG */
+
+#ifdef CONFIG_MALI_NO_MALI
+static int kbase_common_reg_map(kbase_device *kbdev)
+{
+	return 0;
+}
+static void kbase_common_reg_unmap(kbase_device * const kbdev)
+{
+	return;
+}
+#else /* CONFIG_MALI_NO_MALI */
+static int kbase_common_reg_map(kbase_device *kbdev)
+{
+	int err = -ENOMEM;
+
+	kbdev->reg_res = request_mem_region(kbdev->reg_start, kbdev->reg_size, dev_name(kbdev->dev));
+	if (!kbdev->reg_res) {
+		dev_err(kbdev->dev, "Register window unavailable\n");
+		err = -EIO;
+		goto out_region;
+	}
+
+	kbdev->reg = ioremap(kbdev->reg_start, kbdev->reg_size);
+	if (!kbdev->reg) {
+		dev_err(kbdev->dev, "Can't remap register window\n");
+		err = -EINVAL;
+		goto out_ioremap;
+	}
+
+	return 0;
+
+ out_ioremap:
+	release_resource(kbdev->reg_res);
+	kfree(kbdev->reg_res);
+ out_region:
+	return err;
+}
+
+static void kbase_common_reg_unmap(kbase_device * const kbdev)
+{
+	iounmap(kbdev->reg);
+	release_resource(kbdev->reg_res);
+	kfree(kbdev->reg_res);
+}
+#endif /* CONFIG_MALI_NO_MALI */
+
+
+static int kbase_common_device_init(kbase_device *kbdev)
+{
+	int err = -ENOMEM;
+	mali_error mali_err;
+	enum {
+		inited_mem = (1u << 0),
+		inited_job_slot = (1u << 1),
+		inited_pm = (1u << 2),
+		inited_js = (1u << 3),
+		inited_irqs = (1u << 4),
+		inited_debug = (1u << 5),
+		inited_js_softstop = (1u << 6),
+#if MALI_CUSTOMER_RELEASE == 0
+		inited_js_timeouts = (1u << 7),
+		inited_force_replay = (1u << 13),
+#endif /* MALI_CUSTOMER_RELEASE == 0 */
+		inited_pm_runtime_init = (1u << 8),
+#ifdef CONFIG_DEBUG_FS
+		inited_gpu_memory = (1u << 9),
+#endif /* CONFIG_DEBUG_FS */
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+		inited_sc_split = (1u << 11),
+#endif /* CONFIG_MALI_DEBUG_SHADER_SPLIT_FS */
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+		inited_timeline = (1u << 12),
+#endif /* CONFIG_MALI_TRACE_LINE */
+		inited_pm_powerup = (1u << 14),
+	};
+
+	int inited = 0;
+
+	dev_set_drvdata(kbdev->dev, kbdev);
+
+	kbdev->mdev.minor = MISC_DYNAMIC_MINOR;
+	kbdev->mdev.name = kbdev->devname;
+	kbdev->mdev.fops = &kbase_fops;
+	kbdev->mdev.parent = get_device(kbdev->dev);
+
+	scnprintf(kbdev->devname, DEVNAME_SIZE, "%s%d", kbase_drv_name, kbase_dev_nr++);
+
+	if (misc_register(&kbdev->mdev)) {
+		dev_err(kbdev->dev, "Couldn't register misc dev %s\n", kbdev->devname);
+		err = -EINVAL;
+		goto out_misc;
+	}
+
+	if (device_create_file(kbdev->dev, &dev_attr_power_policy)) {
+		dev_err(kbdev->dev, "Couldn't create power_policy sysfs file\n");
+		goto out_file;
+	}
+
+	if (device_create_file(kbdev->dev, &dev_attr_core_availability_policy)) {
+		dev_err(kbdev->dev, "Couldn't create core_availability_policy sysfs file\n");
+		goto out_file_core_availability_policy;
+	}
+
+	if (device_create_file(kbdev->dev, &dev_attr_core_mask)) {
+		dev_err(kbdev->dev, "Couldn't create core_mask sysfs file\n");
+		goto out_file_core_mask;
+	}
+
+	down(&kbase_dev_list_lock);
+	list_add(&kbdev->entry, &kbase_dev_list);
+	up(&kbase_dev_list_lock);
+	dev_info(kbdev->dev, "Probed as %s\n", dev_name(kbdev->mdev.this_device));
+
+	mali_err = kbase_pm_init(kbdev);
+	if (MALI_ERROR_NONE != mali_err)
+		goto out_partial;
+
+	inited |= inited_pm;
+
+	if (kbdev->pm.callback_power_runtime_init) {
+		mali_err = kbdev->pm.callback_power_runtime_init(kbdev);
+		if (MALI_ERROR_NONE != mali_err)
+			goto out_partial;
+
+		inited |= inited_pm_runtime_init;
+	}
+
+	mali_err = kbase_mem_init(kbdev);
+	if (MALI_ERROR_NONE != mali_err)
+		goto out_partial;
+
+	inited |= inited_mem;
+
+	mali_err = kbase_job_slot_init(kbdev);
+	if (MALI_ERROR_NONE != mali_err)
+		goto out_partial;
+
+	inited |= inited_job_slot;
+
+	mali_err = kbasep_js_devdata_init(kbdev);
+	if (MALI_ERROR_NONE != mali_err)
+		goto out_partial;
+
+	inited |= inited_js;
+
+	err = kbase_install_interrupts(kbdev);
+	if (err)
+		goto out_partial;
+
+	inited |= inited_irqs;
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+	if (device_create_file(kbdev->dev, &dev_attr_sc_split))
+	{
+		dev_err(kbdev->dev, "Couldn't create sc_split sysfs file\n");
+		goto out_partial;
+	}
+
+	inited |= inited_sc_split;
+#endif /* CONFIG_MALI_DEBUG_SHADER_SPLIT_FS */
+
+#ifdef CONFIG_DEBUG_FS
+	if (kbasep_gpu_memory_debugfs_init(kbdev)) {
+		dev_err(kbdev->dev, "Couldn't create gpu_memory debugfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_gpu_memory;
+#endif /* CONFIG_DEBUG_FS */
+
+#ifdef CONFIG_MALI_DEBUG
+
+	if (device_create_file(kbdev->dev, &dev_attr_debug_command)) {
+		dev_err(kbdev->dev, "Couldn't create debug_command sysfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_debug;
+
+	if (device_create_file(kbdev->dev, &dev_attr_js_softstop_always)) {
+		dev_err(kbdev->dev, "Couldn't create js_softstop_always sysfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_js_softstop;
+#endif /* CONFIG_MALI_DEBUG */
+
+#if MALI_CUSTOMER_RELEASE == 0
+	if (device_create_file(kbdev->dev, &dev_attr_js_timeouts)) {
+		dev_err(kbdev->dev, "Couldn't create js_timeouts sysfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_js_timeouts;
+
+	if (device_create_file(kbdev->dev, &dev_attr_force_replay)) {
+		dev_err(kbdev->dev, "Couldn't create force_replay sysfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_force_replay;
+#endif /* MALI_CUSTOMER_RELEASE */
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	if (kbasep_trace_timeline_debugfs_init(kbdev)) {
+		dev_err(kbdev->dev, "Couldn't create mali_timeline_defs debugfs file\n");
+		goto out_partial;
+	}
+	inited |= inited_timeline;
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+
+	mali_err = kbase_pm_powerup(kbdev);
+	if (MALI_ERROR_NONE == mali_err) {
+		inited |= inited_pm_powerup;
+#ifdef CONFIG_MALI_DEBUG
+#ifndef CONFIG_MALI_NO_MALI
+		if (MALI_ERROR_NONE != kbasep_common_test_interrupt_handlers(kbdev)) {
+			dev_err(kbdev->dev, "Interrupt assigment check failed.\n");
+			err = -EINVAL;
+			goto out_partial;
+		}
+#endif /* CONFIG_MALI_NO_MALI */
+#endif /* CONFIG_MALI_DEBUG */
+
+		/* intialise the kctx list */
+		mutex_init(&kbdev->kctx_list_lock);
+		INIT_LIST_HEAD(&kbdev->kctx_list);
+		return 0;
+	} else {
+		/* Failed to power up the GPU. */
+		dev_err(kbdev->dev, "GPU power up failed.\n");
+		err = -ENODEV;
+	}
+
+ out_partial:
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	if (inited & inited_timeline)
+		kbasep_trace_timeline_debugfs_term(kbdev);
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+#if MALI_CUSTOMER_RELEASE == 0
+	if (inited & inited_force_replay)
+		device_remove_file(kbdev->dev, &dev_attr_force_replay);
+	if (inited & inited_js_timeouts)
+		device_remove_file(kbdev->dev, &dev_attr_js_timeouts);
+#endif /* MALI_CUSTOMER_RELEASE */
+#ifdef CONFIG_MALI_DEBUG
+	if (inited & inited_js_softstop)
+		device_remove_file(kbdev->dev, &dev_attr_js_softstop_always);
+
+	if (inited & inited_debug)
+		device_remove_file(kbdev->dev, &dev_attr_debug_command);
+
+#endif /* CONFIG_MALI_DEBUG */
+
+#ifdef CONFIG_DEBUG_FS
+	if (inited & inited_gpu_memory)
+		kbasep_gpu_memory_debugfs_term(kbdev);
+#endif /* CONFIG_DEBUG_FS */
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+	if (inited & inited_sc_split)
+	{
+		device_remove_file(kbdev->dev, &dev_attr_sc_split);
+	}
+#endif /* CONFIG_MALI_DEBUG_SHADER_SPLIT_FS */
+
+	if (inited & inited_js)
+		kbasep_js_devdata_halt(kbdev);
+
+	if (inited & inited_job_slot)
+		kbase_job_slot_halt(kbdev);
+
+	if (inited & inited_mem)
+		kbase_mem_halt(kbdev);
+
+	if (inited & inited_pm_powerup)
+		kbase_pm_halt(kbdev);
+
+	if (inited & inited_irqs)
+		kbase_release_interrupts(kbdev);
+
+	if (inited & inited_js)
+		kbasep_js_devdata_term(kbdev);
+
+	if (inited & inited_job_slot)
+		kbase_job_slot_term(kbdev);
+
+	if (inited & inited_mem)
+		kbase_mem_term(kbdev);
+
+	if (inited & inited_pm_runtime_init) {
+		if (kbdev->pm.callback_power_runtime_term)
+			kbdev->pm.callback_power_runtime_term(kbdev);
+	}
+
+	if (inited & inited_pm)
+		kbase_pm_term(kbdev);
+
+	down(&kbase_dev_list_lock);
+	list_del(&kbdev->entry);
+	up(&kbase_dev_list_lock);
+
+	device_remove_file(kbdev->dev, &dev_attr_core_mask);
+ out_file_core_mask:
+	device_remove_file(kbdev->dev, &dev_attr_core_availability_policy);
+ out_file_core_availability_policy:
+	device_remove_file(kbdev->dev, &dev_attr_power_policy);
+ out_file:
+	misc_deregister(&kbdev->mdev);
+ out_misc:
+	put_device(kbdev->dev);
+	return err;
+}
+
+static int kbase_platform_device_probe(struct platform_device *pdev)
+{
+	struct kbase_device *kbdev;
+	struct resource *reg_res;
+	kbase_attribute *platform_data;
+	int err;
+	int i;
+	struct mali_base_gpu_core_props *core_props;
+#ifdef CONFIG_MALI_NO_MALI
+	mali_error mali_err;
+#endif /* CONFIG_MALI_NO_MALI */
+#ifdef CONFIG_OF
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+	kbase_platform_config *config;
+	int attribute_count;
+	config = kbase_get_platform_config();
+	attribute_count = kbasep_get_config_attribute_count(config->attributes);
+
+	err = platform_device_add_data(pdev, config->attributes,
+			attribute_count * sizeof(config->attributes[0]));
+	if (err)
+		return err;
+#endif /* CONFIG_MALI_PLATFORM_FAKE */
+#endif /* CONFIG_OF */
+
+	kbdev = kbase_device_alloc();
+	if (!kbdev) {
+		dev_err(&pdev->dev, "Can't allocate device\n");
+		err = -ENOMEM;
+		goto out;
+	}
+#ifdef CONFIG_MALI_NO_MALI
+	mali_err = midg_device_create(kbdev);
+	if (MALI_ERROR_NONE != mali_err) {
+		dev_err(&pdev->dev, "Can't initialize dummy model\n");
+		err = -ENOMEM;
+		goto out_midg;
+	}
+#endif /* CONFIG_MALI_NO_MALI */
+
+	kbdev->dev = &pdev->dev;
+	platform_data = (kbase_attribute *) kbdev->dev->platform_data;
+
+	if (NULL == platform_data) {
+		dev_err(kbdev->dev, "Platform data not specified\n");
+		err = -ENOENT;
+		goto out_free_dev;
+	}
+
+	if (MALI_TRUE != kbasep_validate_configuration_attributes(kbdev, platform_data)) {
+		dev_err(kbdev->dev, "Configuration attributes failed to validate\n");
+		err = -EINVAL;
+		goto out_free_dev;
+	}
+	kbdev->config_attributes = platform_data;
+
+	/* 3 IRQ resources */
+	for (i = 0; i < 3; i++) {
+		struct resource *irq_res;
+		int irqtag;
+
+		irq_res = platform_get_resource(pdev, IORESOURCE_IRQ, i);
+		if (!irq_res) {
+			dev_err(kbdev->dev, "No IRQ resource at index %d\n", i);
+			err = -ENOENT;
+			goto out_free_dev;
+		}
+
+#ifdef CONFIG_OF
+		if (!strcmp(irq_res->name, "JOB"))
+			irqtag = JOB_IRQ_TAG;
+		else if (!strcmp(irq_res->name, "MMU"))
+			irqtag = MMU_IRQ_TAG;
+		else if (!strcmp(irq_res->name, "GPU"))
+			irqtag = GPU_IRQ_TAG;
+		else {
+			dev_err(&pdev->dev, "Invalid irq res name: '%s'\n",
+				irq_res->name);
+			err = -EINVAL;
+			goto out_free_dev;
+		}
+#else
+		irqtag = i;
+#endif /* CONFIG_OF */
+		kbdev->irqs[irqtag].irq = irq_res->start;
+		kbdev->irqs[irqtag].flags = (irq_res->flags & IRQF_TRIGGER_MASK);
+	}
+
+	/* the first memory resource is the physical address of the GPU registers */
+	reg_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!reg_res) {
+		dev_err(kbdev->dev, "Invalid register resource\n");
+		err = -ENOENT;
+		goto out_free_dev;
+	}
+
+	kbdev->reg_start = reg_res->start;
+	kbdev->reg_size = resource_size(reg_res);
+
+	err = kbase_common_reg_map(kbdev);
+	if (err)
+		goto out_free_dev;
+
+#ifdef CONFIG_DEBUG_FS
+	kbdev->mali_debugfs_directory = debugfs_create_dir("mali", NULL);
+	if (NULL == kbdev->mali_debugfs_directory) {
+		dev_err(kbdev->dev, "Couldn't create mali debugfs directory\n");
+		goto out_reg_unmap;
+	}
+#endif /* CONFIG_DEBUG_FS */
+
+	if (MALI_ERROR_NONE != kbase_device_init(kbdev)) {
+		dev_err(kbdev->dev, "Can't initialize device\n");
+		err = -ENOMEM;
+		goto out_debugfs_remove;
+	}
+
+	/* obtain min/max configured gpu frequencies */
+	core_props = &(kbdev->gpu_props.props.core_props);
+	core_props->gpu_freq_khz_min = kbasep_get_config_value(kbdev, platform_data, KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN);
+	core_props->gpu_freq_khz_max = kbasep_get_config_value(kbdev, platform_data, KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX);
+	kbdev->gpu_props.irq_throttle_time_us = kbasep_get_config_value(kbdev, platform_data, KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US);
+
+	err = kbase_common_device_init(kbdev);
+	if (err) {
+		dev_err(kbdev->dev, "Failed kbase_common_device_init\n");
+		goto out_term_dev;
+	}
+	return 0;
+
+out_term_dev:
+	kbase_device_term(kbdev);
+out_debugfs_remove:
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(kbdev->mali_debugfs_directory);
+out_reg_unmap:
+#endif /* CONFIG_DEBUG_FS */
+	kbase_common_reg_unmap(kbdev);
+out_free_dev:
+#ifdef CONFIG_MALI_NO_MALI
+	midg_device_destroy(kbdev);
+out_midg:
+#endif /* CONFIG_MALI_NO_MALI */
+	kbase_device_free(kbdev);
+out:
+	return err;
+}
+
+static int kbase_common_device_remove(struct kbase_device *kbdev)
+{
+	if (kbdev->pm.callback_power_runtime_term)
+		kbdev->pm.callback_power_runtime_term(kbdev);
+
+	/* Remove the sys power policy file */
+	device_remove_file(kbdev->dev, &dev_attr_power_policy);
+	device_remove_file(kbdev->dev, &dev_attr_core_availability_policy);
+	device_remove_file(kbdev->dev, &dev_attr_core_mask);
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	kbasep_trace_timeline_debugfs_term(kbdev);
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+
+#ifdef CONFIG_MALI_DEBUG
+	device_remove_file(kbdev->dev, &dev_attr_js_softstop_always);
+	device_remove_file(kbdev->dev, &dev_attr_debug_command);
+#endif /* CONFIG_MALI_DEBUG */
+#if MALI_CUSTOMER_RELEASE == 0
+	device_remove_file(kbdev->dev, &dev_attr_js_timeouts);
+	device_remove_file(kbdev->dev, &dev_attr_force_replay);
+#endif /* MALI_CUSTOMER_RELEASE */
+#ifdef CONFIG_DEBUG_FS
+	kbasep_gpu_memory_debugfs_term(kbdev);
+#endif
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+	device_remove_file(kbdev->dev, &dev_attr_sc_split);
+#endif /* CONFIG_MALI_DEBUG_SHADER_SPLIT_FS */
+
+	kbasep_js_devdata_halt(kbdev);
+	kbase_job_slot_halt(kbdev);
+	kbase_mem_halt(kbdev);
+	kbase_pm_halt(kbdev);
+
+	kbase_release_interrupts(kbdev);
+
+	kbasep_js_devdata_term(kbdev);
+	kbase_job_slot_term(kbdev);
+	kbase_mem_term(kbdev);
+	kbase_pm_term(kbdev);
+
+	down(&kbase_dev_list_lock);
+	list_del(&kbdev->entry);
+	up(&kbase_dev_list_lock);
+
+	misc_deregister(&kbdev->mdev);
+	put_device(kbdev->dev);
+	kbase_common_reg_unmap(kbdev);
+	kbase_device_term(kbdev);
+#ifdef CONFIG_DEBUG_FS
+	debugfs_remove(kbdev->mali_debugfs_directory);
+#endif /* CONFIG_DEBUG_FS */
+#ifdef CONFIG_MALI_NO_MALI
+	midg_device_destroy(kbdev);
+#endif /* CONFIG_MALI_NO_MALI */
+	kbase_device_free(kbdev);
+
+	return 0;
+}
+
+static int kbase_platform_device_remove(struct platform_device *pdev)
+{
+	struct kbase_device *kbdev = to_kbase_device(&pdev->dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	return kbase_common_device_remove(kbdev);
+}
+
+/** Suspend callback from the OS.
+ *
+ * This is called by Linux when the device should suspend.
+ *
+ * @param dev  The device to suspend
+ *
+ * @return A standard Linux error code
+ */
+static int kbase_device_suspend(struct device *dev)
+{
+	struct kbase_device *kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	kbase_pm_suspend(kbdev);
+	return 0;
+}
+
+/** Resume callback from the OS.
+ *
+ * This is called by Linux when the device should resume from suspension.
+ *
+ * @param dev  The device to resume
+ *
+ * @return A standard Linux error code
+ */
+static int kbase_device_resume(struct device *dev)
+{
+	struct kbase_device *kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	kbase_pm_resume(kbdev);
+	return 0;
+}
+
+/** Runtime suspend callback from the OS.
+ *
+ * This is called by Linux when the device should prepare for a condition in which it will
+ * not be able to communicate with the CPU(s) and RAM due to power management.
+ *
+ * @param dev  The device to suspend
+ *
+ * @return A standard Linux error code
+ */
+#ifdef CONFIG_PM_RUNTIME
+static int kbase_device_runtime_suspend(struct device *dev)
+{
+	struct kbase_device *kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	if (kbdev->pm.callback_power_runtime_off) {
+		kbdev->pm.callback_power_runtime_off(kbdev);
+		dev_dbg(dev, "runtime suspend\n");
+	}
+	return 0;
+}
+#endif /* CONFIG_PM_RUNTIME */
+
+/** Runtime resume callback from the OS.
+ *
+ * This is called by Linux when the device should go into a fully active state.
+ *
+ * @param dev  The device to suspend
+ *
+ * @return A standard Linux error code
+ */
+
+#ifdef CONFIG_PM_RUNTIME
+int kbase_device_runtime_resume(struct device *dev)
+{
+	int ret = 0;
+	struct kbase_device *kbdev = to_kbase_device(dev);
+
+	if (!kbdev)
+		return -ENODEV;
+
+	if (kbdev->pm.callback_power_runtime_on) {
+		ret = kbdev->pm.callback_power_runtime_on(kbdev);
+		dev_dbg(dev, "runtime resume\n");
+	}
+	return ret;
+}
+#endif /* CONFIG_PM_RUNTIME */
+
+/** Runtime idle callback from the OS.
+ *
+ * This is called by Linux when the device appears to be inactive and it might be
+ * placed into a low power state
+ *
+ * @param dev  The device to suspend
+ *
+ * @return A standard Linux error code
+ */
+
+#ifdef CONFIG_PM_RUNTIME
+static int kbase_device_runtime_idle(struct device *dev)
+{
+	/* Avoid pm_runtime_suspend being called */
+	return 1;
+}
+#endif /* CONFIG_PM_RUNTIME */
+
+/** The power management operations for the platform driver.
+ */
+static const struct dev_pm_ops kbase_pm_ops = {
+	.suspend = kbase_device_suspend,
+	.resume = kbase_device_resume,
+#ifdef CONFIG_PM_RUNTIME
+	.runtime_suspend = kbase_device_runtime_suspend,
+	.runtime_resume = kbase_device_runtime_resume,
+	.runtime_idle = kbase_device_runtime_idle,
+#endif /* CONFIG_PM_RUNTIME */
+};
+
+#ifdef CONFIG_OF
+static const struct of_device_id kbase_dt_ids[] = {
+	{ .compatible = "fujitsu,malit6xx" },
+	{ .compatible = "arm,malit6xx" },
+	{ .compatible = "arm,mali-midgard" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, kbase_dt_ids);
+#endif
+
+static struct platform_driver kbase_platform_driver = {
+	.probe = kbase_platform_device_probe,
+	.remove = kbase_platform_device_remove,
+	.driver = {
+		   .name = kbase_drv_name,
+		   .owner = THIS_MODULE,
+		   .pm = &kbase_pm_ops,
+		   .of_match_table = of_match_ptr(kbase_dt_ids),
+	},
+};
+
+/*
+ * The driver will not provide a shortcut to create the Mali platform device
+ * anymore when using Device Tree.
+ */
+#ifdef CONFIG_OF
+module_platform_driver(kbase_platform_driver);
+#else /* CONFIG_MALI_PLATFORM_FAKE */
+
+extern int kbase_platform_early_init(void);
+
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+extern int kbase_platform_fake_register(void);
+extern void kbase_platform_fake_unregister(void);
+#endif
+
+static int __init kbase_driver_init(void)
+{
+	int ret;
+
+	ret = kbase_platform_early_init();
+	if (ret)
+		return ret;
+
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+	ret = kbase_platform_fake_register();
+	if (ret)
+		return ret;
+#endif
+	ret = platform_driver_register(&kbase_platform_driver);
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+	if (ret)
+		kbase_platform_fake_unregister();
+#endif
+
+	return ret;
+}
+
+static void __exit kbase_driver_exit(void)
+{
+	platform_driver_unregister(&kbase_platform_driver);
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+	kbase_platform_fake_unregister();
+#endif
+}
+
+module_init(kbase_driver_init);
+module_exit(kbase_driver_exit);
+
+#endif /* CONFIG_OF */
+
+MODULE_LICENSE("GPL");
+MODULE_VERSION(MALI_RELEASE_NAME);
+
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+/* Create the trace points (otherwise we just get code to call a tracepoint) */
+#define CREATE_TRACE_POINTS
+#include "mali_linux_trace.h"
+
+void kbase_trace_mali_pm_status(u32 event, u64 value)
+{
+	trace_mali_pm_status(event, value);
+}
+
+void kbase_trace_mali_pm_power_off(u32 event, u64 value)
+{
+	trace_mali_pm_power_off(event, value);
+}
+
+void kbase_trace_mali_pm_power_on(u32 event, u64 value)
+{
+	trace_mali_pm_power_on(event, value);
+}
+
+void kbase_trace_mali_job_slots_event(u32 event, const kbase_context *kctx, u8 atom_id)
+{
+	trace_mali_job_slots_event(event, (kctx != NULL ? kctx->tgid : 0), (kctx != NULL ? kctx->pid : 0), atom_id);
+}
+
+void kbase_trace_mali_page_fault_insert_pages(int event, u32 value)
+{
+	trace_mali_page_fault_insert_pages(event, value);
+}
+
+void kbase_trace_mali_mmu_as_in_use(int event)
+{
+	trace_mali_mmu_as_in_use(event);
+}
+
+void kbase_trace_mali_mmu_as_released(int event)
+{
+	trace_mali_mmu_as_released(event);
+}
+
+void kbase_trace_mali_total_alloc_pages_change(long long int event)
+{
+	trace_mali_total_alloc_pages_change(event);
+}
+#endif /* CONFIG_MALI_GATOR_SUPPORT */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.c
new file mode 100644
index 0000000..b37d22a
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.c
@@ -0,0 +1,124 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_cpuprops.c
+ * Base kernel property query APIs
+ */
+
+#include "mali_kbase.h"
+#include "mali_kbase_cpuprops.h"
+#include "mali_kbase_uku.h"
+#include <mali_kbase_config.h>
+#include <linux/cache.h>
+#include <linux/cpufreq.h>
+#if defined(CONFIG_ARM) || defined(CONFIG_ARM64)
+#include <asm/cputype.h>
+#endif
+
+#define KBASE_DEFAULT_CPU_NUM 0
+
+#define L1_DCACHE_LINE_SIZE_LOG2 L1_CACHE_SHIFT
+
+/**
+ * @brief Macros used to extract cpu id info
+ * @see Doc's for Main ID register
+ */
+#define KBASE_CPUPROPS_ID_GET_REV(cpuid)    (  (cpuid) & 0x0F         )  /* [3:0]   Revision                            */
+#define KBASE_CPUPROPS_ID_GET_PART_NR(cpuid)( ((cpuid) >>  4) & 0xFFF )  /* [15:4]  Part number                         */
+#define KBASE_CPUPROPS_ID_GET_ARCH(cpuid)   ( ((cpuid) >> 16) & 0x0F  )  /* [19:16] Architecture                        */
+#define KBASE_CPUPROPS_ID_GET_VARIANT(cpuid)( ((cpuid) >> 20) & 0x0F  )  /* [23:20] Variant                             */
+#define KBASE_CPUPROPS_ID_GET_CODE(cpuid)   ( ((cpuid) >> 24) & 0xFF  )  /* [31:23] ASCII code of implementer trademark */
+
+/*Below value sourced from OSK*/
+#define L1_DCACHE_SIZE ((u32)0x00008000)
+
+
+/**
+ * @brief Retrieves detailed CPU info from given cpu_val ( ID reg )
+ *
+ * @param kbase_props CPU props to be filled-in with cpu id info
+ *
+ */
+#if defined(CONFIG_ARM) || defined(CONFIG_ARM64) 
+static void kbasep_cpuprops_uk_get_cpu_id_info(kbase_uk_cpuprops * const kbase_props)
+{
+	kbase_props->props.cpu_id.id           = read_cpuid_id();
+
+	kbase_props->props.cpu_id.valid        = 1;
+	kbase_props->props.cpu_id.rev          = KBASE_CPUPROPS_ID_GET_REV(kbase_props->props.cpu_id.id);
+	kbase_props->props.cpu_id.part         = KBASE_CPUPROPS_ID_GET_PART_NR(kbase_props->props.cpu_id.id);
+	kbase_props->props.cpu_id.arch         = KBASE_CPUPROPS_ID_GET_ARCH(kbase_props->props.cpu_id.id);
+	kbase_props->props.cpu_id.variant      = KBASE_CPUPROPS_ID_GET_VARIANT(kbase_props->props.cpu_id.id);
+	kbase_props->props.cpu_id.implementer  = KBASE_CPUPROPS_ID_GET_CODE(kbase_props->props.cpu_id.id);
+}
+#else
+static void kbasep_cpuprops_uk_get_cpu_id_info(kbase_uk_cpuprops * const kbase_props)
+{
+	kbase_props->props.cpu_id.id           = 0;
+	kbase_props->props.cpu_id.valid        = 0;
+	kbase_props->props.cpu_id.rev          = 0;
+	kbase_props->props.cpu_id.part         = 0;
+	kbase_props->props.cpu_id.arch         = 0;
+	kbase_props->props.cpu_id.variant      = 0;
+	kbase_props->props.cpu_id.implementer  = 'N';
+}
+#endif
+
+int kbase_cpuprops_get_default_clock_speed(u32 * const clock_speed)
+{
+	KBASE_DEBUG_ASSERT(NULL != clock_speed);
+
+	*clock_speed = 100;
+	return 0;
+}
+
+mali_error kbase_cpuprops_uk_get_props(kbase_context *kctx, kbase_uk_cpuprops * const kbase_props)
+{
+	unsigned int max_cpu_freq;
+
+	kbase_props->props.cpu_l1_dcache_line_size_log2 = L1_DCACHE_LINE_SIZE_LOG2;
+	kbase_props->props.cpu_l1_dcache_size = L1_DCACHE_SIZE;
+	kbase_props->props.cpu_flags = BASE_CPU_PROPERTY_FLAG_LITTLE_ENDIAN;
+
+	kbase_props->props.nr_cores = NR_CPUS;
+	kbase_props->props.cpu_page_size_log2 = PAGE_SHIFT;
+	kbase_props->props.available_memory_size = totalram_pages << PAGE_SHIFT;
+
+	kbasep_cpuprops_uk_get_cpu_id_info(kbase_props);
+
+	/* check if kernel supports dynamic frequency scaling */
+	max_cpu_freq = cpufreq_quick_get_max( KBASE_DEFAULT_CPU_NUM );
+	if ( max_cpu_freq != 0 )
+	{
+		/* convert from kHz to mHz */
+		kbase_props->props.max_cpu_clock_speed_mhz = max_cpu_freq / 1000 ;
+	}
+	else 
+	{
+		/* fallback if CONFIG_CPU_FREQ turned off */
+		int result;
+		kbase_cpuprops_clock_speed_function kbase_cpuprops_uk_get_clock_speed;
+
+		kbase_cpuprops_uk_get_clock_speed = (kbase_cpuprops_clock_speed_function) kbasep_get_config_value(kctx->kbdev, kctx->kbdev->config_attributes, KBASE_CONFIG_ATTR_CPU_SPEED_FUNC);
+		result = kbase_cpuprops_uk_get_clock_speed(&kbase_props->props.max_cpu_clock_speed_mhz);
+		if (result != 0)
+			return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	return MALI_ERROR_NONE;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.h
new file mode 100644
index 0000000..0f669b7
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_cpuprops.h
@@ -0,0 +1,56 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_cpuprops.h
+ * Base kernel property query APIs
+ */
+
+#ifndef _KBASE_CPUPROPS_H_
+#define _KBASE_CPUPROPS_H_
+
+#include <malisw/mali_malisw.h>
+
+/* Forward declarations */
+struct kbase_uk_cpuprops;
+
+/**
+ * @brief Default implementation of @ref KBASE_CONFIG_ATTR_CPU_SPEED_FUNC.
+ *
+ * This function sets clock_speed to 100, so will be an underestimate for
+ * any real system.
+ *
+ * See @ref kbase_cpuprops_clock_speed_function for details on the parameters
+ * and return value.
+ */
+int kbase_cpuprops_get_default_clock_speed(u32 * const clock_speed);
+
+/**
+ * @brief Provides CPU properties data.
+ *
+ * Fill the kbase_uk_cpuprops with values from CPU configuration.
+ *
+ * @param kctx         The kbase context
+ * @param kbase_props  A copy of the kbase_uk_cpuprops structure from userspace
+ *
+ * @return MALI_ERROR_NONE on success. Any other value indicates failure.
+ */
+mali_error kbase_cpuprops_uk_get_props(kbase_context *kctx, struct kbase_uk_cpuprops * const kbase_props);
+
+#endif /*_KBASE_CPUPROPS_H_*/
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.c
new file mode 100644
index 0000000..247ca40
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.c
@@ -0,0 +1,39 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+
+kbasep_debug_assert_cb kbasep_debug_assert_registered_cb = {
+	NULL,
+	NULL
+};
+
+void kbase_debug_assert_register_hook(kbase_debug_assert_hook *func, void *param)
+{
+	kbasep_debug_assert_registered_cb.func = func;
+	kbasep_debug_assert_registered_cb.param = param;
+}
+
+void kbasep_debug_assert_call_hook(void)
+{
+	if (kbasep_debug_assert_registered_cb.func != NULL)
+		kbasep_debug_assert_registered_cb.func(kbasep_debug_assert_registered_cb.param);
+}
+KBASE_EXPORT_SYMBOL(kbasep_debug_assert_call_hook);
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.h
new file mode 100644
index 0000000..8e388ec
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_debug.h
@@ -0,0 +1,164 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_DEBUG_H
+#define _KBASE_DEBUG_H
+
+#include <linux/bug.h>
+
+/** @brief If equals to 0, a trace containing the file, line, and function will be displayed before each message. */
+#define KBASE_DEBUG_SKIP_TRACE 0
+
+/** @brief If different from 0, the trace will only contain the file and line. */
+#define KBASE_DEBUG_SKIP_FUNCTION_NAME 0
+
+/** @brief Disable the asserts tests if set to 1. Default is to disable the asserts in release. */
+#ifndef KBASE_DEBUG_DISABLE_ASSERTS
+#ifdef CONFIG_MALI_DEBUG
+#define KBASE_DEBUG_DISABLE_ASSERTS 0
+#else
+#define KBASE_DEBUG_DISABLE_ASSERTS 1
+#endif
+#endif				/* KBASE_DEBUG_DISABLE_ASSERTS */
+
+/** Function type that is called on an KBASE_DEBUG_ASSERT() or KBASE_DEBUG_ASSERT_MSG() */
+typedef void (kbase_debug_assert_hook) (void *);
+
+typedef struct kbasep_debug_assert_cb {
+	kbase_debug_assert_hook *func;
+	void *param;
+} kbasep_debug_assert_cb;
+
+/**
+ * @def KBASEP_DEBUG_PRINT_TRACE
+ * @brief Private macro containing the format of the trace to display before every message
+ * @sa KBASE_DEBUG_SKIP_TRACE, KBASE_DEBUG_SKIP_FUNCTION_NAME
+ */
+#if KBASE_DEBUG_SKIP_TRACE == 0
+#define KBASEP_DEBUG_PRINT_TRACE \
+		"In file: " __FILE__ " line: " CSTD_STR2(__LINE__)
+#if KBASE_DEBUG_SKIP_FUNCTION_NAME == 0
+#define KBASEP_DEBUG_PRINT_FUNCTION CSTD_FUNC
+#else
+#define KBASEP_DEBUG_PRINT_FUNCTION ""
+#endif
+#else
+#define KBASEP_DEBUG_PRINT_TRACE ""
+#endif
+
+/**
+ * @def KBASEP_DEBUG_ASSERT_OUT(trace, function, ...)
+ * @brief (Private) system printing function associated to the @see KBASE_DEBUG_ASSERT_MSG event.
+ * @param trace location in the code from where the message is printed
+ * @param function function from where the message is printed
+ * @param ... Format string followed by format arguments.
+ * @note function parameter cannot be concatenated with other strings
+ */
+/* Select the correct system output function*/
+#ifdef CONFIG_MALI_DEBUG
+#define KBASEP_DEBUG_ASSERT_OUT(trace, function, ...)\
+		do { \
+			pr_err("Mali<ASSERT>: %s function:%s ", trace, function);\
+			pr_err(__VA_ARGS__);\
+			pr_err("\n");\
+		} while (MALI_FALSE)
+#else
+#define KBASEP_DEBUG_ASSERT_OUT(trace, function, ...) CSTD_NOP()
+#endif
+
+#ifdef CONFIG_MALI_DEBUG
+#define KBASE_CALL_ASSERT_HOOK() kbasep_debug_assert_call_hook();
+#else
+#define KBASE_CALL_ASSERT_HOOK() CSTD_NOP();
+#endif
+
+/**
+ * @def KBASE_DEBUG_ASSERT(expr)
+ * @brief Calls @see KBASE_PRINT_ASSERT and prints the expression @a expr if @a expr is false
+ *
+ * @note This macro does nothing if the flag @see KBASE_DEBUG_DISABLE_ASSERTS is set to 1
+ *
+ * @param expr Boolean expression
+ */
+#define KBASE_DEBUG_ASSERT(expr) \
+	KBASE_DEBUG_ASSERT_MSG(expr, #expr)
+
+#if KBASE_DEBUG_DISABLE_ASSERTS
+#define KBASE_DEBUG_ASSERT_MSG(expr, ...) CSTD_NOP()
+#else
+	/**
+	 * @def KBASE_DEBUG_ASSERT_MSG(expr, ...)
+	 * @brief Calls @see KBASEP_DEBUG_ASSERT_OUT and prints the given message if @a expr is false
+	 *
+	 * @note This macro does nothing if the flag @see KBASE_DEBUG_DISABLE_ASSERTS is set to 1
+	 *
+	 * @param expr Boolean expression
+	 * @param ...  Message to display when @a expr is false, as a format string followed by format arguments.
+	 */
+#define KBASE_DEBUG_ASSERT_MSG(expr, ...) \
+		do { \
+			if (MALI_FALSE == (expr)) { \
+				KBASEP_DEBUG_ASSERT_OUT(KBASEP_DEBUG_PRINT_TRACE, KBASEP_DEBUG_PRINT_FUNCTION, __VA_ARGS__);\
+				KBASE_CALL_ASSERT_HOOK();\
+				BUG();\
+			} \
+		} while (MALI_FALSE)
+#endif				/* KBASE_DEBUG_DISABLE_ASSERTS */
+
+/**
+ * @def KBASE_DEBUG_CODE( X )
+ * @brief Executes the code inside the macro only in debug mode
+ *
+ * @param X Code to compile only in debug mode.
+ */
+#ifdef CONFIG_MALI_DEBUG
+#define KBASE_DEBUG_CODE(X) X
+#else
+#define KBASE_DEBUG_CODE(X) CSTD_NOP()
+#endif				/* CONFIG_MALI_DEBUG */
+
+/** @} */
+
+/**
+ * @brief Register a function to call on ASSERT
+ *
+ * Such functions will \b only be called during Debug mode, and for debugging
+ * features \b only. Do not rely on them to be called in general use.
+ *
+ * To disable the hook, supply NULL to \a func.
+ *
+ * @note This function is not thread-safe, and should only be used to
+ * register/deregister once in the module's lifetime.
+ *
+ * @param[in] func the function to call when an assert is triggered.
+ * @param[in] param the parameter to pass to \a func when calling it
+ */
+void kbase_debug_assert_register_hook(kbase_debug_assert_hook *func, void *param);
+
+/**
+ * @brief Call a debug assert hook previously registered with kbase_debug_assert_register_hook()
+ *
+ * @note This function is not thread-safe with respect to multiple threads
+ * registering functions and parameters with
+ * kbase_debug_assert_register_hook(). Otherwise, thread safety is the
+ * responsibility of the registered hook.
+ */
+void kbasep_debug_assert_call_hook(void);
+
+#endif				/* _KBASE_DEBUG_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_defs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_defs.h
new file mode 100644
index 0000000..ebf28dc
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_defs.h
@@ -0,0 +1,949 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_defs.h
+ *
+ * Defintions (types, defines, etcs) common to Kbase. They are placed here to
+ * allow the hierarchy of header files to work.
+ */
+
+#ifndef _KBASE_DEFS_H_
+#define _KBASE_DEFS_H_
+
+#include <mali_kbase_config.h>
+#include <mali_base_hwconfig.h>
+#include <mali_kbase_mem_lowlevel.h>
+#include <mali_kbase_mem_alloc.h>
+
+
+#include <linux/atomic.h>
+#include <linux/mempool.h>
+#include <linux/slab.h>
+
+#ifdef CONFIG_KDS
+#include <linux/kds.h>
+#endif				/* CONFIG_KDS */
+
+#ifdef CONFIG_SYNC
+#include "sync.h"
+#endif				/* CONFIG_SYNC */
+
+/** Enable SW tracing when set */
+#ifdef CONFIG_MALI_MIDGARD_ENABLE_TRACE
+#define KBASE_TRACE_ENABLE 1
+#endif
+
+#ifndef KBASE_TRACE_ENABLE
+#ifdef CONFIG_MALI_DEBUG
+#define KBASE_TRACE_ENABLE 1
+#else
+#define KBASE_TRACE_ENABLE 0
+#endif				/* CONFIG_MALI_DEBUG */
+#endif				/* KBASE_TRACE_ENABLE */
+
+/** Dump Job slot trace on error (only active if KBASE_TRACE_ENABLE != 0) */
+#define KBASE_TRACE_DUMP_ON_JOB_SLOT_ERROR 1
+
+/**
+ * Number of milliseconds before resetting the GPU when a job cannot be "zapped" from the hardware.
+ * Note that the time is actually ZAP_TIMEOUT+SOFT_STOP_RESET_TIMEOUT between the context zap starting and the GPU
+ * actually being reset to give other contexts time for their jobs to be soft-stopped and removed from the hardware
+ * before resetting.
+ */
+#define ZAP_TIMEOUT             1000
+
+/** Number of milliseconds before we time out on a GPU soft/hard reset */
+#define RESET_TIMEOUT           500
+
+/**
+ * Prevent soft-stops from occuring in scheduling situations
+ *
+ * This is not due to HW issues, but when scheduling is desired to be more predictable.
+ *
+ * Therefore, soft stop may still be disabled due to HW issues.
+ *
+ * @note Soft stop will still be used for non-scheduling purposes e.g. when terminating a context.
+ *
+ * @note if not in use, define this value to 0 instead of \#undef'ing it
+ */
+#define KBASE_DISABLE_SCHEDULING_SOFT_STOPS 0
+
+/**
+ * Prevent hard-stops from occuring in scheduling situations
+ *
+ * This is not due to HW issues, but when scheduling is desired to be more predictable.
+ *
+ * @note Hard stop will still be used for non-scheduling purposes e.g. when terminating a context.
+ *
+ * @note if not in use, define this value to 0 instead of \#undef'ing it
+ */
+#define KBASE_DISABLE_SCHEDULING_HARD_STOPS 0
+
+/* Forward declarations+defintions */
+typedef struct kbase_context kbase_context;
+typedef struct kbase_jd_atom kbasep_jd_atom;
+typedef struct kbase_device kbase_device;
+
+/**
+ * The maximum number of Job Slots to support in the Hardware.
+ *
+ * You can optimize this down if your target devices will only ever support a
+ * small number of job slots.
+ */
+#define BASE_JM_MAX_NR_SLOTS        16
+
+/**
+ * The maximum number of Address Spaces to support in the Hardware.
+ *
+ * You can optimize this down if your target devices will only ever support a
+ * small number of Address Spaces
+ */
+#define BASE_MAX_NR_AS              16
+
+/* mmu */
+#define ENTRY_IS_ATE        1ULL
+#define ENTRY_IS_INVAL      2ULL
+#define ENTRY_IS_PTE        3ULL
+
+#define MIDGARD_MMU_VA_BITS 48
+
+#define ENTRY_ATTR_BITS (7ULL << 2)	/* bits 4:2 */
+#define ENTRY_RD_BIT (1ULL << 6)
+#define ENTRY_WR_BIT (1ULL << 7)
+#define ENTRY_SHARE_BITS (3ULL << 8)	/* bits 9:8 */
+#define ENTRY_ACCESS_BIT (1ULL << 10)
+#define ENTRY_NX_BIT (1ULL << 54)
+
+#define ENTRY_FLAGS_MASK (ENTRY_ATTR_BITS | ENTRY_RD_BIT | ENTRY_WR_BIT | ENTRY_SHARE_BITS | ENTRY_ACCESS_BIT | ENTRY_NX_BIT)
+
+#if MIDGARD_MMU_VA_BITS > 39
+#define MIDGARD_MMU_TOPLEVEL    0
+#else
+#define MIDGARD_MMU_TOPLEVEL    1
+#endif
+
+#define GROWABLE_FLAGS_REQUIRED (KBASE_REG_PF_GROW)
+#define GROWABLE_FLAGS_MASK     (GROWABLE_FLAGS_REQUIRED | KBASE_REG_FREE)
+
+/** setting in kbase_context::as_nr that indicates it's invalid */
+#define KBASEP_AS_NR_INVALID     (-1)
+
+#define KBASE_LOCK_REGION_MAX_SIZE (63)
+#define KBASE_LOCK_REGION_MIN_SIZE (11)
+
+#define KBASE_TRACE_SIZE_LOG2 8	/* 256 entries */
+#define KBASE_TRACE_SIZE (1 << KBASE_TRACE_SIZE_LOG2)
+#define KBASE_TRACE_MASK ((1 << KBASE_TRACE_SIZE_LOG2)-1)
+
+#include "mali_kbase_js_defs.h"
+
+#define KBASEP_FORCE_REPLAY_DISABLED 0
+
+/* Maximum force replay limit when randomization is enabled */
+#define KBASEP_FORCE_REPLAY_RANDOM_LIMIT 16
+
+/**
+ * @brief States to model state machine processed by kbasep_js_job_check_ref_cores(), which
+ * handles retaining cores for power management and affinity management.
+ *
+ * The state @ref KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY prevents an attack
+ * where lots of atoms could be submitted before powerup, and each has an
+ * affinity chosen that causes other atoms to have an affinity
+ * violation. Whilst the affinity was not causing violations at the time it
+ * was chosen, it could cause violations thereafter. For example, 1000 jobs
+ * could have had their affinity chosen during the powerup time, so any of
+ * those 1000 jobs could cause an affinity violation later on.
+ *
+ * The attack would otherwise occur because other atoms/contexts have to wait for:
+ * -# the currently running atoms (which are causing the violation) to
+ * finish
+ * -# and, the atoms that had their affinity chosen during powerup to
+ * finish. These are run preferrentially because they don't cause a
+ * violation, but instead continue to cause the violation in others.
+ * -# or, the attacker is scheduled out (which might not happen for just 2
+ * contexts)
+ *
+ * By re-choosing the affinity (which is designed to avoid violations at the
+ * time it's chosen), we break condition (2) of the wait, which minimizes the
+ * problem to just waiting for current jobs to finish (which can be bounded if
+ * the Job Scheduling Policy has a timer).
+ */
+typedef enum {
+	/** Starting state: No affinity chosen, and cores must be requested. kbase_jd_atom::affinity==0 */
+	KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED,
+	/** Cores requested, but waiting for them to be powered. Requested cores given by kbase_jd_atom::affinity */
+	KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES,
+	/** Cores given by kbase_jd_atom::affinity are powered, but affinity might be out-of-date, so must recheck */
+	KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY,
+	/** Cores given by kbase_jd_atom::affinity are powered, and affinity is up-to-date, but must check for violations */
+	KBASE_ATOM_COREREF_STATE_CHECK_AFFINITY_VIOLATIONS,
+	/** Cores are powered, kbase_jd_atom::affinity up-to-date, no affinity violations: atom can be submitted to HW */
+	KBASE_ATOM_COREREF_STATE_READY
+} kbase_atom_coreref_state;
+
+typedef enum {
+	/** Atom is not used */
+	KBASE_JD_ATOM_STATE_UNUSED,
+	/** Atom is queued in JD */
+	KBASE_JD_ATOM_STATE_QUEUED,
+	/** Atom has been given to JS (is runnable/running) */
+	KBASE_JD_ATOM_STATE_IN_JS,
+	/** Atom has been completed, but not yet handed back to userspace */
+	KBASE_JD_ATOM_STATE_COMPLETED
+} kbase_jd_atom_state;
+
+/** Atom has been previously soft-stoppped */
+#define KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED (1<<1)
+/** Atom has been previously retried to execute */
+#define KBASE_KATOM_FLAGS_RERUN (1<<2)
+#define KBASE_KATOM_FLAGS_JOBCHAIN (1<<3)
+
+typedef struct kbase_jd_atom kbase_jd_atom;
+
+struct kbase_jd_atom_dependency
+{
+	struct kbase_jd_atom *atom;
+	u8 dep_type;
+};
+
+/**
+ * @brief The function retrieves a read-only reference to the atom field from 
+ * the  kbase_jd_atom_dependency structure
+ *
+ * @param[in] dep kbase jd atom dependency.
+ *
+ * @return readonly reference to dependent ATOM.
+ */
+static INLINE const struct kbase_jd_atom* const kbase_jd_katom_dep_atom(const struct kbase_jd_atom_dependency* dep)
+{
+	LOCAL_ASSERT(dep != NULL);
+	
+	return (const struct kbase_jd_atom* const )(dep->atom);
+}
+ 
+/**
+ * @brief The function retrieves a read-only reference to the dependency type field from 
+ * the  kbase_jd_atom_dependency structure
+ *
+ * @param[in] dep kbase jd atom dependency.
+ *
+ * @return A dependency type value.
+ */
+static INLINE const u8 kbase_jd_katom_dep_type(const struct kbase_jd_atom_dependency* dep)
+{
+	LOCAL_ASSERT(dep != NULL);
+
+	return dep->dep_type;
+}
+
+/**
+ * @brief Setter macro for dep_atom array entry in kbase_jd_atom
+ *
+ * @param[in] dep    The kbase jd atom dependency.
+ * @param[in] a      The ATOM to be set as a dependency.
+ * @param     type   The ATOM dependency type to be set.
+ *
+ */
+static INLINE void kbase_jd_katom_dep_set(const struct kbase_jd_atom_dependency* const_dep, 
+	struct kbase_jd_atom * a,
+	u8 type)
+{
+	struct kbase_jd_atom_dependency* dep;
+	
+	LOCAL_ASSERT(const_dep != NULL);
+
+	dep = (REINTERPRET_CAST(struct kbase_jd_atom_dependency* )const_dep);
+
+	dep->atom = a;
+	dep->dep_type = type; 
+}
+
+/**
+ * @brief Setter macro for dep_atom array entry in kbase_jd_atom
+ *
+ * @param[in] dep    The kbase jd atom dependency to be cleared.
+ *
+ */
+static INLINE void kbase_jd_katom_dep_clear(const struct kbase_jd_atom_dependency* const_dep)
+{
+	struct kbase_jd_atom_dependency* dep;
+
+	LOCAL_ASSERT(const_dep != NULL);
+
+	dep = (REINTERPRET_CAST(struct kbase_jd_atom_dependency* )const_dep);
+
+	dep->atom = NULL;
+	dep->dep_type = BASE_JD_DEP_TYPE_INVALID; 
+}
+
+struct kbase_ext_res
+{
+	mali_addr64 gpu_address;
+	struct kbase_mem_phy_alloc * alloc;
+};
+
+struct kbase_jd_atom {
+	struct work_struct work;
+	ktime_t start_timestamp;
+
+	base_jd_udata udata;
+	kbase_context *kctx;
+
+	struct list_head dep_head[2];
+	struct list_head dep_item[2];
+	const struct kbase_jd_atom_dependency dep[2];
+
+	u16 nr_extres;
+	struct kbase_ext_res * extres;
+
+	u32 device_nr;
+	u64 affinity;
+	u64 jc;
+	kbase_atom_coreref_state coreref_state;
+#ifdef CONFIG_KDS
+	struct list_head node;
+	struct kds_resource_set *kds_rset;
+	mali_bool kds_dep_satisfied;
+#endif				/* CONFIG_KDS */
+#ifdef CONFIG_SYNC
+	struct sync_fence *fence;
+	struct sync_fence_waiter sync_waiter;
+#endif				/* CONFIG_SYNC */
+
+	/* Note: refer to kbasep_js_atom_retained_state, which will take a copy of some of the following members */
+	base_jd_event_code event_code;
+	base_jd_core_req core_req;	    /**< core requirements */
+	/** Job Slot to retry submitting to if submission from IRQ handler failed
+	 *
+	 * NOTE: see if this can be unified into the another member e.g. the event */
+	int retry_submit_on_slot;
+
+	kbasep_js_policy_job_info sched_info;
+	/* atom priority scaled to nice range with +20 offset 0..39 */
+	int nice_prio;
+
+	int poking;		/* BASE_HW_ISSUE_8316 */
+
+	wait_queue_head_t completed;
+	kbase_jd_atom_state status;
+#ifdef CONFIG_GPU_TRACEPOINTS
+	int work_id;
+#endif
+	/* Assigned after atom is completed. Used to check whether PRLAM-10676 workaround should be applied */
+	int slot_nr;
+
+	u32 atom_flags;
+
+	/* Number of times this atom has been retried. Used by replay soft job.
+	 */
+	int retry_count;
+};
+
+/*
+ * Theory of operations:
+ *
+ * Atom objects are statically allocated within the context structure.
+ *
+ * Each atom is the head of two lists, one for the "left" set of dependencies, one for the "right" set.
+ */
+
+#define KBASE_JD_DEP_QUEUE_SIZE 256
+
+typedef struct kbase_jd_context {
+	struct mutex lock;
+	kbasep_js_kctx_info sched_info;
+	kbase_jd_atom atoms[BASE_JD_ATOM_COUNT];
+
+	/** Tracks all job-dispatch jobs.  This includes those not tracked by
+	 * the scheduler: 'not ready to run' and 'dependency-only' jobs. */
+	u32 job_nr;
+
+	/** Waitq that reflects whether there are no jobs (including SW-only
+	 * dependency jobs). This is set when no jobs are present on the ctx,
+	 * and clear when there are jobs.
+	 *
+	 * @note: Job Dispatcher knows about more jobs than the Job Scheduler:
+	 * the Job Scheduler is unaware of jobs that are blocked on dependencies,
+	 * and SW-only dependency jobs.
+	 *
+	 * This waitq can be waited upon to find out when the context jobs are all
+	 * done/cancelled (including those that might've been blocked on
+	 * dependencies) - and so, whether it can be terminated. However, it should
+	 * only be terminated once it is neither present in the policy-queue (see
+	 * kbasep_js_policy_try_evict_ctx() ) nor the run-pool (see
+	 * kbasep_js_kctx_info::ctx::is_scheduled).
+	 *
+	 * Since the waitq is only set under kbase_jd_context::lock,
+	 * the waiter should also briefly obtain and drop kbase_jd_context::lock to
+	 * guarentee that the setter has completed its work on the kbase_context
+	 *
+	 * This must be updated atomically with:
+	 * - kbase_jd_context::job_nr */
+	wait_queue_head_t zero_jobs_wait;
+
+	/** Job Done workqueue. */
+	struct workqueue_struct *job_done_wq;
+
+	spinlock_t tb_lock;
+	u32 *tb;
+	size_t tb_wrap_offset;
+
+#ifdef CONFIG_KDS
+	struct kds_callback kds_cb;
+#endif				/* CONFIG_KDS */
+#ifdef CONFIG_GPU_TRACEPOINTS
+	atomic_t work_id;
+#endif
+} kbase_jd_context;
+
+typedef struct kbase_jm_slot {
+	/* The number of slots must be a power of two */
+#define BASE_JM_SUBMIT_SLOTS        16
+#define BASE_JM_SUBMIT_SLOTS_MASK   (BASE_JM_SUBMIT_SLOTS - 1)
+
+	struct kbase_jd_atom *submitted[BASE_JM_SUBMIT_SLOTS];
+
+	kbase_context *last_context;
+
+	u8 submitted_head;
+	u8 submitted_nr;
+	u8 job_chain_flag;
+
+} kbase_jm_slot;
+
+typedef enum kbase_midgard_type {
+	KBASE_MALI_T601,
+	KBASE_MALI_T604,
+	KBASE_MALI_T608,
+	KBASE_MALI_COUNT
+} kbase_midgard_type;
+
+typedef struct kbase_device_info {
+	kbase_midgard_type dev_type;
+	u32 features;
+} kbase_device_info;
+
+/** Poking state for BASE_HW_ISSUE_8316  */
+enum {
+	KBASE_AS_POKE_STATE_IN_FLIGHT     = 1<<0,
+	KBASE_AS_POKE_STATE_KILLING_POKE  = 1<<1
+};
+
+/** Poking state for BASE_HW_ISSUE_8316  */
+typedef u32 kbase_as_poke_state;
+
+/**
+ * Important: Our code makes assumptions that a kbase_as structure is always at
+ * kbase_device->as[number]. This is used to recover the containing
+ * kbase_device from a kbase_as structure.
+ *
+ * Therefore, kbase_as structures must not be allocated anywhere else.
+ */
+typedef struct kbase_as {
+	int number;
+
+	struct workqueue_struct *pf_wq;
+	struct work_struct work_pagefault;
+	struct work_struct work_busfault;
+	mali_addr64 fault_addr;
+	u32 fault_status;
+	struct mutex transaction_mutex;
+
+	/* BASE_HW_ISSUE_8316  */
+	struct workqueue_struct *poke_wq;
+	struct work_struct poke_work;
+	/** Protected by kbasep_js_device_data::runpool_irq::lock */
+	int poke_refcount;
+	/** Protected by kbasep_js_device_data::runpool_irq::lock */
+	kbase_as_poke_state poke_state;
+	struct hrtimer poke_timer;
+} kbase_as;
+
+/**
+ * Instrumentation State Machine States
+ */
+typedef enum {
+	/** State where instrumentation is not active */
+	KBASE_INSTR_STATE_DISABLED = 0,
+	/** State machine is active and ready for a command. */
+	KBASE_INSTR_STATE_IDLE,
+	/** Hardware is currently dumping a frame. */
+	KBASE_INSTR_STATE_DUMPING,
+	/** We've requested a clean to occur on a workqueue */
+	KBASE_INSTR_STATE_REQUEST_CLEAN,
+	/** Hardware is currently cleaning and invalidating caches. */
+	KBASE_INSTR_STATE_CLEANING,
+	/** Cache clean completed, and either a) a dump is complete, or
+	 * b) instrumentation can now be setup. */
+	KBASE_INSTR_STATE_CLEANED,
+	/** kbasep_reset_timeout_worker() has started (but not compelted) a
+	 * reset. This generally indicates the current action should be aborted, and
+	 * kbasep_reset_timeout_worker() will handle the cleanup */
+	KBASE_INSTR_STATE_RESETTING,
+	/** An error has occured during DUMPING (page fault). */
+	KBASE_INSTR_STATE_FAULT
+} kbase_instr_state;
+
+typedef struct kbasep_mem_device {
+	atomic_t used_pages;   /* Tracks usage of OS shared memory. Updated
+				   when OS memory is allocated/freed. */
+
+} kbasep_mem_device;
+
+
+
+#define KBASE_TRACE_CODE(X) KBASE_TRACE_CODE_ ## X
+
+typedef enum {
+	/* IMPORTANT: USE OF SPECIAL #INCLUDE OF NON-STANDARD HEADER FILE
+	 * THIS MUST BE USED AT THE START OF THE ENUM */
+#define KBASE_TRACE_CODE_MAKE_CODE(X) KBASE_TRACE_CODE(X)
+#include "mali_kbase_trace_defs.h"
+#undef  KBASE_TRACE_CODE_MAKE_CODE
+	/* Comma on its own, to extend the list */
+	,
+	/* Must be the last in the enum */
+	KBASE_TRACE_CODE_COUNT
+} kbase_trace_code;
+
+#define KBASE_TRACE_FLAG_REFCOUNT (((u8)1) << 0)
+#define KBASE_TRACE_FLAG_JOBSLOT  (((u8)1) << 1)
+
+typedef struct kbase_trace {
+	struct timespec timestamp;
+	u32 thread_id;
+	u32 cpu;
+	void *ctx;
+	mali_bool katom;
+	int atom_number;
+	u64 atom_udata[2];
+	u64 gpu_addr;
+	unsigned long info_val;
+	u8 code;
+	u8 jobslot;
+	u8 refcount;
+	u8 flags;
+} kbase_trace;
+
+/** Event IDs for the power management framework.
+ *
+ * Any of these events might be missed, so they should not be relied upon to
+ * find the precise state of the GPU at a particular time in the
+ * trace. Overall, we should get a high percentage of these events for
+ * statisical purposes, and so a few missing should not be a problem */
+typedef enum kbase_timeline_pm_event {
+	/* helper for tests */
+	KBASEP_TIMELINE_PM_EVENT_FIRST,
+
+	/** Event reserved for backwards compatibility with 'init' events */
+	KBASE_TIMELINE_PM_EVENT_RESERVED_0 = KBASEP_TIMELINE_PM_EVENT_FIRST,
+
+	/** The power state of the device has changed.
+	 *
+	 * Specifically, the device has reached a desired or available state.
+	 */
+	KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED,
+
+	/** The GPU is becoming active.
+	 *
+	 * This event is sent when the first context is about to use the GPU.
+	 */
+	KBASE_TIMELINE_PM_EVENT_GPU_ACTIVE,
+
+	/** The GPU is becoming idle.
+	 *
+	 * This event is sent when the last context has finished using the GPU.
+	 */
+	KBASE_TIMELINE_PM_EVENT_GPU_IDLE,
+
+	/** Event reserved for backwards compatibility with 'policy_change'
+	 * events */
+	KBASE_TIMELINE_PM_EVENT_RESERVED_4,
+
+	/** Event reserved for backwards compatibility with 'system_suspend'
+	 * events */
+	KBASE_TIMELINE_PM_EVENT_RESERVED_5,
+
+	/** Event reserved for backwards compatibility with 'system_resume'
+	 * events */
+	KBASE_TIMELINE_PM_EVENT_RESERVED_6,
+
+	/** The job scheduler is requesting to power up/down cores.
+	 *
+	 * This event is sent when:
+	 * - powered down cores are needed to complete a job
+	 * - powered up cores are not needed anymore
+	 */
+	KBASE_TIMELINE_PM_EVENT_CHANGE_GPU_STATE,
+
+	KBASEP_TIMELINE_PM_EVENT_LAST = KBASE_TIMELINE_PM_EVENT_CHANGE_GPU_STATE,
+} kbase_timeline_pm_event;
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+typedef struct kbase_trace_kctx_timeline {
+	atomic_t jd_atoms_in_flight;
+	u32 owner_tgid;
+} kbase_trace_kctx_timeline;
+
+typedef struct kbase_trace_kbdev_timeline {
+	/** DebugFS entry */
+	struct dentry *dentry;
+
+	/* Note: strictly speaking, not needed, because it's in sync with
+	 * kbase_device::jm_slots[]::submitted_nr
+	 *
+	 * But it's kept as an example of how to add global timeline tracking
+	 * information
+	 *
+	 * The caller must hold kbasep_js_device_data::runpool_irq::lock when
+	 * accessing this */
+	u8 slot_atoms_submitted[BASE_JM_SUBMIT_SLOTS];
+
+	/* Last UID for each PM event */
+	atomic_t pm_event_uid[KBASEP_TIMELINE_PM_EVENT_LAST+1];
+	/* Counter for generating PM event UIDs */
+	atomic_t pm_event_uid_counter;
+	/*
+	 * L2 transition state - MALI_TRUE indicates that the transition is ongoing
+	 * Expected to be protected by pm.power_change_lock */
+	mali_bool l2_transitioning;
+} kbase_trace_kbdev_timeline;
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+
+
+typedef struct kbasep_kctx_list_element {
+	struct list_head link;
+	kbase_context    *kctx;
+} kbasep_kctx_list_element;
+
+#define DEVNAME_SIZE	16
+
+struct kbase_device {
+	/** jm_slots is protected by kbasep_js_device_data::runpool_irq::lock */
+	kbase_jm_slot jm_slots[BASE_JM_MAX_NR_SLOTS];
+	s8 slot_submit_count_irq[BASE_JM_MAX_NR_SLOTS];
+
+	struct list_head entry;
+	struct device *dev;
+	struct miscdevice mdev;
+	u64 reg_start;
+	size_t reg_size;
+	void __iomem *reg;
+	struct resource *reg_res;
+	struct {
+		int irq;
+		int flags;
+	} irqs[3];
+	char devname[DEVNAME_SIZE];
+
+#ifdef CONFIG_MALI_NO_MALI
+	void *model;
+	struct kmem_cache *irq_slab;
+	struct workqueue_struct *irq_workq;
+	atomic_t serving_job_irq;
+	atomic_t serving_gpu_irq;
+	atomic_t serving_mmu_irq;
+	spinlock_t reg_op_lock;
+#endif				/* CONFIG_MALI_NO_MALI */
+
+	kbase_pm_device_data pm;
+	kbasep_js_device_data js_data;
+	kbasep_mem_device memdev;
+
+	kbase_as as[BASE_MAX_NR_AS];
+
+	spinlock_t              mmu_mask_change;
+
+	kbase_gpu_props gpu_props;
+
+	/** List of SW workarounds for HW issues */
+	unsigned long hw_issues_mask[(BASE_HW_ISSUE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
+	/** List of features available */
+	unsigned long hw_features_mask[(BASE_HW_FEATURE_END + BITS_PER_LONG - 1) / BITS_PER_LONG];
+
+	/* Cached present bitmaps - these are the same as the corresponding hardware registers */
+	u64 shader_present_bitmap;
+	u64 tiler_present_bitmap;
+	u64 l2_present_bitmap;
+	u64 l3_present_bitmap;
+
+	/* Bitmaps of cores that are currently in use (running jobs).
+	 * These should be kept up to date by the job scheduler.
+	 *
+	 * pm.power_change_lock should be held when accessing these members.
+	 *
+	 * kbase_pm_check_transitions_nolock() should be called when bits are
+	 * cleared to update the power management system and allow transitions to
+	 * occur. */
+	u64 shader_inuse_bitmap;
+
+	/* Refcount for cores in use */
+	u32 shader_inuse_cnt[64];
+
+	/* Bitmaps of cores the JS needs for jobs ready to run */
+	u64 shader_needed_bitmap;
+
+	/* Refcount for cores needed */
+	u32 shader_needed_cnt[64];
+
+	u32 tiler_inuse_cnt;
+
+	u32 tiler_needed_cnt;
+
+	/* Refcount for tracking users of the l2 cache, e.g. when using hardware counter instrumentation. */
+	u32 l2_users_count;
+
+	/* Bitmaps of cores that are currently available (powered up and the power policy is happy for jobs to be
+	 * submitted to these cores. These are updated by the power management code. The job scheduler should avoid
+	 * submitting new jobs to any cores that are not marked as available.
+	 *
+	 * pm.power_change_lock should be held when accessing these members.
+	 */
+	u64 shader_available_bitmap;
+	u64 tiler_available_bitmap;
+	u64 l2_available_bitmap;
+
+	u64 shader_ready_bitmap;
+	u64 shader_transitioning_bitmap;
+
+	s8 nr_hw_address_spaces;			  /**< Number of address spaces in the GPU (constant after driver initialisation) */
+	s8 nr_user_address_spaces;			  /**< Number of address spaces available to user contexts */
+
+	/* Structure used for instrumentation and HW counters dumping */
+	struct {
+		/* The lock should be used when accessing any of the following members */
+		spinlock_t lock;
+
+		kbase_context *kctx;
+		u64 addr;
+		wait_queue_head_t wait;
+		int triggered;
+		kbase_instr_state state;
+		wait_queue_head_t   cache_clean_wait;
+		struct workqueue_struct *cache_clean_wq;
+		struct work_struct  cache_clean_work;
+
+		kbase_context *suspended_kctx;
+		kbase_uk_hwcnt_setup suspended_state;
+	} hwcnt;
+
+	/* Set when we're about to reset the GPU */
+	atomic_t reset_gpu;
+#define KBASE_RESET_GPU_NOT_PENDING     0	/* The GPU reset isn't pending */
+#define KBASE_RESET_GPU_PREPARED        1	/* kbase_prepare_to_reset_gpu has been called */
+#define KBASE_RESET_GPU_COMMITTED       2	/* kbase_reset_gpu has been called - the reset will now definitely happen
+						 * within the timeout period */
+#define KBASE_RESET_GPU_HAPPENING       3	/* The GPU reset process is currently occuring (timeout has expired or
+						 * kbasep_try_reset_gpu_early was called) */
+
+	/* Work queue and work item for performing the reset in */
+	struct workqueue_struct *reset_workq;
+	struct work_struct reset_work;
+	wait_queue_head_t reset_wait;
+	struct hrtimer reset_timer;
+
+	/*value to be written to the irq_throttle register each time an irq is served */
+	atomic_t irq_throttle_cycles;
+
+	const kbase_attribute *config_attributes;
+
+#if KBASE_TRACE_ENABLE != 0
+	spinlock_t              trace_lock;
+	u16                     trace_first_out;
+	u16                     trace_next_in;
+	kbase_trace            *trace_rbuf;
+#endif
+
+#if MALI_CUSTOMER_RELEASE == 0
+	/* This is used to override the current job scheduler values for
+	 * KBASE_CONFIG_ATTR_JS_STOP_STOP_TICKS_SS
+	 * KBASE_CONFIG_ATTR_JS_STOP_STOP_TICKS_CL
+	 * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS
+	 * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL
+	 * KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS
+	 * KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS
+	 * KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL
+	 * KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS.
+	 *
+	 * These values are set via the js_timeouts sysfs file.
+	 */
+	u32 js_soft_stop_ticks;
+	u32 js_soft_stop_ticks_cl;
+	u32 js_hard_stop_ticks_ss;
+	u32 js_hard_stop_ticks_cl;
+	u32 js_hard_stop_ticks_nss;
+	u32 js_reset_ticks_ss;
+	u32 js_reset_ticks_cl;
+	u32 js_reset_ticks_nss;
+#endif
+
+	struct mutex cacheclean_lock;
+
+	/* Platform specific private data to be accessed by mali_kbase_config_xxx.c only */
+	void *platform_context;
+
+	/** Count of contexts keeping the GPU powered */
+	atomic_t keep_gpu_powered_count;
+
+	/* List of kbase_contexts created */
+	struct list_head        kctx_list;
+	struct mutex            kctx_list_lock;
+
+#ifdef CONFIG_MALI_MIDGARD_RT_PM
+	struct delayed_work runtime_pm_workqueue;
+#endif
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	kbase_trace_kbdev_timeline timeline;
+#endif
+
+#ifdef CONFIG_DEBUG_FS
+	/* directory for debugfs entries */
+	struct dentry *mali_debugfs_directory;
+	/* debugfs entry for gpu_memory */
+	struct dentry *gpu_memory_dentry;
+	/* debugfs entry for trace */
+	struct dentry *trace_dentry;
+#endif /* CONFIG_DEBUG_FS */
+
+	/* fbdump profiling controls set by gator */
+	u32 kbase_profiling_controls[FBDUMP_CONTROL_MAX];
+
+
+#if MALI_CUSTOMER_RELEASE == 0
+	/* Number of jobs that are run before a job is forced to fail and
+	 * replay. May be KBASEP_FORCE_REPLAY_DISABLED, to disable forced
+	 * failures. */
+	int force_replay_limit;
+	/* Count of jobs between forced failures. Incremented on each job. A
+	 * job is forced to fail once this is greater than or equal to
+	 * force_replay_limit. */
+	int force_replay_count;
+	/* Core requirement for jobs to be failed and replayed. May be zero. */
+	base_jd_core_req force_replay_core_req;
+	/* MALI_TRUE if force_replay_limit should be randomized. The random
+	 * value will be in the range of 1 - KBASEP_FORCE_REPLAY_RANDOM_LIMIT.
+	 */
+	mali_bool force_replay_random;
+#endif
+};
+
+struct kbase_context {
+	kbase_device *kbdev;
+	phys_addr_t pgd;
+	struct list_head event_list;
+	struct mutex event_mutex;
+	mali_bool event_closed;
+	struct workqueue_struct *event_workq;
+
+	u64 mem_attrs;
+
+	atomic_t                setup_complete;
+	atomic_t                setup_in_progress;
+
+	mali_bool keep_gpu_powered;
+
+	u64 *mmu_teardown_pages;
+
+	phys_addr_t aliasing_sink_page;
+
+	struct mutex            reg_lock; /* To be converted to a rwlock? */
+	struct rb_root          reg_rbtree; /* Red-Black tree of GPU regions (live regions) */
+
+	unsigned long    cookies;
+	struct kbase_va_region *pending_regions[BITS_PER_LONG];
+	
+	wait_queue_head_t event_queue;
+	pid_t tgid;
+	pid_t pid;
+
+	kbase_jd_context jctx;
+	atomic_t used_pages;
+	atomic_t         nonmapped_pages;
+
+	kbase_mem_allocator osalloc;
+	kbase_mem_allocator * pgd_allocator;
+
+	struct list_head waiting_soft_jobs;
+#ifdef CONFIG_KDS
+	struct list_head waiting_kds_resource;
+#endif
+	/** This is effectively part of the Run Pool, because it only has a valid
+	 * setting (!=KBASEP_AS_NR_INVALID) whilst the context is scheduled in
+	 *
+	 * The kbasep_js_device_data::runpool_irq::lock must be held whilst accessing
+	 * this.
+	 *
+	 * If the context relating to this as_nr is required, you must use
+	 * kbasep_js_runpool_retain_ctx() to ensure that the context doesn't disappear
+	 * whilst you're using it. Alternatively, just hold the kbasep_js_device_data::runpool_irq::lock
+	 * to ensure the context doesn't disappear (but this has restrictions on what other locks
+	 * you can take whilst doing this) */
+	int as_nr;
+
+	/* NOTE:
+	 *
+	 * Flags are in jctx.sched_info.ctx.flags
+	 * Mutable flags *must* be accessed under jctx.sched_info.ctx.jsctx_mutex
+	 *
+	 * All other flags must be added there */
+	spinlock_t         mm_update_lock;
+	struct mm_struct * process_mm;
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	kbase_trace_kctx_timeline timeline;
+#endif
+};
+
+typedef enum kbase_reg_access_type {
+	REG_READ,
+	REG_WRITE
+} kbase_reg_access_type;
+
+typedef enum kbase_share_attr_bits {
+	/* (1ULL << 8) bit is reserved */
+	SHARE_BOTH_BITS = (2ULL << 8),	/* inner and outer shareable coherency */
+	SHARE_INNER_BITS = (3ULL << 8)	/* inner shareable coherency */
+} kbase_share_attr_bits;
+
+/* Conversion helpers for setting up high resolution timers */
+#define HR_TIMER_DELAY_MSEC(x) (ns_to_ktime((x)*1000000U))
+#define HR_TIMER_DELAY_NSEC(x) (ns_to_ktime(x))
+
+/* Maximum number of loops polling the GPU for a cache flush before we assume it must have completed */
+#define KBASE_CLEAN_CACHE_MAX_LOOPS     100000
+/* Maximum number of loops polling the GPU for an AS flush to complete before we assume the GPU has hung */
+#define KBASE_AS_FLUSH_MAX_LOOPS        100000
+
+/* Return values from kbase_replay_process */
+
+/* Replay job has completed */
+#define MALI_REPLAY_STATUS_COMPLETE  0
+/* Replay job is replaying and will continue once replayed jobs have completed.
+ */
+#define MALI_REPLAY_STATUS_REPLAYING 1
+#define MALI_REPLAY_STATUS_MASK      0xff
+/* Caller must call kbasep_js_try_schedule_head_ctx */
+#define MALI_REPLAY_FLAG_JS_RESCHED  0x100
+
+/* Maximum number of times a job can be replayed */
+#define BASEP_JD_REPLAY_LIMIT 15
+
+#endif				/* _KBASE_DEFS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_device.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_device.c
new file mode 100644
index 0000000..6dfff10
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_device.c
@@ -0,0 +1,774 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_device.c
+ * Base kernel device APIs
+ */
+
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_hw.h>
+
+#include <mali_kbase_profiling_gator_api.h>
+
+/* NOTE: Magic - 0x45435254 (TRCE in ASCII).
+ * Supports tracing feature provided in the base module.
+ * Please keep it in sync with the value of base module.
+ */
+#define TRACE_BUFFER_HEADER_SPECIAL 0x45435254
+
+#if defined(CONFIG_MALI_PLATFORM_VEXPRESS) || defined(CONFIG_MALI_PLATFORM_VEXPRESS_VIRTEX7_40MHZ)
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+extern kbase_attribute config_attributes_hw_issue_8408[];
+#endif				/* CONFIG_MALI_PLATFORM_FAKE */
+#endif				/* CONFIG_MALI_PLATFORM_VEXPRESS || CONFIG_MALI_PLATFORM_VEXPRESS_VIRTEX7_40MHZ */
+
+#if KBASE_TRACE_ENABLE != 0
+STATIC CONST char *kbasep_trace_code_string[] = {
+	/* IMPORTANT: USE OF SPECIAL #INCLUDE OF NON-STANDARD HEADER FILE
+	 * THIS MUST BE USED AT THE START OF THE ARRAY */
+#define KBASE_TRACE_CODE_MAKE_CODE(X) # X
+#include "mali_kbase_trace_defs.h"
+#undef  KBASE_TRACE_CODE_MAKE_CODE
+};
+#endif
+
+#define DEBUG_MESSAGE_SIZE 256
+
+STATIC mali_error kbasep_trace_init(kbase_device *kbdev);
+STATIC void kbasep_trace_term(kbase_device *kbdev);
+STATIC void kbasep_trace_hook_wrapper(void *param);
+#if KBASE_TRACE_ENABLE != 0
+STATIC void kbasep_trace_debugfs_init(kbase_device *kbdev);
+#endif
+
+void kbasep_as_do_poke(struct work_struct *work);
+enum hrtimer_restart kbasep_reset_timer_callback(struct hrtimer *data);
+void kbasep_reset_timeout_worker(struct work_struct *data);
+
+kbase_device *kbase_device_alloc(void)
+{
+	return kzalloc(sizeof(kbase_device), GFP_KERNEL);
+}
+
+mali_error kbase_device_init(kbase_device * const kbdev)
+{
+	int i;			/* i used after the for loop, don't reuse ! */
+
+	spin_lock_init(&kbdev->mmu_mask_change);
+
+	/* Initialize platform specific context */
+	if (MALI_FALSE == kbasep_platform_device_init(kbdev))
+		goto fail;
+
+	/* Ensure we can access the GPU registers */
+	kbase_pm_register_access_enable(kbdev);
+
+	/* Find out GPU properties based on the GPU feature registers */
+	kbase_gpuprops_set(kbdev);
+
+	/* Get the list of workarounds for issues on the current HW (identified by the GPU_ID register) */
+	if (MALI_ERROR_NONE != kbase_hw_set_issues_mask(kbdev)) {
+		kbase_pm_register_access_disable(kbdev);
+		goto free_platform;
+	}
+
+	/* Set the list of features available on the current HW (identified by the GPU_ID register) */
+	kbase_hw_set_features_mask(kbdev);
+
+	kbdev->nr_hw_address_spaces = kbdev->gpu_props.num_address_spaces;
+
+	/* We're done accessing the GPU registers for now. */
+	kbase_pm_register_access_disable(kbdev);
+
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
+		const char format[] = "mali_mmu%d";
+		char name[sizeof(format)];
+		const char poke_format[] = "mali_mmu%d_poker";	/* BASE_HW_ISSUE_8316 */
+		char poke_name[sizeof(poke_format)];	/* BASE_HW_ISSUE_8316 */
+
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316)) {
+			if (0 > snprintf(poke_name, sizeof(poke_name), poke_format, i))
+				goto free_workqs;
+		}
+
+		if (0 > snprintf(name, sizeof(name), format, i))
+			goto free_workqs;
+
+		kbdev->as[i].number = i;
+		kbdev->as[i].fault_addr = 0ULL;
+
+		kbdev->as[i].pf_wq = alloc_workqueue(name, 0, 1);
+		if (NULL == kbdev->as[i].pf_wq)
+			goto free_workqs;
+
+		mutex_init(&kbdev->as[i].transaction_mutex);
+
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316)) {
+			struct hrtimer *poking_timer = &kbdev->as[i].poke_timer;
+
+			kbdev->as[i].poke_wq = alloc_workqueue(poke_name, 0, 1);
+			if (NULL == kbdev->as[i].poke_wq) {
+				destroy_workqueue(kbdev->as[i].pf_wq);
+				goto free_workqs;
+			}
+			KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&kbdev->as[i].poke_work));
+			INIT_WORK(&kbdev->as[i].poke_work, kbasep_as_do_poke);
+
+			hrtimer_init(poking_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+
+			poking_timer->function = kbasep_as_poke_timer_callback;
+
+			kbdev->as[i].poke_refcount = 0;
+			kbdev->as[i].poke_state = 0u;
+		}
+	}
+	/* don't change i after this point */
+
+	spin_lock_init(&kbdev->hwcnt.lock);
+
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_DISABLED;
+	init_waitqueue_head(&kbdev->reset_wait);
+	init_waitqueue_head(&kbdev->hwcnt.wait);
+	init_waitqueue_head(&kbdev->hwcnt.cache_clean_wait);
+	INIT_WORK(&kbdev->hwcnt.cache_clean_work, kbasep_cache_clean_worker);
+	kbdev->hwcnt.triggered = 0;
+
+	kbdev->hwcnt.cache_clean_wq = alloc_workqueue("Mali cache cleaning workqueue",
+	                                              0, 1);
+	if (NULL == kbdev->hwcnt.cache_clean_wq)
+		goto free_workqs;
+
+	kbdev->reset_workq = alloc_workqueue("Mali reset workqueue", 0, 1);
+	if (NULL == kbdev->reset_workq)
+		goto free_cache_clean_workq;
+
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&kbdev->reset_work));
+	INIT_WORK(&kbdev->reset_work, kbasep_reset_timeout_worker);
+
+	hrtimer_init(&kbdev->reset_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	kbdev->reset_timer.function = kbasep_reset_timer_callback;
+
+	if (kbasep_trace_init(kbdev) != MALI_ERROR_NONE)
+		goto free_reset_workq;
+
+	mutex_init(&kbdev->cacheclean_lock);
+	atomic_set(&kbdev->keep_gpu_powered_count, 0);
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+	for (i = 0; i < BASE_JM_SUBMIT_SLOTS; ++i)
+		kbdev->timeline.slot_atoms_submitted[i] = 0;
+
+	for (i = 0; i <= KBASEP_TIMELINE_PM_EVENT_LAST; ++i)
+		atomic_set(&kbdev->timeline.pm_event_uid[i], 0);
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+
+	/* fbdump profiling controls set to 0 - fbdump not enabled until changed by gator */
+	for (i = 0; i < FBDUMP_CONTROL_MAX; i++)
+		kbdev->kbase_profiling_controls[i] = 0;
+
+		kbase_debug_assert_register_hook(&kbasep_trace_hook_wrapper, kbdev);
+
+#if defined(CONFIG_MALI_PLATFORM_VEXPRESS) || defined(CONFIG_MALI_PLATFORM_VEXPRESS_VIRTEX7_40MHZ)
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+	/* BASE_HW_ISSUE_8408 requires a configuration with different timeouts for
+	 * the vexpress platform */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8408))
+		kbdev->config_attributes = config_attributes_hw_issue_8408;
+#endif				/* CONFIG_MALI_PLATFORM_FAKE */
+#endif				/* CONFIG_MALI_PLATFORM_VEXPRESS || CONFIG_MALI_PLATFORM_VEXPRESS_VIRTEX7_40MHZ */
+
+	return MALI_ERROR_NONE;
+
+ free_reset_workq:
+	destroy_workqueue(kbdev->reset_workq);
+ free_cache_clean_workq:
+	destroy_workqueue(kbdev->hwcnt.cache_clean_wq);
+ free_workqs:
+	while (i > 0) {
+		i--;
+		destroy_workqueue(kbdev->as[i].pf_wq);
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316))
+			destroy_workqueue(kbdev->as[i].poke_wq);
+	}
+ free_platform:
+	kbasep_platform_device_term(kbdev);
+ fail:
+	return MALI_ERROR_FUNCTION_FAILED;
+}
+
+void kbase_device_term(kbase_device *kbdev)
+{
+	int i;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+#if KBASE_TRACE_ENABLE != 0
+	kbase_debug_assert_register_hook(NULL, NULL);
+#endif
+
+	kbasep_trace_term(kbdev);
+
+	destroy_workqueue(kbdev->reset_workq);
+	destroy_workqueue(kbdev->hwcnt.cache_clean_wq);
+
+	for (i = 0; i < kbdev->nr_hw_address_spaces; i++) {
+		destroy_workqueue(kbdev->as[i].pf_wq);
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316))
+			destroy_workqueue(kbdev->as[i].poke_wq);
+	}
+
+	kbasep_platform_device_term(kbdev);
+}
+
+void kbase_device_free(kbase_device *kbdev)
+{
+	kfree(kbdev);
+}
+
+void kbase_device_trace_buffer_install(kbase_context *kctx, u32 *tb, size_t size)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(tb);
+
+	/* set up the header */
+	/* magic number in the first 4 bytes */
+	tb[0] = TRACE_BUFFER_HEADER_SPECIAL;
+	/* Store (write offset = 0, wrap counter = 0, transaction active = no)
+	 * write offset 0 means never written.
+	 * Offsets 1 to (wrap_offset - 1) used to store values when trace started
+	 */
+	tb[1] = 0;
+
+	/* install trace buffer */
+	spin_lock_irqsave(&kctx->jctx.tb_lock, flags);
+	kctx->jctx.tb_wrap_offset = size / 8;
+	kctx->jctx.tb = tb;
+	spin_unlock_irqrestore(&kctx->jctx.tb_lock, flags);
+}
+
+void kbase_device_trace_buffer_uninstall(kbase_context *kctx)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kctx);
+	spin_lock_irqsave(&kctx->jctx.tb_lock, flags);
+	kctx->jctx.tb = NULL;
+	kctx->jctx.tb_wrap_offset = 0;
+	spin_unlock_irqrestore(&kctx->jctx.tb_lock, flags);
+}
+
+void kbase_device_trace_register_access(kbase_context *kctx, kbase_reg_access_type type, u16 reg_offset, u32 reg_value)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&kctx->jctx.tb_lock, flags);
+	if (kctx->jctx.tb) {
+		u16 wrap_count;
+		u16 write_offset;
+		u32 *tb = kctx->jctx.tb;
+		u32 header_word;
+
+		header_word = tb[1];
+		KBASE_DEBUG_ASSERT(0 == (header_word & 0x1));
+
+		wrap_count = (header_word >> 1) & 0x7FFF;
+		write_offset = (header_word >> 16) & 0xFFFF;
+
+		/* mark as transaction in progress */
+		tb[1] |= 0x1;
+		mb();
+
+		/* calculate new offset */
+		write_offset++;
+		if (write_offset == kctx->jctx.tb_wrap_offset) {
+			/* wrap */
+			write_offset = 1;
+			wrap_count++;
+			wrap_count &= 0x7FFF;	/* 15bit wrap counter */
+		}
+
+		/* store the trace entry at the selected offset */
+		tb[write_offset * 2 + 0] = (reg_offset & ~0x3) | ((type == REG_WRITE) ? 0x1 : 0x0);
+		tb[write_offset * 2 + 1] = reg_value;
+		mb();
+
+		/* new header word */
+		header_word = (write_offset << 16) | (wrap_count << 1) | 0x0;	/* transaction complete */
+		tb[1] = header_word;
+	}
+	spin_unlock_irqrestore(&kctx->jctx.tb_lock, flags);
+}
+
+void kbase_reg_write(kbase_device *kbdev, u16 offset, u32 value, kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_powered);
+	KBASE_DEBUG_ASSERT(kctx == NULL || kctx->as_nr != KBASEP_AS_NR_INVALID);
+	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
+	dev_dbg(kbdev->dev, "w: reg %04x val %08x", offset, value);
+	kbase_os_reg_write(kbdev, offset, value);
+	if (kctx && kctx->jctx.tb)
+		kbase_device_trace_register_access(kctx, REG_WRITE, offset, value);
+}
+
+KBASE_EXPORT_TEST_API(kbase_reg_write)
+
+u32 kbase_reg_read(kbase_device *kbdev, u16 offset, kbase_context *kctx)
+{
+	u32 val;
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_powered);
+	KBASE_DEBUG_ASSERT(kctx == NULL || kctx->as_nr != KBASEP_AS_NR_INVALID);
+	KBASE_DEBUG_ASSERT(kbdev->dev != NULL);
+	val = kbase_os_reg_read(kbdev, offset);
+	dev_dbg(kbdev->dev, "r: reg %04x val %08x", offset, val);
+	if (kctx && kctx->jctx.tb)
+		kbase_device_trace_register_access(kctx, REG_READ, offset, val);
+	return val;
+}
+
+KBASE_EXPORT_TEST_API(kbase_reg_read)
+
+void kbase_report_gpu_fault(kbase_device *kbdev, int multiple)
+{
+	u32 status;
+	u64 address;
+
+	status = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTSTATUS), NULL);
+	address = (u64) kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTADDRESS_HI), NULL) << 32;
+	address |= kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTADDRESS_LO), NULL);
+
+	dev_warn(kbdev->dev, "GPU Fault 0x%08x (%s) at 0x%016llx", status & 0xFF, kbase_exception_name(status), address);
+	if (multiple)
+		dev_warn(kbdev->dev, "There were multiple GPU faults - some have not been reported\n");
+}
+
+void kbase_gpu_interrupt(kbase_device *kbdev, u32 val)
+{
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_IRQ, NULL, NULL, 0u, val);
+	if (val & GPU_FAULT)
+		kbase_report_gpu_fault(kbdev, val & MULTIPLE_GPU_FAULTS);
+
+	if (val & RESET_COMPLETED)
+		kbase_pm_reset_done(kbdev);
+
+	if (val & PRFCNT_SAMPLE_COMPLETED)
+		kbase_instr_hwcnt_sample_done(kbdev);
+
+	if (val & CLEAN_CACHES_COMPLETED)
+		kbase_clean_caches_done(kbdev);
+
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, NULL, 0u, val);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), val, NULL);
+
+	/* kbase_pm_check_transitions must be called after the IRQ has been cleared. This is because it might trigger
+	 * further power transitions and we don't want to miss the interrupt raised to notify us that these further
+	 * transitions have finished.
+	 */
+	if (val & POWER_CHANGED_ALL) {
+		mali_bool cores_are_available;
+		unsigned long flags;
+
+		KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_GPU_INTERRUPT_START);
+		spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+		cores_are_available = kbase_pm_check_transitions_nolock(kbdev);
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+		KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_GPU_INTERRUPT_END);
+
+		if (cores_are_available) {
+			/* Fast-path Job Scheduling on PM IRQ */
+			int js;
+			/* Log timelining information that a change in state has completed */
+			kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+
+			spin_lock_irqsave(&kbdev->js_data.runpool_irq.lock, flags);
+			/* A simplified check to ensure the last context hasn't exited
+			 * after dropping the PM lock whilst doing a PM IRQ: any bits set
+			 * in 'submit_allowed' indicate that we have a context in the
+			 * runpool (which can't leave whilst we hold this lock). It is
+			 * sometimes zero even when we have a context in the runpool, but
+			 * that's no problem because we'll be unable to submit jobs
+			 * anyway */
+			if (kbdev->js_data.runpool_irq.submit_allowed)
+				for (js = 0; js < kbdev->gpu_props.num_job_slots; ++js) {
+					mali_bool needs_retry;
+					s8 submitted_count = 0;
+					needs_retry = kbasep_js_try_run_next_job_on_slot_irq_nolock(kbdev, js, &submitted_count);
+					/* Don't need to retry outside of IRQ context - this can
+					 * only happen if we submitted too many in one IRQ, such
+					 * that they were completing faster than we could
+					 * submit. In this case, a job IRQ will fire to cause more
+					 * work to be submitted in some way */
+					CSTD_UNUSED(needs_retry);
+				}
+			spin_unlock_irqrestore(&kbdev->js_data.runpool_irq.lock, flags);
+		}
+	}
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_IRQ_DONE, NULL, NULL, 0u, val);
+}
+
+/*
+ * Device trace functions
+ */
+#if KBASE_TRACE_ENABLE != 0
+
+STATIC mali_error kbasep_trace_init(kbase_device *kbdev)
+{
+	void *rbuf;
+
+	rbuf = kmalloc(sizeof(kbase_trace) * KBASE_TRACE_SIZE, GFP_KERNEL);
+
+	if (!rbuf)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	kbdev->trace_rbuf = rbuf;
+	spin_lock_init(&kbdev->trace_lock);
+	kbasep_trace_debugfs_init(kbdev);
+	return MALI_ERROR_NONE;
+}
+
+STATIC void kbasep_trace_term(kbase_device *kbdev)
+{
+	debugfs_remove(kbdev->trace_dentry);
+	kbdev->trace_dentry= NULL;
+	kfree(kbdev->trace_rbuf);
+}
+
+void kbasep_trace_format_msg(kbase_trace *trace_msg, char *buffer, int len)
+{
+	s32 written = 0;
+
+	/* Initial part of message */
+	written += MAX(snprintf(buffer + written, MAX(len - written, 0), "%d.%.6d,%d,%d,%s,%p,", (int)trace_msg->timestamp.tv_sec, (int)(trace_msg->timestamp.tv_nsec / 1000), trace_msg->thread_id, trace_msg->cpu, kbasep_trace_code_string[trace_msg->code], trace_msg->ctx), 0);
+
+	if (trace_msg->katom != MALI_FALSE) {
+		written += MAX(snprintf(buffer + written, MAX(len - written, 0), "atom %d (ud: 0x%llx 0x%llx)", trace_msg->atom_number, trace_msg->atom_udata[0], trace_msg->atom_udata[1]), 0);
+	}
+
+	written += MAX(snprintf(buffer + written, MAX(len - written, 0), ",%.8llx,", trace_msg->gpu_addr), 0);
+
+	/* NOTE: Could add function callbacks to handle different message types */
+	/* Jobslot present */
+	if ((trace_msg->flags & KBASE_TRACE_FLAG_JOBSLOT) != MALI_FALSE)
+		written += MAX(snprintf(buffer + written, MAX(len - written, 0), "%d", trace_msg->jobslot), 0);
+
+	written += MAX(snprintf(buffer + written, MAX(len - written, 0), ","), 0);
+
+	/* Refcount present */
+	if ((trace_msg->flags & KBASE_TRACE_FLAG_REFCOUNT) != MALI_FALSE)
+		written += MAX(snprintf(buffer + written, MAX(len - written, 0), "%d", trace_msg->refcount), 0);
+
+	written += MAX(snprintf(buffer + written, MAX(len - written, 0), ","), 0);
+
+	/* Rest of message */
+	written += MAX(snprintf(buffer + written, MAX(len - written, 0), "0x%.8lx", trace_msg->info_val), 0);
+
+}
+
+void kbasep_trace_dump_msg(kbase_device *kbdev, kbase_trace *trace_msg)
+{
+	char buffer[DEBUG_MESSAGE_SIZE];
+
+	kbasep_trace_format_msg(trace_msg, buffer, DEBUG_MESSAGE_SIZE);
+	dev_dbg(kbdev->dev, "%s", buffer);
+}
+
+void kbasep_trace_add(kbase_device *kbdev, kbase_trace_code code, void *ctx, kbase_jd_atom *katom, u64 gpu_addr, u8 flags, int refcount, int jobslot, unsigned long info_val)
+{
+	unsigned long irqflags;
+	kbase_trace *trace_msg;
+
+	spin_lock_irqsave(&kbdev->trace_lock, irqflags);
+
+	trace_msg = &kbdev->trace_rbuf[kbdev->trace_next_in];
+
+	/* Fill the message */
+	trace_msg->thread_id = task_pid_nr(current);
+	trace_msg->cpu = task_cpu(current);
+
+	getnstimeofday(&trace_msg->timestamp);
+
+	trace_msg->code = code;
+	trace_msg->ctx = ctx;
+
+	if (NULL == katom) {
+		trace_msg->katom = MALI_FALSE;
+	} else {
+		trace_msg->katom = MALI_TRUE;
+		trace_msg->atom_number = kbase_jd_atom_id(katom->kctx, katom);
+		trace_msg->atom_udata[0] = katom->udata.blob[0];
+		trace_msg->atom_udata[1] = katom->udata.blob[1];
+	}
+
+	trace_msg->gpu_addr = gpu_addr;
+	trace_msg->jobslot = jobslot;
+	trace_msg->refcount = MIN((unsigned int)refcount, 0xFF);
+	trace_msg->info_val = info_val;
+	trace_msg->flags = flags;
+
+	/* Update the ringbuffer indices */
+	kbdev->trace_next_in = (kbdev->trace_next_in + 1) & KBASE_TRACE_MASK;
+	if (kbdev->trace_next_in == kbdev->trace_first_out)
+		kbdev->trace_first_out = (kbdev->trace_first_out + 1) & KBASE_TRACE_MASK;
+
+	/* Done */
+
+	spin_unlock_irqrestore(&kbdev->trace_lock, irqflags);
+}
+
+void kbasep_trace_clear(kbase_device *kbdev)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&kbdev->trace_lock, flags);
+	kbdev->trace_first_out = kbdev->trace_next_in;
+	spin_unlock_irqrestore(&kbdev->trace_lock, flags);
+}
+
+void kbasep_trace_dump(kbase_device *kbdev)
+{
+	unsigned long flags;
+	u32 start;
+	u32 end;
+
+	dev_dbg(kbdev->dev, "Dumping trace:\nsecs,nthread,cpu,code,ctx,katom,gpu_addr,jobslot,refcount,info_val");
+	spin_lock_irqsave(&kbdev->trace_lock, flags);
+	start = kbdev->trace_first_out;
+	end = kbdev->trace_next_in;
+
+	while (start != end) {
+		kbase_trace *trace_msg = &kbdev->trace_rbuf[start];
+		kbasep_trace_dump_msg(kbdev, trace_msg);
+
+		start = (start + 1) & KBASE_TRACE_MASK;
+	}
+	dev_dbg(kbdev->dev, "TRACE_END");
+
+	spin_unlock_irqrestore(&kbdev->trace_lock, flags);
+
+	KBASE_TRACE_CLEAR(kbdev);
+}
+
+STATIC void kbasep_trace_hook_wrapper(void *param)
+{
+	kbase_device *kbdev = (kbase_device *) param;
+	kbasep_trace_dump(kbdev);
+}
+
+#ifdef CONFIG_DEBUG_FS
+struct trace_seq_state {
+	kbase_trace trace_buf[KBASE_TRACE_SIZE];
+	u32 start;
+	u32 end;
+};
+
+void *kbasep_trace_seq_start(struct seq_file *s, loff_t *pos)
+{
+	struct trace_seq_state *state = s->private;
+	int i;
+
+	if (*pos > KBASE_TRACE_SIZE)
+		return NULL;
+	i = state->start + *pos;
+	if ((state->end >= state->start && i >= state->end) ||
+			i >= state->end + KBASE_TRACE_SIZE)
+		return NULL;
+
+	i &= KBASE_TRACE_MASK;
+
+	return &state->trace_buf[i];
+}
+
+void kbasep_trace_seq_stop(struct seq_file *s, void *data)
+{
+}
+
+void *kbasep_trace_seq_next(struct seq_file *s, void *data, loff_t *pos)
+{
+	struct trace_seq_state *state = s->private;
+	int i;
+
+	(*pos)++;
+
+	i = (state->start + *pos) & KBASE_TRACE_MASK;
+	if (i == state->end)
+		return NULL;
+
+	return &state->trace_buf[i];
+}
+
+int kbasep_trace_seq_show(struct seq_file *s, void *data)
+{
+	kbase_trace *trace_msg = data;
+	char buffer[DEBUG_MESSAGE_SIZE];
+
+	kbasep_trace_format_msg(trace_msg, buffer, DEBUG_MESSAGE_SIZE);
+	seq_printf(s, "%s\n", buffer);
+	return 0;
+}
+
+static const struct seq_operations kbasep_trace_seq_ops = {
+	.start = kbasep_trace_seq_start,
+	.next = kbasep_trace_seq_next,
+	.stop = kbasep_trace_seq_stop,
+	.show = kbasep_trace_seq_show,
+};
+
+static int kbasep_trace_debugfs_open(struct inode *inode, struct file *file)
+{
+	kbase_device *kbdev = inode->i_private;
+	unsigned long flags;
+
+	struct trace_seq_state *state;
+
+	state = __seq_open_private(file, &kbasep_trace_seq_ops, sizeof(*state));
+	if (!state)
+		return -ENOMEM;
+
+	spin_lock_irqsave(&kbdev->trace_lock, flags);
+	state->start = kbdev->trace_first_out;
+	state->end = kbdev->trace_next_in;
+	memcpy(state->trace_buf, kbdev->trace_rbuf, sizeof(state->trace_buf));
+	spin_unlock_irqrestore(&kbdev->trace_lock, flags);
+
+	return 0;
+}
+
+static const struct file_operations kbasep_trace_debugfs_fops = {
+	.open = kbasep_trace_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release_private,
+};
+
+STATIC void kbasep_trace_debugfs_init(kbase_device *kbdev)
+{
+	kbdev->trace_dentry = debugfs_create_file("mali_trace", S_IRUGO,
+			kbdev->mali_debugfs_directory, kbdev,
+			&kbasep_trace_debugfs_fops);
+}
+#else
+STATIC void kbasep_trace_debugfs_init(kbase_device *kbdev)
+{
+
+}
+#endif				/* CONFIG_DEBUG_FS */
+
+#else				/* KBASE_TRACE_ENABLE != 0 */
+STATIC mali_error kbasep_trace_init(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+	return MALI_ERROR_NONE;
+}
+
+STATIC void kbasep_trace_term(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+STATIC void kbasep_trace_hook_wrapper(void *param)
+{
+	CSTD_UNUSED(param);
+}
+
+void kbasep_trace_add(kbase_device *kbdev, kbase_trace_code code, void *ctx, kbase_jd_atom *katom, u64 gpu_addr, u8 flags, int refcount, int jobslot, unsigned long info_val)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(code);
+	CSTD_UNUSED(ctx);
+	CSTD_UNUSED(katom);
+	CSTD_UNUSED(gpu_addr);
+	CSTD_UNUSED(flags);
+	CSTD_UNUSED(refcount);
+	CSTD_UNUSED(jobslot);
+	CSTD_UNUSED(info_val);
+}
+
+void kbasep_trace_clear(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+void kbasep_trace_dump(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+#endif				/* KBASE_TRACE_ENABLE != 0 */
+
+void kbase_set_profiling_control(struct kbase_device *kbdev, u32 control, u32 value)
+{
+	switch (control) {
+	case FBDUMP_CONTROL_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RATE:
+		/* fall through */
+	case SW_COUNTER_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RESIZE_FACTOR:
+		kbdev->kbase_profiling_controls[control] = value;
+		break;
+	default:
+		dev_err(kbdev->dev, "Profiling control %d not found\n", control);
+		break;
+	}
+}
+
+u32 kbase_get_profiling_control(struct kbase_device *kbdev, u32 control)
+{
+	u32 ret_value = 0;
+
+	switch (control) {
+	case FBDUMP_CONTROL_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RATE:
+		/* fall through */
+	case SW_COUNTER_ENABLE:
+		/* fall through */
+	case FBDUMP_CONTROL_RESIZE_FACTOR:
+		ret_value = kbdev->kbase_profiling_controls[control];
+		break;
+	default:
+		dev_err(kbdev->dev, "Profiling control %d not found\n", control);
+		break;
+	}
+
+	return ret_value;
+}
+
+/*
+ * Called by gator to control the production of
+ * profiling information at runtime
+ * */
+
+void _mali_profiling_control(u32 action, u32 value)
+{
+	struct kbase_device *kbdev = NULL;
+
+	/* find the first i.e. call with -1 */
+	kbdev = kbase_find_device(-1);
+
+	if (NULL != kbdev) {
+		kbase_set_profiling_control(kbdev, action, value);
+	}
+}
+
+KBASE_EXPORT_SYMBOL(_mali_profiling_control);
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_event.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_event.c
new file mode 100644
index 0000000..1e04869
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_event.c
@@ -0,0 +1,185 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+#include <mali_kbase_debug.h>
+
+STATIC base_jd_udata kbase_event_process(kbase_context *kctx, kbase_jd_atom *katom)
+{
+	base_jd_udata data;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	KBASE_DEBUG_ASSERT(katom->status == KBASE_JD_ATOM_STATE_COMPLETED);
+
+	data = katom->udata;
+
+	KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_sub_return(1, &kctx->timeline.jd_atoms_in_flight));
+
+	mutex_lock(&kctx->jctx.lock);
+	katom->status = KBASE_JD_ATOM_STATE_UNUSED;
+	mutex_unlock(&kctx->jctx.lock);
+
+	wake_up(&katom->completed);
+
+	return data;
+}
+
+int kbase_event_pending(kbase_context *ctx)
+{
+	int ret;
+
+	KBASE_DEBUG_ASSERT(ctx);
+
+	mutex_lock(&ctx->event_mutex);
+	ret = (!list_empty(&ctx->event_list)) || (MALI_TRUE == ctx->event_closed);
+	mutex_unlock(&ctx->event_mutex);
+
+	return ret;
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_pending)
+
+int kbase_event_dequeue(kbase_context *ctx, base_jd_event_v2 *uevent)
+{
+	kbase_jd_atom *atom;
+
+	KBASE_DEBUG_ASSERT(ctx);
+
+	mutex_lock(&ctx->event_mutex);
+
+	if (list_empty(&ctx->event_list)) {
+		if (ctx->event_closed) {
+			/* generate the BASE_JD_EVENT_DRV_TERMINATED message on the fly */
+			mutex_unlock(&ctx->event_mutex);
+			uevent->event_code = BASE_JD_EVENT_DRV_TERMINATED;
+			memset(&uevent->udata, 0, sizeof(uevent->udata));
+			dev_dbg(ctx->kbdev->dev,
+				"event system closed, returning BASE_JD_EVENT_DRV_TERMINATED(0x%X)\n",
+				BASE_JD_EVENT_DRV_TERMINATED);
+			return 0;
+		} else {
+			mutex_unlock(&ctx->event_mutex);
+			return -1;
+		}
+	}
+
+	/* normal event processing */
+	atom = list_entry(ctx->event_list.next, kbase_jd_atom, dep_item[0]);
+	list_del(ctx->event_list.next);
+
+	mutex_unlock(&ctx->event_mutex);
+
+	dev_dbg(ctx->kbdev->dev, "event dequeuing %p\n", (void *)atom);
+	uevent->event_code = atom->event_code;
+	uevent->atom_number = (atom - ctx->jctx.atoms);
+	uevent->udata = kbase_event_process(ctx, atom);
+
+	return 0;
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_dequeue)
+
+static void kbase_event_post_worker(struct work_struct *data)
+{
+	kbase_jd_atom *atom = CONTAINER_OF(data, kbase_jd_atom, work);
+	kbase_context *ctx = atom->kctx;
+
+	if (atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES)
+		kbase_jd_free_external_resources(atom);
+
+	if (atom->core_req & BASE_JD_REQ_EVENT_ONLY_ON_FAILURE) {
+		if (atom->event_code == BASE_JD_EVENT_DONE) {
+			/* Don't report the event */
+			kbase_event_process(ctx, atom);
+			return;
+		}
+	}
+
+	if (atom->core_req & BASEP_JD_REQ_EVENT_NEVER) {
+		/* Don't report the event */
+		kbase_event_process(ctx, atom);
+		return;
+	}
+
+	mutex_lock(&ctx->event_mutex);
+	list_add_tail(&atom->dep_item[0], &ctx->event_list);
+	mutex_unlock(&ctx->event_mutex);
+
+	kbase_event_wakeup(ctx);
+}
+
+void kbase_event_post(kbase_context *ctx, kbase_jd_atom *atom)
+{
+	KBASE_DEBUG_ASSERT(ctx);
+	KBASE_DEBUG_ASSERT(ctx->event_workq);
+	KBASE_DEBUG_ASSERT(atom);
+
+	INIT_WORK(&atom->work, kbase_event_post_worker);
+	queue_work(ctx->event_workq, &atom->work);
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_post)
+
+void kbase_event_close(kbase_context *kctx)
+{
+	mutex_lock(&kctx->event_mutex);
+	kctx->event_closed = MALI_TRUE;
+	mutex_unlock(&kctx->event_mutex);
+	kbase_event_wakeup(kctx);
+}
+
+mali_error kbase_event_init(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx);
+
+	INIT_LIST_HEAD(&kctx->event_list);
+	mutex_init(&kctx->event_mutex);
+	kctx->event_closed = MALI_FALSE;
+	kctx->event_workq = alloc_workqueue("kbase_event", WQ_MEM_RECLAIM, 1);
+
+	if (NULL == kctx->event_workq)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_init)
+
+void kbase_event_cleanup(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(kctx->event_workq);
+
+	flush_workqueue(kctx->event_workq);
+	destroy_workqueue(kctx->event_workq);
+
+	/* We use kbase_event_dequeue to remove the remaining events as that
+	 * deals with all the cleanup needed for the atoms.
+	 *
+	 * Note: use of kctx->event_list without a lock is safe because this must be the last
+	 * thread using it (because we're about to terminate the lock)
+	 */
+	while (!list_empty(&kctx->event_list)) {
+		base_jd_event_v2 event;
+		kbase_event_dequeue(kctx, &event);
+	}
+}
+
+KBASE_EXPORT_TEST_API(kbase_event_cleanup)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gator.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gator.h
new file mode 100644
index 0000000..3c4ffed
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gator.h
@@ -0,0 +1,44 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/* NB taken from gator  */
+/*
+ * List of possible actions to be controlled by Streamline.
+ * The following numbers are used by gator to control the frame buffer dumping and s/w counter reporting.
+ * We cannot use the enums in mali_uk_types.h because they are unknown inside gator.
+ */
+#ifndef _KBASE_GATOR_H_
+#define _KBASE_GATOR_H_
+
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+#define GATOR_MAKE_EVENT(type, number) (((type) << 24) | ((number) << 16))
+#define GATOR_JOB_SLOT_START 1
+#define GATOR_JOB_SLOT_STOP  2
+#define GATOR_JOB_SLOT_SOFT_STOPPED  3
+
+void kbase_trace_mali_job_slots_event(u32 event, const kbase_context *kctx, u8 atom_id);
+void kbase_trace_mali_pm_status(u32 event, u64 value);
+void kbase_trace_mali_pm_power_off(u32 event, u64 value);
+void kbase_trace_mali_pm_power_on(u32 event, u64 value);
+void kbase_trace_mali_page_fault_insert_pages(int event, u32 value);
+void kbase_trace_mali_mmu_as_in_use(int event);
+void kbase_trace_mali_mmu_as_released(int event);
+void kbase_trace_mali_total_alloc_pages_change(long long int event);
+
+#endif /* CONFIG_MALI_GATOR_SUPPORT */
+
+#endif  /* _KBASE_GATOR_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.c
new file mode 100644
index 0000000..e2948b1
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.c
@@ -0,0 +1,100 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <mali_kbase_gpu_memory_debugfs.h>
+
+/** Show callback for the @c gpu_memory debugfs file.
+ *
+ * This function is called to get the contents of the @c gpu_memory debugfs
+ * file. This is a report of current gpu memory usage.
+ *
+ * @param sfile The debugfs entry
+ * @param data Data associated with the entry
+ *
+ * @return 0 if successfully prints data in debugfs entry file
+ *         -1 if it encountered an error
+ */
+
+static int kbasep_gpu_memory_seq_show(struct seq_file *sfile, void *data)
+{
+	ssize_t ret = 0;
+	struct list_head *entry;
+	const struct list_head *kbdev_list;
+	kbdev_list = kbase_dev_list_get();
+	list_for_each(entry, kbdev_list) {
+		struct kbase_device *kbdev = NULL;
+		kbasep_kctx_list_element *element;
+
+		kbdev = list_entry(entry, struct kbase_device, entry);
+		/* output the total memory usage and cap for this device */
+		ret = seq_printf(sfile, "%-16s  %10u\n", \
+				kbdev->devname, \
+				atomic_read(&(kbdev->memdev.used_pages)));
+		mutex_lock(&kbdev->kctx_list_lock);
+		list_for_each_entry(element, &kbdev->kctx_list, link) {
+			/* output the memory usage and cap for each kctx
+			* opened on this device */
+			ret = seq_printf(sfile, "  %s-0x%p %10u\n", \
+				"kctx",
+				element->kctx, \
+				atomic_read(&(element->kctx->used_pages)));
+		}
+		mutex_unlock(&kbdev->kctx_list_lock);
+	}
+	kbase_dev_list_put(kbdev_list);
+	return ret;
+}
+
+/*
+ *  File operations related to debugfs entry for gpu_memory
+ */
+STATIC int kbasep_gpu_memory_debugfs_open(struct inode *in, struct file *file)
+{
+	return single_open(file, kbasep_gpu_memory_seq_show , NULL);
+}
+
+static const struct file_operations kbasep_gpu_memory_debugfs_fops = {
+	.open = kbasep_gpu_memory_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release_private,
+};
+
+/*
+ *  Initialize debugfs entry for gpu_memory
+ */
+mali_error kbasep_gpu_memory_debugfs_init(kbase_device *kbdev)
+{
+	kbdev->gpu_memory_dentry = debugfs_create_file("gpu_memory", \
+					S_IRUGO, \
+					kbdev->mali_debugfs_directory, \
+					NULL, \
+					&kbasep_gpu_memory_debugfs_fops);
+	if (IS_ERR(kbdev->gpu_memory_dentry))
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return MALI_ERROR_NONE;
+}
+
+/*
+ *  Terminate debugfs entry for gpu_memory
+ */
+void kbasep_gpu_memory_debugfs_term(kbase_device *kbdev)
+{
+	debugfs_remove(kbdev->gpu_memory_dentry);
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.h
new file mode 100644
index 0000000..b53a9ff
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpu_memory_debugfs.h
@@ -0,0 +1,43 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_gpu_memory_debugfs.h
+ * Header file for gpu_memory entry in debugfs
+ *
+ */
+
+#ifndef _KBASE_GPU_MEMORY_H
+#define _KBASE_GPU_MEMORY_H
+
+#include <mali_kbase.h>
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+/**
+ * @brief Initialize gpu_memory debugfs entry
+ */
+mali_error kbasep_gpu_memory_debugfs_init(kbase_device *kbdev);
+
+/**
+ * @brief Terminate gpu_memory debugfs entry
+ */
+void kbasep_gpu_memory_debugfs_term(kbase_device *kbdev);
+
+#endif  /*_KBASE_GPU_MEMORY_H*/
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.c
new file mode 100644
index 0000000..0fbefea
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.c
@@ -0,0 +1,336 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_gpuprops.c
+ * Base kernel property query APIs
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gpuprops.h>
+
+/**
+ * @brief Extracts bits from a 32-bit bitfield.
+ * @hideinitializer
+ *
+ * @param[in]    value       The value from which to extract bits.
+ * @param[in]    offset      The first bit to extract (0 being the LSB).
+ * @param[in]    size        The number of bits to extract.
+ * @return                   Bits [@a offset, @a offset + @a size) from @a value.
+ *
+ * @pre offset + size <= 32.
+ */
+/* from mali_cdsb.h */
+#define KBASE_UBFX32(value, offset, size) \
+	(((u32)(value) >> (u32)(offset)) & (u32)((1ULL << (u32)(size)) - 1))
+
+mali_error kbase_gpuprops_uk_get_props(kbase_context *kctx, kbase_uk_gpuprops * const kbase_props)
+{
+	kbase_gpuprops_clock_speed_function get_gpu_speed_mhz;
+	u32 gpu_speed_mhz;
+	int rc = 1;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != kbase_props);
+
+	/* Current GPU speed is requested from the system integrator via the KBASE_CONFIG_ATTR_GPU_SPEED_FUNC function.
+	 * If that function fails, or the function is not provided by the system integrator, we report the maximum
+	 * GPU speed as specified by KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX.
+	 */
+	get_gpu_speed_mhz = (kbase_gpuprops_clock_speed_function) kbasep_get_config_value(kctx->kbdev, kctx->kbdev->config_attributes, KBASE_CONFIG_ATTR_GPU_SPEED_FUNC);
+	if (get_gpu_speed_mhz != NULL) {
+		rc = get_gpu_speed_mhz(&gpu_speed_mhz);
+#ifdef CONFIG_MALI_DEBUG
+		/* Issue a warning message when the reported GPU speed falls outside the min/max range */
+		if (rc == 0) {
+			u32 gpu_speed_khz = gpu_speed_mhz * 1000;
+			if (gpu_speed_khz < kctx->kbdev->gpu_props.props.core_props.gpu_freq_khz_min || gpu_speed_khz > kctx->kbdev->gpu_props.props.core_props.gpu_freq_khz_max)
+				dev_warn(kctx->kbdev->dev, "GPU Speed is outside of min/max range (got %lu Khz, min %lu Khz, max %lu Khz)\n", (unsigned long)gpu_speed_khz, (unsigned long)kctx->kbdev->gpu_props.props.core_props.gpu_freq_khz_min, (unsigned long)kctx->kbdev->gpu_props.props.core_props.gpu_freq_khz_max);
+		}
+#endif				/* CONFIG_MALI_DEBUG */
+	}
+	if (rc != 0)
+		gpu_speed_mhz = kctx->kbdev->gpu_props.props.core_props.gpu_freq_khz_max / 1000;
+
+	kctx->kbdev->gpu_props.props.core_props.gpu_speed_mhz = gpu_speed_mhz;
+
+	memcpy(&kbase_props->props, &kctx->kbdev->gpu_props.props, sizeof(kbase_props->props));
+
+	return MALI_ERROR_NONE;
+}
+
+STATIC void kbase_gpuprops_dump_registers(kbase_device *kbdev, kbase_gpuprops_regdump *regdump)
+{
+	int i;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	KBASE_DEBUG_ASSERT(NULL != regdump);
+
+	/* Fill regdump with the content of the relevant registers */
+	regdump->gpu_id = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(GPU_ID));
+
+	regdump->l2_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L2_FEATURES));
+	regdump->l3_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L3_FEATURES));
+	regdump->tiler_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(TILER_FEATURES));
+	regdump->mem_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(MEM_FEATURES));
+	regdump->mmu_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(MMU_FEATURES));
+	regdump->as_present = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(AS_PRESENT));
+	regdump->js_present = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(JS_PRESENT));
+
+	for (i = 0; i < MIDG_MAX_JOB_SLOTS; i++)
+		regdump->js_features[i] = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(JS_FEATURES_REG(i)));
+
+	for (i = 0; i < BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS; i++)
+		regdump->texture_features[i] = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(TEXTURE_FEATURES_REG(i)));
+
+	regdump->thread_max_threads = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(THREAD_MAX_THREADS));
+	regdump->thread_max_workgroup_size = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(THREAD_MAX_WORKGROUP_SIZE));
+	regdump->thread_max_barrier_size = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(THREAD_MAX_BARRIER_SIZE));
+	regdump->thread_features = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(THREAD_FEATURES));
+
+	regdump->shader_present_lo = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(SHADER_PRESENT_LO));
+	regdump->shader_present_hi = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(SHADER_PRESENT_HI));
+
+	regdump->tiler_present_lo = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(TILER_PRESENT_LO));
+	regdump->tiler_present_hi = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(TILER_PRESENT_HI));
+
+	regdump->l2_present_lo = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L2_PRESENT_LO));
+	regdump->l2_present_hi = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L2_PRESENT_HI));
+
+	regdump->l3_present_lo = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L3_PRESENT_LO));
+	regdump->l3_present_hi = kbase_os_reg_read(kbdev, GPU_CONTROL_REG(L3_PRESENT_HI));
+}
+
+STATIC void kbase_gpuprops_construct_coherent_groups(base_gpu_props * const props)
+{
+	struct mali_base_gpu_coherent_group *current_group;
+	u64 group_present;
+	u64 group_mask;
+	u64 first_set, first_set_prev;
+	u32 num_groups = 0;
+
+	KBASE_DEBUG_ASSERT(NULL != props);
+
+	props->coherency_info.coherency = props->raw_props.mem_features;
+	props->coherency_info.num_core_groups = hweight64(props->raw_props.l2_present);
+
+	if (props->coherency_info.coherency & GROUPS_L3_COHERENT) {
+		/* Group is l3 coherent */
+		group_present = props->raw_props.l3_present;
+	} else if (props->coherency_info.coherency & GROUPS_L2_COHERENT) {
+		/* Group is l2 coherent */
+		group_present = props->raw_props.l2_present;
+	} else {
+		/* Group is l1 coherent */
+		group_present = props->raw_props.shader_present;
+	}
+
+	/*
+	 * The coherent group mask can be computed from the l2/l3 present
+	 * register.
+	 *
+	 * For the coherent group n:
+	 * group_mask[n] = (first_set[n] - 1) & ~(first_set[n-1] - 1)
+	 * where first_set is group_present with only its nth set-bit kept
+	 * (i.e. the position from where a new group starts).
+	 *
+	 * For instance if the groups are l2 coherent and l2_present=0x0..01111:
+	 * The first mask is:
+	 * group_mask[1] = (first_set[1] - 1) & ~(first_set[0] - 1)
+	 *               = (0x0..010     - 1) & ~(0x0..01      - 1)
+	 *               =  0x0..00f
+	 * The second mask is:
+	 * group_mask[2] = (first_set[2] - 1) & ~(first_set[1] - 1)
+	 *               = (0x0..100     - 1) & ~(0x0..010     - 1)
+	 *               =  0x0..0f0
+	 * And so on until all the bits from group_present have been cleared
+	 * (i.e. there is no group left).
+	 */
+
+	current_group = props->coherency_info.group;
+	first_set = group_present & ~(group_present - 1);
+
+	while (group_present != 0 && num_groups < BASE_MAX_COHERENT_GROUPS) {
+		group_present -= first_set;	/* Clear the current group bit */
+		first_set_prev = first_set;
+
+		first_set = group_present & ~(group_present - 1);
+		group_mask = (first_set - 1) & ~(first_set_prev - 1);
+
+		/* Populate the coherent_group structure for each group */
+		current_group->core_mask = group_mask & props->raw_props.shader_present;
+		current_group->num_cores = hweight64(current_group->core_mask);
+
+		num_groups++;
+		current_group++;
+	}
+
+	if (group_present != 0)
+		pr_warn("Too many coherent groups (keeping only %d groups).\n", BASE_MAX_COHERENT_GROUPS);
+
+	props->coherency_info.num_groups = num_groups;
+}
+
+/**
+ * @brief Get the GPU configuration
+ *
+ * Fill the base_gpu_props structure with values from the GPU configuration registers.
+ * Only the raw properties are filled in this function
+ *
+ * @param gpu_props  The base_gpu_props structure
+ * @param kbdev      The kbase_device structure for the device
+ */
+static void kbase_gpuprops_get_props(base_gpu_props * const gpu_props, kbase_device *kbdev)
+{
+	kbase_gpuprops_regdump regdump;
+	int i;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	KBASE_DEBUG_ASSERT(NULL != gpu_props);
+
+	/* Dump relevant registers */
+	kbase_gpuprops_dump_registers(kbdev, &regdump);
+
+	gpu_props->raw_props.gpu_id = regdump.gpu_id;
+	gpu_props->raw_props.tiler_features = regdump.tiler_features;
+	gpu_props->raw_props.mem_features = regdump.mem_features;
+	gpu_props->raw_props.mmu_features = regdump.mmu_features;
+	gpu_props->raw_props.l2_features = regdump.l2_features;
+	gpu_props->raw_props.l3_features = regdump.l3_features;
+
+	gpu_props->raw_props.as_present = regdump.as_present;
+	gpu_props->raw_props.js_present = regdump.js_present;
+	gpu_props->raw_props.shader_present = ((u64) regdump.shader_present_hi << 32) + regdump.shader_present_lo;
+	gpu_props->raw_props.tiler_present = ((u64) regdump.tiler_present_hi << 32) + regdump.tiler_present_lo;
+	gpu_props->raw_props.l2_present = ((u64) regdump.l2_present_hi << 32) + regdump.l2_present_lo;
+	gpu_props->raw_props.l3_present = ((u64) regdump.l3_present_hi << 32) + regdump.l3_present_lo;
+
+	for (i = 0; i < MIDG_MAX_JOB_SLOTS; i++)
+		gpu_props->raw_props.js_features[i] = regdump.js_features[i];
+
+	for (i = 0; i < BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS; i++)
+		gpu_props->raw_props.texture_features[i] = regdump.texture_features[i];
+
+	gpu_props->raw_props.thread_max_barrier_size = regdump.thread_max_barrier_size;
+	gpu_props->raw_props.thread_max_threads = regdump.thread_max_threads;
+	gpu_props->raw_props.thread_max_workgroup_size = regdump.thread_max_workgroup_size;
+	gpu_props->raw_props.thread_features = regdump.thread_features;
+}
+
+/**
+ * @brief Calculate the derived properties
+ *
+ * Fill the base_gpu_props structure with values derived from the GPU configuration registers
+ *
+ * @param gpu_props  The base_gpu_props structure
+ * @param kbdev      The kbase_device structure for the device
+ */
+static void kbase_gpuprops_calculate_props(base_gpu_props * const gpu_props, kbase_device *kbdev)
+{
+	int i;
+
+	/* Populate the base_gpu_props structure */
+	gpu_props->core_props.version_status = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 0U, 4);
+	gpu_props->core_props.minor_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 4U, 8);
+	gpu_props->core_props.major_revision = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 12U, 4);
+	gpu_props->core_props.product_id = KBASE_UBFX32(gpu_props->raw_props.gpu_id, 16U, 16);
+	gpu_props->core_props.log2_program_counter_size = KBASE_GPU_PC_SIZE_LOG2;
+	gpu_props->core_props.gpu_available_memory_size = totalram_pages << PAGE_SHIFT;
+
+	for (i = 0; i < BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS; i++)
+		gpu_props->core_props.texture_features[i] = gpu_props->raw_props.texture_features[i];
+
+	gpu_props->l2_props.log2_line_size = KBASE_UBFX32(gpu_props->raw_props.l2_features, 0U, 8);
+	gpu_props->l2_props.log2_cache_size = KBASE_UBFX32(gpu_props->raw_props.l2_features, 16U, 8);
+	gpu_props->l2_props.num_l2_slices = 1;
+	if (gpu_props->core_props.product_id == GPU_ID_PI_T76X) {
+		gpu_props->l2_props.num_l2_slices = KBASE_UBFX32(gpu_props->raw_props.mem_features, 8U, 4) + 1;
+	}
+
+	gpu_props->l3_props.log2_line_size = KBASE_UBFX32(gpu_props->raw_props.l3_features, 0U, 8);
+	gpu_props->l3_props.log2_cache_size = KBASE_UBFX32(gpu_props->raw_props.l3_features, 16U, 8);
+
+	gpu_props->tiler_props.bin_size_bytes = 1 << KBASE_UBFX32(gpu_props->raw_props.tiler_features, 0U, 6);
+	gpu_props->tiler_props.max_active_levels = KBASE_UBFX32(gpu_props->raw_props.tiler_features, 8U, 4);
+
+	if (gpu_props->raw_props.thread_max_threads == 0)
+		gpu_props->thread_props.max_threads = THREAD_MT_DEFAULT;
+	else
+		gpu_props->thread_props.max_threads = gpu_props->raw_props.thread_max_threads;
+
+	if (gpu_props->raw_props.thread_max_workgroup_size == 0)
+		gpu_props->thread_props.max_workgroup_size = THREAD_MWS_DEFAULT;
+	else
+		gpu_props->thread_props.max_workgroup_size = gpu_props->raw_props.thread_max_workgroup_size;
+
+	if (gpu_props->raw_props.thread_max_barrier_size == 0)
+		gpu_props->thread_props.max_barrier_size = THREAD_MBS_DEFAULT;
+	else
+		gpu_props->thread_props.max_barrier_size = gpu_props->raw_props.thread_max_barrier_size;
+
+	gpu_props->thread_props.max_registers = KBASE_UBFX32(gpu_props->raw_props.thread_features, 0U, 16);
+	gpu_props->thread_props.max_task_queue = KBASE_UBFX32(gpu_props->raw_props.thread_features, 16U, 8);
+	gpu_props->thread_props.max_thread_group_split = KBASE_UBFX32(gpu_props->raw_props.thread_features, 24U, 6);
+	gpu_props->thread_props.impl_tech = KBASE_UBFX32(gpu_props->raw_props.thread_features, 30U, 2);
+
+	/* If values are not specified, then use defaults */
+	if (gpu_props->thread_props.max_registers == 0) {
+		gpu_props->thread_props.max_registers = THREAD_MR_DEFAULT;
+		gpu_props->thread_props.max_task_queue = THREAD_MTQ_DEFAULT;
+		gpu_props->thread_props.max_thread_group_split = THREAD_MTGS_DEFAULT;
+	}
+	/* Initialize the coherent_group structure for each group */
+	kbase_gpuprops_construct_coherent_groups(gpu_props);
+}
+
+void kbase_gpuprops_set(kbase_device *kbdev)
+{
+	kbase_gpu_props *gpu_props;
+	struct midg_raw_gpu_props *raw;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	gpu_props = &kbdev->gpu_props;
+	raw = &gpu_props->props.raw_props;
+
+	/* Initialize the base_gpu_props structure from the hardware */
+	kbase_gpuprops_get_props(&gpu_props->props, kbdev);
+
+	/* Populate the derived properties */
+	kbase_gpuprops_calculate_props(&gpu_props->props, kbdev);
+
+	/* Populate kbase-only fields */
+	gpu_props->l2_props.associativity = KBASE_UBFX32(raw->l2_features, 8U, 8);
+	gpu_props->l2_props.external_bus_width = KBASE_UBFX32(raw->l2_features, 24U, 8);
+
+	gpu_props->l3_props.associativity = KBASE_UBFX32(raw->l3_features, 8U, 8);
+	gpu_props->l3_props.external_bus_width = KBASE_UBFX32(raw->l3_features, 24U, 8);
+
+	gpu_props->mem.core_group = KBASE_UBFX32(raw->mem_features, 0U, 1);
+	gpu_props->mem.supergroup = KBASE_UBFX32(raw->mem_features, 1U, 1);
+
+	gpu_props->mmu.va_bits = KBASE_UBFX32(raw->mmu_features, 0U, 8);
+	gpu_props->mmu.pa_bits = KBASE_UBFX32(raw->mmu_features, 8U, 8);
+
+	gpu_props->num_cores = hweight64(raw->shader_present);
+	gpu_props->num_core_groups = hweight64(raw->l2_present);
+	gpu_props->num_supergroups = hweight64(raw->l3_present);
+	gpu_props->num_address_spaces = hweight32(raw->as_present);
+	gpu_props->num_job_slots = hweight32(raw->js_present);
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.h
new file mode 100644
index 0000000..835c87f
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops.h
@@ -0,0 +1,54 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_gpuprops.h
+ * Base kernel property query APIs
+ */
+
+#ifndef _KBASE_GPUPROPS_H_
+#define _KBASE_GPUPROPS_H_
+
+#include "mali_kbase_gpuprops_types.h"
+
+/* Forward definition - see mali_kbase.h */
+struct kbase_device;
+
+/**
+ * @brief Set up Kbase GPU properties.
+ *
+ * Set up Kbase GPU properties with information from the GPU registers
+ *
+ * @param kbdev		The kbase_device structure for the device
+ */
+void kbase_gpuprops_set(struct kbase_device *kbdev);
+
+/**
+ * @brief Provide GPU properties to userside through UKU call.
+ *
+ * Fill the kbase_uk_gpuprops with values from GPU configuration registers.
+ *
+ * @param kctx		The kbase_context structure
+ * @param kbase_props	A copy of the kbase_uk_gpuprops structure from userspace
+ *
+ * @return MALI_ERROR_NONE on success. Any other value indicates failure.
+ */
+mali_error kbase_gpuprops_uk_get_props(kbase_context *kctx, kbase_uk_gpuprops * const kbase_props);
+
+#endif				/* _KBASE_GPUPROPS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops_types.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops_types.h
new file mode 100644
index 0000000..8793e0a
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_gpuprops_types.h
@@ -0,0 +1,103 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_gpuprops_types.h
+ * Base kernel property query APIs
+ */
+
+#ifndef _KBASE_GPUPROPS_TYPES_H_
+#define _KBASE_GPUPROPS_TYPES_H_
+
+#include "mali_base_kernel.h"
+
+#define KBASE_GPU_SPEED_MHZ    123
+#define KBASE_GPU_PC_SIZE_LOG2 24U
+
+typedef struct kbase_gpuprops_regdump {
+	u32 gpu_id;
+	u32 l2_features;
+	u32 l3_features;
+	u32 tiler_features;
+	u32 mem_features;
+	u32 mmu_features;
+	u32 as_present;
+	u32 js_present;
+
+	u32 js_features[MIDG_MAX_JOB_SLOTS];
+
+	u32 texture_features[BASE_GPU_NUM_TEXTURE_FEATURES_REGISTERS];
+
+	u32 shader_present_lo;
+	u32 shader_present_hi;
+
+	u32 tiler_present_lo;
+	u32 tiler_present_hi;
+
+	u32 l2_present_lo;
+	u32 l2_present_hi;
+
+	u32 l3_present_lo;
+	u32 l3_present_hi;
+
+	u32 thread_max_threads;
+	u32 thread_max_workgroup_size;
+	u32 thread_max_barrier_size;
+	u32 thread_features;
+} kbase_gpuprops_regdump;
+
+typedef struct kbase_gpu_cache_props {
+	u8 associativity;
+	u8 external_bus_width;
+} kbase_gpu_cache_props;
+
+typedef struct kbase_gpu_mem_props {
+	u8 core_group;
+	u8 supergroup;
+} kbase_gpu_mem_props;
+
+typedef struct kbase_gpu_mmu_props {
+	u8 va_bits;
+	u8 pa_bits;
+} kbase_gpu_mmu_props;
+
+typedef struct mali_kbase_gpu_props {
+	/* kernel-only properties */
+	u8 num_cores;
+	u8 num_core_groups;
+	u8 num_supergroups;
+	u8 num_address_spaces;
+	u8 num_job_slots;
+
+	kbase_gpu_cache_props l2_props;
+	kbase_gpu_cache_props l3_props;
+
+	kbase_gpu_mem_props mem;
+	kbase_gpu_mmu_props mmu;
+
+	/**
+	 * Implementation specific irq throttle value (us), should be adjusted during integration.
+	 */
+	int irq_throttle_time_us;
+
+	/* Properties shared with userspace */
+	base_gpu_props props;
+} kbase_gpu_props;
+
+#endif				/* _KBASE_GPUPROPS_TYPES_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.c
new file mode 100644
index 0000000..872d24b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.c
@@ -0,0 +1,142 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Run-time work-arounds helpers
+ */
+
+#include <mali_base_hwconfig.h>
+#include <mali_midg_regmap.h>
+#include "mali_kbase.h"
+#include "mali_kbase_hw.h"
+
+void kbase_hw_set_features_mask(kbase_device *kbdev)
+{
+	const base_hw_feature *features;
+	u32 gpu_id;
+
+	gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
+
+	switch (gpu_id) {
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 0, 0):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 0, 1):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 1, 1):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 1, 9):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 2, 1):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 3, 1):
+	case GPU_ID_MAKE(GPU_ID_PI_T76X, 1, 0, 0):
+		features = base_hw_features_t76x;
+		break;
+	default:
+		features = base_hw_features_generic;
+		break;
+	}
+
+	for (; *features != BASE_HW_FEATURE_END; features++)
+		set_bit(*features, &kbdev->hw_features_mask[0]);
+}
+
+mali_error kbase_hw_set_issues_mask(kbase_device *kbdev)
+{
+	const base_hw_issue *issues;
+	u32 gpu_id;
+	u32 impl_tech;
+
+	gpu_id = kbdev->gpu_props.props.raw_props.gpu_id;
+	impl_tech = kbdev->gpu_props.props.thread_props.impl_tech;
+
+	if (impl_tech != IMPLEMENTATION_MODEL) {
+		switch (gpu_id) {
+		case GPU_ID_MAKE(GPU_ID_PI_T60X, 0, 0, GPU_ID_S_15DEV0):
+			issues = base_hw_issues_t60x_r0p0_15dev0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T60X, 0, 0, GPU_ID_S_EAC):
+			issues = base_hw_issues_t60x_r0p0_eac;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T60X, 0, 1, 0):
+			issues = base_hw_issues_t60x_r0p1;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T62X, 0, 1, 0):
+			issues = base_hw_issues_t62x_r0p1;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T62X, 1, 0, 0):
+		case GPU_ID_MAKE(GPU_ID_PI_T62X, 1, 0, 1):
+			issues = base_hw_issues_t62x_r1p0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T62X, 1, 1, 0):
+			issues = base_hw_issues_t62x_r1p1;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 0, 1):
+			issues = base_hw_issues_t76x_r0p0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 1, 1):
+			issues = base_hw_issues_t76x_r0p1;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 1, 9):
+			issues = base_hw_issues_t76x_r0p1_50rel0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 2, 1):
+			issues = base_hw_issues_t76x_r0p2;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 0, 3, 1):
+			issues = base_hw_issues_t76x_r0p3;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T76X, 1, 0, 0):
+			issues = base_hw_issues_t76x_r1p0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T72X, 0, 0, 0):
+		case GPU_ID_MAKE(GPU_ID_PI_T72X, 0, 0, 1):
+		case GPU_ID_MAKE(GPU_ID_PI_T72X, 0, 0, 2):
+			issues = base_hw_issues_t72x_r0p0;
+			break;
+		case GPU_ID_MAKE(GPU_ID_PI_T72X, 1, 0, 0):
+			issues = base_hw_issues_t72x_r1p0;
+			break;
+		default:
+			dev_err(kbdev->dev, "Unknown GPU ID %x", gpu_id);
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+	} else {
+		/* Software model */
+		switch (gpu_id >> GPU_ID_VERSION_PRODUCT_ID_SHIFT) {
+		case GPU_ID_PI_T60X:
+		case GPU_ID_PI_T62X:
+			issues = base_hw_issues_model_t6xx;
+			break;
+		case GPU_ID_PI_T72X:
+			issues = base_hw_issues_model_t72x;
+			break;
+		case GPU_ID_PI_T76X:
+			issues = base_hw_issues_model_t7xx;
+			break;
+
+		default:
+			dev_err(kbdev->dev, "Unknown GPU ID %x", gpu_id);
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+	}
+
+	dev_info(kbdev->dev, "GPU identified as 0x%04x r%dp%d status %d", (gpu_id & GPU_ID_VERSION_PRODUCT_ID) >> GPU_ID_VERSION_PRODUCT_ID_SHIFT, (gpu_id & GPU_ID_VERSION_MAJOR) >> GPU_ID_VERSION_MAJOR_SHIFT, (gpu_id & GPU_ID_VERSION_MINOR) >> GPU_ID_VERSION_MINOR_SHIFT, (gpu_id & GPU_ID_VERSION_STATUS) >> GPU_ID_VERSION_STATUS_SHIFT);
+
+	for (; *issues != BASE_HW_ISSUE_END; issues++)
+		set_bit(*issues, &kbdev->hw_issues_mask[0]);
+
+	return MALI_ERROR_NONE;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.h
new file mode 100644
index 0000000..4501af7
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_hw.h
@@ -0,0 +1,52 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file
+ * Run-time work-arounds helpers
+ */
+
+#ifndef _KBASE_HW_H_
+#define _KBASE_HW_H_
+
+#include "mali_kbase_defs.h"
+
+/**
+ * @brief Tell whether a work-around should be enabled
+ */
+#define kbase_hw_has_issue(kbdev, issue)\
+	test_bit(issue, &(kbdev)->hw_issues_mask[0])
+
+/**
+ * @brief Tell whether a feature is supported
+ */
+#define kbase_hw_has_feature(kbdev, feature)\
+	test_bit(feature, &(kbdev)->hw_features_mask[0])
+
+/**
+ * @brief Set the HW issues mask depending on the GPU ID
+ */
+mali_error kbase_hw_set_issues_mask(kbase_device *kbdev);
+
+/**
+ * @brief Set the features mask depending on the GPU ID
+ */
+void kbase_hw_set_features_mask(kbase_device *kbdev);
+
+#endif				/* _KBASE_HW_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_instr.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_instr.c
new file mode 100644
index 0000000..0b9f355
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_instr.c
@@ -0,0 +1,618 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_instr.c
+ * Base kernel instrumentation APIs.
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+
+/**
+ * @brief Issue Cache Clean & Invalidate command to hardware
+ */
+static void kbasep_instr_hwcnt_cacheclean(kbase_device *kbdev)
+{
+	unsigned long flags;
+	unsigned long pm_flags;
+	u32 irq_mask;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	/* Wait for any reset to complete */
+	while (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.cache_clean_wait,
+		           kbdev->hwcnt.state != KBASE_INSTR_STATE_RESETTING);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+	KBASE_DEBUG_ASSERT(kbdev->hwcnt.state == KBASE_INSTR_STATE_REQUEST_CLEAN);
+
+	/* Enable interrupt */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, pm_flags);
+	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), irq_mask | CLEAN_CACHES_COMPLETED, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, pm_flags);
+
+	/* clean&invalidate the caches so we're sure the mmu tables for the dump buffer is valid */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, NULL, 0u, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CLEAN_INV_CACHES, NULL);
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_CLEANING;
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+}
+
+STATIC mali_error kbase_instr_hwcnt_enable_internal(kbase_device *kbdev, kbase_context *kctx, kbase_uk_hwcnt_setup *setup)
+{
+	unsigned long flags, pm_flags;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	kbasep_js_device_data *js_devdata;
+	u32 irq_mask;
+	int ret;
+	u64 shader_cores_needed;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	KBASE_DEBUG_ASSERT(NULL != setup);
+	KBASE_DEBUG_ASSERT(NULL == kbdev->hwcnt.suspended_kctx);
+
+	shader_cores_needed = kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_SHADER);
+
+	js_devdata = &kbdev->js_data;
+
+	/* alignment failure */
+	if ((setup->dump_buffer == 0ULL) || (setup->dump_buffer & (2048 - 1)))
+		goto out_err;
+
+	/* Override core availability policy to ensure all cores are available */
+	kbase_pm_ca_instr_enable(kbdev);
+
+	/* Mark the context as active so the GPU is kept turned on */
+	/* A suspend won't happen here, because we're in a syscall from a userspace
+	 * thread. */
+	kbase_pm_context_active(kbdev);
+
+	/* Request the cores early on synchronously - we'll release them on any errors
+	 * (e.g. instrumentation already active) */
+	kbase_pm_request_cores_sync(kbdev, MALI_TRUE, shader_cores_needed);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {
+		/* GPU is being reset */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+
+	if (kbdev->hwcnt.state != KBASE_INSTR_STATE_DISABLED) {
+		/* Instrumentation is already enabled */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		goto out_unrequest_cores;
+	}
+
+	/* Enable interrupt */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, pm_flags);
+	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), irq_mask | PRFCNT_SAMPLE_COMPLETED, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, pm_flags);
+
+	/* In use, this context is the owner */
+	kbdev->hwcnt.kctx = kctx;
+	/* Remember the dump address so we can reprogram it later */
+	kbdev->hwcnt.addr = setup->dump_buffer;
+	/* Remember all the settings for suspend/resume */
+	if (&kbdev->hwcnt.suspended_state != setup)
+		memcpy(&kbdev->hwcnt.suspended_state, setup, sizeof(kbdev->hwcnt.suspended_state));
+
+	/* Request the clean */
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_REQUEST_CLEAN;
+	kbdev->hwcnt.triggered = 0;
+	/* Clean&invalidate the caches so we're sure the mmu tables for the dump buffer is valid */
+	ret = queue_work(kbdev->hwcnt.cache_clean_wq, &kbdev->hwcnt.cache_clean_work);
+	KBASE_DEBUG_ASSERT(ret);
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	/* Wait for cacheclean to complete */
+	wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+
+	KBASE_DEBUG_ASSERT(kbdev->hwcnt.state == KBASE_INSTR_STATE_IDLE);
+
+	/* Schedule the context in */
+	kbasep_js_schedule_privileged_ctx(kbdev, kctx);
+
+	/* Configure */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), (kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT) | PRFCNT_CONFIG_MODE_OFF, kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_LO),     setup->dump_buffer & 0xFFFFFFFF, kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_HI),     setup->dump_buffer >> 32,        kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_JM_EN),       setup->jm_bm,                    kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_SHADER_EN),   setup->shader_bm,                kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_L3_CACHE_EN), setup->l3_cache_bm,              kctx);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_MMU_L2_EN),   setup->mmu_l2_bm,                kctx);
+	/* Due to PRLAM-8186 we need to disable the Tiler before we enable the HW counter dump. */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8186))
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), 0, kctx);
+	else
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), setup->tiler_bm, kctx);
+
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), (kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT) | PRFCNT_CONFIG_MODE_MANUAL, kctx);
+
+	/* If HW has PRLAM-8186 we can now re-enable the tiler HW counters dump */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8186))
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), setup->tiler_bm, kctx);
+	
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {
+		/* GPU is being reset */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_IDLE;
+	kbdev->hwcnt.triggered = 1;
+	wake_up(&kbdev->hwcnt.wait);
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	err = MALI_ERROR_NONE;
+
+	dev_dbg(kbdev->dev, "HW counters dumping set-up for context %p", kctx);
+	return err;
+ out_unrequest_cores:
+	kbase_pm_unrequest_cores(kbdev, MALI_TRUE, shader_cores_needed);
+	kbase_pm_context_idle(kbdev);
+ out_err:
+	return err;
+}
+
+/**
+ * @brief Enable HW counters collection
+ *
+ * Note: will wait for a cache clean to complete
+ */
+mali_error kbase_instr_hwcnt_enable(kbase_context *kctx, kbase_uk_hwcnt_setup *setup)
+{
+	kbase_device *kbdev;
+	mali_bool access_allowed;
+	kbdev = kctx->kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	/* Determine if the calling task has access to this capability */
+	access_allowed = kbase_security_has_capability(kctx, KBASE_SEC_INSTR_HW_COUNTERS_COLLECT, KBASE_SEC_FLAG_NOAUDIT);
+	if (MALI_FALSE == access_allowed)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return kbase_instr_hwcnt_enable_internal(kbdev, kctx, setup);
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_enable)
+
+/**
+ * @brief Disable HW counters collection
+ *
+ * Note: might sleep, waiting for an ongoing dump to complete
+ */
+mali_error kbase_instr_hwcnt_disable(kbase_context *kctx)
+{
+	unsigned long flags, pm_flags;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	u32 irq_mask;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	while (1) {
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+		if (kbdev->hwcnt.state == KBASE_INSTR_STATE_DISABLED) {
+			/* Instrumentation is not enabled */
+			spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+			goto out;
+		}
+
+		if (kbdev->hwcnt.kctx != kctx) {
+			/* Instrumentation has been setup for another context */
+			spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+			goto out;
+		}
+
+		if (kbdev->hwcnt.state == KBASE_INSTR_STATE_IDLE)
+			break;
+
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+		/* Ongoing dump/setup - wait for its completion */
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+
+	}
+
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_DISABLED;
+	kbdev->hwcnt.triggered = 0;
+
+	/* Disable interrupt */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, pm_flags);
+	irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), irq_mask & ~PRFCNT_SAMPLE_COMPLETED, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, pm_flags);
+
+	/* Disable the counters */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), 0, kctx);
+
+	kbdev->hwcnt.kctx = NULL;
+	kbdev->hwcnt.addr = 0ULL;
+
+	kbase_pm_ca_instr_disable(kbdev);
+
+	kbase_pm_unrequest_cores(kbdev, MALI_TRUE, kbase_pm_get_present_cores(kbdev, KBASE_PM_CORE_SHADER));
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	/* Release the context. This had its own Power Manager Active reference */
+	kbasep_js_release_privileged_ctx(kbdev, kctx);
+
+	/* Also release our Power Manager Active reference */
+	kbase_pm_context_idle(kbdev);
+
+	dev_dbg(kbdev->dev, "HW counters dumping disabled for context %p", kctx);
+
+	err = MALI_ERROR_NONE;
+
+ out:
+	return err;
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_disable)
+
+/**
+ * @brief Configure HW counters collection
+ */
+mali_error kbase_instr_hwcnt_setup(kbase_context *kctx, kbase_uk_hwcnt_setup *setup)
+{
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	if (NULL == setup) {
+		/* Bad parameter - abort */
+		goto out;
+	}
+
+	if (setup->dump_buffer != 0ULL) {
+		/* Enable HW counters */
+		err = kbase_instr_hwcnt_enable(kctx, setup);
+	} else {
+		/* Disable HW counters */
+		err = kbase_instr_hwcnt_disable(kctx);
+	}
+
+ out:
+	return err;
+}
+
+/**
+ * @brief Issue Dump command to hardware
+ *
+ * Notes:
+ * - does not sleep
+ */
+mali_error kbase_instr_hwcnt_dump_irq(kbase_context *kctx)
+{
+	unsigned long flags;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.kctx != kctx) {
+		/* The instrumentation has been setup for another context */
+		goto unlock;
+	}
+
+	if (kbdev->hwcnt.state != KBASE_INSTR_STATE_IDLE) {
+		/* HW counters are disabled or another dump is ongoing, or we're resetting */
+		goto unlock;
+	}
+
+	kbdev->hwcnt.triggered = 0;
+
+	/* Mark that we're dumping - the PF handler can signal that we faulted */
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_DUMPING;
+
+	/* Reconfigure the dump address */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_LO), kbdev->hwcnt.addr & 0xFFFFFFFF, NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_HI), kbdev->hwcnt.addr >> 32, NULL);
+
+	/* Start dumping */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_PRFCNT_SAMPLE, NULL, NULL, kbdev->hwcnt.addr, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_PRFCNT_SAMPLE, kctx);
+
+	dev_dbg(kbdev->dev, "HW counters dumping done for context %p", kctx);
+
+	err = MALI_ERROR_NONE;
+
+ unlock:
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+	return err;
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_dump_irq)
+
+/**
+ * @brief Tell whether the HW counters dump has completed
+ *
+ * Notes:
+ * - does not sleep
+ * - success will be set to MALI_TRUE if the dump succeeded or
+ *   MALI_FALSE on failure
+ */
+mali_bool kbase_instr_hwcnt_dump_complete(kbase_context *kctx, mali_bool * const success)
+{
+	unsigned long flags;
+	mali_bool complete = MALI_FALSE;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	KBASE_DEBUG_ASSERT(NULL != success);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_IDLE) {
+		*success = MALI_TRUE;
+		complete = MALI_TRUE;
+	} else if (kbdev->hwcnt.state == KBASE_INSTR_STATE_FAULT) {
+		*success = MALI_FALSE;
+		complete = MALI_TRUE;
+		kbdev->hwcnt.state = KBASE_INSTR_STATE_IDLE;
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	return complete;
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_dump_complete)
+
+/**
+ * @brief Issue Dump command to hardware and wait for completion
+ */
+mali_error kbase_instr_hwcnt_dump(kbase_context *kctx)
+{
+	unsigned long flags;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	err = kbase_instr_hwcnt_dump_irq(kctx);
+	if (MALI_ERROR_NONE != err) {
+		/* Can't dump HW counters */
+		goto out;
+	}
+
+	/* Wait for dump & cacheclean to complete */
+	wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {
+		/* GPU is being reset */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_FAULT) {
+		err = MALI_ERROR_FUNCTION_FAILED;
+		kbdev->hwcnt.state = KBASE_INSTR_STATE_IDLE;
+	} else {
+		/* Dump done */
+		KBASE_DEBUG_ASSERT(kbdev->hwcnt.state == KBASE_INSTR_STATE_IDLE);
+		err = MALI_ERROR_NONE;
+	}
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+ out:
+	return err;
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_dump)
+
+/**
+ * @brief Clear the HW counters
+ */
+mali_error kbase_instr_hwcnt_clear(kbase_context *kctx)
+{
+	unsigned long flags;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {
+		/* GPU is being reset */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+
+	/* Check it's the context previously set up and we're not already dumping */
+	if (kbdev->hwcnt.kctx != kctx || kbdev->hwcnt.state != KBASE_INSTR_STATE_IDLE)
+		goto out;
+
+	/* Clear the counters */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_PRFCNT_CLEAR, NULL, NULL, 0u, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_PRFCNT_CLEAR, kctx);
+
+	err = MALI_ERROR_NONE;
+
+ out:
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+	return err;
+}
+KBASE_EXPORT_SYMBOL(kbase_instr_hwcnt_clear)
+
+/**
+ * Workqueue for handling cache cleaning
+ */
+void kbasep_cache_clean_worker(struct work_struct *data)
+{
+	kbase_device *kbdev;
+	unsigned long flags;
+
+	kbdev = container_of(data, kbase_device, hwcnt.cache_clean_work);
+
+	mutex_lock(&kbdev->cacheclean_lock);
+	kbasep_instr_hwcnt_cacheclean(kbdev);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	/* Wait for our condition, and any reset to complete */
+	while (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING
+		   || kbdev->hwcnt.state == KBASE_INSTR_STATE_CLEANING) {
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.cache_clean_wait,
+		           (kbdev->hwcnt.state != KBASE_INSTR_STATE_RESETTING
+		            && kbdev->hwcnt.state != KBASE_INSTR_STATE_CLEANING));
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+	KBASE_DEBUG_ASSERT(kbdev->hwcnt.state == KBASE_INSTR_STATE_CLEANED);
+
+	/* All finished and idle */
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_IDLE;
+	kbdev->hwcnt.triggered = 1;
+	wake_up(&kbdev->hwcnt.wait);
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+	mutex_unlock(&kbdev->cacheclean_lock);
+}
+
+/**
+ * @brief Dump complete interrupt received
+ */
+void kbase_instr_hwcnt_sample_done(kbase_device *kbdev)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_FAULT) {
+		kbdev->hwcnt.triggered = 1;
+		wake_up(&kbdev->hwcnt.wait);
+	} else if (kbdev->hwcnt.state == KBASE_INSTR_STATE_DUMPING) {
+		int ret;
+		/* Always clean and invalidate the cache after a successful dump */
+		kbdev->hwcnt.state = KBASE_INSTR_STATE_REQUEST_CLEAN;
+		ret = queue_work(kbdev->hwcnt.cache_clean_wq, &kbdev->hwcnt.cache_clean_work);
+		KBASE_DEBUG_ASSERT(ret);
+	}
+	/* NOTE: In the state KBASE_INSTR_STATE_RESETTING, We're in a reset,
+	 * and the instrumentation state hasn't been restored yet -
+	 * kbasep_reset_timeout_worker() will do the rest of the work */
+
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+}
+
+/**
+ * @brief Cache clean interrupt received
+ */
+void kbase_clean_caches_done(kbase_device *kbdev)
+{
+	u32 irq_mask;
+
+	if (kbdev->hwcnt.state != KBASE_INSTR_STATE_DISABLED) {
+		unsigned long flags;
+		unsigned long pm_flags;
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+		/* Disable interrupt */
+		spin_lock_irqsave(&kbdev->pm.power_change_lock, pm_flags);
+		irq_mask = kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), NULL);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), irq_mask & ~CLEAN_CACHES_COMPLETED, NULL);
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, pm_flags);
+
+		/* Wakeup... */
+		if (kbdev->hwcnt.state == KBASE_INSTR_STATE_CLEANING) {
+			/* Only wake if we weren't resetting */
+			kbdev->hwcnt.state = KBASE_INSTR_STATE_CLEANED;
+			wake_up(&kbdev->hwcnt.cache_clean_wait);
+		}
+		/* NOTE: In the state KBASE_INSTR_STATE_RESETTING, We're in a reset,
+		 * and the instrumentation state hasn't been restored yet -
+		 * kbasep_reset_timeout_worker() will do the rest of the work */
+
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+	}
+}
+
+
+/* Disable instrumentation and wait for any existing dump to complete
+ * It's assumed that there's only one privileged context
+ * Safe to do this without lock when doing an OS suspend, because it only
+ * changes in response to user-space IOCTLs */
+void kbase_instr_hwcnt_suspend(kbase_device *kbdev)
+{
+	kbase_context *kctx;
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(!kbdev->hwcnt.suspended_kctx);
+
+	kctx = kbdev->hwcnt.kctx;
+	kbdev->hwcnt.suspended_kctx = kctx;
+
+	/* Relevant state was saved into hwcnt.suspended_state when enabling the
+	 * counters */
+
+	if (kctx)
+	{
+		KBASE_DEBUG_ASSERT(kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED);
+		kbase_instr_hwcnt_disable(kctx);
+	}
+}
+
+void kbase_instr_hwcnt_resume(kbase_device *kbdev)
+{
+	kbase_context *kctx;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	kctx = kbdev->hwcnt.suspended_kctx;
+	kbdev->hwcnt.suspended_kctx = NULL;
+
+	if (kctx)
+	{
+		mali_error err;
+		err = kbase_instr_hwcnt_enable_internal(kbdev, kctx, &kbdev->hwcnt.suspended_state);
+		WARN(err != MALI_ERROR_NONE,
+		     "Failed to restore instrumented hardware counters on resume\n");
+	}
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jd.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jd.c
new file mode 100644
index 0000000..4e808ba
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jd.c
@@ -0,0 +1,1639 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#if defined(CONFIG_DMA_SHARED_BUFFER)
+#include <linux/dma-buf.h>
+#endif				/* defined(CONFIG_DMA_SHARED_BUFFER) */
+#ifdef CONFIG_COMPAT
+#include <linux/compat.h>
+#endif
+#include <mali_kbase.h>
+#include <mali_kbase_uku.h>
+#include <mali_kbase_js_affinity.h>
+#include <mali_kbase_10969_workaround.h>
+#ifdef CONFIG_UMP
+#include <linux/ump.h>
+#endif				/* CONFIG_UMP */
+#include <linux/random.h>
+
+#define beenthere(kctx,f, a...)  dev_dbg(kctx->kbdev->dev, "%s:" f, __func__, ##a)
+
+
+/*
+ * This is the kernel side of the API. Only entry points are:
+ * - kbase_jd_submit(): Called from userspace to submit a single bag
+ * - kbase_jd_done(): Called from interrupt context to track the
+ *   completion of a job.
+ * Callouts:
+ * - to the job manager (enqueue a job)
+ * - to the event subsystem (signals the completion/failure of bag/job-chains).
+ */
+
+static void *get_compat_pointer(const kbase_pointer *p)
+{
+#ifdef CONFIG_COMPAT
+	if (is_compat_task())
+		return compat_ptr(p->compat_value);
+	else
+#endif
+		return p->value;
+}
+
+/* Runs an atom, either by handing to the JS or by immediately running it in the case of soft-jobs
+ *
+ * Returns whether the JS needs a reschedule.
+ *
+ * Note that the caller must also check the atom status and
+ * if it is KBASE_JD_ATOM_STATE_COMPLETED must call jd_done_nolock
+ */
+static int jd_run_atom(kbase_jd_atom *katom)
+{
+	kbase_context *kctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(katom->status != KBASE_JD_ATOM_STATE_UNUSED);
+
+	if ((katom->core_req & BASEP_JD_REQ_ATOM_TYPE) == BASE_JD_REQ_DEP) {
+		/* Dependency only atom */
+		katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+		return 0;
+	} else if (katom->core_req & BASE_JD_REQ_SOFT_JOB) {
+		/* Soft-job */
+		if ((katom->core_req & BASEP_JD_REQ_ATOM_TYPE)
+						  == BASE_JD_REQ_SOFT_REPLAY) {
+			int status = kbase_replay_process(katom);
+
+			if ((status & MALI_REPLAY_STATUS_MASK)
+					       == MALI_REPLAY_STATUS_REPLAYING)
+				return status & MALI_REPLAY_FLAG_JS_RESCHED;
+			else
+				katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+			return 0;
+		} else if (kbase_process_soft_job(katom) == 0) {
+			kbase_finish_soft_job(katom);
+			katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+		} else {
+			/* The job has not completed */
+			list_add_tail(&katom->dep_item[0], &kctx->waiting_soft_jobs);
+		}
+		return 0;
+	}
+
+	katom->status = KBASE_JD_ATOM_STATE_IN_JS;
+	/* Queue an action about whether we should try scheduling a context */
+	return kbasep_js_add_job(kctx, katom);
+}
+
+#ifdef CONFIG_KDS
+
+/* Add the katom to the kds waiting list.
+ * Atoms must be added to the waiting list after a successful call to kds_async_waitall.
+ * The caller must hold the kbase_jd_context.lock */
+
+static void kbase_jd_kds_waiters_add(kbase_jd_atom *katom)
+{
+	kbase_context *kctx;
+	KBASE_DEBUG_ASSERT(katom);
+
+	kctx = katom->kctx;
+
+	list_add_tail(&katom->node, &kctx->waiting_kds_resource);
+}
+
+/* Remove the katom from the kds waiting list.
+ * Atoms must be removed from the waiting list before a call to kds_resource_set_release_sync.
+ * The supplied katom must first have been added to the list with a call to kbase_jd_kds_waiters_add.
+ * The caller must hold the kbase_jd_context.lock */
+
+static void kbase_jd_kds_waiters_remove(kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(katom);
+	list_del(&katom->node);
+}
+
+static void kds_dep_clear(void *callback_parameter, void *callback_extra_parameter)
+{
+	kbase_jd_atom *katom;
+	kbase_jd_context *ctx;
+	kbase_device *kbdev;
+
+	katom = (kbase_jd_atom *) callback_parameter;
+	KBASE_DEBUG_ASSERT(katom);
+	ctx = &katom->kctx->jctx;
+	kbdev = katom->kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	mutex_lock(&ctx->lock);
+
+	/* KDS resource has already been satisfied (e.g. due to zapping) */
+	if (katom->kds_dep_satisfied)
+		goto out;
+
+	/* This atom's KDS dependency has now been met */
+	katom->kds_dep_satisfied = MALI_TRUE;
+
+	/* Check whether the atom's other dependencies were already met */
+	if (!kbase_jd_katom_dep_atom(&katom->dep[0]) && !kbase_jd_katom_dep_atom(&katom->dep[1])) {
+		/* katom dep complete, attempt to run it */
+		mali_bool resched = MALI_FALSE;
+		resched = jd_run_atom(katom);
+
+		if (katom->status == KBASE_JD_ATOM_STATE_COMPLETED) {
+			/* The atom has already finished */
+			resched |= jd_done_nolock(katom);
+		}
+
+		if (resched)
+			kbasep_js_try_schedule_head_ctx(kbdev);
+	}
+ out:
+	mutex_unlock(&ctx->lock);
+}
+
+void kbase_cancel_kds_wait_job(kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(katom);
+
+	/* Prevent job_done_nolock from being called twice on an atom when
+	 *  there is a race between job completion and cancellation */
+
+	if ( katom->status == KBASE_JD_ATOM_STATE_QUEUED ) {
+		/* Wait was cancelled - zap the atom */
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		if (jd_done_nolock(katom)) {
+			kbasep_js_try_schedule_head_ctx( katom->kctx->kbdev );
+		}
+	}
+}
+#endif				/* CONFIG_KDS */
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+static mali_error kbase_jd_umm_map(kbase_context *kctx, struct kbase_va_region *reg)
+{
+	struct sg_table *sgt;
+	struct scatterlist *s;
+	int i;
+	phys_addr_t *pa;
+	mali_error err;
+	size_t count = 0;
+
+	KBASE_DEBUG_ASSERT(reg->alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM);
+	KBASE_DEBUG_ASSERT(NULL == reg->alloc->imported.umm.sgt);
+	sgt = dma_buf_map_attachment(reg->alloc->imported.umm.dma_attachment, DMA_BIDIRECTIONAL);
+
+	if (IS_ERR_OR_NULL(sgt))
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	/* save for later */
+	reg->alloc->imported.umm.sgt = sgt;
+
+	pa = kbase_get_phy_pages(reg);
+	KBASE_DEBUG_ASSERT(pa);
+
+	for_each_sg(sgt->sgl, s, sgt->nents, i) {
+		int j;
+		size_t pages = PFN_UP(sg_dma_len(s));
+
+		WARN_ONCE(sg_dma_len(s) & (PAGE_SIZE-1),
+		"sg_dma_len(s)=%u is not a multiple of PAGE_SIZE\n",
+		sg_dma_len(s));
+
+		WARN_ONCE(sg_dma_address(s) & (PAGE_SIZE-1),
+		"sg_dma_address(s)=%llx is not aligned to PAGE_SIZE\n",
+		(unsigned long long) sg_dma_address(s));
+
+		for (j = 0; (j < pages) && (count < reg->nr_pages); j++, count++)
+			*pa++ = sg_dma_address(s) + (j << PAGE_SHIFT);
+		WARN_ONCE(j < pages,
+		"sg list from dma_buf_map_attachment > dma_buf->size=%zu\n",
+		reg->alloc->imported.umm.dma_buf->size);
+	}
+
+	if (WARN_ONCE(count < reg->nr_pages,
+				  "sg list from dma_buf_map_attachment < dma_buf->size=%zu\n",
+				  reg->alloc->imported.umm.dma_buf->size)) {
+		err = MALI_ERROR_FUNCTION_FAILED;
+		goto out;
+	}
+
+	/* Update nents as we now have pages to map */
+	reg->alloc->nents = count;
+
+	err = kbase_mmu_insert_pages(kctx, reg->start_pfn, kbase_get_phy_pages(reg), kbase_reg_current_backed_size(reg), reg->flags | KBASE_REG_GPU_WR | KBASE_REG_GPU_RD);
+
+out:
+	if (MALI_ERROR_NONE != err) {
+		dma_buf_unmap_attachment(reg->alloc->imported.umm.dma_attachment, reg->alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
+		reg->alloc->imported.umm.sgt = NULL;
+	}
+
+	return err;
+}
+
+static void kbase_jd_umm_unmap(kbase_context *kctx, struct kbase_mem_phy_alloc *alloc)
+{
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(alloc);
+	KBASE_DEBUG_ASSERT(alloc->imported.umm.dma_attachment);
+	KBASE_DEBUG_ASSERT(alloc->imported.umm.sgt);
+	dma_buf_unmap_attachment(alloc->imported.umm.dma_attachment,
+			alloc->imported.umm.sgt, DMA_BIDIRECTIONAL);
+	alloc->imported.umm.sgt = NULL;
+	alloc->nents = 0;
+}
+#endif				/* CONFIG_DMA_SHARED_BUFFER */
+
+void kbase_jd_free_external_resources(kbase_jd_atom *katom)
+{
+#ifdef CONFIG_KDS
+	if (katom->kds_rset) {
+		kbase_jd_context * jctx = &katom->kctx->jctx;
+
+		/*
+		 * As the atom is no longer waiting, remove it from
+		 * the waiting list.
+		 */
+
+		mutex_lock(&jctx->lock);
+		kbase_jd_kds_waiters_remove( katom );
+		mutex_unlock(&jctx->lock);
+
+		/* Release the kds resource or cancel if zapping */
+		kds_resource_set_release_sync(&katom->kds_rset);
+	}
+#endif				/* CONFIG_KDS */
+}
+
+static void kbase_jd_post_external_resources(kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(katom);
+	KBASE_DEBUG_ASSERT(katom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES);
+
+#ifdef CONFIG_KDS
+	/* Prevent the KDS resource from triggering the atom in case of zapping */
+	if (katom->kds_rset)
+		katom->kds_dep_satisfied = MALI_TRUE;
+#endif				/* CONFIG_KDS */
+
+	kbase_gpu_vm_lock(katom->kctx);
+	/* only roll back if extres is non-NULL */
+	if (katom->extres) {
+		u32 res_no;
+		res_no = katom->nr_extres;
+		while (res_no-- > 0) {
+			struct kbase_mem_phy_alloc *alloc = katom->extres[res_no].alloc;
+#ifdef CONFIG_DMA_SHARED_BUFFER
+			if (alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
+				alloc->imported.umm.current_mapping_usage_count--;
+
+				if (0 == alloc->imported.umm.current_mapping_usage_count) {
+					struct kbase_va_region *reg;
+					reg = kbase_region_tracker_find_region_base_address(
+					          katom->kctx, katom->extres[res_no].gpu_address);
+
+					if (reg && reg->alloc == alloc) {
+						kbase_mmu_teardown_pages(katom->kctx, reg->start_pfn,
+						    kbase_reg_current_backed_size(reg));
+					}
+
+					kbase_jd_umm_unmap(katom->kctx, alloc);
+				}
+			}
+#endif	/* CONFIG_DMA_SHARED_BUFFER */
+			kbase_mem_phy_alloc_put(katom->extres[res_no].alloc);
+		}
+		kfree(katom->extres);
+		katom->extres = NULL;
+	}
+	kbase_gpu_vm_unlock(katom->kctx);
+}
+
+#if (defined(CONFIG_KDS) && defined(CONFIG_UMP)) || defined(CONFIG_DMA_SHARED_BUFFER_USES_KDS)
+static void add_kds_resource(struct kds_resource *kds_res, struct kds_resource **kds_resources, u32 *kds_res_count, unsigned long *kds_access_bitmap, mali_bool exclusive)
+{
+	u32 i;
+
+	for (i = 0; i < *kds_res_count; i++) {
+		/* Duplicate resource, ignore */
+		if (kds_resources[i] == kds_res)
+			return;
+	}
+
+	kds_resources[*kds_res_count] = kds_res;
+	if (exclusive)
+		set_bit(*kds_res_count, kds_access_bitmap);
+	(*kds_res_count)++;
+}
+#endif
+
+/*
+ * Set up external resources needed by this job.
+ *
+ * jctx.lock must be held when this is called.
+ */
+
+static mali_error kbase_jd_pre_external_resources(kbase_jd_atom *katom, const base_jd_atom_v2 *user_atom)
+{
+	mali_error err_ret_val = MALI_ERROR_FUNCTION_FAILED;
+	u32 res_no;
+#ifdef CONFIG_KDS
+	u32 kds_res_count = 0;
+	struct kds_resource **kds_resources = NULL;
+	unsigned long *kds_access_bitmap = NULL;
+#endif				/* CONFIG_KDS */
+	struct base_external_resource * input_extres;
+
+	KBASE_DEBUG_ASSERT(katom);
+	KBASE_DEBUG_ASSERT(katom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES);
+
+	/* no resources encoded, early out */
+	if (!katom->nr_extres)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	katom->extres = kmalloc(sizeof(*katom->extres) * katom->nr_extres, GFP_KERNEL);
+	if (NULL == katom->extres) {
+		err_ret_val = MALI_ERROR_OUT_OF_MEMORY;
+		goto early_err_out;
+	}
+
+	/* copy user buffer to the end of our real buffer.
+	 * Make sure the struct sizes haven't changed in a way
+	 * we don't support */
+	BUILD_BUG_ON(sizeof(*input_extres) > sizeof(*katom->extres));
+	input_extres = (struct base_external_resource*)(((unsigned char *)katom->extres) + (sizeof(*katom->extres) - sizeof(*input_extres)) * katom->nr_extres);
+
+	if (copy_from_user(input_extres, get_compat_pointer(&user_atom->extres_list), sizeof(*input_extres) * katom->nr_extres) != 0) {
+		err_ret_val = MALI_ERROR_FUNCTION_FAILED;
+		goto early_err_out;
+	}
+#ifdef CONFIG_KDS
+	/* assume we have to wait for all */
+	KBASE_DEBUG_ASSERT(0 != katom->nr_extres);
+	kds_resources = kmalloc(sizeof(struct kds_resource *) * katom->nr_extres, GFP_KERNEL);
+
+	if (NULL == kds_resources) {
+		err_ret_val = MALI_ERROR_OUT_OF_MEMORY;
+		goto early_err_out;
+	}
+
+	KBASE_DEBUG_ASSERT(0 != katom->nr_extres);
+	kds_access_bitmap = kzalloc(sizeof(unsigned long) * ((katom->nr_extres + BITS_PER_LONG - 1) / BITS_PER_LONG), GFP_KERNEL);
+
+	if (NULL == kds_access_bitmap) {
+		err_ret_val = MALI_ERROR_OUT_OF_MEMORY;
+		goto early_err_out;
+	}
+#endif				/* CONFIG_KDS */
+
+	/* need to keep the GPU VM locked while we set up UMM buffers */
+	kbase_gpu_vm_lock(katom->kctx);
+	for (res_no = 0; res_no < katom->nr_extres; res_no++) {
+		base_external_resource *res;
+		kbase_va_region *reg;
+
+		res = &input_extres[res_no];
+		reg = kbase_region_tracker_find_region_enclosing_address(katom->kctx, res->ext_resource & ~BASE_EXT_RES_ACCESS_EXCLUSIVE);
+		/* did we find a matching region object? */
+		if (NULL == reg || (reg->flags & KBASE_REG_FREE)) {
+			/* roll back */
+			goto failed_loop;
+		}
+
+		/* decide what needs to happen for this resource */
+		switch (reg->alloc->type) {
+		case BASE_TMEM_IMPORT_TYPE_UMP:
+			{
+#if defined(CONFIG_KDS) && defined(CONFIG_UMP)
+				struct kds_resource *kds_res;
+				kds_res = ump_dd_kds_resource_get(reg->alloc->imported.ump_handle);
+				if (kds_res)
+					add_kds_resource(kds_res, kds_resources, &kds_res_count, kds_access_bitmap, res->ext_resource & BASE_EXT_RES_ACCESS_EXCLUSIVE);
+#endif				/*defined(CONFIG_KDS) && defined(CONFIG_UMP) */
+				break;
+			}
+#ifdef CONFIG_DMA_SHARED_BUFFER
+		case BASE_TMEM_IMPORT_TYPE_UMM:
+			{
+#ifdef CONFIG_DMA_SHARED_BUFFER_USES_KDS
+				struct kds_resource *kds_res;
+				kds_res = get_dma_buf_kds_resource(reg->alloc->imported.umm.dma_buf);
+				if (kds_res)
+					add_kds_resource(kds_res, kds_resources, &kds_res_count, kds_access_bitmap, res->ext_resource & BASE_EXT_RES_ACCESS_EXCLUSIVE);
+#endif
+				reg->alloc->imported.umm.current_mapping_usage_count++;
+				if (1 == reg->alloc->imported.umm.current_mapping_usage_count) {
+					/* use a local variable to not pollute err_ret_val
+					 * with a potential success value as some other gotos depend
+					 * on the default error code stored in err_ret_val */
+					mali_error tmp;
+					tmp = kbase_jd_umm_map(katom->kctx, reg);
+					if (MALI_ERROR_NONE != tmp) {
+						/* failed to map this buffer, roll back */
+						err_ret_val = tmp;
+						reg->alloc->imported.umm.current_mapping_usage_count--;
+						goto failed_loop;
+					}
+				}
+				break;
+			}
+#endif
+		default:
+			goto failed_loop;
+		}
+
+		/* finish with updating out array with the data we found */
+		/* NOTE: It is important that this is the last thing we do (or
+		 * at least not before the first write) as we overwrite elements
+		 * as we loop and could be overwriting ourself, so no writes
+		 * until the last read for an element.
+		 * */
+		katom->extres[res_no].gpu_address = reg->start_pfn << PAGE_SHIFT; /* save the start_pfn (as an address, not pfn) to use fast lookup later */
+		katom->extres[res_no].alloc = kbase_mem_phy_alloc_get(reg->alloc);
+	}
+	/* successfully parsed the extres array */
+	/* drop the vm lock before we call into kds */
+	kbase_gpu_vm_unlock(katom->kctx);
+
+#ifdef CONFIG_KDS
+	if (kds_res_count) {
+		int wait_failed;
+		/* We have resources to wait for with kds */
+		katom->kds_dep_satisfied = MALI_FALSE;
+
+		wait_failed = kds_async_waitall(&katom->kds_rset,
+										&katom->kctx->jctx.kds_cb,
+										katom,
+										NULL,
+										kds_res_count,
+										kds_access_bitmap,
+										kds_resources);
+		if (wait_failed) {
+			goto failed_kds_setup;
+		} else {
+			kbase_jd_kds_waiters_add( katom );
+		}
+	} else {
+		/* Nothing to wait for, so kds dep met */
+		katom->kds_dep_satisfied = MALI_TRUE;
+	}
+	kfree(kds_resources);
+	kfree(kds_access_bitmap);
+#endif				/* CONFIG_KDS */
+
+	/* all done OK */
+	return MALI_ERROR_NONE;
+
+/* error handling section */
+
+#ifdef CONFIG_KDS
+ failed_kds_setup:
+
+	/* lock before we unmap */
+	kbase_gpu_vm_lock(katom->kctx);
+#endif				/* CONFIG_KDS */
+
+ failed_loop:
+	/* undo the loop work */
+	while (res_no-- > 0) {
+		struct kbase_mem_phy_alloc *alloc = katom->extres[res_no].alloc;
+#ifdef CONFIG_DMA_SHARED_BUFFER
+		if (alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM) {
+			alloc->imported.umm.current_mapping_usage_count--;
+
+			if (0 == alloc->imported.umm.current_mapping_usage_count) {
+				struct kbase_va_region *reg;
+				reg = kbase_region_tracker_find_region_base_address(
+				          katom->kctx, katom->extres[res_no].gpu_address);
+
+				if (reg && reg->alloc == alloc) {
+					kbase_mmu_teardown_pages(katom->kctx, reg->start_pfn,
+					    kbase_reg_current_backed_size(reg));
+				}
+
+				kbase_jd_umm_unmap(katom->kctx, alloc);
+			}
+		}
+#endif				/* CONFIG_DMA_SHARED_BUFFER */
+		kbase_mem_phy_alloc_put(alloc);
+	}
+	kbase_gpu_vm_unlock(katom->kctx);
+
+ early_err_out:
+	kfree(katom->extres);
+	katom->extres = NULL;
+#ifdef CONFIG_KDS
+	kfree(kds_resources);
+	kfree(kds_access_bitmap);
+#endif				/* CONFIG_KDS */
+	return err_ret_val;
+}
+
+STATIC INLINE void jd_resolve_dep(struct list_head *out_list, kbase_jd_atom *katom, u8 d)
+{
+	u8 other_d = !d;
+
+	while (!list_empty(&katom->dep_head[d])) {
+		kbase_jd_atom *dep_atom = list_entry(katom->dep_head[d].next, kbase_jd_atom, dep_item[d]);
+		list_del(katom->dep_head[d].next);
+
+		kbase_jd_katom_dep_clear(&dep_atom->dep[d]);
+		
+		if (katom->event_code != BASE_JD_EVENT_DONE) {
+			/* Atom failed, so remove the other dependencies and immediately fail the atom */
+			if (kbase_jd_katom_dep_atom(&dep_atom->dep[other_d])) {
+				list_del(&dep_atom->dep_item[other_d]);
+				kbase_jd_katom_dep_clear(&dep_atom->dep[other_d]);
+			}
+#ifdef CONFIG_KDS
+			if (!dep_atom->kds_dep_satisfied) {
+				/* Just set kds_dep_satisfied to true. If the callback happens after this then it will early out and
+				 * do nothing. If the callback doesn't happen then kbase_jd_post_external_resources will clean up
+				 */
+				dep_atom->kds_dep_satisfied = MALI_TRUE;
+			}
+#endif
+
+			/* at this point a dependency to the failed job is already removed */
+			if ( !( kbase_jd_katom_dep_type(&dep_atom->dep[d]) == BASE_JD_DEP_TYPE_ORDER &&
+					katom->event_code > BASE_JD_EVENT_ACTIVE) )
+			{
+				dep_atom->event_code = katom->event_code;
+				KBASE_DEBUG_ASSERT(dep_atom->status != KBASE_JD_ATOM_STATE_UNUSED);
+				dep_atom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+			}
+
+			list_add_tail(&dep_atom->dep_item[0], out_list);
+		} else if (!kbase_jd_katom_dep_atom(&dep_atom->dep[other_d])) {
+#ifdef CONFIG_KDS
+			if (dep_atom->kds_dep_satisfied)
+#endif
+				list_add_tail(&dep_atom->dep_item[0], out_list);
+		}
+	}
+}
+
+KBASE_EXPORT_TEST_API(jd_resolve_dep)
+
+#if MALI_CUSTOMER_RELEASE == 0
+static void jd_force_failure(kbase_device *kbdev, kbase_jd_atom *katom)
+{
+	kbdev->force_replay_count++;
+
+	if (kbdev->force_replay_count >= kbdev->force_replay_limit) {
+		kbdev->force_replay_count = 0;
+		katom->event_code = BASE_JD_EVENT_DATA_INVALID_FAULT;
+
+		if (kbdev->force_replay_random)
+			kbdev->force_replay_limit =
+			   (prandom_u32() % KBASEP_FORCE_REPLAY_RANDOM_LIMIT) + 1;
+
+		dev_info(kbdev->dev, "force_replay : promoting to error\n");
+	}
+}
+
+/** Test to see if atom should be forced to fail.
+ *
+ * This function will check if an atom has a replay job as a dependent. If so
+ * then it will be considered for forced failure. */
+static void jd_check_force_failure(kbase_jd_atom *katom)
+{
+	struct kbase_context *kctx = katom->kctx;
+	kbase_device *kbdev = kctx->kbdev;
+	int i;
+	if ((kbdev->force_replay_limit == KBASEP_FORCE_REPLAY_DISABLED) ||
+	    (katom->core_req & BASEP_JD_REQ_EVENT_NEVER))
+		return;
+	for (i = 1; i < BASE_JD_ATOM_COUNT; i++) {
+		if (kbase_jd_katom_dep_atom(&kctx->jctx.atoms[i].dep[0]) == katom ||
+		    kbase_jd_katom_dep_atom(&kctx->jctx.atoms[i].dep[1]) == katom) {
+			kbase_jd_atom *dep_atom = &kctx->jctx.atoms[i];
+
+			if ((dep_atom->core_req & BASEP_JD_REQ_ATOM_TYPE) ==
+						     BASE_JD_REQ_SOFT_REPLAY &&
+			    (dep_atom->core_req & kbdev->force_replay_core_req)
+					     == kbdev->force_replay_core_req) {
+				jd_force_failure(kbdev, katom);
+				return;
+			}
+		}
+	}
+}
+#endif
+
+static mali_bool jd_replay(kbase_jd_atom *katom)
+{
+	int status = kbase_replay_process(katom);
+
+	if ((status & MALI_REPLAY_STATUS_MASK) ==
+						MALI_REPLAY_STATUS_REPLAYING) {
+		if (status & MALI_REPLAY_FLAG_JS_RESCHED)
+			return MALI_TRUE;
+	}
+	return MALI_FALSE;
+}
+
+/*
+ * Perform the necessary handling of an atom that has finished running
+ * on the GPU.
+ *
+ * Note that if this is a soft-job that has had kbase_prepare_soft_job called on it then the caller
+ * is responsible for calling kbase_finish_soft_job *before* calling this function.
+ *
+ * The caller must hold the kbase_jd_context.lock.
+ */
+mali_bool jd_done_nolock(kbase_jd_atom *katom)
+{
+	struct kbase_context *kctx = katom->kctx;
+	kbase_device *kbdev = kctx->kbdev;
+	struct list_head completed_jobs;
+	struct list_head runnable_jobs;
+	mali_bool need_to_try_schedule_context = MALI_FALSE;
+	int i;
+
+	INIT_LIST_HEAD(&completed_jobs);
+	INIT_LIST_HEAD(&runnable_jobs);
+
+	KBASE_DEBUG_ASSERT(katom->status != KBASE_JD_ATOM_STATE_UNUSED);
+
+#if MALI_CUSTOMER_RELEASE == 0
+	jd_check_force_failure(katom);
+#endif
+
+
+	/* This is needed in case an atom is failed due to being invalid, this
+	 * can happen *before* the jobs that the atom depends on have completed */
+	for (i = 0; i < 2; i++) {
+		if ( kbase_jd_katom_dep_atom(&katom->dep[i])) {
+			list_del(&katom->dep_item[i]);
+			kbase_jd_katom_dep_clear(&katom->dep[i]);
+		}
+	}
+
+	/* With PRLAM-10817 or PRLAM-10959 the last tile of a fragment job being soft-stopped can fail with
+	 * BASE_JD_EVENT_TILE_RANGE_FAULT.
+	 *
+	 * So here if the fragment job failed with TILE_RANGE_FAULT and it has been soft-stopped, then we promote the
+	 * error code to BASE_JD_EVENT_DONE
+	 */
+
+	if ((kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10817) || kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10959)) &&
+		  katom->event_code == BASE_JD_EVENT_TILE_RANGE_FAULT) {
+		if ( ( katom->core_req & BASE_JD_REQ_FS ) && (katom->atom_flags & KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED) ) {
+			/* Promote the failure to job done */
+			katom->event_code = BASE_JD_EVENT_DONE;
+			katom->atom_flags = katom->atom_flags & (~KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED);
+		}
+	}
+
+	katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+	list_add_tail(&katom->dep_item[0], &completed_jobs);
+
+	while (!list_empty(&completed_jobs)) {
+		katom = list_entry(completed_jobs.prev, kbase_jd_atom, dep_item[0]);
+		list_del(completed_jobs.prev);
+
+		KBASE_DEBUG_ASSERT(katom->status == KBASE_JD_ATOM_STATE_COMPLETED);
+
+		for (i = 0; i < 2; i++)
+			jd_resolve_dep(&runnable_jobs, katom, i);
+
+		if (katom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES)
+			kbase_jd_post_external_resources(katom);
+
+		while (!list_empty(&runnable_jobs)) {
+			kbase_jd_atom *node = list_entry(runnable_jobs.prev, kbase_jd_atom, dep_item[0]);
+			list_del(runnable_jobs.prev);
+
+			KBASE_DEBUG_ASSERT(node->status != KBASE_JD_ATOM_STATE_UNUSED);
+
+			if (katom->event_code == BASE_JD_EVENT_DONE) {
+				need_to_try_schedule_context |= jd_run_atom(node);
+			} else {
+				node->event_code = katom->event_code;
+				node->status = KBASE_JD_ATOM_STATE_COMPLETED;
+
+				if ((node->core_req & BASEP_JD_REQ_ATOM_TYPE)
+						  == BASE_JD_REQ_SOFT_REPLAY) {
+					need_to_try_schedule_context |=
+							       jd_replay(node);
+				} else if (node->core_req &
+							BASE_JD_REQ_SOFT_JOB) {
+					kbase_finish_soft_job(node);
+				}
+			}
+
+			if (node->status == KBASE_JD_ATOM_STATE_COMPLETED)
+				list_add_tail(&node->dep_item[0], &completed_jobs);
+		}
+
+		kbase_event_post(kctx, katom);
+
+		/* Decrement and check the TOTAL number of jobs. This includes
+		 * those not tracked by the scheduler: 'not ready to run' and
+		 * 'dependency-only' jobs. */
+		if (--kctx->jctx.job_nr == 0)
+			wake_up(&kctx->jctx.zero_jobs_wait);	/* All events are safely queued now, and we can signal any waiter
+								 * that we've got no more jobs (so we can be safely terminated) */
+	}
+
+	return need_to_try_schedule_context;
+}
+
+KBASE_EXPORT_TEST_API(jd_done_nolock)
+
+#ifdef CONFIG_GPU_TRACEPOINTS
+enum {
+	CORE_REQ_DEP_ONLY,
+	CORE_REQ_SOFT,
+	CORE_REQ_COMPUTE,
+	CORE_REQ_FRAGMENT,
+	CORE_REQ_VERTEX,
+	CORE_REQ_TILER,
+	CORE_REQ_FRAGMENT_VERTEX,
+	CORE_REQ_FRAGMENT_VERTEX_TILER,
+	CORE_REQ_FRAGMENT_TILER,
+	CORE_REQ_VERTEX_TILER,
+	CORE_REQ_UNKNOWN
+};
+static const char * const core_req_strings[] = {
+	"Dependency Only Job",
+	"Soft Job",
+	"Compute Shader Job",
+	"Fragment Shader Job",
+	"Vertex/Geometry Shader Job",
+	"Tiler Job",
+	"Fragment Shader + Vertex/Geometry Shader Job",
+	"Fragment Shader + Vertex/Geometry Shader Job + Tiler Job",
+	"Fragment Shader + Tiler Job",
+	"Vertex/Geometry Shader Job + Tiler Job",
+	"Unknown Job"
+};
+static const char *kbasep_map_core_reqs_to_string(base_jd_core_req core_req)
+{
+	if (core_req & BASE_JD_REQ_SOFT_JOB)
+		return core_req_strings[CORE_REQ_SOFT];
+	if (core_req & BASE_JD_REQ_ONLY_COMPUTE)
+		return core_req_strings[CORE_REQ_COMPUTE];
+	switch (core_req & (BASE_JD_REQ_FS | BASE_JD_REQ_CS | BASE_JD_REQ_T)) {
+	case BASE_JD_REQ_DEP:
+		return core_req_strings[CORE_REQ_DEP_ONLY];
+	case BASE_JD_REQ_FS:
+		return core_req_strings[CORE_REQ_FRAGMENT];
+	case BASE_JD_REQ_CS:
+		return core_req_strings[CORE_REQ_VERTEX];
+	case BASE_JD_REQ_T:
+		return core_req_strings[CORE_REQ_TILER];
+	case (BASE_JD_REQ_FS | BASE_JD_REQ_CS):
+		return core_req_strings[CORE_REQ_FRAGMENT_VERTEX];
+	case (BASE_JD_REQ_FS | BASE_JD_REQ_T):
+		return core_req_strings[CORE_REQ_FRAGMENT_TILER];
+	case (BASE_JD_REQ_CS | BASE_JD_REQ_T):
+		return core_req_strings[CORE_REQ_VERTEX_TILER];
+	case (BASE_JD_REQ_FS | BASE_JD_REQ_CS | BASE_JD_REQ_T):
+		return core_req_strings[CORE_REQ_FRAGMENT_VERTEX_TILER];
+	}
+	return core_req_strings[CORE_REQ_UNKNOWN];
+}
+#endif
+
+mali_bool jd_submit_atom(kbase_context *kctx,
+			 const base_jd_atom_v2 *user_atom,
+			 kbase_jd_atom *katom)
+{
+	kbase_jd_context *jctx = &kctx->jctx;
+	base_jd_core_req core_req;
+	int queued = 0;
+	int i;
+	mali_bool ret;
+
+	/* Update the TOTAL number of jobs. This includes those not tracked by
+	 * the scheduler: 'not ready to run' and 'dependency-only' jobs. */
+	jctx->job_nr++;
+
+	core_req = user_atom->core_req;
+
+	katom->udata = user_atom->udata;
+	katom->kctx = kctx;
+	katom->nr_extres = user_atom->nr_extres;
+	katom->extres = NULL;
+	katom->device_nr = user_atom->device_nr;
+
+	katom->affinity = 0;
+	katom->jc = user_atom->jc;
+	katom->coreref_state = KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED;
+	katom->core_req = core_req;
+	katom->nice_prio = user_atom->prio;
+	katom->atom_flags = 0;
+	katom->retry_count = 0;
+
+	
+#ifdef CONFIG_KDS
+	/* Start by assuming that the KDS dependencies are satisfied,
+	 * kbase_jd_pre_external_resources will correct this if there are dependencies */
+	katom->kds_dep_satisfied = MALI_TRUE;
+	katom->kds_rset = NULL;
+#endif				/* CONFIG_KDS */
+
+
+	/* Don't do anything if there is a mess up with dependencies.
+	   This is done in a separate cycle to check both the dependencies at ones, otherwise 
+	   it will be extra complexity to deal with 1st dependency ( just added to the list )
+	   if only the 2nd one has invalid config.
+	 */
+	for (i = 0; i < 2; i++) {
+		int dep_atom_number = user_atom->pre_dep[i].atom_id;
+		base_jd_dep_type dep_atom_type = user_atom->pre_dep[i].dependency_type;
+
+		if (dep_atom_number) {
+			if ( dep_atom_type != BASE_JD_DEP_TYPE_ORDER && dep_atom_type != BASE_JD_DEP_TYPE_DATA )
+			{
+				katom->event_code = BASE_JD_EVENT_JOB_CONFIG_FAULT;
+				katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+				ret = jd_done_nolock(katom);
+				goto out;
+			}
+		}
+	}
+	
+	/* Add dependencies */
+	for (i = 0; i < 2; i++) {
+		int dep_atom_number = user_atom->pre_dep[i].atom_id;
+		base_jd_dep_type dep_atom_type = user_atom->pre_dep[i].dependency_type;
+
+		kbase_jd_katom_dep_clear(&katom->dep[i]);
+
+		if (dep_atom_number) {
+			kbase_jd_atom *dep_atom = &jctx->atoms[dep_atom_number];
+
+			if (dep_atom->status == KBASE_JD_ATOM_STATE_UNUSED || dep_atom->status == KBASE_JD_ATOM_STATE_COMPLETED) {
+				if (dep_atom->event_code != BASE_JD_EVENT_DONE) {
+					/* don't stop this atom if it has an order dependency only to the failed one,
+					 try to submit it throught the normal path */
+					if ( dep_atom_type == BASE_JD_DEP_TYPE_ORDER &&
+							dep_atom->event_code > BASE_JD_EVENT_ACTIVE) {
+						continue;
+					}
+
+					if (i == 1 && kbase_jd_katom_dep_atom(&katom->dep[0])) {
+						/* Remove the previous dependency */
+						list_del(&katom->dep_item[0]);
+						kbase_jd_katom_dep_clear(&katom->dep[0]);
+					}
+					
+					/* Atom has completed, propagate the error code if any */
+					katom->event_code = dep_atom->event_code;
+					katom->status = KBASE_JD_ATOM_STATE_QUEUED;
+					if ((katom->core_req & 
+							BASEP_JD_REQ_ATOM_TYPE)
+						  == BASE_JD_REQ_SOFT_REPLAY) {
+						int status =
+						   kbase_replay_process(katom);
+
+						if ((status &
+						       MALI_REPLAY_STATUS_MASK)
+					     == MALI_REPLAY_STATUS_REPLAYING) {
+							ret = (status &
+						  MALI_REPLAY_FLAG_JS_RESCHED);
+							goto out;
+						}
+					}					
+					ret = jd_done_nolock(katom);
+					
+					goto out;
+				}
+			} else {
+				/* Atom is in progress, add this atom to the list */
+				list_add_tail(&katom->dep_item[i], &dep_atom->dep_head[i]);
+				kbase_jd_katom_dep_set(&katom->dep[i], dep_atom, dep_atom_type);
+				queued = 1;
+			}
+		}
+	}
+
+	/* These must occur after the above loop to ensure that an atom that
+	 * depends on a previous atom with the same number behaves as expected */
+	katom->event_code = BASE_JD_EVENT_DONE;
+	katom->status = KBASE_JD_ATOM_STATE_QUEUED;
+
+	/* Reject atoms with job chain = NULL, as these cause issues with soft-stop */
+	if (0 == katom->jc && (katom->core_req & BASEP_JD_REQ_ATOM_TYPE) != BASE_JD_REQ_DEP) {
+		dev_warn(kctx->kbdev->dev, "Rejecting atom with jc = NULL");
+		katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+		ret = jd_done_nolock(katom);
+		goto out;
+	}
+
+	/* Reject atoms with an invalid device_nr */
+	if ((katom->core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) &&
+	    (katom->device_nr >= kctx->kbdev->gpu_props.num_core_groups)) {
+		dev_warn(kctx->kbdev->dev, "Rejecting atom with invalid device_nr %d", katom->device_nr);
+		katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+		ret = jd_done_nolock(katom);
+		goto out;
+	}
+
+	/*
+	 * If the priority is increased we need to check the caller has security caps to do this, if
+	 * priority is decreased then this is ok as the result will have no negative impact on other
+	 * processes running.
+	 */
+	if (0 > katom->nice_prio) {
+		mali_bool access_allowed;
+		access_allowed = kbase_security_has_capability(kctx, KBASE_SEC_MODIFY_PRIORITY, KBASE_SEC_FLAG_NOAUDIT);
+		if (!access_allowed) {
+			/* For unprivileged processes - a negative priority is interpreted as zero */
+			katom->nice_prio = 0;
+		}
+	}
+
+	/* Scale priority range to use NICE range */
+	if (katom->nice_prio) {
+		/* Remove sign for calculation */
+		int nice_priority = katom->nice_prio + 128;
+		/* Fixed point maths to scale from ..255 to 0..39 (NICE range with +20 offset) */
+		katom->nice_prio = (((20 << 16) / 128) * nice_priority) >> 16;
+	}
+
+	if (katom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES) {
+		/* handle what we need to do to access the external resources */
+		if (MALI_ERROR_NONE != kbase_jd_pre_external_resources(katom, user_atom)) {
+			/* setup failed (no access, bad resource, unknown resource types, etc.) */
+			katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+			ret = jd_done_nolock(katom);
+			goto out;
+		}
+	}
+
+	/* Initialize the jobscheduler policy for this atom. Function will
+	 * return error if the atom is malformed.
+	 *
+	 * Soft-jobs never enter the job scheduler but have their own initialize method.
+	 *
+	 * If either fail then we immediately complete the atom with an error.
+	 */
+	if ((katom->core_req & BASE_JD_REQ_SOFT_JOB) == 0) {
+		kbasep_js_policy *js_policy = &(kctx->kbdev->js_data.policy);
+		if (MALI_ERROR_NONE != kbasep_js_policy_init_job(js_policy, kctx, katom)) {
+			katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+			ret = jd_done_nolock(katom);
+			goto out;
+		}
+	} else {
+		/* Soft-job */
+		if (MALI_ERROR_NONE != kbase_prepare_soft_job(katom)) {
+			katom->event_code = BASE_JD_EVENT_JOB_INVALID;
+			ret = jd_done_nolock(katom);
+			goto out;
+		}
+	}
+
+#ifdef CONFIG_GPU_TRACEPOINTS
+	katom->work_id = atomic_inc_return(&jctx->work_id);
+	trace_gpu_job_enqueue((u32)kctx, katom->work_id, kbasep_map_core_reqs_to_string(katom->core_req));
+#endif
+
+	if (queued) {
+		ret = MALI_FALSE;
+		goto out;
+	}
+#ifdef CONFIG_KDS
+	if (!katom->kds_dep_satisfied) {
+		/* Queue atom due to KDS dependency */
+		ret = MALI_FALSE;
+		goto out;
+	}
+#endif				/* CONFIG_KDS */
+
+	if ((katom->core_req & BASEP_JD_REQ_ATOM_TYPE)
+						  == BASE_JD_REQ_SOFT_REPLAY) {
+		int status = kbase_replay_process(katom);
+
+		if ((status & MALI_REPLAY_STATUS_MASK)
+					       == MALI_REPLAY_STATUS_REPLAYING)
+			ret = status & MALI_REPLAY_FLAG_JS_RESCHED;
+		else
+			ret = jd_done_nolock(katom);
+
+		goto out;
+	} else if (katom->core_req & BASE_JD_REQ_SOFT_JOB) {
+		if (kbase_process_soft_job(katom) == 0) {
+			kbase_finish_soft_job(katom);
+			ret = jd_done_nolock(katom);
+			goto out;
+		}
+		/* The job has not yet completed */
+		list_add_tail(&katom->dep_item[0], &kctx->waiting_soft_jobs);
+		ret = MALI_FALSE;
+	} else if ((katom->core_req & BASEP_JD_REQ_ATOM_TYPE) != BASE_JD_REQ_DEP) {
+		katom->status = KBASE_JD_ATOM_STATE_IN_JS;
+		ret = kbasep_js_add_job(kctx, katom);
+	} else {
+		/* This is a pure dependency. Resolve it immediately */
+		ret = jd_done_nolock(katom);
+	}
+
+ out:
+	return ret;
+}
+
+mali_error kbase_jd_submit(kbase_context *kctx, const kbase_uk_job_submit *submit_data)
+{
+	kbase_jd_context *jctx = &kctx->jctx;
+	mali_error err = MALI_ERROR_NONE;
+	int i;
+	mali_bool need_to_try_schedule_context = MALI_FALSE;
+	kbase_device *kbdev;
+	void *user_addr;
+
+	/*
+	 * kbase_jd_submit isn't expected to fail and so all errors with the jobs
+	 * are reported by immediately falling them (through event system)
+	 */
+	kbdev = kctx->kbdev;
+
+	beenthere(kctx,"%s", "Enter");
+
+	if ((kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_SUBMIT_DISABLED) != 0) {
+		dev_err(kbdev->dev, "Attempt to submit to a context that has SUBMIT_DISABLED set on it");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	if (submit_data->stride != sizeof(base_jd_atom_v2)) {
+		dev_err(kbdev->dev, "Stride passed to job_submit doesn't match kernel");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	user_addr = get_compat_pointer(&submit_data->addr);
+
+	KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_add_return(submit_data->nr_atoms, &kctx->timeline.jd_atoms_in_flight));
+
+	for (i = 0; i < submit_data->nr_atoms; i++) {
+		base_jd_atom_v2 user_atom;
+		kbase_jd_atom *katom;
+
+		if (copy_from_user(&user_atom, user_addr, sizeof(user_atom)) != 0) {
+			err = MALI_ERROR_FUNCTION_FAILED;
+			KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, atomic_sub_return(submit_data->nr_atoms - i, &kctx->timeline.jd_atoms_in_flight));
+			break;
+		}
+
+		user_addr = (void *)((uintptr_t) user_addr + submit_data->stride);
+
+		mutex_lock(&jctx->lock);
+		katom = &jctx->atoms[user_atom.atom_number];
+
+		while (katom->status != KBASE_JD_ATOM_STATE_UNUSED) {
+			/* Atom number is already in use, wait for the atom to
+			 * complete
+			 */
+			mutex_unlock(&jctx->lock);
+		
+			/* This thread will wait for the atom to complete. Due
+			 * to thread scheduling we are not sure that the other
+			 * thread that owns the atom will also schedule the
+			 * context, so we force the scheduler to be active and
+			 * hence eventually schedule this context at some point
+			 * later.
+			 */
+			kbasep_js_try_schedule_head_ctx(kctx->kbdev);
+			if (wait_event_killable(katom->completed,
+				katom->status == KBASE_JD_ATOM_STATE_UNUSED)) {
+				/* We're being killed so the result code
+				 * doesn't really matter
+				 */
+				return MALI_ERROR_NONE;
+			}
+			mutex_lock(&jctx->lock);
+		}
+
+		need_to_try_schedule_context |=
+				       jd_submit_atom(kctx, &user_atom, katom);
+		mutex_unlock(&jctx->lock);
+	}
+
+	if (need_to_try_schedule_context)
+		kbasep_js_try_schedule_head_ctx(kbdev);
+
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_jd_submit)
+
+static void kbasep_jd_cacheclean(kbase_device *kbdev)
+{
+	/* Limit the number of loops to avoid a hang if the interrupt is missed */
+	u32 max_loops = KBASE_CLEAN_CACHE_MAX_LOOPS;
+
+	mutex_lock(&kbdev->cacheclean_lock);
+
+	/* use GPU_COMMAND completion solution */
+	/* clean & invalidate the caches */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_CLEAN_INV_CACHES, NULL, NULL, 0u, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CLEAN_INV_CACHES, NULL);
+
+	/* wait for cache flush to complete before continuing */
+	while (--max_loops && (kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL) & CLEAN_CACHES_COMPLETED) == 0)
+		;
+
+	/* clear the CLEAN_CACHES_COMPLETED irq */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_IRQ_CLEAR, NULL, NULL, 0u, CLEAN_CACHES_COMPLETED);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), CLEAN_CACHES_COMPLETED, NULL);
+	KBASE_DEBUG_ASSERT_MSG(kbdev->hwcnt.state != KBASE_INSTR_STATE_CLEANING,
+	    "Instrumentation code was cleaning caches, but Job Management code cleared their IRQ - Instrumentation code will now hang.");
+
+	mutex_unlock(&kbdev->cacheclean_lock);
+}
+
+/**
+ * This function:
+ * - requeues the job from the runpool (if it was soft-stopped/removed from NEXT registers)
+ * - removes it from the system if it finished/failed/was cancelled.
+ * - resolves dependencies to add dependent jobs to the context, potentially starting them if necessary (which may add more references to the context)
+ * - releases the reference to the context from the no-longer-running job.
+ * - Handles retrying submission outside of IRQ context if it failed from within IRQ context.
+ */
+static void jd_done_worker(struct work_struct *data)
+{
+	kbase_jd_atom *katom = container_of(data, kbase_jd_atom, work);
+	kbase_jd_context *jctx;
+	kbase_context *kctx;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_policy *js_policy;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	u64 cache_jc = katom->jc;
+	kbasep_js_atom_retained_state katom_retained_state;
+
+	/* Soft jobs should never reach this function */
+	KBASE_DEBUG_ASSERT((katom->core_req & BASE_JD_REQ_SOFT_JOB) == 0);
+
+	kctx = katom->kctx;
+	jctx = &kctx->jctx;
+	kbdev = kctx->kbdev;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+
+	KBASE_TRACE_ADD(kbdev, JD_DONE_WORKER, kctx, katom, katom->jc, 0);
+	/*
+	 * Begin transaction on JD context and JS context
+	 */
+	mutex_lock(&jctx->lock);
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+
+	/* This worker only gets called on contexts that are scheduled *in*. This is
+	 * because it only happens in response to an IRQ from a job that was
+	 * running.
+	 */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled != MALI_FALSE);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_6787) && katom->event_code != BASE_JD_EVENT_DONE && !(katom->event_code & BASE_JD_SW_EVENT))
+		kbasep_jd_cacheclean(kbdev);  /* cache flush when jobs complete with non-done codes */
+	else if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10676)) {
+		if (kbdev->gpu_props.num_core_groups > 1 && 
+		    !(katom->affinity & kbdev->gpu_props.props.coherency_info.group[0].core_mask) &&
+		    (katom->affinity & kbdev->gpu_props.props.coherency_info.group[1].core_mask)) {
+			dev_dbg(kbdev->dev, "JD: Flushing cache due to PRLAM-10676\n");
+			kbasep_jd_cacheclean(kbdev);
+		}
+	}
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10969)            &&
+	    (katom->core_req & BASE_JD_REQ_FS)                        &&
+	    katom->event_code == BASE_JD_EVENT_TILE_RANGE_FAULT       &&
+	    (katom->atom_flags & KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED) &&
+	    !(katom->atom_flags & KBASE_KATOM_FLAGS_RERUN)){
+		dev_dbg(kbdev->dev,
+				       "Soft-stopped fragment shader job got a TILE_RANGE_FAULT." \
+				       "Possible HW issue, trying SW workaround\n" );
+		if (kbasep_10969_workaround_clamp_coordinates(katom)){
+			/* The job had a TILE_RANGE_FAULT after was soft-stopped.
+			 * Due to an HW issue we try to execute the job
+			 * again.
+			 */
+			dev_dbg(kbdev->dev, "Clamping has been executed, try to rerun the job\n" );
+			katom->event_code = BASE_JD_EVENT_STOPPED;
+			katom->atom_flags |= KBASE_KATOM_FLAGS_RERUN;
+
+			/* The atom will be requeued, but requeing does not submit more
+			 * jobs. If this was the last job, we must also ensure that more
+			 * jobs will be run on slot 0 - this is a Fragment job. */
+			kbasep_js_set_job_retry_submit_slot(katom, 0);
+		}
+	}
+
+	/* If job was rejected due to BASE_JD_EVENT_PM_EVENT but was not
+	 * specifically targeting core group 1, then re-submit targeting core
+	 * group 0 */
+	if (katom->event_code == BASE_JD_EVENT_PM_EVENT && !(katom->core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP)) {
+		katom->event_code = BASE_JD_EVENT_STOPPED;
+		/* Don't need to worry about any previously set retry-slot - it's
+		 * impossible for it to have been set previously, because we guarantee
+		 * kbase_jd_done() was called with done_code==0 on this atom */
+		kbasep_js_set_job_retry_submit_slot(katom, 1);
+	}
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316))
+		kbase_as_poking_timer_release_atom(kbdev, kctx, katom);
+
+	/* Release cores this job was using (this might power down unused cores, and
+	 * cause extra latency if a job submitted here - such as depenedent jobs -
+	 * would use those cores) */
+	kbasep_js_job_check_deref_cores(kbdev, katom);
+
+	/* Retain state before the katom disappears */
+	kbasep_js_atom_retained_state_copy(&katom_retained_state, katom);
+
+	if (!kbasep_js_has_atom_finished(&katom_retained_state)) {
+		unsigned long flags;
+		/* Requeue the atom on soft-stop / removed from NEXT registers */
+		dev_dbg(kbdev->dev, "JS: Soft Stopped/Removed from next on Ctx %p; Requeuing", kctx);
+
+		mutex_lock(&js_devdata->runpool_mutex);
+		kbasep_js_clear_job_retry_submit(katom);
+
+		KBASE_TIMELINE_ATOM_READY(kctx, kbase_jd_atom_id(kctx, katom));
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+		kbasep_js_policy_enqueue_job(js_policy, katom);
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+		/* A STOPPED/REMOVED job must cause a re-submit to happen, in case it
+		 * was the last job left. Crucially, work items on work queues can run
+		 * out of order e.g. on different CPUs, so being able to submit from
+		 * the IRQ handler is not a good indication that we don't need to run
+		 * jobs; the submitted job could be processed on the work-queue
+		 * *before* the stopped job, even though it was submitted after. */
+		{
+			int tmp;
+			KBASE_DEBUG_ASSERT(kbasep_js_get_atom_retry_submit_slot(&katom_retained_state, &tmp) != MALI_FALSE);
+			CSTD_UNUSED(tmp);
+		}
+
+		mutex_unlock(&js_devdata->runpool_mutex);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+	} else {
+		/* Remove the job from the system for all other reasons */
+		mali_bool need_to_try_schedule_context;
+
+		kbasep_js_remove_job(kbdev, kctx, katom);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+		/* jd_done_nolock() requires the jsctx_mutex lock to be dropped */
+
+		need_to_try_schedule_context = jd_done_nolock(katom);
+
+		/* This ctx is already scheduled in, so return value guarenteed FALSE */
+		KBASE_DEBUG_ASSERT(need_to_try_schedule_context == MALI_FALSE);
+	}
+	/* katom may have been freed now, do not use! */
+
+	/*
+	 * Transaction complete
+	 */
+	mutex_unlock(&jctx->lock);
+
+	/* Job is now no longer running, so can now safely release the context
+	 * reference, and handle any actions that were logged against the atom's retained state */
+	kbasep_js_runpool_release_ctx_and_katom_retained_state(kbdev, kctx, &katom_retained_state);
+
+	KBASE_TRACE_ADD(kbdev, JD_DONE_WORKER_END, kctx, NULL, cache_jc, 0);
+}
+
+/**
+ * Work queue job cancel function
+ * Only called as part of 'Zapping' a context (which occurs on termination)
+ * Operates serially with the jd_done_worker() on the work queue.
+ *
+ * This can only be called on contexts that aren't scheduled.
+ *
+ * @note We don't need to release most of the resources that would occur on
+ * kbase_jd_done() or jd_done_worker(), because the atoms here must not be
+ * running (by virtue of only being called on contexts that aren't
+ * scheduled). The only resources that are an exception to this are:
+ * - those held by kbasep_js_job_check_ref_cores(), because these resources are
+ *   held for non-running atoms as well as running atoms.
+ */
+static void jd_cancel_worker(struct work_struct *data)
+{
+	kbase_jd_atom *katom = container_of(data, kbase_jd_atom, work);
+	kbase_jd_context *jctx;
+	kbase_context *kctx;
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool need_to_try_schedule_context;
+	kbase_device *kbdev;
+
+	/* Soft jobs should never reach this function */
+	KBASE_DEBUG_ASSERT((katom->core_req & BASE_JD_REQ_SOFT_JOB) == 0);
+
+	kctx = katom->kctx;
+	kbdev = kctx->kbdev;
+	jctx = &kctx->jctx;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	KBASE_TRACE_ADD(kbdev, JD_CANCEL_WORKER, kctx, katom, katom->jc, 0);
+
+	/* This only gets called on contexts that are scheduled out. Hence, we must
+	 * make sure we don't de-ref the number of running jobs (there aren't
+	 * any), nor must we try to schedule out the context (it's already
+	 * scheduled out).
+	 */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled == MALI_FALSE);
+
+	/* Release cores this job was using (this might power down unused cores) */
+	kbasep_js_job_check_deref_cores(kctx->kbdev, katom);
+
+	/* Scheduler: Remove the job from the system */
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	kbasep_js_remove_cancelled_job(kbdev, kctx, katom);
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+	mutex_lock(&jctx->lock);
+
+	need_to_try_schedule_context = jd_done_nolock(katom);
+	/* Because we're zapping, we're not adding any more jobs to this ctx, so no need to
+	 * schedule the context. There's also no need for the jsctx_mutex to have been taken
+	 * around this too. */
+	KBASE_DEBUG_ASSERT(need_to_try_schedule_context == MALI_FALSE);
+
+	/* katom may have been freed now, do not use! */
+	mutex_unlock(&jctx->lock);
+
+}
+
+/**
+ * @brief Complete a job that has been removed from the Hardware
+ *
+ * This must be used whenever a job has been removed from the Hardware, e.g.:
+ * - An IRQ indicates that the job finished (for both error and 'done' codes)
+ * - The job was evicted from the JSn_HEAD_NEXT registers during a Soft/Hard stop.
+ *
+ * Some work is carried out immediately, and the rest is deferred onto a workqueue
+ *
+ * This can be called safely from atomic context.
+ *
+ * The caller must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ */
+void kbase_jd_done(kbase_jd_atom *katom, int slot_nr, ktime_t *end_timestamp,
+                   kbasep_js_atom_done_code done_code)
+{
+	kbase_context *kctx;
+	kbase_device *kbdev;
+	KBASE_DEBUG_ASSERT(katom);
+	kctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(kctx);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	if (done_code & KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT)
+		katom->event_code = BASE_JD_EVENT_REMOVED_FROM_NEXT;
+
+	kbase_timeline_job_slot_done(kbdev, kctx, katom, slot_nr, done_code);
+
+	KBASE_TRACE_ADD(kbdev, JD_DONE, kctx, katom, katom->jc, 0);
+
+	kbasep_js_job_done_slot_irq(katom, slot_nr, end_timestamp, done_code);
+
+	katom->slot_nr = slot_nr;
+
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+	INIT_WORK(&katom->work, jd_done_worker);
+	queue_work(kctx->jctx.job_done_wq, &katom->work);
+}
+
+KBASE_EXPORT_TEST_API(kbase_jd_done)
+
+void kbase_jd_cancel(kbase_device *kbdev, kbase_jd_atom *katom)
+{
+	kbase_context *kctx;
+	kbasep_js_kctx_info *js_kctx_info;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	KBASE_DEBUG_ASSERT(NULL != katom);
+	kctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	KBASE_TRACE_ADD(kbdev, JD_CANCEL, kctx, katom, katom->jc, 0);
+
+	/* This should only be done from a context that is not scheduled */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled == MALI_FALSE);
+
+	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+	INIT_WORK(&katom->work, jd_cancel_worker);
+	queue_work(kctx->jctx.job_done_wq, &katom->work);
+}
+
+typedef struct zap_reset_data {
+	/* The stages are:
+	 * 1. The timer has never been called
+	 * 2. The zap has timed out, all slots are soft-stopped - the GPU reset will happen.
+	 *    The GPU has been reset when kbdev->reset_waitq is signalled
+	 *
+	 * (-1 - The timer has been cancelled)
+	 */
+	int stage;
+	kbase_device *kbdev;
+	struct hrtimer timer;
+	spinlock_t lock;
+} zap_reset_data;
+
+static enum hrtimer_restart zap_timeout_callback(struct hrtimer *timer)
+{
+	zap_reset_data *reset_data = container_of(timer, zap_reset_data, timer);
+	kbase_device *kbdev = reset_data->kbdev;
+	unsigned long flags;
+
+	spin_lock_irqsave(&reset_data->lock, flags);
+
+	if (reset_data->stage == -1)
+		goto out;
+
+	if (kbase_prepare_to_reset_gpu(kbdev)) {
+		dev_err(kbdev->dev, "Issueing GPU soft-reset because jobs failed to be killed (within %d ms) as part of context termination (e.g. process exit)\n", ZAP_TIMEOUT);
+		kbase_reset_gpu(kbdev);
+	}
+
+	reset_data->stage = 2;
+
+ out:
+	spin_unlock_irqrestore(&reset_data->lock, flags);
+
+	return HRTIMER_NORESTART;
+}
+
+void kbase_jd_zap_context(kbase_context *kctx)
+{
+	kbase_jd_atom *katom;
+	struct list_head *entry;
+	kbase_device *kbdev;
+	zap_reset_data reset_data;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kctx);
+
+	kbdev = kctx->kbdev;
+
+	KBASE_TRACE_ADD(kbdev, JD_ZAP_CONTEXT, kctx, NULL, 0u, 0u);
+	kbase_job_zap_context(kctx);
+
+	mutex_lock(&kctx->jctx.lock);
+
+	/*
+	 * While holding the kbase_jd_context lock clean up jobs which are known to kbase but are
+	 * queued outside the job scheduler.
+	 */
+
+	list_for_each( entry, &kctx->waiting_soft_jobs) {
+		katom = list_entry(entry, kbase_jd_atom, dep_item[0]);
+		kbase_cancel_soft_job(katom);
+	}
+	/* kctx->waiting_soft_jobs is not valid after this point */
+
+#ifdef CONFIG_KDS
+
+	/* For each job waiting on a kds resource, cancel the wait and force the job to
+	 * complete early, this is done so that we don't leave jobs outstanding waiting
+	 * on kds resources which may never be released when contexts are zapped, resulting
+	 * in a hang.
+	 *
+	 * Note that we can safely iterate over the list as the kbase_jd_context lock is held,
+	 * this prevents items being removed when calling job_done_nolock in kbase_cancel_kds_wait_job.
+	 */
+
+	list_for_each( entry, &kctx->waiting_kds_resource) {
+		katom = list_entry(entry, kbase_jd_atom, node);
+
+		kbase_cancel_kds_wait_job(katom);
+	}
+#endif
+
+	mutex_unlock(&kctx->jctx.lock);
+
+	hrtimer_init_on_stack(&reset_data.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	reset_data.timer.function = zap_timeout_callback;
+
+	spin_lock_init(&reset_data.lock);
+
+	reset_data.kbdev = kbdev;
+	reset_data.stage = 1;
+
+	hrtimer_start(&reset_data.timer, HR_TIMER_DELAY_MSEC(ZAP_TIMEOUT), HRTIMER_MODE_REL);
+
+	/* Wait for all jobs to finish, and for the context to be not-scheduled
+	 * (due to kbase_job_zap_context(), we also guarentee it's not in the JS
+	 * policy queue either */
+	wait_event(kctx->jctx.zero_jobs_wait, kctx->jctx.job_nr == 0);
+	wait_event(kctx->jctx.sched_info.ctx.is_scheduled_wait, kctx->jctx.sched_info.ctx.is_scheduled == MALI_FALSE);
+
+	spin_lock_irqsave(&reset_data.lock, flags);
+	if (reset_data.stage == 1) {
+		/* The timer hasn't run yet - so cancel it */
+		reset_data.stage = -1;
+	}
+	spin_unlock_irqrestore(&reset_data.lock, flags);
+
+	hrtimer_cancel(&reset_data.timer);
+
+	if (reset_data.stage == 2) {
+		/* The reset has already started.
+		 * Wait for the reset to complete
+		 */
+		wait_event(kbdev->reset_wait, atomic_read(&kbdev->reset_gpu) == KBASE_RESET_GPU_NOT_PENDING);
+	}
+	destroy_hrtimer_on_stack(&reset_data.timer);
+
+	dev_dbg(kbdev->dev, "Zap: Finished Context %p", kctx);
+
+	/* Ensure that the signallers of the waitqs have finished */
+	mutex_lock(&kctx->jctx.lock);
+	mutex_lock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
+	mutex_unlock(&kctx->jctx.sched_info.ctx.jsctx_mutex);
+	mutex_unlock(&kctx->jctx.lock);
+}
+
+KBASE_EXPORT_TEST_API(kbase_jd_zap_context)
+
+mali_error kbase_jd_init(kbase_context *kctx)
+{
+	int i;
+	mali_error mali_err = MALI_ERROR_NONE;
+#ifdef CONFIG_KDS
+	int err;
+#endif				/* CONFIG_KDS */
+
+	KBASE_DEBUG_ASSERT(kctx);
+
+	kctx->jctx.job_done_wq = alloc_workqueue("mali_jd", 0, 1);
+	if (NULL == kctx->jctx.job_done_wq) {
+		mali_err = MALI_ERROR_OUT_OF_MEMORY;
+		goto out1;
+	}
+
+	for (i = 0; i < BASE_JD_ATOM_COUNT; i++) {
+		init_waitqueue_head(&kctx->jctx.atoms[i].completed);
+
+		INIT_LIST_HEAD(&kctx->jctx.atoms[i].dep_head[0]);
+		INIT_LIST_HEAD(&kctx->jctx.atoms[i].dep_head[1]);
+
+		/* Catch userspace attempting to use an atom which doesn't exist as a pre-dependency */
+		kctx->jctx.atoms[i].event_code = BASE_JD_EVENT_JOB_INVALID;
+		kctx->jctx.atoms[i].status = KBASE_JD_ATOM_STATE_UNUSED;
+	}
+
+	mutex_init(&kctx->jctx.lock);
+
+	init_waitqueue_head(&kctx->jctx.zero_jobs_wait);
+
+	spin_lock_init(&kctx->jctx.tb_lock);
+
+#ifdef CONFIG_KDS
+	err = kds_callback_init(&kctx->jctx.kds_cb, 0, kds_dep_clear);
+	if (0 != err) {
+		mali_err = MALI_ERROR_FUNCTION_FAILED;
+		goto out2;
+	}
+#endif				/* CONFIG_KDS */
+
+	kctx->jctx.job_nr = 0;
+
+	return MALI_ERROR_NONE;
+
+#ifdef CONFIG_KDS
+ out2:
+	destroy_workqueue(kctx->jctx.job_done_wq);
+#endif				/* CONFIG_KDS */
+ out1:
+	return mali_err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_jd_init)
+
+void kbase_jd_exit(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx);
+
+#ifdef CONFIG_KDS
+	kds_callback_term(&kctx->jctx.kds_cb);
+#endif				/* CONFIG_KDS */
+	/* Work queue is emptied by this */
+	destroy_workqueue(kctx->jctx.job_done_wq);
+}
+
+KBASE_EXPORT_TEST_API(kbase_jd_exit)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.c
new file mode 100644
index 0000000..d3c6643
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.c
@@ -0,0 +1,1420 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_jm.c
+ * Base kernel job manager APIs
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_js_affinity.h>
+#include <mali_kbase_hw.h>
+
+#include "mali_kbase_jm.h"
+
+#define beenthere(kctx, f, a...)  dev_dbg(kctx->kbdev->dev, "%s:" f, __func__, ##a)
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+u64 mali_js0_affinity_mask = 0xFFFFFFFFFFFFFFFFULL;
+u64 mali_js1_affinity_mask = 0xFFFFFFFFFFFFFFFFULL;
+u64 mali_js2_affinity_mask = 0xFFFFFFFFFFFFFFFFULL;
+#endif
+
+
+static void kbasep_try_reset_gpu_early(kbase_device *kbdev);
+
+#ifdef CONFIG_GPU_TRACEPOINTS
+static char *kbasep_make_job_slot_string(int js, char *js_string)
+{
+	sprintf(js_string, "job_slot_%i", js);
+	return js_string;
+}
+#endif
+
+static void kbase_job_hw_submit(kbase_device *kbdev, kbase_jd_atom *katom, int js)
+{
+	kbase_context *kctx;
+	u32 cfg;
+	u64 jc_head = katom->jc;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(katom);
+
+	kctx = katom->kctx;
+
+	/* Command register must be available */
+	KBASE_DEBUG_ASSERT(kbasep_jm_is_js_free(kbdev, js, kctx));
+	/* Affinity is not violating */
+	kbase_js_debug_log_current_affinities(kbdev);
+	KBASE_DEBUG_ASSERT(!kbase_js_affinity_would_violate(kbdev, js, katom->affinity));
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), jc_head & 0xFFFFFFFF, kctx);
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), jc_head >> 32, kctx);
+
+#ifdef CONFIG_MALI_DEBUG_SHADER_SPLIT_FS
+	{
+		u64 mask;
+		u32 value;
+
+		if( 0 == js )
+		{
+			mask = mali_js0_affinity_mask;
+		}
+		else if( 1 == js )
+		{
+			mask = mali_js1_affinity_mask;
+		}
+		else
+		{
+			mask = mali_js2_affinity_mask;
+		}
+
+		value = katom->affinity & (mask & 0xFFFFFFFF);
+
+		kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_LO), value, kctx);
+
+		value = (katom->affinity >> 32) & ((mask>>32) & 0xFFFFFFFF);
+		kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_HI), value, kctx);
+	}
+#else
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_LO), katom->affinity & 0xFFFFFFFF, kctx);
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_AFFINITY_NEXT_HI), katom->affinity >> 32, kctx);
+#endif
+
+	/* start MMU, medium priority, cache clean/flush on end, clean/flush on start */
+	cfg = kctx->as_nr | JSn_CONFIG_END_FLUSH_CLEAN_INVALIDATE | JSn_CONFIG_START_MMU | JSn_CONFIG_START_FLUSH_CLEAN_INVALIDATE | JSn_CONFIG_THREAD_PRI(8);
+
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION)) {
+		if (!kbdev->jm_slots[js].job_chain_flag) {
+			cfg |= JSn_CONFIG_JOB_CHAIN_FLAG;
+			katom->atom_flags |= KBASE_KATOM_FLAGS_JOBCHAIN;
+			kbdev->jm_slots[js].job_chain_flag = MALI_TRUE;
+		} else {
+			katom->atom_flags &= ~KBASE_KATOM_FLAGS_JOBCHAIN;
+			kbdev->jm_slots[js].job_chain_flag = MALI_FALSE;
+		}
+	}
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_CONFIG_NEXT), cfg, kctx);
+
+	/* Write an approximate start timestamp.
+	 * It's approximate because there might be a job in the HEAD register. In
+	 * such cases, we'll try to make a better approximation in the IRQ handler
+	 * (up to the KBASE_JS_IRQ_THROTTLE_TIME_US). */
+	katom->start_timestamp = ktime_get();
+
+	/* GO ! */
+	dev_dbg(kbdev->dev, "JS: Submitting atom %p from ctx %p to js[%d] with head=0x%llx, affinity=0x%llx", katom, kctx, js, jc_head, katom->affinity);
+
+	KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_SUBMIT, kctx, katom, jc_head, js, (u32) katom->affinity);
+
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	kbase_trace_mali_job_slots_event(GATOR_MAKE_EVENT(GATOR_JOB_SLOT_START, js), kctx, kbase_jd_atom_id(kctx, katom)); 
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+#ifdef CONFIG_GPU_TRACEPOINTS
+	if (kbasep_jm_nr_jobs_submitted(&kbdev->jm_slots[js]) == 1)
+	{
+		/* If this is the only job on the slot, trace it as starting */
+		char js_string[16];
+		trace_gpu_sched_switch(kbasep_make_job_slot_string(js, js_string), ktime_to_ns(katom->start_timestamp), (u32)katom->kctx, 0, katom->work_id);
+		kbdev->jm_slots[js].last_context = katom->kctx;
+	}
+#endif
+	kbase_timeline_job_slot_submit(kbdev, kctx, katom, js);
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_START, katom->kctx);
+}
+
+void kbase_job_submit_nolock(kbase_device *kbdev, kbase_jd_atom *katom, int js)
+{
+	kbase_jm_slot *jm_slots;
+	base_jd_core_req core_req;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(katom);
+
+	jm_slots = kbdev->jm_slots;
+
+	core_req = katom->core_req;
+	if (core_req & BASE_JD_REQ_ONLY_COMPUTE) {
+		unsigned long flags;
+		int device_nr = (core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) ? katom->device_nr : 0;
+		KBASE_DEBUG_ASSERT(device_nr < 2);
+		spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+		kbasep_pm_record_job_status(kbdev);
+		kbdev->pm.metrics.active_cl_ctx[device_nr]++;
+		spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+	} else {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+		kbasep_pm_record_job_status(kbdev);
+		kbdev->pm.metrics.active_gl_ctx++;
+		spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+	}
+
+	/*
+	 * We can have:
+	 * - one job already done (pending interrupt),
+	 * - one running,
+	 * - one ready to be run.
+	 * Hence a maximum of 3 inflight jobs. We have a 4 job
+	 * queue, which I hope will be enough...
+	 */
+	kbasep_jm_enqueue_submit_slot(&jm_slots[js], katom);
+	kbase_job_hw_submit(kbdev, katom, js);
+}
+
+void kbase_job_done_slot(kbase_device *kbdev, int s, u32 completion_code, u64 job_tail, ktime_t *end_timestamp)
+{
+	kbase_jm_slot *slot;
+	kbase_jd_atom *katom;
+	mali_addr64 jc_head;
+	kbase_context *kctx;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	if (completion_code != BASE_JD_EVENT_DONE && completion_code != BASE_JD_EVENT_STOPPED)
+		dev_err(kbdev->dev, "t6xx: GPU fault 0x%02lx from job slot %d\n", (unsigned long)completion_code, s);
+
+	/* IMPORTANT: this function must only contain work necessary to complete a
+	 * job from a Real IRQ (and not 'fake' completion, e.g. from
+	 * Soft-stop). For general work that must happen no matter how the job was
+	 * removed from the hardware, place it in kbase_jd_done() */
+
+	slot = &kbdev->jm_slots[s];
+	katom = kbasep_jm_dequeue_submit_slot(slot);
+
+	/* If the katom completed is because it's a dummy job for HW workarounds, then take no further action */
+	if (kbasep_jm_is_dummy_workaround_job(kbdev, katom)) {
+		KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_JOB_DONE, NULL, NULL, 0, s, completion_code);
+		return;
+	}
+
+	jc_head = katom->jc;
+	kctx = katom->kctx;
+
+	KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_JOB_DONE, kctx, katom, jc_head, s, completion_code);
+
+	if (completion_code != BASE_JD_EVENT_DONE && completion_code != BASE_JD_EVENT_STOPPED) {
+
+#if KBASE_TRACE_DUMP_ON_JOB_SLOT_ERROR != 0
+		KBASE_TRACE_DUMP(kbdev);
+#endif
+	}
+	if (job_tail != 0) {
+		mali_bool was_updated = (job_tail != jc_head);
+		/* Some of the job has been executed, so we update the job chain address to where we should resume from */
+		katom->jc = job_tail;
+		if (was_updated)
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_UPDATE_HEAD, kctx, katom, job_tail, s);
+	}
+
+	/* Only update the event code for jobs that weren't cancelled */
+	if (katom->event_code != BASE_JD_EVENT_JOB_CANCELLED)
+		katom->event_code = (base_jd_event_code) completion_code;
+
+	kbase_device_trace_register_access(kctx, REG_WRITE, JOB_CONTROL_REG(JOB_IRQ_CLEAR), 1 << s);
+
+	/* Complete the job, and start new ones
+	 *
+	 * Also defer remaining work onto the workqueue:
+	 * - Re-queue Soft-stopped jobs
+	 * - For any other jobs, queue the job back into the dependency system
+	 * - Schedule out the parent context if necessary, and schedule a new one in.
+	 */
+#ifdef CONFIG_GPU_TRACEPOINTS
+	if (kbasep_jm_nr_jobs_submitted(slot) != 0) {
+		kbase_jd_atom *katom;
+		char js_string[16];
+		katom = kbasep_jm_peek_idx_submit_slot(slot, 0);        /* The atom in the HEAD */
+		trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(*end_timestamp), (u32)katom->kctx, 0, katom->work_id);
+		slot->last_context = katom->kctx;
+	} else {
+		char js_string[16];
+		trace_gpu_sched_switch(kbasep_make_job_slot_string(s, js_string), ktime_to_ns(ktime_get()), 0, 0, 0);
+		slot->last_context = 0;
+	}
+#endif
+	kbase_jd_done(katom, s, end_timestamp, KBASE_JS_ATOM_DONE_START_NEW_ATOMS);
+}
+
+/**
+ * Update the start_timestamp of the job currently in the HEAD, based on the
+ * fact that we got an IRQ for the previous set of completed jobs.
+ *
+ * The estimate also takes into account the KBASE_JS_IRQ_THROTTLE_TIME_US and
+ * the time the job was submitted, to work out the best estimate (which might
+ * still result in an over-estimate to the calculated time spent)
+ */
+STATIC void kbasep_job_slot_update_head_start_timestamp(kbase_device *kbdev, kbase_jm_slot *slot, ktime_t end_timestamp)
+{
+	KBASE_DEBUG_ASSERT(slot);
+
+	if (kbasep_jm_nr_jobs_submitted(slot) > 0) {
+		kbase_jd_atom *katom;
+		ktime_t new_timestamp;
+		ktime_t timestamp_diff;
+		katom = kbasep_jm_peek_idx_submit_slot(slot, 0);	/* The atom in the HEAD */
+
+		KBASE_DEBUG_ASSERT(katom != NULL);
+
+		if (kbasep_jm_is_dummy_workaround_job(kbdev, katom) != MALI_FALSE) {
+			/* Don't access the members of HW workaround 'dummy' jobs */
+			return;
+		}
+
+		/* Account for any IRQ Throttle time - makes an overestimate of the time spent by the job */
+		new_timestamp = ktime_sub_ns(end_timestamp, KBASE_JS_IRQ_THROTTLE_TIME_US * 1000);
+		timestamp_diff = ktime_sub(new_timestamp, katom->start_timestamp);
+		if (ktime_to_ns(timestamp_diff) >= 0) {
+			/* Only update the timestamp if it's a better estimate than what's currently stored.
+			 * This is because our estimate that accounts for the throttle time may be too much
+			 * of an overestimate */
+			katom->start_timestamp = new_timestamp;
+		}
+	}
+}
+
+void kbase_job_done(kbase_device *kbdev, u32 done)
+{
+	unsigned long flags;
+	int i;
+	u32 count = 0;
+	ktime_t end_timestamp = ktime_get();
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_TRACE_ADD(kbdev, JM_IRQ, NULL, NULL, 0, done);
+
+	memset(&kbdev->slot_submit_count_irq[0], 0, sizeof(kbdev->slot_submit_count_irq));
+
+	/* write irq throttle register, this will prevent irqs from occurring until
+	 * the given number of gpu clock cycles have passed */
+	{
+		int irq_throttle_cycles = atomic_read(&kbdev->irq_throttle_cycles);
+		kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_THROTTLE), irq_throttle_cycles, NULL);
+	}
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	while (done) {
+		kbase_jm_slot *slot;
+		u32 failed = done >> 16;
+
+		/* treat failed slots as finished slots */
+		u32 finished = (done & 0xFFFF) | failed;
+
+		/* Note: This is inherently unfair, as we always check
+		 * for lower numbered interrupts before the higher
+		 * numbered ones.*/
+		i = ffs(finished) - 1;
+		KBASE_DEBUG_ASSERT(i >= 0);
+
+		slot = &kbdev->jm_slots[i];
+
+		do {
+			int nr_done;
+			u32 active;
+			u32 completion_code = BASE_JD_EVENT_DONE;	/* assume OK */
+			u64 job_tail = 0;
+
+			if (failed & (1u << i)) {
+				/* read out the job slot status code if the job slot reported failure */
+				completion_code = kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_STATUS), NULL);
+
+				switch (completion_code) {
+				case BASE_JD_EVENT_STOPPED:
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+					kbase_trace_mali_job_slots_event(GATOR_MAKE_EVENT(GATOR_JOB_SLOT_SOFT_STOPPED, i), NULL, 0);
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+					/* Soft-stopped job - read the value of JS<n>_TAIL so that the job chain can be resumed */
+					job_tail = (u64) kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_TAIL_LO), NULL) | ((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_TAIL_HI), NULL) << 32);
+					break;
+				case BASE_JD_EVENT_NOT_STARTED:
+					/* PRLAM-10673 can cause a TERMINATED job to come back as NOT_STARTED, but the error interrupt helps us detect it */
+					completion_code = BASE_JD_EVENT_TERMINATED;
+					/* fall throught */
+				default:
+					dev_warn(kbdev->dev, "error detected from slot %d, job status 0x%08x (%s)", i, completion_code, kbase_exception_name(completion_code));
+				}
+			}
+
+			kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), done & ((1 << i) | (1 << (i + 16))), NULL);
+			active = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_JS_STATE), NULL);
+
+			if (((active >> i) & 1) == 0 && (((done >> (i + 16)) & 1) == 0)) {
+				/* There is a potential race we must work around:
+				 *
+				 *  1. A job slot has a job in both current and next registers
+				 *  2. The job in current completes successfully, the IRQ handler reads RAWSTAT
+				 *     and calls this function with the relevant bit set in "done"
+				 *  3. The job in the next registers becomes the current job on the GPU
+				 *  4. Sometime before the JOB_IRQ_CLEAR line above the job on the GPU _fails_
+				 *  5. The IRQ_CLEAR clears the done bit but not the failed bit. This atomically sets
+				 *     JOB_IRQ_JS_STATE. However since both jobs have now completed the relevant bits
+				 *     for the slot are set to 0.
+				 *
+				 * If we now did nothing then we'd incorrectly assume that _both_ jobs had completed
+				 * successfully (since we haven't yet observed the fail bit being set in RAWSTAT).
+				 *
+				 * So at this point if there are no active jobs left we check to see if RAWSTAT has a failure
+				 * bit set for the job slot. If it does we know that there has been a new failure that we
+				 * didn't previously know about, so we make sure that we record this in active (but we wait
+				 * for the next loop to deal with it).
+				 *
+				 * If we were handling a job failure (i.e. done has the relevant high bit set) then we know that
+				 * the value read back from JOB_IRQ_JS_STATE is the correct number of remaining jobs because
+				 * the failed job will have prevented any futher jobs from starting execution.
+				 */
+				u32 rawstat = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT), NULL);
+
+				if ((rawstat >> (i + 16)) & 1) {
+					/* There is a failed job that we've missed - add it back to active */
+					active |= (1u << i);
+				}
+			}
+
+			dev_dbg(kbdev->dev, "Job ended with status 0x%08X\n", completion_code);
+
+			nr_done = kbasep_jm_nr_jobs_submitted(slot);
+			nr_done -= (active >> i) & 1;
+			nr_done -= (active >> (i + 16)) & 1;
+
+			if (nr_done <= 0) {
+				dev_warn(kbdev->dev, "Spurious interrupt on slot %d", i);
+				goto spurious;
+			}
+
+			count += nr_done;
+
+			while (nr_done) {
+				if (nr_done == 1) {
+					kbase_job_done_slot(kbdev, i, completion_code, job_tail, &end_timestamp);
+				} else {
+					/* More than one job has completed. Since this is not the last job being reported this time it
+					 * must have passed. This is because the hardware will not allow further jobs in a job slot to
+					 * complete until the faile job is cleared from the IRQ status.
+					 */
+					kbase_job_done_slot(kbdev, i, BASE_JD_EVENT_DONE, 0, &end_timestamp);
+				}
+				nr_done--;
+			}
+
+ spurious:
+			done = kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT), NULL);
+
+			if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10883)) {
+				/* Workaround for missing interrupt caused by PRLAM-10883 */
+				if (((active >> i) & 1) && (0 == kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_STATUS), NULL))) {
+					/* Force job slot to be processed again */
+					done |= (1u << i);
+				}
+			}
+
+			failed = done >> 16;
+			finished = (done & 0xFFFF) | failed;
+		} while (finished & (1 << i));
+
+		kbasep_job_slot_update_head_start_timestamp(kbdev, slot, end_timestamp);
+	}
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	if (atomic_read(&kbdev->reset_gpu) == KBASE_RESET_GPU_COMMITTED) {
+		/* If we're trying to reset the GPU then we might be able to do it early
+		 * (without waiting for a timeout) because some jobs have completed
+		 */
+		kbasep_try_reset_gpu_early(kbdev);
+	}
+
+	KBASE_TRACE_ADD(kbdev, JM_IRQ_END, NULL, NULL, 0, count);
+}
+KBASE_EXPORT_TEST_API(kbase_job_done)
+
+static mali_bool kbasep_soft_stop_allowed(kbase_device *kbdev, u16 core_reqs)
+{
+	mali_bool soft_stops_allowed = MALI_TRUE;
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8408)) {
+		if ((core_reqs & BASE_JD_REQ_T) != 0)
+			soft_stops_allowed = MALI_FALSE;
+	}
+	return soft_stops_allowed;
+}
+
+static mali_bool kbasep_hard_stop_allowed(kbase_device *kbdev, u16 core_reqs)
+{
+	mali_bool hard_stops_allowed = MALI_TRUE;
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8394)) {
+		if ((core_reqs & BASE_JD_REQ_T) != 0)
+			hard_stops_allowed = MALI_FALSE;
+	}
+	return hard_stops_allowed;
+}
+
+static void kbasep_job_slot_soft_or_hard_stop_do_action(kbase_device *kbdev, int js, u32 action, u16 core_reqs, kbase_jd_atom * target_katom )
+{
+	kbase_context *kctx = target_katom->kctx;
+#if KBASE_TRACE_ENABLE
+	u32 status_reg_before;
+	u64 job_in_head_before;
+	u32 status_reg_after;
+
+	/* Check the head pointer */
+	job_in_head_before = ((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_LO), NULL))
+	    | (((u64) kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_HI), NULL)) << 32);
+	status_reg_before = kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_STATUS), NULL);
+#endif
+
+	if (action == JSn_COMMAND_SOFT_STOP) {
+		mali_bool soft_stop_allowed = kbasep_soft_stop_allowed(kbdev, core_reqs);
+		if (!soft_stop_allowed) {
+#ifdef CONFIG_MALI_DEBUG
+			dev_dbg(kbdev->dev, "Attempt made to soft-stop a job that cannot be soft-stopped. core_reqs = 0x%X", (unsigned int)core_reqs);
+#endif				/* CONFIG_MALI_DEBUG */
+			return;
+		}
+
+		/* We are about to issue a soft stop, so mark the atom as having been soft stopped */
+		target_katom->atom_flags |= KBASE_KATOM_FLAG_BEEN_SOFT_STOPPPED;
+	}
+
+	if (action == JSn_COMMAND_HARD_STOP) {
+		mali_bool hard_stop_allowed = kbasep_hard_stop_allowed(kbdev, core_reqs);
+		if (!hard_stop_allowed) {
+			/* Jobs can be hard-stopped for the following reasons:
+			 *  * CFS decides the job has been running too long (and soft-stop has not occurred).
+			 *    In this case the GPU will be reset by CFS if the job remains on the GPU.
+			 *
+			 *  * The context is destroyed, kbase_jd_zap_context will attempt to hard-stop the job. However
+			 *    it also has a watchdog which will cause the GPU to be reset if the job remains on the GPU.
+			 *
+			 *  * An (unhandled) MMU fault occurred. As long as BASE_HW_ISSUE_8245 is defined then
+			 *    the GPU will be reset.
+			 *
+			 * All three cases result in the GPU being reset if the hard-stop fails,
+			 * so it is safe to just return and ignore the hard-stop request.
+			 */
+			dev_warn(kbdev->dev, "Attempt made to hard-stop a job that cannot be hard-stopped. core_reqs = 0x%X", (unsigned int)core_reqs);
+			return;
+		}
+	}
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8316) && action == JSn_COMMAND_SOFT_STOP) {
+		int i;
+		kbase_jm_slot *slot;
+		slot = &kbdev->jm_slots[js];
+
+		for (i = 0; i < kbasep_jm_nr_jobs_submitted(slot); i++) {
+			kbase_jd_atom *katom;
+
+			katom = kbasep_jm_peek_idx_submit_slot(slot, i);
+
+			KBASE_DEBUG_ASSERT(katom);
+
+			if (kbasep_jm_is_dummy_workaround_job(kbdev, katom) != MALI_FALSE) {
+				/* Don't access the members of HW workaround 'dummy' jobs
+				 *
+				 * This assumes that such jobs can't cause HW_ISSUE_8316, and could only be blocked
+				 * by other jobs causing HW_ISSUE_8316 (which will get poked/or eventually get killed) */
+				continue;
+			}
+
+			/* For HW_ISSUE_8316, only 'bad' jobs attacking the system can
+			 * cause this issue: normally, all memory should be allocated in
+			 * multiples of 4 pages, and growable memory should be changed size
+			 * in multiples of 4 pages.
+			 *
+			 * Whilst such 'bad' jobs can be cleared by a GPU reset, the
+			 * locking up of a uTLB entry caused by the bad job could also
+			 * stall other ASs, meaning that other ASs' jobs don't complete in
+			 * the 'grace' period before the reset. We don't want to lose other
+			 * ASs' jobs when they would normally complete fine, so we must
+			 * 'poke' the MMU regularly to help other ASs complete */
+			kbase_as_poking_timer_retain_atom(kbdev, katom->kctx, katom);
+		}
+	}
+
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION)) {
+		if (action == JSn_COMMAND_SOFT_STOP)
+			action = (target_katom->atom_flags & KBASE_KATOM_FLAGS_JOBCHAIN) ? 
+				 JSn_COMMAND_SOFT_STOP_1:
+		         JSn_COMMAND_SOFT_STOP_0;
+		else
+			action = (target_katom->atom_flags & KBASE_KATOM_FLAGS_JOBCHAIN) ? 
+				 JSn_COMMAND_HARD_STOP_1:
+		         JSn_COMMAND_HARD_STOP_0;
+	}
+
+	kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND), action, kctx);
+
+#if KBASE_TRACE_ENABLE
+	status_reg_after = kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_STATUS), NULL);
+	if (status_reg_after == BASE_JD_EVENT_ACTIVE) {
+		kbase_jm_slot *slot;
+		kbase_jd_atom *head;
+		kbase_context *head_kctx;
+
+		slot = &kbdev->jm_slots[js];
+		head = kbasep_jm_peek_idx_submit_slot(slot, slot->submitted_nr - 1);
+		head_kctx = head->kctx;
+
+		/* We don't need to check kbasep_jm_is_dummy_workaround_job( head ) here:
+		 * - Members are not indirected through
+		 * - The members will all be zero anyway
+		 */
+		if (status_reg_before == BASE_JD_EVENT_ACTIVE)
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_CHECK_HEAD, head_kctx, head, job_in_head_before, js);
+		else
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_CHECK_HEAD, NULL, NULL, 0, js);
+
+		switch(action) {
+		case JSn_COMMAND_SOFT_STOP:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP, head_kctx, head, head->jc, js);
+			break;
+		case JSn_COMMAND_SOFT_STOP_0:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP_0, head_kctx, head, head->jc, js);
+			break;
+		case JSn_COMMAND_SOFT_STOP_1:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP_1, head_kctx, head, head->jc, js);
+			break;
+		case JSn_COMMAND_HARD_STOP:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP, head_kctx, head, head->jc, js);
+			break;
+		case JSn_COMMAND_HARD_STOP_0:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP_0, head_kctx, head, head->jc, js);
+			break;
+		case JSn_COMMAND_HARD_STOP_1:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP_1, head_kctx, head, head->jc, js);
+			break;
+		default:
+			BUG();
+			break;
+		}
+	} else {
+		if (status_reg_before == BASE_JD_EVENT_ACTIVE)
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_CHECK_HEAD, NULL, NULL, job_in_head_before, js);
+		else
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_CHECK_HEAD, NULL, NULL, 0, js);
+
+		switch(action) {
+		case JSn_COMMAND_SOFT_STOP:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP, NULL, NULL, 0, js);
+			break;
+		case JSn_COMMAND_SOFT_STOP_0:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP_0, NULL, NULL, 0, js);
+			break;
+		case JSn_COMMAND_SOFT_STOP_1:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SOFTSTOP_1, NULL, NULL, 0, js);
+			break;
+		case JSn_COMMAND_HARD_STOP:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP, NULL, NULL, 0, js);
+			break;
+		case JSn_COMMAND_HARD_STOP_0:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP_0, NULL, NULL, 0, js);
+			break;
+		case JSn_COMMAND_HARD_STOP_1:
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_HARDSTOP_1, NULL, NULL, 0, js);
+			break;
+		default:
+			BUG();
+			break;
+		}
+	}
+#endif
+}
+
+/* Helper macros used by kbasep_job_slot_soft_or_hard_stop */
+#define JM_SLOT_MAX_JOB_SUBMIT_REGS    2
+#define JM_JOB_IS_CURRENT_JOB_INDEX(n) (1 == n)	/* Index of the last job to process */
+#define JM_JOB_IS_NEXT_JOB_INDEX(n)    (2 == n)	/* Index of the prior to last job to process */
+
+/** Soft or hard-stop a slot
+ *
+ * This function safely ensures that the correct job is either hard or soft-stopped.
+ * It deals with evicting jobs from the next registers where appropriate.
+ *
+ * This does not attempt to stop or evict jobs that are 'dummy' jobs for HW workarounds.
+ *
+ * @param kbdev         The kbase device
+ * @param kctx          The context to soft/hard-stop job(s) from (or NULL is all jobs should be targeted)
+ * @param js            The slot that the job(s) are on
+ * @param target_katom  The atom that should be targeted (or NULL if all jobs from the context should be targeted)
+ * @param action        The action to perform, either JSn_COMMAND_HARD_STOP or JSn_COMMAND_SOFT_STOP
+ */
+static void kbasep_job_slot_soft_or_hard_stop(kbase_device *kbdev, kbase_context *kctx, int js, kbase_jd_atom *target_katom, u32 action)
+{
+	kbase_jd_atom *katom;
+	u8 i;
+	u8 jobs_submitted;
+	kbase_jm_slot *slot;
+	u16 core_reqs;
+	kbasep_js_device_data *js_devdata;
+	mali_bool can_safely_stop = kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_JOBCHAIN_DISAMBIGUATION);
+
+	KBASE_DEBUG_ASSERT(action == JSn_COMMAND_HARD_STOP || action == JSn_COMMAND_SOFT_STOP);
+	KBASE_DEBUG_ASSERT(kbdev);
+	js_devdata = &kbdev->js_data;
+
+	slot = &kbdev->jm_slots[js];
+	KBASE_DEBUG_ASSERT(slot);
+	lockdep_assert_held(&js_devdata->runpool_irq.lock);
+
+	jobs_submitted = kbasep_jm_nr_jobs_submitted(slot);
+
+	KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, 1);
+	KBASE_TRACE_ADD_SLOT_INFO(kbdev, JM_SLOT_SOFT_OR_HARD_STOP, kctx, NULL, 0u, js, jobs_submitted);
+
+	if (jobs_submitted > JM_SLOT_MAX_JOB_SUBMIT_REGS)
+		i = jobs_submitted - JM_SLOT_MAX_JOB_SUBMIT_REGS;
+	else
+		i = 0;
+
+	/* Loop through all jobs that have been submitted to the slot and haven't completed */
+	for (; i < jobs_submitted; i++) {
+		katom = kbasep_jm_peek_idx_submit_slot(slot, i);
+
+		if (kctx && katom->kctx != kctx)
+			continue;
+
+		if (target_katom && katom != target_katom)
+			continue;
+
+		if (kbasep_jm_is_dummy_workaround_job(kbdev, katom))
+			continue;
+
+		core_reqs = katom->core_req;
+	
+		if (JM_JOB_IS_CURRENT_JOB_INDEX(jobs_submitted - i)) {
+			/* The last job in the slot, check if there is a job in the next register */
+			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) == 0)
+				kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, action, core_reqs, katom);
+			else {
+				/* The job is in the next registers */
+				beenthere(kctx, "clearing job from next registers on slot %d", js);
+				kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
+				/* Check to see if we did remove a job from the next registers */
+				if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+					/* The job was successfully cleared from the next registers, requeue it */
+					kbase_jd_atom *dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+					KBASE_DEBUG_ASSERT(dequeued_katom == katom);
+					jobs_submitted--;
+
+					/* Set the next registers to NULL */
+					kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), 0, NULL);
+					kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), 0, NULL);
+
+					/* As the job is removed from the next registers we undo the associated
+					 * update to the job_chain_flag for the job slot. */
+					if (can_safely_stop)
+						slot->job_chain_flag = !slot->job_chain_flag;
+
+					KBASE_TRACE_ADD_SLOT(kbdev, JM_SLOT_EVICT, dequeued_katom->kctx, dequeued_katom, dequeued_katom->jc, js);
+
+					/* Complete the job, indicate it took no time, but don't submit any more at this point */
+					kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+				} else {
+					/* The job transitioned into the current registers before we managed to evict it,
+					 * in this case we fall back to soft/hard-stopping the job */
+					beenthere(kctx, "missed job in next register, soft/hard-stopping slot %d", js);
+					kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, action, core_reqs, katom);
+				}
+			}
+		} else if (JM_JOB_IS_NEXT_JOB_INDEX(jobs_submitted - i)) {
+			/* There's a job after this one, check to see if that job is in the next registers.
+             * If so, we need to pay attention to not accidently stop that one when issueing
+             * the command to stop the one pointed to by the head registers (as the one in the head
+             * may finish in the mean time and the one in the next moves to the head). Either the hardware
+			 * has support for this using job chain disambiguation or we need to evict the job
+			 * from the next registers first to ensure we can safely stop the one pointed to by
+			 * the head registers. */
+			if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL) != 0) {
+				kbase_jd_atom *check_next_atom;
+				/* It is - we should remove that job and soft/hard-stop the slot */
+
+				/* Only proceed when the next job isn't a HW workaround 'dummy' job
+				 *
+				 * This can't be an ASSERT due to MMU fault code:
+				 * - This first hard-stops the job that caused the fault
+				 * - Under HW Issue 8245, it will then reset the GPU
+				 *  - This causes a Soft-stop to occur on all slots
+				 * - By the time of the soft-stop, we may (depending on timing) still have:
+				 *  - The original job in HEAD, if it's not finished the hard-stop
+				 *  - The dummy workaround job in NEXT
+				 *
+				 * Other cases could be coded in future that cause back-to-back Soft/Hard
+				 * stops with dummy workaround jobs in place, e.g. MMU handler code and Job
+				 * Scheduler watchdog timer running in parallel.
+				 *
+				 * Note, the index i+1 is valid to peek from: i == jobs_submitted-2, therefore
+				 * i+1 == jobs_submitted-1 */
+				check_next_atom = kbasep_jm_peek_idx_submit_slot(slot, i + 1);
+				if (kbasep_jm_is_dummy_workaround_job(kbdev, check_next_atom) != MALI_FALSE)
+					continue;
+
+				if (!can_safely_stop) {
+					beenthere(kctx, "clearing job from next registers on slot %d", js);
+					kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
+
+					/* Check to see if we did remove a job from the next registers */
+					if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+						/* We did remove a job from the next registers, requeue it */
+						kbase_jd_atom *dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+						KBASE_DEBUG_ASSERT(dequeued_katom != NULL);
+						jobs_submitted--;
+
+						/* Set the next registers to NULL */
+						kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), 0, NULL);
+						kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), 0, NULL);
+
+						KBASE_TRACE_ADD_SLOT(kbdev, JM_SLOT_EVICT, dequeued_katom->kctx, dequeued_katom, dequeued_katom->jc, js);
+
+						/* Complete the job, indicate it took no time, but don't submit any more at this point */
+						kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+					} else {
+						/* We missed the job, that means the job we're interested in left the hardware before
+						 * we managed to do anything, so we can proceed to the next job */
+						continue;
+					}
+				}
+
+				/* Next is now free, so we can soft/hard-stop the slot */
+				beenthere(kctx, "soft/hard-stopped slot %d (there was a job in next which was successfully cleared)\n", js);
+				kbasep_job_slot_soft_or_hard_stop_do_action(kbdev, js, action, core_reqs, katom);
+			}
+			/* If there was no job in the next registers, then the job we were
+			 * interested in has finished, so we need not take any action
+			 */
+		}
+	}
+
+	KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, 0);
+}
+
+void kbase_job_kill_jobs_from_context(kbase_context *kctx)
+{
+	unsigned long flags;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	int i;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	/* Cancel any remaining running jobs for this kctx  */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* Invalidate all jobs in context, to prevent re-submitting */
+	for (i = 0; i < BASE_JD_ATOM_COUNT; i++)
+		kctx->jctx.atoms[i].event_code = BASE_JD_EVENT_JOB_CANCELLED;
+
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
+		kbase_job_slot_hardstop(kctx, i, NULL);
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+}
+
+void kbase_job_zap_context(kbase_context *kctx)
+{
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	int i;
+	mali_bool evict_success;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	/*
+	 * Critical assumption: No more submission is possible outside of the
+	 * workqueue. This is because the OS *must* prevent U/K calls (IOCTLs)
+	 * whilst the kbase_context is terminating.
+	 */
+
+	/* First, atomically do the following:
+	 * - mark the context as dying
+	 * - try to evict it from the policy queue */
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	js_kctx_info->ctx.is_dying = MALI_TRUE;
+
+	dev_dbg(kbdev->dev, "Zap: Try Evict Ctx %p", kctx);
+	mutex_lock(&js_devdata->queue_mutex);
+	evict_success = kbasep_js_policy_try_evict_ctx(&js_devdata->policy, kctx);
+	mutex_unlock(&js_devdata->queue_mutex);
+
+	/*
+	 * At this point we know:
+	 * - If eviction succeeded, it was in the policy queue, but now no longer is
+	 *  - We must cancel the jobs here. No Power Manager active reference to
+	 * release.
+	 *  - This happens asynchronously - kbase_jd_zap_context() will wait for
+	 * those jobs to be killed.
+	 * - If eviction failed, then it wasn't in the policy queue. It is one of
+	 * the following:
+	 *  - a. it didn't have any jobs, and so is not in the Policy Queue or the
+	 * Run Pool (not scheduled)
+	 *   - Hence, no more work required to cancel jobs. No Power Manager active
+	 * reference to release.
+	 *  - b. it was in the middle of a scheduling transaction (and thus must
+	 * have at least 1 job). This can happen from a syscall or a kernel thread.
+	 * We still hold the jsctx_mutex, and so the thread must be waiting inside
+	 * kbasep_js_try_schedule_head_ctx(), before checking whether the runpool
+	 * is full. That thread will continue after we drop the mutex, and will
+	 * notice the context is dying. It will rollback the transaction, killing
+	 * all jobs at the same time. kbase_jd_zap_context() will wait for those
+	 * jobs to be killed.
+	 *   - Hence, no more work required to cancel jobs, or to release the Power
+	 * Manager active reference.
+	 *  - c. it is scheduled, and may or may not be running jobs
+	 * - We must cause it to leave the runpool by stopping it from submitting
+	 * any more jobs. When it finally does leave,
+	 * kbasep_js_runpool_requeue_or_kill_ctx() will kill all remaining jobs
+	 * (because it is dying), release the Power Manager active reference, and
+	 * will not requeue the context in the policy queue. kbase_jd_zap_context()
+	 * will wait for those jobs to be killed.
+	 *  - Hence, work required just to make it leave the runpool. Cancelling
+	 * jobs and releasing the Power manager active reference will be handled
+	 * when it leaves the runpool.
+	 */
+
+	if (evict_success != MALI_FALSE || js_kctx_info->ctx.is_scheduled == MALI_FALSE) {
+		/* The following events require us to kill off remaining jobs and
+		 * update PM book-keeping:
+		 * - we evicted it correctly (it must have jobs to be in the Policy Queue)
+		 *
+		 * These events need no action, but take this path anyway:
+		 * - Case a: it didn't have any jobs, and was never in the Queue
+		 * - Case b: scheduling transaction will be partially rolled-back (this
+		 * already cancels the jobs)
+		 */
+
+		KBASE_TRACE_ADD(kbdev, JM_ZAP_NON_SCHEDULED, kctx, NULL, 0u, js_kctx_info->ctx.is_scheduled);
+
+		dev_dbg(kbdev->dev, "Zap: Ctx %p evict_success=%d, scheduled=%d", kctx, evict_success, js_kctx_info->ctx.is_scheduled);
+
+		if (evict_success != MALI_FALSE) {
+			/* Only cancel jobs when we evicted from the policy queue. No Power
+			 * Manager active reference was held.
+			 *
+			 * Having is_dying set ensures that this kills, and doesn't requeue */
+			kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx, MALI_FALSE);
+		}
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+	} else {
+		unsigned long flags;
+		mali_bool was_retained;
+		/* Case c: didn't evict, but it is scheduled - it's in the Run Pool */
+		KBASE_TRACE_ADD(kbdev, JM_ZAP_SCHEDULED, kctx, NULL, 0u, js_kctx_info->ctx.is_scheduled);
+		dev_dbg(kbdev->dev, "Zap: Ctx %p is in RunPool", kctx);
+
+		/* Disable the ctx from submitting any more jobs */
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+		kbasep_js_clear_submit_allowed(js_devdata, kctx);
+
+		/* Retain and (later) release the context whilst it is is now disallowed from submitting
+		 * jobs - ensures that someone somewhere will be removing the context later on */
+		was_retained = kbasep_js_runpool_retain_ctx_nolock(kbdev, kctx);
+
+		/* Since it's scheduled and we have the jsctx_mutex, it must be retained successfully */
+		KBASE_DEBUG_ASSERT(was_retained != MALI_FALSE);
+
+		dev_dbg(kbdev->dev, "Zap: Ctx %p Kill Any Running jobs", kctx);
+		/* Cancel any remaining running jobs for this kctx - if any. Submit is disallowed
+		 * which takes effect immediately, so no more new jobs will appear after we do this.  */
+		for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
+			kbase_job_slot_hardstop(kctx, i, NULL);
+
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+		dev_dbg(kbdev->dev, "Zap: Ctx %p Release (may or may not schedule out immediately)", kctx);
+		kbasep_js_runpool_release_ctx(kbdev, kctx);
+	}
+	KBASE_TRACE_ADD(kbdev, JM_ZAP_DONE, kctx, NULL, 0u, 0u);
+
+	/* After this, you must wait on both the kbase_jd_context::zero_jobs_wait
+	 * and the kbasep_js_kctx_info::ctx::is_scheduled_waitq - to wait for the
+	 * jobs to be destroyed, and the context to be de-scheduled (if it was on
+	 * the runpool).
+	 *
+	 * kbase_jd_zap_context() will do this. */
+}
+KBASE_EXPORT_TEST_API(kbase_job_zap_context)
+
+mali_error kbase_job_slot_init(kbase_device *kbdev)
+{
+	int i;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
+		kbasep_jm_init_submit_slot(&kbdev->jm_slots[i]);
+
+	return MALI_ERROR_NONE;
+}
+KBASE_EXPORT_TEST_API(kbase_job_slot_init)
+
+void kbase_job_slot_halt(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+void kbase_job_slot_term(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev);
+}
+KBASE_EXPORT_TEST_API(kbase_job_slot_term)
+
+/**
+ * Soft-stop the specified job slot
+ *
+ * The job slot lock must be held when calling this function.
+ * The job slot must not already be in the process of being soft-stopped.
+ *
+ * Where possible any job in the next register is evicted before the soft-stop.
+ *
+ * @param kbdev         The kbase device
+ * @param js            The job slot to soft-stop
+ * @param target_katom  The job that should be soft-stopped (or NULL for any job)
+ */
+void kbase_job_slot_softstop(kbase_device *kbdev, int js, kbase_jd_atom *target_katom)
+{
+	kbasep_job_slot_soft_or_hard_stop(kbdev, NULL, js, target_katom, JSn_COMMAND_SOFT_STOP);
+}
+
+/**
+ * Hard-stop the specified job slot
+ *
+ * The job slot lock must be held when calling this function.
+ *
+ * @param kctx		The kbase context that contains the job(s) that should
+ *			be hard-stopped
+ * @param js		The job slot to hard-stop
+ * @param target_katom	The job that should be hard-stopped (or NULL for all
+ *			jobs from the context)
+ */
+void kbase_job_slot_hardstop(kbase_context *kctx, int js,
+				kbase_jd_atom *target_katom)
+{
+	kbase_device *kbdev = kctx->kbdev;
+
+	kbasep_job_slot_soft_or_hard_stop(kbdev, kctx, js, target_katom,
+						JSn_COMMAND_HARD_STOP);
+	if (kbase_hw_has_issue(kctx->kbdev, BASE_HW_ISSUE_8401) ||
+		kbase_hw_has_issue(kctx->kbdev, BASE_HW_ISSUE_9510) ||
+		(kbase_hw_has_issue(kctx->kbdev, BASE_HW_ISSUE_T76X_3542) &&
+		(target_katom == NULL || target_katom->core_req & BASE_JD_REQ_FS_AFBC))) {
+		/* MIDBASE-2916 if a fragment job with AFBC encoding is
+		 * hardstopped, ensure to do a soft reset also in order to
+		 * clear the GPU status.
+		 * Workaround for HW issue 8401 has an issue,so after
+		 * hard-stopping just reset the GPU. This will ensure that the
+		 * jobs leave the GPU.*/
+		if (kbase_prepare_to_reset_gpu_locked(kbdev)) {
+			dev_err(kbdev->dev, "Issueing GPU\
+			soft-reset after hard stopping due to hardware issue");
+			kbase_reset_gpu_locked(kbdev);
+		}
+	}
+}
+
+
+void kbase_debug_dump_registers(kbase_device *kbdev)
+{
+	int i;
+	dev_err(kbdev->dev, "Register state:");
+	dev_err(kbdev->dev, "  GPU_IRQ_RAWSTAT=0x%08x GPU_STATUS=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_STATUS), NULL));
+	dev_err(kbdev->dev, "  JOB_IRQ_RAWSTAT=0x%08x JOB_IRQ_JS_STATE=0x%08x JOB_IRQ_THROTTLE=0x%08x",
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_RAWSTAT), NULL),
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_JS_STATE), NULL),
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_THROTTLE), NULL));
+	for (i = 0; i < 3; i++) {
+		dev_err(kbdev->dev, "  JS%d_STATUS=0x%08x      JS%d_HEAD_LO=0x%08x",
+			i, kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_STATUS),
+					NULL),
+			i, kbase_reg_read(kbdev, JOB_SLOT_REG(i, JSn_HEAD_LO),
+					NULL));
+	}
+	dev_err(kbdev->dev, "  MMU_IRQ_RAWSTAT=0x%08x GPU_FAULTSTATUS=0x%08x",
+		kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_RAWSTAT), NULL),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_FAULTSTATUS), NULL));
+	dev_err(kbdev->dev, "  GPU_IRQ_MASK=0x%08x    JOB_IRQ_MASK=0x%08x     MMU_IRQ_MASK=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), NULL),
+		kbase_reg_read(kbdev, JOB_CONTROL_REG(JOB_IRQ_MASK), NULL),
+		kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK), NULL));
+	dev_err(kbdev->dev, "  PWR_OVERRIDE0=0x%08x   PWR_OVERRIDE1=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(PWR_OVERRIDE0), NULL),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(PWR_OVERRIDE1), NULL));
+	dev_err(kbdev->dev, "  SHADER_CONFIG=0x%08x   L2_MMU_CONFIG=0x%08x",
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(SHADER_CONFIG), NULL),
+		kbase_reg_read(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG), NULL));
+}
+
+void kbasep_reset_timeout_worker(struct work_struct *data)
+{
+	unsigned long flags;
+	kbase_device *kbdev;
+	int i;
+	ktime_t end_timestamp = ktime_get();
+	kbasep_js_device_data *js_devdata;
+	kbase_uk_hwcnt_setup hwcnt_setup = { {0} };
+	kbase_instr_state bckp_state;
+
+	KBASE_DEBUG_ASSERT(data);
+
+	kbdev = container_of(data, kbase_device, reset_work);
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_TRACE_ADD(kbdev, JM_BEGIN_RESET_WORKER, NULL, NULL, 0u, 0);
+
+	/* Make sure the timer has completed - this cannot be done from interrupt context,
+	 * so this cannot be done within kbasep_try_reset_gpu_early. */
+	hrtimer_cancel(&kbdev->reset_timer);
+
+	if (kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE)) {
+		/* This would re-activate the GPU. Since it's already idle, there's no
+		 * need to reset it */
+		atomic_set(&kbdev->reset_gpu, KBASE_RESET_GPU_NOT_PENDING);
+		wake_up(&kbdev->reset_wait);
+		return;
+	}
+
+	mutex_lock(&kbdev->pm.lock);
+	/* We hold the pm lock, so there ought to be a current policy */
+	KBASE_DEBUG_ASSERT(kbdev->pm.pm_current_policy);
+
+	/* All slot have been soft-stopped and we've waited SOFT_STOP_RESET_TIMEOUT for the slots to clear, at this point
+	 * we assume that anything that is still left on the GPU is stuck there and we'll kill it when we reset the GPU */
+
+	dev_err(kbdev->dev, "Resetting GPU (allowing up to %d ms)", RESET_TIMEOUT);
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+
+	if (kbdev->hwcnt.state == KBASE_INSTR_STATE_RESETTING) {	/*the same interrupt handler preempted itself */
+		/* GPU is being reset */
+		spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+		wait_event(kbdev->hwcnt.wait, kbdev->hwcnt.triggered != 0);
+		spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	}
+	/* Save the HW counters setup */
+	if (kbdev->hwcnt.kctx != NULL) {
+		kbase_context *kctx = kbdev->hwcnt.kctx;
+		hwcnt_setup.dump_buffer = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_LO), kctx) & 0xffffffff;
+		hwcnt_setup.dump_buffer |= (mali_addr64) kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_HI), kctx) << 32;
+		hwcnt_setup.jm_bm = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_JM_EN), kctx);
+		hwcnt_setup.shader_bm = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_SHADER_EN), kctx);
+		hwcnt_setup.tiler_bm = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), kctx);
+		hwcnt_setup.l3_cache_bm = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_L3_CACHE_EN), kctx);
+		hwcnt_setup.mmu_l2_bm = kbase_reg_read(kbdev, GPU_CONTROL_REG(PRFCNT_MMU_L2_EN), kctx);
+	}
+
+	/* Output the state of some interesting registers to help in the
+	 * debugging of GPU resets */
+	kbase_debug_dump_registers(kbdev);
+
+	bckp_state = kbdev->hwcnt.state;
+	kbdev->hwcnt.state = KBASE_INSTR_STATE_RESETTING;
+	kbdev->hwcnt.triggered = 0;
+	/* Disable IRQ to avoid IRQ handlers to kick in after releaseing the spinlock;
+	 * this also clears any outstanding interrupts */
+	kbase_pm_disable_interrupts(kbdev);
+	/* Ensure that any IRQ handlers have finished */
+	kbase_synchronize_irqs(kbdev);
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	/* Reset the GPU */
+	kbase_pm_init_hw(kbdev, MALI_TRUE);
+	/* IRQs were re-enabled by kbase_pm_init_hw, and GPU is still powered */
+
+	spin_lock_irqsave(&kbdev->hwcnt.lock, flags);
+	/* Restore the HW counters setup */
+	if (kbdev->hwcnt.kctx != NULL) {
+		kbase_context *kctx = kbdev->hwcnt.kctx;
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), (kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT) | PRFCNT_CONFIG_MODE_OFF, kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_LO),     hwcnt_setup.dump_buffer & 0xFFFFFFFF, kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_BASE_HI),     hwcnt_setup.dump_buffer >> 32,        kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_JM_EN),       hwcnt_setup.jm_bm,                    kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_SHADER_EN),   hwcnt_setup.shader_bm,                kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_L3_CACHE_EN), hwcnt_setup.l3_cache_bm,              kctx);
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_MMU_L2_EN),   hwcnt_setup.mmu_l2_bm,                kctx);
+
+		/* Due to PRLAM-8186 we need to disable the Tiler before we enable the HW counter dump. */
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8186))
+			kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), 0, kctx);
+		else
+			kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), hwcnt_setup.tiler_bm, kctx);
+
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_CONFIG), (kctx->as_nr << PRFCNT_CONFIG_AS_SHIFT) | PRFCNT_CONFIG_MODE_MANUAL, kctx);
+
+		/* If HW has PRLAM-8186 we can now re-enable the tiler HW counters dump */
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8186))
+			kbase_reg_write(kbdev, GPU_CONTROL_REG(PRFCNT_TILER_EN), hwcnt_setup.tiler_bm, kctx);
+	}
+	kbdev->hwcnt.state = bckp_state;
+	switch(kbdev->hwcnt.state) {
+	/* Cases for waking kbasep_cache_clean_worker worker */
+	case KBASE_INSTR_STATE_CLEANED:
+		/* Cache-clean IRQ occurred, but we reset:
+		 * Wakeup incase the waiter saw RESETTING */
+	case KBASE_INSTR_STATE_REQUEST_CLEAN:
+		/* After a clean was requested, but before the regs were written:
+		 * Wakeup incase the waiter saw RESETTING */
+		wake_up(&kbdev->hwcnt.cache_clean_wait);
+		break;
+	case KBASE_INSTR_STATE_CLEANING:
+		/* Either:
+		 * 1) We've not got the Cache-clean IRQ yet: it was lost, or:
+		 * 2) We got it whilst resetting: it was voluntarily lost
+		 *
+		 * So, move to the next state and wakeup: */
+		kbdev->hwcnt.state = KBASE_INSTR_STATE_CLEANED;
+		wake_up(&kbdev->hwcnt.cache_clean_wait);
+		break;
+
+	/* Cases for waking anyone else */
+	case KBASE_INSTR_STATE_DUMPING:
+		/* If dumping, abort the dump, because we may've lost the IRQ */
+		kbdev->hwcnt.state = KBASE_INSTR_STATE_IDLE;
+		kbdev->hwcnt.triggered = 1;
+		wake_up(&kbdev->hwcnt.wait);
+		break;
+	case KBASE_INSTR_STATE_DISABLED:
+	case KBASE_INSTR_STATE_IDLE:
+	case KBASE_INSTR_STATE_FAULT:
+		/* Every other reason: wakeup in that state */
+		kbdev->hwcnt.triggered = 1;
+		wake_up(&kbdev->hwcnt.wait);
+		break;
+
+	/* Unhandled cases */
+	case KBASE_INSTR_STATE_RESETTING:
+	default:
+		BUG();
+		break;
+	}
+	spin_unlock_irqrestore(&kbdev->hwcnt.lock, flags);
+
+	/* Complete any jobs that were still on the GPU */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++) {
+		int nr_done;
+		kbase_jm_slot *slot = &kbdev->jm_slots[i];
+
+		nr_done = kbasep_jm_nr_jobs_submitted(slot);
+		while (nr_done) {
+			dev_err(kbdev->dev, "Job stuck in slot %d on the GPU was cancelled", i);
+			kbase_job_done_slot(kbdev, i, BASE_JD_EVENT_JOB_CANCELLED, 0, &end_timestamp);
+			nr_done--;
+		}
+	}
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	mutex_lock(&js_devdata->runpool_mutex);
+
+	/* Reprogram the GPU's MMU */
+	for (i = 0; i < BASE_MAX_NR_AS; i++) {
+		if (js_devdata->runpool_irq.per_as_data[i].kctx) {
+			kbase_as *as = &kbdev->as[i];
+			mutex_lock(&as->transaction_mutex);
+			kbase_mmu_update(js_devdata->runpool_irq.per_as_data[i].kctx);
+			mutex_unlock(&as->transaction_mutex);
+		}
+	}
+
+	atomic_set(&kbdev->reset_gpu, KBASE_RESET_GPU_NOT_PENDING);
+	wake_up(&kbdev->reset_wait);
+	dev_err(kbdev->dev, "Reset complete");
+
+	/* Find out what cores are required now */
+	kbase_pm_update_cores_state(kbdev);
+
+	/* Synchronously request and wait for those cores, because if
+	 * instrumentation is enabled it would need them immediately. */
+	kbase_pm_check_transitions_sync(kbdev);
+
+	/* Try submitting some jobs to restart processing */
+	if (js_devdata->nr_user_contexts_running > 0) {
+		KBASE_TRACE_ADD(kbdev, JM_SUBMIT_AFTER_RESET, NULL, NULL, 0u, 0);
+
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+		kbasep_js_try_run_next_job_nolock(kbdev);
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	}
+	mutex_unlock(&js_devdata->runpool_mutex);
+	mutex_unlock(&kbdev->pm.lock);
+
+	kbase_pm_context_idle(kbdev);
+	KBASE_TRACE_ADD(kbdev, JM_END_RESET_WORKER, NULL, NULL, 0u, 0);
+}
+
+enum hrtimer_restart kbasep_reset_timer_callback(struct hrtimer *timer)
+{
+	kbase_device *kbdev = container_of(timer, kbase_device, reset_timer);
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	/* Reset still pending? */
+	if (atomic_cmpxchg(&kbdev->reset_gpu, KBASE_RESET_GPU_COMMITTED, KBASE_RESET_GPU_HAPPENING) == KBASE_RESET_GPU_COMMITTED)
+		queue_work(kbdev->reset_workq, &kbdev->reset_work);
+
+	return HRTIMER_NORESTART;
+}
+
+/*
+ * If all jobs are evicted from the GPU then we can reset the GPU
+ * immediately instead of waiting for the timeout to elapse
+ */
+
+static void kbasep_try_reset_gpu_early_locked(kbase_device *kbdev)
+{
+	int i;
+	int pending_jobs = 0;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	/* Count the number of jobs */
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++) {
+		kbase_jm_slot *slot = &kbdev->jm_slots[i];
+		pending_jobs += kbasep_jm_nr_jobs_submitted(slot);
+	}
+
+	if (pending_jobs > 0) {
+		/* There are still jobs on the GPU - wait */
+		return;
+	}
+
+	/* Check that the reset has been committed to (i.e. kbase_reset_gpu has been called), and that no other
+	 * thread beat this thread to starting the reset */
+	if (atomic_cmpxchg(&kbdev->reset_gpu, KBASE_RESET_GPU_COMMITTED, KBASE_RESET_GPU_HAPPENING) != KBASE_RESET_GPU_COMMITTED) {
+		/* Reset has already occurred */
+		return;
+	}
+	queue_work(kbdev->reset_workq, &kbdev->reset_work);
+}
+
+static void kbasep_try_reset_gpu_early(kbase_device *kbdev)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+
+	js_devdata = &kbdev->js_data;
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	kbasep_try_reset_gpu_early_locked(kbdev);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+}
+
+/*
+ * Prepare for resetting the GPU.
+ * This function just soft-stops all the slots to ensure that as many jobs as possible are saved.
+ *
+ * The function returns a boolean which should be interpreted as follows:
+ * - MALI_TRUE - Prepared for reset, kbase_reset_gpu should be called.
+ * - MALI_FALSE - Another thread is performing a reset, kbase_reset_gpu should not be called.
+ *
+ * @return See description
+ */
+mali_bool kbase_prepare_to_reset_gpu_locked(kbase_device *kbdev)
+{
+	int i;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	if (atomic_cmpxchg(&kbdev->reset_gpu, KBASE_RESET_GPU_NOT_PENDING, KBASE_RESET_GPU_PREPARED) != KBASE_RESET_GPU_NOT_PENDING) {
+		/* Some other thread is already resetting the GPU */
+		return MALI_FALSE;
+	}
+
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; i++)
+		kbase_job_slot_softstop(kbdev, i, NULL);
+
+	return MALI_TRUE;
+}
+
+mali_bool kbase_prepare_to_reset_gpu(kbase_device *kbdev)
+{
+	unsigned long flags;
+	mali_bool ret;
+	kbasep_js_device_data *js_devdata;
+
+	js_devdata = &kbdev->js_data;
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	ret = kbase_prepare_to_reset_gpu_locked(kbdev);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return ret;
+}
+KBASE_EXPORT_TEST_API(kbase_prepare_to_reset_gpu)
+
+/*
+ * This function should be called after kbase_prepare_to_reset_gpu iff it returns MALI_TRUE.
+ * It should never be called without a corresponding call to kbase_prepare_to_reset_gpu.
+ *
+ * After this function is called (or not called if kbase_prepare_to_reset_gpu returned MALI_FALSE),
+ * the caller should wait for kbdev->reset_waitq to be signalled to know when the reset has completed.
+ */
+void kbase_reset_gpu(kbase_device *kbdev)
+{
+	u32 timeout_ms;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	/* Note this is an assert/atomic_set because it is a software issue for a race to be occuring here */
+	KBASE_DEBUG_ASSERT(atomic_read(&kbdev->reset_gpu) == KBASE_RESET_GPU_PREPARED);
+	atomic_set(&kbdev->reset_gpu, KBASE_RESET_GPU_COMMITTED);
+
+	timeout_ms = kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS);
+	dev_err(kbdev->dev, "Preparing to soft-reset GPU: Waiting (upto %d ms) for all jobs to complete soft-stop\n", timeout_ms);
+	hrtimer_start(&kbdev->reset_timer, HR_TIMER_DELAY_MSEC(timeout_ms), HRTIMER_MODE_REL);
+
+	/* Try resetting early */
+	kbasep_try_reset_gpu_early(kbdev);
+}
+KBASE_EXPORT_TEST_API(kbase_reset_gpu)
+
+void kbase_reset_gpu_locked(kbase_device *kbdev)
+{
+	u32 timeout_ms;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	/* Note this is an assert/atomic_set because it is a software issue for a race to be occuring here */
+	KBASE_DEBUG_ASSERT(atomic_read(&kbdev->reset_gpu) == KBASE_RESET_GPU_PREPARED);
+	atomic_set(&kbdev->reset_gpu, KBASE_RESET_GPU_COMMITTED);
+
+	timeout_ms = kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS);
+	dev_err(kbdev->dev, "Preparing to soft-reset GPU: Waiting (upto %d ms) for all jobs to complete soft-stop\n", timeout_ms);
+	hrtimer_start(&kbdev->reset_timer, HR_TIMER_DELAY_MSEC(timeout_ms), HRTIMER_MODE_REL);
+
+	/* Try resetting early */
+	kbasep_try_reset_gpu_early_locked(kbdev);
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.h
new file mode 100644
index 0000000..bd2b70d
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_jm.h
@@ -0,0 +1,199 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_jm.h
+ * Job Manager Low-level APIs.
+ */
+
+#ifndef _KBASE_JM_H_
+#define _KBASE_JM_H_
+
+#include <mali_kbase_hw.h>
+#include <mali_kbase_debug.h>
+#include <linux/atomic.h>
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_jm Job Manager Low-level APIs
+ * @{
+ *
+ */
+
+static INLINE int kbasep_jm_is_js_free(kbase_device *kbdev, int js, kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(0 <= js && js < kbdev->gpu_props.num_job_slots);
+
+	return !kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), kctx);
+}
+
+/**
+ * This checks that:
+ * - there is enough space in the GPU's buffers (JSn_NEXT and JSn_HEAD registers) to accomodate the job.
+ * - there is enough space to track the job in a our Submit Slots. Note that we have to maintain space to
+ *   requeue one job in case the next registers on the hardware need to be cleared.
+ */
+static INLINE mali_bool kbasep_jm_is_submit_slots_free(kbase_device *kbdev, int js, kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(0 <= js && js < kbdev->gpu_props.num_job_slots);
+
+	if (atomic_read(&kbdev->reset_gpu) != KBASE_RESET_GPU_NOT_PENDING) {
+		/* The GPU is being reset - so prevent submission */
+		return MALI_FALSE;
+	}
+
+	return (mali_bool) (kbasep_jm_is_js_free(kbdev, js, kctx)
+			    && kbdev->jm_slots[js].submitted_nr < (BASE_JM_SUBMIT_SLOTS - 2));
+}
+
+/**
+ * Initialize a submit slot
+ */
+static INLINE void kbasep_jm_init_submit_slot(kbase_jm_slot *slot)
+{
+	slot->submitted_nr = 0;
+	slot->submitted_head = 0;
+}
+
+/**
+ * Find the atom at the idx'th element in the queue without removing it, starting at the head with idx==0.
+ */
+static INLINE kbase_jd_atom *kbasep_jm_peek_idx_submit_slot(kbase_jm_slot *slot, u8 idx)
+{
+	u8 pos;
+	kbase_jd_atom *katom;
+
+	KBASE_DEBUG_ASSERT(idx < BASE_JM_SUBMIT_SLOTS);
+
+	pos = (slot->submitted_head + idx) & BASE_JM_SUBMIT_SLOTS_MASK;
+	katom = slot->submitted[pos];
+
+	return katom;
+}
+
+/**
+ * Pop front of the submitted
+ */
+static INLINE kbase_jd_atom *kbasep_jm_dequeue_submit_slot(kbase_jm_slot *slot)
+{
+	u8 pos;
+	kbase_jd_atom *katom;
+
+	pos = slot->submitted_head & BASE_JM_SUBMIT_SLOTS_MASK;
+	katom = slot->submitted[pos];
+	slot->submitted[pos] = NULL;	/* Just to catch bugs... */
+	KBASE_DEBUG_ASSERT(katom);
+
+	/* rotate the buffers */
+	slot->submitted_head = (slot->submitted_head + 1) & BASE_JM_SUBMIT_SLOTS_MASK;
+	slot->submitted_nr--;
+
+	dev_dbg(katom->kctx->kbdev->dev, "katom %p new head %u", (void *)katom, (unsigned int)slot->submitted_head);
+
+	return katom;
+}
+
+/* Pop back of the submitted queue (unsubmit a job)
+ */
+static INLINE kbase_jd_atom *kbasep_jm_dequeue_tail_submit_slot(kbase_jm_slot *slot)
+{
+	u8 pos;
+
+	slot->submitted_nr--;
+
+	pos = (slot->submitted_head + slot->submitted_nr) & BASE_JM_SUBMIT_SLOTS_MASK;
+
+	return slot->submitted[pos];
+}
+
+static INLINE u8 kbasep_jm_nr_jobs_submitted(kbase_jm_slot *slot)
+{
+	return slot->submitted_nr;
+}
+
+/**
+ * Push back of the submitted
+ */
+static INLINE void kbasep_jm_enqueue_submit_slot(kbase_jm_slot *slot, kbase_jd_atom *katom)
+{
+	u8 nr;
+	u8 pos;
+	nr = slot->submitted_nr++;
+	KBASE_DEBUG_ASSERT(nr < BASE_JM_SUBMIT_SLOTS);
+
+	pos = (slot->submitted_head + nr) & BASE_JM_SUBMIT_SLOTS_MASK;
+	slot->submitted[pos] = katom;
+}
+
+/**
+ * @brief Query whether a job peeked/dequeued from the submit slots is a
+ * 'dummy' job that is used for hardware workaround purposes.
+ *
+ * Any time a job is peeked/dequeued from the submit slots, this should be
+ * queried on that job.
+ *
+ * If a \a atom is indicated as being a dummy job, then you <b>must not attempt
+ * to use \a atom</b>. This is because its members will not necessarily be
+ * initialized, and so could lead to a fault if they were used.
+ *
+ * @param[in] kbdev kbase device pointer
+ * @param[in] atom The atom to query
+ *
+ * @return    MALI_TRUE if \a atom is for a dummy job, in which case you must not
+ *            attempt to use it.
+ * @return    MALI_FALSE otherwise, and \a atom is safe to use.
+ */
+static INLINE mali_bool kbasep_jm_is_dummy_workaround_job(kbase_device *kbdev, kbase_jd_atom *atom)
+{
+	/* Query the set of workaround jobs here */
+	/* none exists today */
+	return MALI_FALSE;
+}
+
+/**
+ * @brief Submit a job to a certain job-slot
+ *
+ * The caller must check kbasep_jm_is_submit_slots_free() != MALI_FALSE before calling this.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold the kbasep_js_device_data::runpoool_irq::lock
+ */
+void kbase_job_submit_nolock(kbase_device *kbdev, kbase_jd_atom *katom, int js);
+
+/**
+ * @brief Complete the head job on a particular job-slot
+ */
+void kbase_job_done_slot(kbase_device *kbdev, int s, u32 completion_code, u64 job_tail, ktime_t *end_timestamp);
+
+	  /** @} *//* end group kbase_jm */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_JM_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.c
new file mode 100644
index 0000000..aeeea68
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.c
@@ -0,0 +1,2144 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/*
+ * Job Scheduler Implementation
+ */
+#include <mali_kbase.h>
+#include <mali_kbase_js.h>
+#include <mali_kbase_js_affinity.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_hw.h>
+
+#include "mali_kbase_jm.h"
+#include <mali_kbase_defs.h>
+
+/*
+ * Private types
+ */
+
+/** Bitpattern indicating the result of releasing a context */
+enum {
+	/** The context was descheduled - caller should try scheduling in a new one
+	 * to keep the runpool full */
+	KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED = (1u << 0),
+};
+
+typedef u32 kbasep_js_release_result;
+
+/*
+ * Private function prototypes
+ */
+STATIC INLINE void kbasep_js_deref_permon_check_and_disable_cycle_counter(kbase_device *kbdev, kbase_jd_atom *katom);
+
+STATIC INLINE void kbasep_js_ref_permon_check_and_enable_cycle_counter(kbase_device *kbdev, kbase_jd_atom *katom);
+
+STATIC kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state);
+
+/** Helper for trace subcodes */
+#if KBASE_TRACE_ENABLE != 0
+STATIC int kbasep_js_trace_get_refcnt(kbase_device *kbdev, kbase_context *kctx)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		kbasep_js_per_as_data *js_per_as_data;
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return refcnt;
+}
+#else				/* KBASE_TRACE_ENABLE != 0 */
+STATIC int kbasep_js_trace_get_refcnt(kbase_device *kbdev, kbase_context *kctx)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(kctx);
+	return 0;
+}
+#endif				/* KBASE_TRACE_ENABLE != 0 */
+
+/*
+ * Private types
+ */
+enum {
+	JS_DEVDATA_INIT_NONE = 0,
+	JS_DEVDATA_INIT_CONSTANTS = (1 << 0),
+	JS_DEVDATA_INIT_POLICY = (1 << 1),
+	JS_DEVDATA_INIT_ALL = ((1 << 2) - 1)
+};
+
+enum {
+	JS_KCTX_INIT_NONE = 0,
+	JS_KCTX_INIT_CONSTANTS = (1 << 0),
+	JS_KCTX_INIT_POLICY = (1 << 1),
+	JS_KCTX_INIT_ALL = ((1 << 2) - 1)
+};
+
+/*
+ * Private functions
+ */
+
+/**
+ * Check if the job had performance monitoring enabled and decrement the count.  If no jobs require
+ * performance monitoring, then the cycle counters will be disabled in the GPU.
+ *
+ * No locks need to be held - locking is handled further down
+ *
+ * This function does not sleep.
+ */
+
+STATIC INLINE void kbasep_js_deref_permon_check_and_disable_cycle_counter(kbase_device *kbdev, kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+
+	if (katom->core_req & BASE_JD_REQ_PERMON)
+		kbase_pm_release_gpu_cycle_counter(kbdev);
+}
+
+/**
+ * Check if the job has performance monitoring enabled and keep a count of it.  If at least one
+ * job requires performance monitoring, then the cycle counters will be enabled in the GPU.
+ *
+ * No locks need to be held - locking is handled further down
+ *
+ * This function does not sleep.
+ */
+
+STATIC INLINE void kbasep_js_ref_permon_check_and_enable_cycle_counter(kbase_device *kbdev, kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+
+	if (katom->core_req & BASE_JD_REQ_PERMON)
+		kbase_pm_request_gpu_cycle_counter(kbdev);
+}
+
+/*
+ * The following locking conditions are made on the caller:
+ * - The caller must hold the kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - The caller must hold the kbasep_js_device_data::runpool_mutex
+ */
+STATIC INLINE void runpool_inc_context_count(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	BUG_ON(!mutex_is_locked(&js_devdata->runpool_mutex));
+
+	/* Track total contexts */
+	KBASE_DEBUG_ASSERT(js_devdata->nr_all_contexts_running < S8_MAX);
+	++(js_devdata->nr_all_contexts_running);
+
+	if ((js_kctx_info->ctx.flags & KBASE_CTX_FLAG_SUBMIT_DISABLED) == 0) {
+		/* Track contexts that can submit jobs */
+		KBASE_DEBUG_ASSERT(js_devdata->nr_user_contexts_running < S8_MAX);
+		++(js_devdata->nr_user_contexts_running);
+	}
+}
+
+/*
+ * The following locking conditions are made on the caller:
+ * - The caller must hold the kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - The caller must hold the kbasep_js_device_data::runpool_mutex
+ */
+STATIC INLINE void runpool_dec_context_count(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	BUG_ON(!mutex_is_locked(&js_devdata->runpool_mutex));
+
+	/* Track total contexts */
+	--(js_devdata->nr_all_contexts_running);
+	KBASE_DEBUG_ASSERT(js_devdata->nr_all_contexts_running >= 0);
+
+	if ((js_kctx_info->ctx.flags & KBASE_CTX_FLAG_SUBMIT_DISABLED) == 0) {
+		/* Track contexts that can submit jobs */
+		--(js_devdata->nr_user_contexts_running);
+		KBASE_DEBUG_ASSERT(js_devdata->nr_user_contexts_running >= 0);
+	}
+}
+
+/**
+ * @brief check whether the runpool is full for a specified context
+ *
+ * If kctx == NULL, then this makes the least restrictive check on the
+ * runpool. A specific context that is supplied immediately after could fail
+ * the check, even under the same conditions.
+ *
+ * Therefore, once a context is obtained you \b must re-check it with this
+ * function, since the return value could change to MALI_FALSE.
+ *
+ * The following locking conditions are made on the caller:
+ * - In all cases, the caller must hold kbasep_js_device_data::runpool_mutex
+ * - When kctx != NULL the caller must hold the kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - When kctx == NULL, then the caller need not hold any jsctx_mutex locks (but it doesn't do any harm to do so).
+ */
+STATIC mali_bool check_is_runpool_full(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	mali_bool is_runpool_full;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+	BUG_ON(!mutex_is_locked(&js_devdata->runpool_mutex));
+
+	/* Regardless of whether a context is submitting or not, can't have more than there
+	 * are HW address spaces */
+	is_runpool_full = (mali_bool) (js_devdata->nr_all_contexts_running >= kbdev->nr_hw_address_spaces);
+
+	if (kctx != NULL && (kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_SUBMIT_DISABLED) == 0) {
+		BUG_ON(!mutex_is_locked(&kctx->jctx.sched_info.ctx.jsctx_mutex));
+		/* Contexts that submit might use less of the address spaces available, due to HW
+		 * workarounds.  In which case, the runpool is also full when the number of
+		 * submitting contexts exceeds the number of submittable address spaces.
+		 *
+		 * Both checks must be made: can have nr_user_address_spaces == nr_hw_address spaces,
+		 * and at the same time can have nr_user_contexts_running < nr_all_contexts_running. */
+		is_runpool_full |= (mali_bool) (js_devdata->nr_user_contexts_running >= kbdev->nr_user_address_spaces);
+	}
+
+	return is_runpool_full;
+}
+
+STATIC base_jd_core_req core_reqs_from_jsn_features(u16 features) /* JS<n>_FEATURE register value */
+{
+	base_jd_core_req core_req = 0u;
+
+	if ((features & JSn_FEATURE_SET_VALUE_JOB) != 0)
+		core_req |= BASE_JD_REQ_V;
+
+	if ((features & JSn_FEATURE_CACHE_FLUSH_JOB) != 0)
+		core_req |= BASE_JD_REQ_CF;
+
+	if ((features & JSn_FEATURE_COMPUTE_JOB) != 0)
+		core_req |= BASE_JD_REQ_CS;
+
+	if ((features & JSn_FEATURE_TILER_JOB) != 0)
+		core_req |= BASE_JD_REQ_T;
+
+	if ((features & JSn_FEATURE_FRAGMENT_JOB) != 0)
+		core_req |= BASE_JD_REQ_FS;
+
+	return core_req;
+}
+
+/**
+ * Picks and reserves an address space.
+ *
+ * When this function returns, the address space returned is reserved and
+ * cannot be picked for another context until it is released.
+ *
+ * The caller must ensure there \b is a free address space before calling this.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_mutex
+ *
+ * @return a non-NULL pointer to a kbase_as that is not in use by any other context
+ */
+STATIC kbase_as *pick_free_addr_space(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	kbase_as *current_as;
+	long ffs_result;
+	js_devdata = &kbdev->js_data;
+
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+
+	/* Find the free address space */
+	ffs_result = ffs(js_devdata->as_free) - 1;
+
+	/* ASSERT that we should've found a free one */
+	KBASE_DEBUG_ASSERT(0 <= ffs_result && ffs_result < kbdev->nr_hw_address_spaces);
+	/* Ensure no-one else picks this one */
+	js_devdata->as_free &= ~((u16) (1u << ffs_result));
+
+	current_as = &kbdev->as[ffs_result];
+
+	return current_as;
+}
+
+/**
+ * Release an address space, making it available for being picked again.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_mutex
+ */
+STATIC INLINE void release_addr_space(kbase_device *kbdev, int kctx_as_nr)
+{
+	kbasep_js_device_data *js_devdata;
+	u16 as_bit = (1u << kctx_as_nr);
+
+	js_devdata = &kbdev->js_data;
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+
+	/* The address space must not already be free */
+	KBASE_DEBUG_ASSERT(!(js_devdata->as_free & as_bit));
+
+	js_devdata->as_free |= as_bit;
+}
+
+/**
+ * Assign an Address Space (AS) to a context, and add the context to the Policy.
+ *
+ * This includes:
+ * - setting up the global runpool_irq structure and the context on the AS
+ * - Activating the MMU on the AS
+ * - Allowing jobs to be submitted on the AS
+ *
+ * Locking conditions:
+ * - Caller must hold the kbasep_js_kctx_info::jsctx_mutex
+ * - Caller must hold the kbasep_js_device_data::runpool_mutex
+ * - Caller must hold AS transaction mutex
+ * - Caller must hold Runpool IRQ lock
+ */
+STATIC void assign_and_activate_kctx_addr_space(kbase_device *kbdev, kbase_context *kctx, kbase_as *current_as)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_per_as_data *js_per_as_data;
+	int as_nr;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(current_as != NULL);
+
+	js_devdata = &kbdev->js_data;
+	as_nr = current_as->number;
+
+	lockdep_assert_held(&kctx->jctx.sched_info.ctx.jsctx_mutex);
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+	lockdep_assert_held(&current_as->transaction_mutex);
+	lockdep_assert_held(&js_devdata->runpool_irq.lock);
+
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+	/* Attribute handling */
+	kbasep_js_ctx_attr_runpool_retain_ctx(kbdev, kctx);
+
+	/* Assign addr space */
+	kctx->as_nr = as_nr;
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	kbase_trace_mali_mmu_as_in_use(kctx->as_nr);
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+	/* Activate this address space on the MMU */
+	kbase_mmu_update(kctx);
+
+	/* Allow it to run jobs */
+	kbasep_js_set_submit_allowed(js_devdata, kctx);
+
+	/* Book-keeping */
+	js_per_as_data->kctx = kctx;
+	js_per_as_data->as_busy_refcount = 0;
+
+	/* Lastly, add the context to the policy's runpool - this really allows it to run jobs */
+	kbasep_js_policy_runpool_add_ctx(&js_devdata->policy, kctx);
+
+}
+
+void kbasep_js_try_run_next_job_nolock(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	int js;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+	lockdep_assert_held(&js_devdata->runpool_irq.lock);
+
+	/* It's cheap and simple to retest this here - otherwise we burden the
+	 * caller with it. In some cases, we do this higher up to optimize out the
+	 * spinlock. */
+	if (js_devdata->nr_user_contexts_running == 0)
+		return; /* No contexts present - the GPU might be powered off, so just return */
+
+	for (js = 0; js < kbdev->gpu_props.num_job_slots; ++js)
+		kbasep_js_try_run_next_job_on_slot_nolock(kbdev, js);
+}
+
+/** Hold the kbasep_js_device_data::runpool_irq::lock for this */
+mali_bool kbasep_js_runpool_retain_ctx_nolock(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_per_as_data *js_per_as_data;
+	mali_bool result = MALI_FALSE;
+	int as_nr;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_devdata = &kbdev->js_data;
+
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		int new_refcnt;
+
+		KBASE_DEBUG_ASSERT(as_nr >= 0);
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		KBASE_DEBUG_ASSERT(js_per_as_data->kctx != NULL);
+
+		new_refcnt = ++(js_per_as_data->as_busy_refcount);
+		KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_RETAIN_CTX_NOLOCK, kctx, NULL, 0u, new_refcnt);
+		result = MALI_TRUE;
+	}
+
+	return result;
+}
+
+/*
+ * Functions private to KBase ('Protected' functions)
+ */
+void kbase_js_try_run_jobs(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	mutex_lock(&js_devdata->runpool_mutex);
+	if (js_devdata->nr_user_contexts_running != 0) {
+		/* Only try running jobs when we have contexts present, otherwise the GPU might be powered off.  */
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+		kbasep_js_try_run_next_job_nolock(kbdev);
+
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	}
+	mutex_unlock(&js_devdata->runpool_mutex);
+}
+
+void kbase_js_try_run_jobs_on_slot(kbase_device *kbdev, int js)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	mutex_lock(&js_devdata->runpool_mutex);
+	if (js_devdata->nr_user_contexts_running != 0) {
+		/* Only try running jobs when we have contexts present, otherwise the GPU might be powered off.  */
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+		kbasep_js_try_run_next_job_on_slot_nolock(kbdev, js);
+
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	}
+	mutex_unlock(&js_devdata->runpool_mutex);
+}
+
+mali_error kbasep_js_devdata_init(kbase_device * const kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	mali_error err;
+	int i;
+	u16 as_present;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	KBASE_DEBUG_ASSERT(js_devdata->init_status == JS_DEVDATA_INIT_NONE);
+
+	/* These two must be recalculated if nr_hw_address_spaces changes (e.g. for HW workarounds) */
+	as_present = (1U << kbdev->nr_hw_address_spaces) - 1;
+	kbdev->nr_user_address_spaces = kbdev->nr_hw_address_spaces;
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		mali_bool use_workaround_for_security;
+		use_workaround_for_security = (mali_bool) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE);
+		if (use_workaround_for_security != MALI_FALSE) {
+			dev_dbg(kbdev->dev, "GPU has HW ISSUE 8987, and driver configured for security workaround: 1 address space only");
+			kbdev->nr_user_address_spaces = 1;
+		}
+	}
+#ifdef CONFIG_MALI_DEBUG
+	/* Soft-stop will be disabled on a single context by default unless softstop_always is set */
+	js_devdata->softstop_always = MALI_FALSE;
+#endif				/* CONFIG_MALI_DEBUG */
+	js_devdata->nr_all_contexts_running = 0;
+	js_devdata->nr_user_contexts_running = 0;
+	js_devdata->as_free = as_present;	/* All ASs initially free */
+	js_devdata->runpool_irq.submit_allowed = 0u;	/* No ctx allowed to submit */
+	memset(js_devdata->runpool_irq.ctx_attr_ref_count, 0, sizeof(js_devdata->runpool_irq.ctx_attr_ref_count));
+	memset(js_devdata->runpool_irq.slot_affinities, 0, sizeof(js_devdata->runpool_irq.slot_affinities));
+	js_devdata->runpool_irq.slots_blocked_on_affinity = 0u;
+	memset(js_devdata->runpool_irq.slot_affinity_refcount, 0, sizeof(js_devdata->runpool_irq.slot_affinity_refcount));
+	INIT_LIST_HEAD(&js_devdata->suspended_soft_jobs_list);
+
+	/* Config attributes */
+	js_devdata->scheduling_tick_ns = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS);
+	js_devdata->soft_stop_ticks = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS);
+	js_devdata->soft_stop_ticks_cl = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL);
+	js_devdata->hard_stop_ticks_ss = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS);
+	js_devdata->hard_stop_ticks_cl = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL);
+	js_devdata->hard_stop_ticks_nss = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS);
+	js_devdata->gpu_reset_ticks_ss = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS);
+	js_devdata->gpu_reset_ticks_cl = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL);
+	js_devdata->gpu_reset_ticks_nss = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS);
+	js_devdata->ctx_timeslice_ns = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS);
+	js_devdata->cfs_ctx_runtime_init_slices = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES);
+	js_devdata->cfs_ctx_runtime_min_slices = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES);
+
+	dev_dbg(kbdev->dev, "JS Config Attribs: ");
+	dev_dbg(kbdev->dev, "\tscheduling_tick_ns:%u", js_devdata->scheduling_tick_ns);
+	dev_dbg(kbdev->dev, "\tsoft_stop_ticks:%u", js_devdata->soft_stop_ticks);
+	dev_dbg(kbdev->dev, "\tsoft_stop_ticks_cl:%u", js_devdata->soft_stop_ticks_cl);
+	dev_dbg(kbdev->dev, "\thard_stop_ticks_ss:%u", js_devdata->hard_stop_ticks_ss);
+	dev_dbg(kbdev->dev, "\thard_stop_ticks_cl:%u", js_devdata->hard_stop_ticks_cl);
+	dev_dbg(kbdev->dev, "\thard_stop_ticks_nss:%u", js_devdata->hard_stop_ticks_nss);
+	dev_dbg(kbdev->dev, "\tgpu_reset_ticks_ss:%u", js_devdata->gpu_reset_ticks_ss);
+	dev_dbg(kbdev->dev, "\tgpu_reset_ticks_cl:%u", js_devdata->gpu_reset_ticks_cl);
+	dev_dbg(kbdev->dev, "\tgpu_reset_ticks_nss:%u", js_devdata->gpu_reset_ticks_nss);
+	dev_dbg(kbdev->dev, "\tctx_timeslice_ns:%u", js_devdata->ctx_timeslice_ns);
+	dev_dbg(kbdev->dev, "\tcfs_ctx_runtime_init_slices:%u", js_devdata->cfs_ctx_runtime_init_slices);
+	dev_dbg(kbdev->dev, "\tcfs_ctx_runtime_min_slices:%u", js_devdata->cfs_ctx_runtime_min_slices);
+
+#if KBASE_DISABLE_SCHEDULING_SOFT_STOPS != 0
+	dev_dbg(kbdev->dev, "Job Scheduling Policy Soft-stops disabled, ignoring value for soft_stop_ticks==%u at %uns per tick. Other soft-stops may still occur.", js_devdata->soft_stop_ticks, js_devdata->scheduling_tick_ns);
+#endif
+#if KBASE_DISABLE_SCHEDULING_HARD_STOPS != 0
+	dev_dbg(kbdev->dev, "Job Scheduling Policy Hard-stops disabled, ignoring values for hard_stop_ticks_ss==%d and hard_stop_ticks_nss==%u at %uns per tick. Other hard-stops may still occur.", js_devdata->hard_stop_ticks_ss, js_devdata->hard_stop_ticks_nss, js_devdata->scheduling_tick_ns);
+#endif
+#if KBASE_DISABLE_SCHEDULING_SOFT_STOPS != 0 && KBASE_DISABLE_SCHEDULING_HARD_STOPS != 0
+	dev_dbg(kbdev->dev, "Note: The JS policy's tick timer (if coded) will still be run, but do nothing.");
+#endif
+
+	/* setup the number of irq throttle cycles base on given time */
+	{
+		int irq_throttle_time_us = kbdev->gpu_props.irq_throttle_time_us;
+		int irq_throttle_cycles = kbasep_js_convert_us_to_gpu_ticks_max_freq(kbdev, irq_throttle_time_us);
+		atomic_set(&kbdev->irq_throttle_cycles, irq_throttle_cycles);
+	}
+
+	/* Clear the AS data, including setting NULL pointers */
+	memset(&js_devdata->runpool_irq.per_as_data[0], 0, sizeof(js_devdata->runpool_irq.per_as_data));
+
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; ++i)
+		js_devdata->js_reqs[i] = core_reqs_from_jsn_features(kbdev->gpu_props.props.raw_props.js_features[i]);
+
+	js_devdata->init_status |= JS_DEVDATA_INIT_CONSTANTS;
+
+	/* On error, we could continue on: providing none of the below resources
+	 * rely on the ones above */
+
+	mutex_init(&js_devdata->runpool_mutex);
+	mutex_init(&js_devdata->queue_mutex);
+	spin_lock_init(&js_devdata->runpool_irq.lock);
+
+	err = kbasep_js_policy_init(kbdev);
+	if (err == MALI_ERROR_NONE)
+		js_devdata->init_status |= JS_DEVDATA_INIT_POLICY;
+
+	/* On error, do no cleanup; this will be handled by the caller(s), since
+	 * we've designed this resource to be safe to terminate on init-fail */
+	if (js_devdata->init_status != JS_DEVDATA_INIT_ALL)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return MALI_ERROR_NONE;
+}
+
+void kbasep_js_devdata_halt(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+void kbasep_js_devdata_term(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	if ((js_devdata->init_status & JS_DEVDATA_INIT_CONSTANTS)) {
+		s8 zero_ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT] = { 0, };
+		/* The caller must de-register all contexts before calling this */
+		KBASE_DEBUG_ASSERT(js_devdata->nr_all_contexts_running == 0);
+		KBASE_DEBUG_ASSERT(memcmp(js_devdata->runpool_irq.ctx_attr_ref_count, zero_ctx_attr_ref_count, sizeof(js_devdata->runpool_irq.ctx_attr_ref_count)) == 0);
+		CSTD_UNUSED(zero_ctx_attr_ref_count);
+	}
+	if ((js_devdata->init_status & JS_DEVDATA_INIT_POLICY))
+		kbasep_js_policy_term(&js_devdata->policy);
+
+	js_devdata->init_status = JS_DEVDATA_INIT_NONE;
+}
+
+mali_error kbasep_js_kctx_init(kbase_context * const kctx)
+{
+	kbase_device *kbdev;
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_error err;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_kctx_info = &kctx->jctx.sched_info;
+	KBASE_DEBUG_ASSERT(js_kctx_info->init_status == JS_KCTX_INIT_NONE);
+
+	js_kctx_info->ctx.nr_jobs = 0;
+	js_kctx_info->ctx.is_scheduled = MALI_FALSE;
+	js_kctx_info->ctx.is_dying = MALI_FALSE;
+	memset(js_kctx_info->ctx.ctx_attr_ref_count, 0, sizeof(js_kctx_info->ctx.ctx_attr_ref_count));
+
+	/* Initially, the context is disabled from submission until the create flags are set */
+	js_kctx_info->ctx.flags = KBASE_CTX_FLAG_SUBMIT_DISABLED;
+
+	js_kctx_info->init_status |= JS_KCTX_INIT_CONSTANTS;
+
+	/* On error, we could continue on: providing none of the below resources
+	 * rely on the ones above */
+	mutex_init(&js_kctx_info->ctx.jsctx_mutex);
+
+	init_waitqueue_head(&js_kctx_info->ctx.is_scheduled_wait);
+
+	err = kbasep_js_policy_init_ctx(kbdev, kctx);
+	if (err == MALI_ERROR_NONE)
+		js_kctx_info->init_status |= JS_KCTX_INIT_POLICY;
+
+	/* On error, do no cleanup; this will be handled by the caller(s), since
+	 * we've designed this resource to be safe to terminate on init-fail */
+	if (js_kctx_info->init_status != JS_KCTX_INIT_ALL)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return MALI_ERROR_NONE;
+}
+
+void kbasep_js_kctx_term(kbase_context *kctx)
+{
+	kbase_device *kbdev;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_policy *js_policy;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbdev = kctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_policy = &kbdev->js_data.policy;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	if ((js_kctx_info->init_status & JS_KCTX_INIT_CONSTANTS)) {
+		/* The caller must de-register all jobs before calling this */
+		KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled == MALI_FALSE);
+		KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs == 0);
+	}
+
+	if ((js_kctx_info->init_status & JS_KCTX_INIT_POLICY))
+		kbasep_js_policy_term_ctx(js_policy, kctx);
+
+	js_kctx_info->init_status = JS_KCTX_INIT_NONE;
+}
+
+/* Evict jobs from the NEXT registers
+ *
+ * The caller must hold:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex
+ * - kbasep_js_device_data::runpool_mutex
+ */
+STATIC void kbasep_js_runpool_evict_next_jobs(kbase_device *kbdev, kbase_context *kctx)
+{
+	unsigned long flags;
+	int js;
+	kbasep_js_device_data *js_devdata;
+
+	js_devdata = &kbdev->js_data;
+
+	BUG_ON(!mutex_is_locked(&kctx->jctx.sched_info.ctx.jsctx_mutex));
+	BUG_ON(!mutex_is_locked(&js_devdata->runpool_mutex));
+
+	/* Prevent contexts in the runpool from submitting jobs */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* There's no need to prevent contexts in the runpool from submitting jobs,
+	 * because we complete this operation by the time we release the
+	 * runpool_irq.lock */
+
+	/* Evict jobs from the NEXT registers */
+	for (js = 0; js < kbdev->gpu_props.num_job_slots; js++) {
+		kbase_jm_slot *slot;
+		kbase_jd_atom *tail;
+
+		if (!kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), NULL)) {
+			/* No job in the NEXT register */
+			continue;
+		}
+
+		slot = &kbdev->jm_slots[js];
+		tail = kbasep_jm_peek_idx_submit_slot(slot, slot->submitted_nr - 1);
+
+		KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, 1);
+		/* Clearing job from next registers */
+		kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_COMMAND_NEXT), JSn_COMMAND_NOP, NULL);
+
+		/* Check to see if we did remove a job from the next registers */
+		if (kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), NULL) != 0 || kbase_reg_read(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), NULL) != 0) {
+			/* The job was successfully cleared from the next registers, requeue it */
+			kbase_jd_atom *dequeued_katom = kbasep_jm_dequeue_tail_submit_slot(slot);
+			KBASE_DEBUG_ASSERT(dequeued_katom == tail);
+
+			/* Set the next registers to NULL */
+			kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_LO), 0, NULL);
+			kbase_reg_write(kbdev, JOB_SLOT_REG(js, JSn_HEAD_NEXT_HI), 0, NULL);
+
+			KBASE_TRACE_ADD_SLOT(kbdev, JM_SLOT_EVICT, dequeued_katom->kctx, dequeued_katom, dequeued_katom->jc, js);
+
+			/* Complete the job, indicate that it took no time, and don't start
+			 * new atoms */
+			kbase_jd_done(dequeued_katom, js, NULL, KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT);
+		}
+		KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, 0);
+	}
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+}
+
+/**
+ * Fast start a higher priority job
+ * If the runpool is full, the lower priority contexts with no running jobs
+ * will be evicted from the runpool
+ *
+ * If \a kctx_new is NULL, the first context with no running jobs will be evicted
+ *
+ * The following locking conditions are made on the caller:
+ * - The caller must \b not hold \a kctx_new's
+ * kbasep_js_kctx_info::ctx::jsctx_mutex, or that mutex of any ctx in the
+ * runpool. This is because \a kctx_new's jsctx_mutex and one of the other
+ * scheduled ctx's jsctx_mutex will be obtained internally.
+ * - it must \em not hold kbasep_js_device_data::runpool_irq::lock (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (again, it's used
+ * internally).
+ */
+STATIC void kbasep_js_runpool_attempt_fast_start_ctx(kbase_device *kbdev, kbase_context *kctx_new)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_new;
+	kbasep_js_policy *js_policy;
+	kbasep_js_per_as_data *js_per_as_data;
+	int evict_as_nr;
+	kbasep_js_atom_retained_state katom_retained_state;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+
+	if (kctx_new != NULL) {
+		js_kctx_new = &kctx_new->jctx.sched_info;
+		mutex_lock(&js_kctx_new->ctx.jsctx_mutex);
+	} else {
+		js_kctx_new = NULL;
+		CSTD_UNUSED(js_kctx_new);
+	}
+
+	/* Setup a dummy katom_retained_state */
+	kbasep_js_atom_retained_state_init_invalid(&katom_retained_state);
+
+	mutex_lock(&js_devdata->runpool_mutex);
+
+	/* If the runpool is full and either there is no specified context or the specified context is not dying, then
+	   attempt to fast start the specified context or evict the first context with no running jobs. */
+	if (check_is_runpool_full(kbdev, kctx_new) && 
+            (!js_kctx_new || (js_kctx_new && !js_kctx_new->ctx.is_dying))) {
+		/* No free address spaces - attempt to evict non-running lower priority context */
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+		for (evict_as_nr = 0; evict_as_nr < kbdev->nr_hw_address_spaces; evict_as_nr++) {
+			kbase_context *kctx_evict;
+			js_per_as_data = &js_devdata->runpool_irq.per_as_data[evict_as_nr];
+			kctx_evict = js_per_as_data->kctx;
+
+			/* Look for the AS which is not currently running */
+			if (0 == js_per_as_data->as_busy_refcount && kctx_evict != NULL) {
+				/* Now compare the scheduled priority we are considering evicting with the new ctx priority
+				 * and take into consideration if the scheduled priority is a realtime policy or not.
+				 * Note that the lower the number, the higher the priority
+				 */
+				if ((kctx_new == NULL) || kbasep_js_policy_ctx_has_priority(js_policy, kctx_evict, kctx_new)) {
+					mali_bool retain_result;
+					kbasep_js_release_result release_result;
+					KBASE_TRACE_ADD(kbdev, JS_FAST_START_EVICTS_CTX, kctx_evict, NULL, 0u, (uintptr_t)kctx_new);
+
+					/* Retain the ctx to work on it - this shouldn't be able to fail */
+					retain_result = kbasep_js_runpool_retain_ctx_nolock(kbdev, kctx_evict);
+					KBASE_DEBUG_ASSERT(retain_result != MALI_FALSE);
+					CSTD_UNUSED(retain_result);
+
+					/* This will cause the context to be scheduled out on the next runpool_release_ctx(),
+					 * and also stop its refcount increasing */
+					kbasep_js_clear_submit_allowed(js_devdata, kctx_evict);
+
+					spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+					mutex_unlock(&js_devdata->runpool_mutex);
+					if (kctx_new != NULL)
+						mutex_unlock(&js_kctx_new->ctx.jsctx_mutex);
+
+					/* Stop working on the target context, start working on the kctx_evict context */
+
+					mutex_lock(&kctx_evict->jctx.sched_info.ctx.jsctx_mutex);
+					mutex_lock(&js_devdata->runpool_mutex);
+					release_result = kbasep_js_runpool_release_ctx_internal(kbdev, kctx_evict, &katom_retained_state);
+					mutex_unlock(&js_devdata->runpool_mutex);
+					/* Only requeue if actually descheduled, which is more robust in case
+					 * something else retains it (e.g. two high priority contexts racing
+					 * to evict the same lower priority context) */
+					if ((release_result & KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED) != 0u)
+						kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx_evict, MALI_TRUE);
+
+					mutex_unlock(&kctx_evict->jctx.sched_info.ctx.jsctx_mutex);
+
+					/* release_result isn't propogated further:
+					 * - the caller will be scheduling in a context anyway
+					 * - which will also cause new jobs to run */
+
+					/* ctx fast start has taken place */
+					return;
+				}
+			}
+		}
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	}
+
+	/* ctx fast start has not taken place */
+	mutex_unlock(&js_devdata->runpool_mutex);
+	if (kctx_new != NULL)
+		mutex_unlock(&js_kctx_new->ctx.jsctx_mutex);
+}
+
+mali_bool kbasep_js_add_job(kbase_context *kctx, kbase_jd_atom *atom)
+{
+	unsigned long flags;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy *js_policy;
+
+	mali_bool policy_queue_updated = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(atom != NULL);
+	lockdep_assert_held(&kctx->jctx.lock);
+
+	kbdev = kctx->kbdev;
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	KBASE_TIMELINE_ATOM_READY(kctx, kbase_jd_atom_id(kctx, atom));
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	/* Policy-specific initialization of atoms (which cannot fail). Anything that
+	 * could've failed must've been done at kbasep_jd_policy_init_job() time. */
+	kbasep_js_policy_register_job(js_policy, kctx, atom);
+
+	/*
+	 * Begin Runpool transaction
+	 */
+	mutex_lock(&js_devdata->runpool_mutex);
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_ADD_JOB, kctx, atom, atom->jc, kbasep_js_trace_get_refcnt(kbdev, kctx));
+
+	/* Refcount ctx.nr_jobs */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs < U32_MAX);
+	++(js_kctx_info->ctx.nr_jobs);
+
+	/* Setup any scheduling information */
+	kbasep_js_clear_job_retry_submit(atom);
+
+	/* Lock for state available during IRQ */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* Context Attribute Refcounting */
+	kbasep_js_ctx_attr_ctx_retain_atom(kbdev, kctx, atom);
+
+	/* Enqueue the job in the policy, causing it to be scheduled if the
+	 * parent context gets scheduled */
+	kbasep_js_policy_enqueue_job(js_policy, atom);
+
+	if (js_kctx_info->ctx.is_scheduled != MALI_FALSE) {
+		/* Handle an already running context - try to run the new job, in case it
+		 * matches requirements that aren't matched by any other job in the Run
+		 * Pool */
+		kbasep_js_try_run_next_job_nolock(kbdev);
+	}
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	mutex_unlock(&js_devdata->runpool_mutex);
+	/* End runpool transaction */
+
+	if (js_kctx_info->ctx.is_scheduled == MALI_FALSE) {
+		if (js_kctx_info->ctx.is_dying) {
+			/* A job got added while/after kbase_job_zap_context() was called
+			 * on a non-scheduled context (e.g. KDS dependency resolved). Kill
+			 * that job by killing the context. */
+			kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx, MALI_FALSE);
+		} else if (js_kctx_info->ctx.nr_jobs == 1) {
+			/* Handle Refcount going from 0 to 1: schedule the context on the Policy Queue */
+			KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled == MALI_FALSE);
+			dev_dbg(kbdev->dev, "JS: Enqueue Context %p", kctx);
+
+			mutex_lock(&js_devdata->queue_mutex);
+			kbasep_js_policy_enqueue_ctx(js_policy, kctx);
+			mutex_unlock(&js_devdata->queue_mutex);
+
+			/* Policy Queue was updated - caller must try to schedule the head context
+			 * We also try to encourage a fast-start from here. */
+			policy_queue_updated = MALI_TRUE;
+		}
+	}
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+	/* If the runpool is full and this job has a higher priority than the
+	 * non-running job in the runpool - evict it so this higher priority job
+	 * starts faster.  Fast-starting requires the jsctx_mutex to be dropped,
+	 * because it works on multiple ctxs
+	 *
+	 * Note: If the context is being killed with kbase_job_zap_context(), then
+	 * kctx can't disappear after the jsctx_mutex was dropped. This is because
+	 * the caller holds kctx->jctx.lock */
+	if (policy_queue_updated)
+		kbasep_js_runpool_attempt_fast_start_ctx(kbdev, kctx);
+
+	return policy_queue_updated;
+}
+
+void kbasep_js_remove_job(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *atom)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy *js_policy;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(atom != NULL);
+
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_REMOVE_JOB, kctx, atom, atom->jc, kbasep_js_trace_get_refcnt(kbdev, kctx));
+
+	/* De-refcount ctx.nr_jobs */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.nr_jobs > 0);
+	--(js_kctx_info->ctx.nr_jobs);
+
+	/* De-register the job from the system */
+	kbasep_js_policy_deregister_job(js_policy, kctx, atom);
+}
+
+void kbasep_js_remove_cancelled_job(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	unsigned long flags;
+	kbasep_js_atom_retained_state katom_retained_state;
+	kbasep_js_device_data *js_devdata;
+	mali_bool attr_state_changed;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	kbasep_js_atom_retained_state_copy(&katom_retained_state, katom);
+	kbasep_js_remove_job(kbdev, kctx, katom);
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* The atom has 'finished' (will not be re-run), so no need to call
+	 * kbasep_js_has_atom_finished().
+	 *
+	 * This is because it returns MALI_FALSE for soft-stopped atoms, but we
+	 * want to override that, because we're cancelling an atom regardless of
+	 * whether it was soft-stopped or not */
+	attr_state_changed = kbasep_js_ctx_attr_ctx_release_atom(kbdev, kctx, &katom_retained_state);
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	if (attr_state_changed != MALI_FALSE) {
+		/* A change in runpool ctx attributes might mean we can run more jobs
+		 * than before. */
+		kbase_js_try_run_jobs(kbdev);
+	}
+}
+
+mali_bool kbasep_js_runpool_retain_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	mali_bool result;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	/* KBASE_TRACE_ADD_REFCOUNT( kbdev, JS_RETAIN_CTX, kctx, NULL, 0,
+	   kbasep_js_trace_get_refcnt(kbdev, kctx)); */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	result = kbasep_js_runpool_retain_ctx_nolock(kbdev, kctx);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return result;
+}
+
+kbase_context *kbasep_js_runpool_lookup_ctx(kbase_device *kbdev, int as_nr)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	kbase_context *found_kctx = NULL;
+	kbasep_js_per_as_data *js_per_as_data;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(0 <= as_nr && as_nr < BASE_MAX_NR_AS);
+	js_devdata = &kbdev->js_data;
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	found_kctx = js_per_as_data->kctx;
+
+	if (found_kctx != NULL)
+		++(js_per_as_data->as_busy_refcount);
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return found_kctx;
+}
+
+/**
+ * @brief Try running more jobs after releasing a context and/or atom
+ *
+ * This collates a set of actions that must happen whilst
+ * kbasep_js_device_data::runpool_irq::lock is held.
+ *
+ * This includes running more jobs when:
+ * - The previously released kctx caused a ctx attribute change
+ * - The released atom caused a ctx attribute change
+ * - Slots were previously blocked due to affinity restrictions
+ * - Submission during IRQ handling failed
+ */
+STATIC void kbasep_js_run_jobs_after_ctx_and_atom_release(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state, mali_bool runpool_ctx_attr_change)
+{
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(katom_retained_state != NULL);
+	js_devdata = &kbdev->js_data;
+
+	lockdep_assert_held(&kctx->jctx.sched_info.ctx.jsctx_mutex);
+	lockdep_assert_held(&js_devdata->runpool_mutex);
+	lockdep_assert_held(&js_devdata->runpool_irq.lock);
+
+	if (js_devdata->nr_user_contexts_running != 0) {
+		mali_bool retry_submit;
+		int retry_jobslot;
+
+		retry_submit = kbasep_js_get_atom_retry_submit_slot(katom_retained_state, &retry_jobslot);
+
+		if (runpool_ctx_attr_change != MALI_FALSE) {
+			/* A change in runpool ctx attributes might mean we can run more jobs
+			 * than before  */
+			kbasep_js_try_run_next_job_nolock(kbdev);
+
+			/* A retry submit on all slots has now happened, so don't need to do it again */
+			retry_submit = MALI_FALSE;
+		}
+
+		/* Submit on any slots that might've had atoms blocked by the affinity of
+		 * a completed atom.
+		 *
+		 * If no atom has recently completed, then this is harmelss */
+		kbase_js_affinity_submit_to_blocked_slots(kbdev);
+
+		/* If the IRQ handler failed to get a job from the policy, try again from
+		 * outside the IRQ handler
+		 * NOTE: We may've already cleared retry_submit from submitting above */
+		if (retry_submit != MALI_FALSE) {
+			KBASE_TRACE_ADD_SLOT(kbdev, JD_DONE_TRY_RUN_NEXT_JOB, kctx, NULL, 0u, retry_jobslot);
+			kbasep_js_try_run_next_job_on_slot_nolock(kbdev, retry_jobslot);
+		}
+	}
+}
+
+/**
+ * Internal function to release the reference on a ctx and an atom's "retained
+ * state", only taking the runpool and as transaction mutexes
+ *
+ * This also starts more jobs running in the case of an ctx-attribute state change
+ *
+ * This does none of the followup actions for scheduling:
+ * - It does not schedule in a new context
+ * - It does not requeue or handle dying contexts
+ *
+ * For those tasks, just call kbasep_js_runpool_release_ctx() instead
+ *
+ * Requires:
+ * - Context is scheduled in, and kctx->as_nr matches kctx_as_nr
+ * - Context has a non-zero refcount
+ * - Caller holds js_kctx_info->ctx.jsctx_mutex
+ * - Caller holds js_devdata->runpool_mutex
+ */
+STATIC kbasep_js_release_result kbasep_js_runpool_release_ctx_internal(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_policy *js_policy;
+	kbasep_js_per_as_data *js_per_as_data;
+
+	kbasep_js_release_result release_result = 0u;
+	mali_bool runpool_ctx_attr_change = MALI_FALSE;
+	int kctx_as_nr;
+	kbase_as *current_as;
+	int new_ref_count;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+
+	/* Ensure context really is scheduled in */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled != MALI_FALSE);
+
+	/* kctx->as_nr and js_per_as_data are only read from here. The caller's
+	 * js_ctx_mutex provides a barrier that ensures they are up-to-date.
+	 *
+	 * They will not change whilst we're reading them, because the refcount
+	 * is non-zero (and we ASSERT on that last fact).
+	 */
+	kctx_as_nr = kctx->as_nr;
+	KBASE_DEBUG_ASSERT(kctx_as_nr != KBASEP_AS_NR_INVALID);
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[kctx_as_nr];
+	KBASE_DEBUG_ASSERT(js_per_as_data->as_busy_refcount > 0);
+
+	/*
+	 * Transaction begins on AS and runpool_irq
+	 *
+	 * Assert about out calling contract
+	 */
+	current_as = &kbdev->as[kctx_as_nr];
+	mutex_lock(&current_as->transaction_mutex);
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	KBASE_DEBUG_ASSERT(kctx_as_nr == kctx->as_nr);
+	KBASE_DEBUG_ASSERT(js_per_as_data->as_busy_refcount > 0);
+
+	/* Update refcount */
+	new_ref_count = --(js_per_as_data->as_busy_refcount);
+
+	/* Release the atom if it finished (i.e. wasn't soft-stopped) */
+	if (kbasep_js_has_atom_finished(katom_retained_state) != MALI_FALSE)
+		runpool_ctx_attr_change |= kbasep_js_ctx_attr_ctx_release_atom(kbdev, kctx, katom_retained_state);
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_RELEASE_CTX, kctx, NULL, 0u, new_ref_count);
+
+	if (new_ref_count == 1 && kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED
+		&& !kbase_pm_is_suspending(kbdev) ) {
+		/* Context is kept scheduled into an address space even when there are no jobs, in this case we have
+		 * to handle the situation where all jobs have been evicted from the GPU and submission is disabled.
+		 *
+		 * At this point we re-enable submission to allow further jobs to be executed
+		 */
+		kbasep_js_set_submit_allowed(js_devdata, kctx);
+	}
+
+	/* Make a set of checks to see if the context should be scheduled out */
+	if (new_ref_count == 0 && (kctx->jctx.sched_info.ctx.nr_jobs == 0 || kbasep_js_is_submit_allowed(js_devdata, kctx) == MALI_FALSE)) {
+		/* Last reference, and we've been told to remove this context from the Run Pool */
+		dev_dbg(kbdev->dev, "JS: RunPool Remove Context %p because as_busy_refcount=%d, jobs=%d, allowed=%d", kctx, new_ref_count, js_kctx_info->ctx.nr_jobs, kbasep_js_is_submit_allowed(js_devdata, kctx));
+
+		kbasep_js_policy_runpool_remove_ctx(js_policy, kctx);
+
+		/* Stop any more refcounts occuring on the context */
+		js_per_as_data->kctx = NULL;
+
+		/* Ensure we prevent the context from submitting any new jobs
+		 * e.g. from kbasep_js_try_run_next_job_on_slot_irq_nolock()  */
+		kbasep_js_clear_submit_allowed(js_devdata, kctx);
+
+		/* Disable the MMU on the affected address space, and indicate it's invalid */
+		kbase_mmu_disable(kctx);
+
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+		kbase_trace_mali_mmu_as_released(kctx->as_nr);
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+
+		kctx->as_nr = KBASEP_AS_NR_INVALID;
+
+		/* Ctx Attribute handling
+		 *
+		 * Releasing atoms attributes must either happen before this, or after
+		 * 'is_scheduled' is changed, otherwise we double-decount the attributes*/
+		runpool_ctx_attr_change |= kbasep_js_ctx_attr_runpool_release_ctx(kbdev, kctx);
+
+		/* Early update of context count, to optimize the
+		 * kbasep_js_run_jobs_after_ctx_and_atom_release() call */
+		runpool_dec_context_count(kbdev, kctx);
+
+		/* Releasing the context and katom retained state can allow more jobs to run */
+		kbasep_js_run_jobs_after_ctx_and_atom_release(kbdev, kctx, katom_retained_state, runpool_ctx_attr_change);
+
+		/*
+		 * Transaction ends on AS and runpool_irq:
+		 *
+		 * By this point, the AS-related data is now clear and ready for re-use.
+		 *
+		 * Since releases only occur once for each previous successful retain, and no more
+		 * retains are allowed on this context, no other thread will be operating in this
+		 * code whilst we are
+		 */
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+		mutex_unlock(&current_as->transaction_mutex);
+
+		/* Free up the address space */
+		release_addr_space(kbdev, kctx_as_nr);
+		/* Note: Don't reuse kctx_as_nr now */
+
+		/* Synchronize with any policy timers */
+		kbasep_js_policy_runpool_timers_sync(js_policy);
+
+		/* update book-keeping info */
+		js_kctx_info->ctx.is_scheduled = MALI_FALSE;
+		/* Signal any waiter that the context is not scheduled, so is safe for
+		 * termination - once the jsctx_mutex is also dropped, and jobs have
+		 * finished. */
+		wake_up(&js_kctx_info->ctx.is_scheduled_wait);
+
+		/* Queue an action to occur after we've dropped the lock */
+		release_result |= KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED;
+
+	} else {
+		kbasep_js_run_jobs_after_ctx_and_atom_release(kbdev, kctx, katom_retained_state, runpool_ctx_attr_change);
+
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+		mutex_unlock(&current_as->transaction_mutex);
+	}
+
+	return release_result;
+}
+
+void kbasep_js_runpool_requeue_or_kill_ctx(kbase_device *kbdev, kbase_context *kctx, mali_bool has_pm_ref)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy *js_policy;
+	kbasep_js_kctx_info *js_kctx_info;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+	js_policy = &kbdev->js_data.policy;
+	js_devdata = &kbdev->js_data;
+
+	/* This is called if and only if you've you've detached the context from
+	 * the Runpool or the Policy Queue, and not added it back to the Runpool */
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled == MALI_FALSE);
+
+	if (js_kctx_info->ctx.is_dying != MALI_FALSE) {
+		/* Dying: don't requeue, but kill all jobs on the context. This happens
+		 * asynchronously */
+		dev_dbg(kbdev->dev, "JS: ** Killing Context %p on RunPool Remove **", kctx);
+		kbasep_js_policy_foreach_ctx_job(js_policy, kctx, &kbase_jd_cancel, MALI_TRUE);
+	} else if (js_kctx_info->ctx.nr_jobs > 0) {
+		/* Not dying, has jobs: de-ref core counts from each job before addding
+		 * back to the queue */
+		kbasep_js_policy_foreach_ctx_job(js_policy, kctx, &kbasep_js_job_check_deref_cores, MALI_FALSE);
+
+		dev_dbg(kbdev->dev, "JS: Requeue Context %p", kctx);
+		mutex_lock(&js_devdata->queue_mutex);
+		kbasep_js_policy_enqueue_ctx(js_policy, kctx);
+		mutex_unlock(&js_devdata->queue_mutex);
+	} else {
+		/* Not dying, no jobs: don't add back to the queue */
+		dev_dbg(kbdev->dev, "JS: Idling Context %p (not requeued)", kctx);
+	}
+
+	if (has_pm_ref) {
+		/* In all cases where we had a pm active refcount, release it */
+		kbase_pm_context_idle(kbdev);
+	}
+}
+
+void kbasep_js_runpool_release_ctx_and_katom_retained_state(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_release_result release_result;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+	js_devdata = &kbdev->js_data;
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	mutex_lock(&js_devdata->runpool_mutex);
+	release_result = kbasep_js_runpool_release_ctx_internal(kbdev, kctx, katom_retained_state);
+
+	/* Drop the runpool mutex to allow requeing kctx */
+	mutex_unlock(&js_devdata->runpool_mutex);
+	if ((release_result & KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED) != 0u)
+		kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx, MALI_TRUE);
+
+	/* Drop the jsctx_mutex to allow scheduling in a new context */
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+	if ((release_result & KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED) != 0u) {
+		/* We've freed up an address space, so let's try to schedule in another
+		 * context
+		 *
+		 * Note: if there's a context to schedule in, then it also tries to run
+		 * another job, in case the new context has jobs satisfying requirements
+		 * that no other context/job in the runpool does */
+		kbasep_js_try_schedule_head_ctx(kbdev);
+	}
+}
+
+void kbasep_js_runpool_release_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_atom_retained_state katom_retained_state;
+
+	kbasep_js_atom_retained_state_init_invalid(&katom_retained_state);
+
+	kbasep_js_runpool_release_ctx_and_katom_retained_state(kbdev, kctx, &katom_retained_state);
+}
+
+/** Variant of kbasep_js_runpool_release_ctx() that doesn't call into
+ * kbasep_js_try_schedule_head_ctx() */
+STATIC void kbasep_js_runpool_release_ctx_no_schedule(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_release_result release_result;
+	kbasep_js_atom_retained_state katom_retained_state_struct;
+	kbasep_js_atom_retained_state *katom_retained_state = &katom_retained_state_struct;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+	js_devdata = &kbdev->js_data;
+	kbasep_js_atom_retained_state_init_invalid(katom_retained_state);
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	mutex_lock(&js_devdata->runpool_mutex);
+	release_result = kbasep_js_runpool_release_ctx_internal(kbdev, kctx, katom_retained_state);
+
+	/* Drop the runpool mutex to allow requeing kctx */
+	mutex_unlock(&js_devdata->runpool_mutex);
+	if ((release_result & KBASEP_JS_RELEASE_RESULT_WAS_DESCHEDULED) != 0u)
+		kbasep_js_runpool_requeue_or_kill_ctx(kbdev, kctx, MALI_TRUE);
+
+	/* Drop the jsctx_mutex to allow scheduling in a new context */
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+	/* NOTE: could return release_result if the caller would like to know
+	 * whether it should schedule a new context, but currently no callers do */
+}
+
+
+/**
+ * @brief Handle retaining cores for power management and affinity management,
+ * ensuring that cores are powered up and won't violate affinity restrictions.
+ *
+ * This function enters at the following @ref kbase_atom_coreref_state states:
+ *
+ * - NO_CORES_REQUESTED,
+ * - WAITING_FOR_REQUESTED_CORES,
+ * - RECHECK_AFFINITY,
+ *
+ * The transitions are as folows:
+ * - NO_CORES_REQUESTED -> WAITING_FOR_REQUESTED_CORES
+ * - WAITING_FOR_REQUESTED_CORES -> ( WAITING_FOR_REQUESTED_CORES or RECHECK_AFFINITY )
+ * - RECHECK_AFFINITY -> ( WAITING_FOR_REQUESTED_CORES or CHECK_AFFINITY_VIOLATIONS )
+ * - CHECK_AFFINITY_VIOLATIONS -> ( RECHECK_AFFINITY or READY )
+ *
+ * The caller must hold:
+ * - kbasep_js_device_data::runpool_irq::lock
+ *
+ * @return MALI_FALSE when the function makes a transition to the same or lower state, indicating
+ * that the cores are not ready.
+ * @return MALI_TRUE once READY state is reached, indicating that the cores are 'ready' and won't
+ * violate affinity restrictions.
+ *
+ */
+STATIC mali_bool kbasep_js_job_check_ref_cores(kbase_device *kbdev, int js, kbase_jd_atom *katom)
+{
+	/* The most recently checked affinity. Having this at this scope allows us
+	 * to guarantee that we've checked the affinity in this function call. */
+	u64 recently_chosen_affinity = 0;
+	mali_bool chosen_affinity = MALI_FALSE;
+	mali_bool retry;
+
+	do {
+		retry = MALI_FALSE;
+
+		/* NOTE: The following uses a number of FALLTHROUGHs to optimize the
+		 * calls to this function. Ending of the function is indicated by BREAK OUT */
+		switch (katom->coreref_state) {
+			/* State when job is first attempted to be run */
+		case KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED:
+			KBASE_DEBUG_ASSERT(katom->affinity == 0);
+
+			/* Compute affinity */
+			if (MALI_FALSE == kbase_js_choose_affinity(&recently_chosen_affinity, kbdev, katom, js)) {
+				/* No cores are currently available */
+				/* *** BREAK OUT: No state transition *** */
+				break;
+			}		
+
+			chosen_affinity = MALI_TRUE;
+
+			/* Request the cores */
+			kbase_pm_request_cores(kbdev, katom->core_req & BASE_JD_REQ_T, recently_chosen_affinity);
+
+			katom->affinity = recently_chosen_affinity;
+
+			/* Proceed to next state */
+			katom->coreref_state = KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES;
+
+			/* ***FALLTHROUGH: TRANSITION TO HIGHER STATE*** */
+
+		case KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES:
+			{
+				kbase_pm_cores_ready cores_ready;
+				KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+
+				cores_ready = kbase_pm_register_inuse_cores(kbdev, katom->core_req & BASE_JD_REQ_T, katom->affinity);
+				if (cores_ready == KBASE_NEW_AFFINITY) {
+					/* Affinity no longer valid - return to previous state */
+					kbasep_js_job_check_deref_cores(kbdev, katom);
+					KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_REGISTER_INUSE_FAILED, katom->kctx, katom, katom->jc, js, (u32) katom->affinity);
+					/* *** BREAK OUT: Return to previous state, retry *** */
+					retry = MALI_TRUE;
+					break;
+				}
+				if (cores_ready == KBASE_CORES_NOT_READY) {
+					/* Stay in this state and return, to retry at this state later */
+					KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_REGISTER_INUSE_FAILED, katom->kctx, katom, katom->jc, js, (u32) katom->affinity);
+					/* *** BREAK OUT: No state transition *** */
+					break;
+				}
+				/* Proceed to next state */
+				katom->coreref_state = KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY;
+			}
+
+			/* ***FALLTHROUGH: TRANSITION TO HIGHER STATE*** */
+
+		case KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY:
+			KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+
+			/* Optimize out choosing the affinity twice in the same function call */
+			if (chosen_affinity == MALI_FALSE) {
+				/* See if the affinity changed since a previous call. */
+				if (MALI_FALSE == kbase_js_choose_affinity(&recently_chosen_affinity, kbdev, katom, js)) {
+					/* No cores are currently available */
+					kbasep_js_job_check_deref_cores(kbdev, katom);
+					KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_REQUEST_ON_RECHECK_FAILED, katom->kctx, katom, katom->jc, js, (u32) recently_chosen_affinity);
+					/* *** BREAK OUT: Transition to lower state *** */
+					break;
+				}		
+				chosen_affinity = MALI_TRUE;
+			}
+
+			/* Now see if this requires a different set of cores */
+			if (recently_chosen_affinity != katom->affinity) {
+				kbase_pm_cores_ready cores_ready;
+
+				kbase_pm_request_cores(kbdev, katom->core_req & BASE_JD_REQ_T, recently_chosen_affinity);
+
+				/* Register new cores whilst we still hold the old ones, to minimize power transitions */
+				cores_ready = kbase_pm_register_inuse_cores(kbdev, katom->core_req & BASE_JD_REQ_T, recently_chosen_affinity);
+				kbasep_js_job_check_deref_cores(kbdev, katom);
+
+				/* Fixup the state that was reduced by deref_cores: */
+				katom->coreref_state = KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY;
+				katom->affinity = recently_chosen_affinity;
+				if (cores_ready == KBASE_NEW_AFFINITY) {
+					/* Affinity no longer valid - return to previous state */
+					katom->coreref_state = KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES;
+					kbasep_js_job_check_deref_cores(kbdev, katom);
+					KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_REGISTER_INUSE_FAILED, katom->kctx, katom, katom->jc, js, (u32) katom->affinity);
+					/* *** BREAK OUT: Return to previous state, retry *** */
+					retry = MALI_TRUE;
+					break;
+				}
+				/* Now might be waiting for powerup again, with a new affinity */
+				if (cores_ready == KBASE_CORES_NOT_READY) {
+					/* Return to previous state */
+					katom->coreref_state = KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES;
+					KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_REGISTER_ON_RECHECK_FAILED, katom->kctx, katom, katom->jc, js, (u32) katom->affinity);
+					/* *** BREAK OUT: Transition to lower state *** */
+					break;
+				}
+			}
+			/* Proceed to next state */
+			katom->coreref_state = KBASE_ATOM_COREREF_STATE_CHECK_AFFINITY_VIOLATIONS;
+
+			/* ***FALLTHROUGH: TRANSITION TO HIGHER STATE*** */
+		case KBASE_ATOM_COREREF_STATE_CHECK_AFFINITY_VIOLATIONS:
+			KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+			KBASE_DEBUG_ASSERT(katom->affinity == recently_chosen_affinity);
+
+			/* Note: this is where the caller must've taken the runpool_irq.lock */
+
+			/* Check for affinity violations - if there are any, then we just ask
+			 * the caller to requeue and try again later */
+			if (kbase_js_affinity_would_violate(kbdev, js, katom->affinity) != MALI_FALSE) {
+				/* Cause a re-attempt to submit from this slot on the next job complete */
+				kbase_js_affinity_slot_blocked_an_atom(kbdev, js);
+				/* Return to previous state */
+				katom->coreref_state = KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY;
+				/* *** BREAK OUT: Transition to lower state *** */
+				KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_CORE_REF_AFFINITY_WOULD_VIOLATE, katom->kctx, katom, katom->jc, js, (u32) katom->affinity);
+				break;
+			}
+
+			/* No affinity violations would result, so the cores are ready */
+			katom->coreref_state = KBASE_ATOM_COREREF_STATE_READY;
+			/* *** BREAK OUT: Cores Ready *** */
+			break;
+
+		default:
+			KBASE_DEBUG_ASSERT_MSG(MALI_FALSE, "Unhandled kbase_atom_coreref_state %d", katom->coreref_state);
+			break;
+		}
+	} while (retry != MALI_FALSE);
+
+	return (katom->coreref_state == KBASE_ATOM_COREREF_STATE_READY);
+}
+
+void kbasep_js_job_check_deref_cores(kbase_device *kbdev, struct kbase_jd_atom *katom)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+
+	switch (katom->coreref_state) {
+	case KBASE_ATOM_COREREF_STATE_READY:
+		/* State where atom was submitted to the HW - just proceed to power-down */
+		KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+
+		/* *** FALLTHROUGH *** */
+
+	case KBASE_ATOM_COREREF_STATE_RECHECK_AFFINITY:
+		/* State where cores were registered */
+		KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+		kbase_pm_release_cores(kbdev, katom->core_req & BASE_JD_REQ_T, katom->affinity);
+
+		/* Note: We do not clear the state for kbase_js_affinity_slot_blocked_an_atom().
+		 * That is handled after finishing the job. This might be slightly
+		 * suboptimal for some corner cases, but is otherwise not a problem
+		 * (and resolves itself after the next job completes). */
+
+		break;
+
+	case KBASE_ATOM_COREREF_STATE_WAITING_FOR_REQUESTED_CORES:
+		/* State where cores were requested, but not registered */
+		KBASE_DEBUG_ASSERT(katom->affinity != 0 || (katom->core_req & BASE_JD_REQ_T));
+		kbase_pm_unrequest_cores(kbdev, katom->core_req & BASE_JD_REQ_T, katom->affinity);
+		break;
+
+	case KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED:
+		/* Initial state - nothing required */
+		KBASE_DEBUG_ASSERT(katom->affinity == 0);
+		break;
+
+	default:
+		KBASE_DEBUG_ASSERT_MSG(MALI_FALSE, "Unhandled coreref_state: %d", katom->coreref_state);
+		break;
+	}
+
+	katom->affinity = 0;
+	katom->coreref_state = KBASE_ATOM_COREREF_STATE_NO_CORES_REQUESTED;
+}
+
+/*
+ * Note: this function is quite similar to kbasep_js_try_run_next_job_on_slot()
+ */
+mali_bool kbasep_js_try_run_next_job_on_slot_irq_nolock(kbase_device *kbdev, int js, s8 *submit_count)
+{
+	kbasep_js_device_data *js_devdata;
+	mali_bool cores_ready;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	/* The caller of this function may not be aware of Ctx Attribute state changes so we
+	 * must recheck if the given slot is still valid. Otherwise do not try to run.
+	 */
+	if (kbase_js_can_run_job_on_slot_no_lock(kbdev, js)) {
+		/* Keep submitting while there's space to run a job on this job-slot,
+		 * and there are jobs to get that match its requirements (see 'break'
+		 * statement below) */
+		while (*submit_count < KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ && kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE) {
+			kbase_jd_atom *dequeued_atom;
+			mali_bool has_job = MALI_FALSE;
+
+			/* Dequeue a job that matches the requirements */
+			has_job = kbasep_js_policy_dequeue_job(kbdev, js, &dequeued_atom);
+
+			if (has_job != MALI_FALSE) {
+				/* NOTE: since the runpool_irq lock is currently held and acts across
+				 * all address spaces, any context whose busy refcount has reached
+				 * zero won't yet be scheduled out whilst we're trying to run jobs
+				 * from it */
+				kbase_context *parent_ctx = dequeued_atom->kctx;
+				mali_bool retain_success;
+
+				/* Retain/power up the cores it needs, check if cores are ready */
+				cores_ready = kbasep_js_job_check_ref_cores(kbdev, js, dequeued_atom);
+
+				if (cores_ready != MALI_TRUE && dequeued_atom->event_code != BASE_JD_EVENT_PM_EVENT) {
+					/* The job can't be submitted until the cores are ready, requeue the job */
+					kbasep_js_policy_enqueue_job(&kbdev->js_data.policy, dequeued_atom);
+					break;
+				}
+
+				/* ASSERT that the Policy picked a job from an allowed context */
+				KBASE_DEBUG_ASSERT(kbasep_js_is_submit_allowed(js_devdata, parent_ctx));
+
+				/* Retain the context to stop it from being scheduled out
+				 * This is released when the job finishes */
+				retain_success = kbasep_js_runpool_retain_ctx_nolock(kbdev, parent_ctx);
+				KBASE_DEBUG_ASSERT(retain_success != MALI_FALSE);
+				CSTD_UNUSED(retain_success);
+
+				/* Retain the affinity on the slot */
+				kbase_js_affinity_retain_slot_cores(kbdev, js, dequeued_atom->affinity);
+
+				/* Check if this job needs the cycle counter enabled before submission */
+				kbasep_js_ref_permon_check_and_enable_cycle_counter(kbdev, dequeued_atom);
+
+				if (dequeued_atom->event_code == BASE_JD_EVENT_PM_EVENT) {
+					dev_warn(kbdev->dev, "Rejecting atom due to BASE_JD_EVENT_PM_EVENT\n");
+					/* The job has failed due to the specified core group being unavailable */
+					kbase_jd_done(dequeued_atom, js, NULL, 0);
+				} else {
+					/* Submit the job */
+					kbase_job_submit_nolock(kbdev, dequeued_atom, js);
+
+					++(*submit_count);
+				}
+			} else {
+				/* No more jobs - stop submitting for this slot */
+				break;
+			}
+		}
+	}
+
+	/* Indicate whether a retry in submission should be tried on a different
+	 * dequeue function. These are the reasons why it *must* happen:
+	 * - the KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ threshold was reached
+	 *   and new scheduling must be performed outside of IRQ mode.
+	 *
+	 * Failure to indicate this correctly could stop further jobs being processed.
+	 *
+	 * However, we do not _need_ to indicate a retry for the following:
+	 * - kbasep_js_policy_dequeue_job() couldn't get a job. In which case,
+	 *   there's no point re-trying outside of IRQ, because the result will be
+	 *   the same until job dependencies are resolved, or user-space provides
+	 *   more jobs. In both those cases, we try to run jobs anyway, so
+	 *   processing does not stop.
+	 * - kbasep_jm_is_submit_slots_free() was MALI_FALSE, indicating jobs were
+	 *   already running. When those jobs complete, that will still cause events
+	 *   that cause us to resume job submission.
+	 * - kbase_js_can_run_job_on_slot_no_lock() was MALI_FALSE - this is for
+	 *   Ctx Attribute handling. That _can_ change outside of IRQ context, but
+	 *   is handled explicitly by kbasep_js_runpool_release_ctx_and_katom_retained_state().
+	 */
+	return (mali_bool) (*submit_count >= KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ);
+}
+
+void kbasep_js_try_run_next_job_on_slot_nolock(kbase_device *kbdev, int js)
+{
+	kbasep_js_device_data *js_devdata;
+	mali_bool has_job;
+	mali_bool cores_ready;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	KBASE_DEBUG_ASSERT(js_devdata->nr_user_contexts_running > 0);
+
+	/* Keep submitting while there's space to run a job on this job-slot,
+	 * and there are jobs to get that match its requirements (see 'break'
+	 * statement below) */
+	if (kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE) {
+		/* The caller of this function may not be aware of Ctx Attribute state changes so we
+		 * must recheck if the given slot is still valid. Otherwise do not try to run.
+		 */
+		if (kbase_js_can_run_job_on_slot_no_lock(kbdev, js)) {
+			do {
+				kbase_jd_atom *dequeued_atom;
+
+				/* Dequeue a job that matches the requirements */
+				has_job = kbasep_js_policy_dequeue_job(kbdev, js, &dequeued_atom);
+
+				if (has_job != MALI_FALSE) {
+					/* NOTE: since the runpool_irq lock is currently held and acts across
+					 * all address spaces, any context whose busy refcount has reached
+					 * zero won't yet be scheduled out whilst we're trying to run jobs
+					 * from it */
+					kbase_context *parent_ctx = dequeued_atom->kctx;
+					mali_bool retain_success;
+
+					/* Retain/power up the cores it needs, check if cores are ready */
+					cores_ready = kbasep_js_job_check_ref_cores(kbdev, js, dequeued_atom);
+
+					if (cores_ready != MALI_TRUE && dequeued_atom->event_code != BASE_JD_EVENT_PM_EVENT) {
+						/* The job can't be submitted until the cores are ready, requeue the job */
+						kbasep_js_policy_enqueue_job(&kbdev->js_data.policy, dequeued_atom);
+						break;
+					}
+					/* ASSERT that the Policy picked a job from an allowed context */
+					KBASE_DEBUG_ASSERT(kbasep_js_is_submit_allowed(js_devdata, parent_ctx));
+
+					/* Retain the context to stop it from being scheduled out
+					 * This is released when the job finishes */
+					retain_success = kbasep_js_runpool_retain_ctx_nolock(kbdev, parent_ctx);
+					KBASE_DEBUG_ASSERT(retain_success != MALI_FALSE);
+					CSTD_UNUSED(retain_success);
+
+					/* Retain the affinity on the slot */
+					kbase_js_affinity_retain_slot_cores(kbdev, js, dequeued_atom->affinity);
+
+					/* Check if this job needs the cycle counter enabled before submission */
+					kbasep_js_ref_permon_check_and_enable_cycle_counter(kbdev, dequeued_atom);
+
+					if (dequeued_atom->event_code == BASE_JD_EVENT_PM_EVENT) {
+						dev_warn(kbdev->dev, "Rejecting atom due to BASE_JD_EVENT_PM_EVENT\n");
+						/* The job has failed due to the specified core group being unavailable */
+						kbase_jd_done(dequeued_atom, js, NULL, 0);
+					} else {
+						/* Submit the job */
+						kbase_job_submit_nolock(kbdev, dequeued_atom, js);
+					}
+				}
+
+			} while (kbasep_jm_is_submit_slots_free(kbdev, js, NULL) != MALI_FALSE && has_job != MALI_FALSE);
+		}
+	}
+}
+
+void kbasep_js_try_schedule_head_ctx(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	mali_bool has_kctx;
+	kbase_context *head_kctx;
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool is_runpool_full;
+	kbase_as *new_address_space;
+	unsigned long flags;
+	mali_bool head_kctx_suspended = MALI_FALSE;
+	int pm_active_err;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	js_devdata = &kbdev->js_data;
+
+	/* We *don't* make a speculative check on whether we can fit a context in the
+	 * runpool, because most of our use-cases assume 2 or fewer contexts, and
+	 * so we will usually have enough address spaces free.
+	 *
+	 * In any case, the check will be done later on once we have a context */
+
+	/* Grab the context off head of queue - if there is one */
+	mutex_lock(&js_devdata->queue_mutex);
+	has_kctx = kbasep_js_policy_dequeue_head_ctx(&js_devdata->policy, &head_kctx);
+	mutex_unlock(&js_devdata->queue_mutex);
+
+	if (has_kctx == MALI_FALSE) {
+		/* No ctxs to run - nothing to do */
+		return;
+	}
+	js_kctx_info = &head_kctx->jctx.sched_info;
+
+	dev_dbg(kbdev->dev, "JS: Dequeue Context %p", head_kctx);
+
+	pm_active_err = kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE);
+
+	/*
+	 * Atomic transaction on the Context and Run Pool begins
+	 */
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	mutex_lock(&js_devdata->runpool_mutex);
+
+	/* Check to see if we shouldn't add the context to run Run Pool:
+	 * - it can't take the specified context, and so is 'full'. This may be
+	 * 'full' even when there are addres spaces available, since some contexts
+	 * are allowed in whereas others may not due to HW workarounds
+	 * - A suspend is taking place
+	 * - The context is dying due to kbase_job_zap_context() */
+	is_runpool_full = check_is_runpool_full(kbdev, head_kctx);
+	if (is_runpool_full || pm_active_err || js_kctx_info->ctx.is_dying) {
+		/* Roll back the transaction so far and return */
+		mutex_unlock(&js_devdata->runpool_mutex);
+
+		/* Note: If a Power Management active reference was taken, it's released by
+		 * this: */
+		kbasep_js_runpool_requeue_or_kill_ctx(kbdev, head_kctx, !pm_active_err);
+
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+		return;
+	}
+
+	/* From the point on, the Power Management active reference is released
+	 * only if kbasep_js_runpool_release_ctx() causes the context to be removed
+	 * from the runpool */
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_TRY_SCHEDULE_HEAD_CTX, head_kctx, NULL, 0u, kbasep_js_trace_get_refcnt(kbdev, head_kctx));
+
+#if MALI_CUSTOMER_RELEASE == 0
+	if (js_devdata->nr_user_contexts_running == 0) {
+		/* Only when there are no other contexts submitting jobs:
+		 * Latch in run-time job scheduler timeouts that were set through js_timeouts sysfs file */
+		if (kbdev->js_soft_stop_ticks != 0)
+			js_devdata->soft_stop_ticks = kbdev->js_soft_stop_ticks;
+
+		if (kbdev->js_soft_stop_ticks_cl != 0)
+			js_devdata->soft_stop_ticks_cl = kbdev->js_soft_stop_ticks_cl;
+
+		if (kbdev->js_hard_stop_ticks_ss != 0)
+			js_devdata->hard_stop_ticks_ss = kbdev->js_hard_stop_ticks_ss;
+
+		if (kbdev->js_hard_stop_ticks_cl != 0)
+			js_devdata->hard_stop_ticks_cl = kbdev->js_hard_stop_ticks_cl;
+
+		if (kbdev->js_hard_stop_ticks_nss != 0)
+			js_devdata->hard_stop_ticks_nss = kbdev->js_hard_stop_ticks_nss;
+
+		if (kbdev->js_reset_ticks_ss != 0)
+			js_devdata->gpu_reset_ticks_ss = kbdev->js_reset_ticks_ss;
+
+		if (kbdev->js_reset_ticks_cl != 0)
+			js_devdata->gpu_reset_ticks_cl = kbdev->js_reset_ticks_cl;
+
+		if (kbdev->js_reset_ticks_nss != 0)
+			js_devdata->gpu_reset_ticks_nss = kbdev->js_reset_ticks_nss;
+	}
+#endif
+
+	runpool_inc_context_count(kbdev, head_kctx);
+	/* Cause any future waiter-on-termination to wait until the context is
+	 * descheduled */
+	js_kctx_info->ctx.is_scheduled = MALI_TRUE;
+	wake_up(&js_kctx_info->ctx.is_scheduled_wait);
+
+	/* Pick the free address space (guaranteed free by check_is_runpool_full() ) */
+	new_address_space = pick_free_addr_space(kbdev);
+
+	/* Lock the address space whilst working on it */
+	mutex_lock(&new_address_space->transaction_mutex);
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* Do all the necessaries to assign the address space (inc. update book-keeping info)
+	 * Add the context to the Run Pool, and allow it to run jobs */
+	assign_and_activate_kctx_addr_space(kbdev, head_kctx, new_address_space);
+
+	/* NOTE: If Linux allows, then we can drop the new_address_space->transaction mutex here */
+
+	if ((js_kctx_info->ctx.flags & KBASE_CTX_FLAG_PRIVILEGED) != 0) {
+		/* We need to retain it to keep the corresponding address space */
+		kbasep_js_runpool_retain_ctx_nolock(kbdev, head_kctx);
+	}
+
+	/* Re-check for suspending: a suspend could've occurred after we
+	 * pm_context_active'd, and all the contexts could've been removed from the
+	 * runpool before we took this lock. In this case, we don't want to allow
+	 * this context to run jobs, we just want it out immediately.
+	 *
+	 * The DMB required to read the suspend flag was issued recently as part of
+	 * the runpool_irq locking. If a suspend occurs *after* that lock was taken
+	 * (i.e. this condition doesn't execute), then the kbasep_js_suspend() code
+	 * will cleanup this context instead (by virtue of it being called strictly
+	 * after the suspend flag is set, and will wait for this lock to drop) */
+	if (kbase_pm_is_suspending(kbdev)) {
+		/* Cause it to leave at some later point */
+		mali_bool retained;
+		retained = kbasep_js_runpool_retain_ctx_nolock(kbdev, head_kctx);
+		KBASE_DEBUG_ASSERT(retained);
+		kbasep_js_clear_submit_allowed(js_devdata, head_kctx);
+		head_kctx_suspended = MALI_TRUE;
+	}
+
+	/* Try to run the next job, in case this context has jobs that match the
+	 * job slot requirements, but none of the other currently running contexts
+	 * do */
+	kbasep_js_try_run_next_job_nolock(kbdev);
+
+	/* Transaction complete */
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+	mutex_unlock(&new_address_space->transaction_mutex);
+	mutex_unlock(&js_devdata->runpool_mutex);
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+	/* Note: after this point, the context could potentially get scheduled out immediately */
+
+	if (head_kctx_suspended) {
+		/* Finishing forcing out the context due to a suspend. Use a variant of
+		 * kbasep_js_runpool_release_ctx() that doesn't schedule a new context,
+		 * to prevent a risk of recursion back into this function */
+		kbasep_js_runpool_release_ctx_no_schedule(kbdev, head_kctx);
+	}
+	return;
+}
+
+void kbasep_js_schedule_privileged_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	kbasep_js_device_data *js_devdata;
+	mali_bool is_scheduled;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	/* This must never be attempted whilst suspending - i.e. it should only
+	 * happen in response to a syscall from a user-space thread */
+	BUG_ON(kbase_pm_is_suspending(kbdev));
+
+	kbase_pm_request_l2_caches(kbdev);
+
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	/* Mark the context as privileged */
+	js_kctx_info->ctx.flags |= KBASE_CTX_FLAG_PRIVILEGED;
+
+	is_scheduled = js_kctx_info->ctx.is_scheduled;
+	if (is_scheduled == MALI_FALSE) {
+		mali_bool is_runpool_full;
+
+		/* Add the context to the runpool */
+		mutex_lock(&js_devdata->queue_mutex);
+		kbasep_js_policy_enqueue_ctx(&js_devdata->policy, kctx);
+		mutex_unlock(&js_devdata->queue_mutex);
+
+		mutex_lock(&js_devdata->runpool_mutex);
+		{
+			is_runpool_full = check_is_runpool_full(kbdev, kctx);
+			if (is_runpool_full != MALI_FALSE) {
+				/* Evict jobs from the NEXT registers to free an AS asap */
+				kbasep_js_runpool_evict_next_jobs(kbdev, kctx);
+			}
+		}
+		mutex_unlock(&js_devdata->runpool_mutex);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+		/* Fast-starting requires the jsctx_mutex to be dropped, because it works on multiple ctxs */
+
+		if (is_runpool_full != MALI_FALSE) {
+			/* Evict non-running contexts from the runpool */
+			kbasep_js_runpool_attempt_fast_start_ctx(kbdev, NULL);
+		}
+		/* Try to schedule the context in */
+		kbasep_js_try_schedule_head_ctx(kbdev);
+
+		/* Wait for the context to be scheduled in */
+		wait_event(kctx->jctx.sched_info.ctx.is_scheduled_wait, kctx->jctx.sched_info.ctx.is_scheduled == MALI_TRUE);
+	} else {
+		/* Already scheduled in - We need to retain it to keep the corresponding address space */
+		kbasep_js_runpool_retain_ctx(kbdev, kctx);
+		mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+	}
+}
+
+void kbasep_js_release_privileged_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	/* We don't need to use the address space anymore */
+	mutex_lock(&js_kctx_info->ctx.jsctx_mutex);
+	js_kctx_info->ctx.flags &= (~KBASE_CTX_FLAG_PRIVILEGED);
+	mutex_unlock(&js_kctx_info->ctx.jsctx_mutex);
+
+	kbase_pm_release_l2_caches(kbdev);
+
+	/* Release the context - it will be scheduled out if there is no pending job */
+	kbasep_js_runpool_release_ctx(kbdev, kctx);
+}
+
+void kbasep_js_job_done_slot_irq(kbase_jd_atom *katom, int slot_nr,
+                                 ktime_t *end_timestamp,
+                                 kbasep_js_atom_done_code done_code)
+{
+	kbase_device *kbdev;
+	kbasep_js_policy *js_policy;
+	kbasep_js_device_data *js_devdata;
+	mali_bool submit_retry_needed = MALI_TRUE;	/* If we don't start jobs here, start them from the workqueue */
+	ktime_t tick_diff;
+	u64 microseconds_spent = 0u;
+	kbase_context *parent_ctx;
+
+	KBASE_DEBUG_ASSERT(katom);
+	parent_ctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(parent_ctx);
+	kbdev = parent_ctx->kbdev;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	js_devdata = &kbdev->js_data;
+	js_policy = &kbdev->js_data.policy;
+
+	lockdep_assert_held(&js_devdata->runpool_irq.lock);
+
+	/*
+	 * Release resources before submitting new jobs (bounds the refcount of
+	 * the resource to BASE_JM_SUBMIT_SLOTS)
+	 */
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	kbase_trace_mali_job_slots_event(GATOR_MAKE_EVENT(GATOR_JOB_SLOT_STOP, slot_nr), NULL, 0);
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+
+	/* Check if submitted jobs no longer require the cycle counter to be enabled */
+	kbasep_js_deref_permon_check_and_disable_cycle_counter(kbdev, katom);
+
+	/* Release the affinity from the slot - must happen before next submission to this slot */
+	kbase_js_affinity_release_slot_cores(kbdev, slot_nr, katom->affinity);
+	kbase_js_debug_log_current_affinities(kbdev);
+	/* Calculate the job's time used */
+	if (end_timestamp != NULL) {
+		/* Only calculating it for jobs that really run on the HW (e.g. removed
+		 * from next jobs never actually ran, so really did take zero time) */
+		tick_diff = ktime_sub(*end_timestamp, katom->start_timestamp);
+
+		microseconds_spent = ktime_to_ns(tick_diff);
+		do_div(microseconds_spent, 1000);
+
+		/* Round up time spent to the minimum timer resolution */
+		if (microseconds_spent < KBASEP_JS_TICK_RESOLUTION_US)
+			microseconds_spent = KBASEP_JS_TICK_RESOLUTION_US;
+	}
+
+	/* Log the result of the job (completion status, and time spent). */
+	kbasep_js_policy_log_job_result(js_policy, katom, microseconds_spent);
+	/* Determine whether the parent context's timeslice is up */
+	if (kbasep_js_policy_should_remove_ctx(js_policy, parent_ctx) != MALI_FALSE)
+		kbasep_js_clear_submit_allowed(js_devdata, parent_ctx);
+
+	if (done_code & KBASE_JS_ATOM_DONE_START_NEW_ATOMS) {
+		/* Submit a new job (if there is one) to help keep the GPU's HEAD and NEXT registers full */
+		KBASE_TRACE_ADD_SLOT(kbdev, JS_JOB_DONE_TRY_RUN_NEXT_JOB, parent_ctx, katom, katom->jc, slot_nr);
+
+		submit_retry_needed = kbasep_js_try_run_next_job_on_slot_irq_nolock(kbdev, slot_nr, &kbdev->slot_submit_count_irq[slot_nr]);
+	}
+
+	if (submit_retry_needed != MALI_FALSE || katom->event_code == BASE_JD_EVENT_STOPPED) {
+		/* The extra condition on STOPPED jobs is needed because they may be
+		 * the only job present, but they won't get re-run until the JD work
+		 * queue activates. Crucially, work queues can run items out of order
+		 * e.g. on different CPUs, so being able to submit from the IRQ handler
+		 * is not a good indication that we don't need to run jobs; the
+		 * submitted job could be processed on the work-queue *before* the
+		 * stopped job, even though it was submitted after.
+		 *
+		 * Therefore, we must try to run it, otherwise it might not get run at
+		 * all after this. */
+
+		KBASE_TRACE_ADD_SLOT(kbdev, JS_JOB_DONE_RETRY_NEEDED, parent_ctx, katom, katom->jc, slot_nr);
+		kbasep_js_set_job_retry_submit_slot(katom, slot_nr);
+	}
+}
+
+void kbasep_js_suspend(kbase_device *kbdev)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	int i;
+	u16 retained = 0u;
+	int nr_privileged_ctx = 0;
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(kbase_pm_is_suspending(kbdev));
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	/* Prevent all contexts from submitting */
+	js_devdata->runpool_irq.submit_allowed = 0;
+
+	/* Retain each of the contexts, so we can cause it to leave even if it had
+	 * no refcount to begin with */
+	for (i = BASE_MAX_NR_AS - 1; i >= 0; --i) {
+		kbasep_js_per_as_data *js_per_as_data = &js_devdata->runpool_irq.per_as_data[i];
+		kbase_context *kctx = js_per_as_data->kctx;
+		retained = retained << 1;
+
+		if (kctx) {
+			++(js_per_as_data->as_busy_refcount);
+			retained |= 1u;
+			/* We can only cope with up to 1 privileged context - the
+			 * instrumented context. It'll be suspended by disabling
+			 * instrumentation */
+			if (kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED)
+				KBASE_DEBUG_ASSERT(++nr_privileged_ctx == 1);
+		}
+	}
+	CSTD_UNUSED(nr_privileged_ctx);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	/* De-ref the previous retain to ensure each context gets pulled out
+	 * sometime later. */
+	for (i = 0;
+		 i < BASE_MAX_NR_AS;
+		 ++i, retained = retained >> 1) {
+		kbasep_js_per_as_data *js_per_as_data = &js_devdata->runpool_irq.per_as_data[i];
+		kbase_context *kctx = js_per_as_data->kctx;
+
+		if (retained & 1u)
+			kbasep_js_runpool_release_ctx(kbdev,kctx);
+	}
+
+	/* Caller must wait for all Power Manager active references to be dropped */
+}
+
+void kbasep_js_resume(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	int i;
+	KBASE_DEBUG_ASSERT(kbdev);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_DEBUG_ASSERT(!kbase_pm_is_suspending(kbdev));
+
+	/* Schedule in as many contexts as address spaces. This also starts atoms. */
+	for (i = 0 ; i < kbdev->nr_hw_address_spaces; ++i)
+	{
+		kbasep_js_try_schedule_head_ctx(kbdev);
+	}
+	/* JS Resume complete */
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.h
new file mode 100644
index 0000000..7e325c2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js.h
@@ -0,0 +1,931 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js.h
+ * Job Scheduler APIs.
+ */
+
+#ifndef _KBASE_JS_H_
+#define _KBASE_JS_H_
+
+#include <malisw/mali_malisw.h>
+
+#include "mali_kbase_js_defs.h"
+#include "mali_kbase_js_policy.h"
+#include "mali_kbase_defs.h"
+#include "mali_kbase_debug.h"
+
+#include "mali_kbase_js_ctx_attr.h"
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js Job Scheduler Internal APIs
+ * @{
+ *
+ * These APIs are Internal to KBase and are available for use by the
+ * @ref kbase_js_policy "Job Scheduler Policy APIs"
+ */
+
+/**
+ * @brief Initialize the Job Scheduler
+ *
+ * The kbasep_js_device_data sub-structure of \a kbdev must be zero
+ * initialized before passing to the kbasep_js_devdata_init() function. This is
+ * to give efficient error path code.
+ */
+mali_error kbasep_js_devdata_init(kbase_device * const kbdev);
+
+/**
+ * @brief Halt the Job Scheduler.
+ *
+ * It is safe to call this on \a kbdev even if it the kbasep_js_device_data
+ * sub-structure was never initialized/failed initialization, to give efficient
+ * error-path code.
+ *
+ * For this to work, the kbasep_js_device_data sub-structure of \a kbdev must
+ * be zero initialized before passing to the kbasep_js_devdata_init()
+ * function. This is to give efficient error path code.
+ *
+ * It is a Programming Error to call this whilst there are still kbase_context
+ * structures registered with this scheduler.
+ *
+ */
+void kbasep_js_devdata_halt(kbase_device *kbdev);
+
+/**
+ * @brief Terminate the Job Scheduler
+ *
+ * It is safe to call this on \a kbdev even if it the kbasep_js_device_data
+ * sub-structure was never initialized/failed initialization, to give efficient
+ * error-path code.
+ *
+ * For this to work, the kbasep_js_device_data sub-structure of \a kbdev must
+ * be zero initialized before passing to the kbasep_js_devdata_init()
+ * function. This is to give efficient error path code.
+ *
+ * It is a Programming Error to call this whilst there are still kbase_context
+ * structures registered with this scheduler.
+ */
+void kbasep_js_devdata_term(kbase_device *kbdev);
+
+/**
+ * @brief Initialize the Scheduling Component of a kbase_context on the Job Scheduler.
+ *
+ * This effectively registers a kbase_context with a Job Scheduler.
+ *
+ * It does not register any jobs owned by the kbase_context with the scheduler.
+ * Those must be separately registered by kbasep_js_add_job().
+ *
+ * The kbase_context must be zero intitialized before passing to the
+ * kbase_js_init() function. This is to give efficient error path code.
+ */
+mali_error kbasep_js_kctx_init(kbase_context * const kctx);
+
+/**
+ * @brief Terminate the Scheduling Component of a kbase_context on the Job Scheduler
+ *
+ * This effectively de-registers a kbase_context from its Job Scheduler
+ *
+ * It is safe to call this on a kbase_context that has never had or failed
+ * initialization of its jctx.sched_info member, to give efficient error-path
+ * code.
+ *
+ * For this to work, the kbase_context must be zero intitialized before passing
+ * to the kbase_js_init() function.
+ *
+ * It is a Programming Error to call this whilst there are still jobs
+ * registered with this context.
+ */
+void kbasep_js_kctx_term(kbase_context *kctx);
+
+/**
+ * @brief Add a job chain to the Job Scheduler, and take necessary actions to
+ * schedule the context/run the job.
+ *
+ * This atomically does the following:
+ * - Update the numbers of jobs information
+ * - Add the job to the run pool if necessary (part of init_job)
+ *
+ * Once this is done, then an appropriate action is taken:
+ * - If the ctx is scheduled, it attempts to start the next job (which might be
+ * this added job)
+ * - Otherwise, and if this is the first job on the context, it enqueues it on
+ * the Policy Queue
+ *
+ * The Policy's Queue can be updated by this in the following ways:
+ * - In the above case that this is the first job on the context
+ * - If the job is high priority and the context is not scheduled, then it
+ * could cause the Policy to schedule out a low-priority context, allowing
+ * this context to be scheduled in.
+ *
+ * If the context is already scheduled on the RunPool, then adding a job to it
+ * is guarenteed not to update the Policy Queue. And so, the caller is
+ * guarenteed to not need to try scheduling a context from the Run Pool - it
+ * can safely assert that the result is MALI_FALSE.
+ *
+ * It is a programming error to have more than U32_MAX jobs in flight at a time.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it must \em not hold kbasep_js_device_data::runpool_irq::lock (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (again, it's used internally).
+ *
+ * @return MALI_TRUE indicates that the Policy Queue was updated, and so the
+ * caller will need to try scheduling a context onto the Run Pool.
+ * @return MALI_FALSE indicates that no updates were made to the Policy Queue,
+ * so no further action is required from the caller. This is \b always returned
+ * when the context is currently scheduled.
+ */
+mali_bool kbasep_js_add_job(kbase_context *kctx, kbase_jd_atom *atom);
+
+/**
+ * @brief Remove a job chain from the Job Scheduler, except for its 'retained state'.
+ *
+ * Completely removing a job requires several calls:
+ * - kbasep_js_copy_atom_retained_state(), to capture the 'retained state' of
+ *   the atom
+ * - kbasep_js_remove_job(), to partially remove the atom from the Job Scheduler
+ * - kbasep_js_runpool_release_ctx_and_katom_retained_state(), to release the
+ *   remaining state held as part of the job having been run.
+ *
+ * In the common case of atoms completing normally, this set of actions is more optimal for spinlock purposes than having kbasep_js_remove_job() handle all of the actions.
+ *
+ * In the case of cancelling atoms, it is easier to call kbasep_js_remove_cancelled_job(), which handles all the necessary actions.
+ *
+ * It is a programming error to call this when:
+ * - \a atom is not a job belonging to kctx.
+ * - \a atom has already been removed from the Job Scheduler.
+ * - \a atom is still in the runpool:
+ *  - it has not been removed with kbasep_js_policy_dequeue_job()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job_irq()
+ *
+ * Do not use this for removing jobs being killed by kbase_jd_cancel() - use
+ * kbasep_js_remove_cancelled_job() instead.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ *
+ */
+void kbasep_js_remove_job(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *atom);
+
+/**
+ * @brief Completely remove a job chain from the Job Scheduler, in the case
+ * where the job chain was cancelled.
+ *
+ * This is a variant of kbasep_js_remove_job() that takes care of removing all
+ * of the retained state too. This is generally useful for cancelled atoms,
+ * which need not be handled in an optimal way.
+ *
+ * It is a programming error to call this when:
+ * - \a atom is not a job belonging to kctx.
+ * - \a atom has already been removed from the Job Scheduler.
+ * - \a atom is still in the runpool:
+ *  - it is not being killed with kbasep_jd_cancel()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job()
+ *  - or, it has not been removed with kbasep_js_policy_dequeue_job_irq()
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this could be
+ * obtained internally)
+ */
+void kbasep_js_remove_cancelled_job(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+ * @brief Refcount a context as being busy, preventing it from being scheduled
+ * out.
+ *
+ * @note This function can safely be called from IRQ context.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, because
+ * it will be used internally.
+ *
+ * @return value != MALI_FALSE if the retain succeeded, and the context will not be scheduled out.
+ * @return MALI_FALSE if the retain failed (because the context is being/has been scheduled out).
+ */
+mali_bool kbasep_js_runpool_retain_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Refcount a context as being busy, preventing it from being scheduled
+ * out.
+ *
+ * @note This function can safely be called from IRQ context.
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ *
+ * @return value != MALI_FALSE if the retain succeeded, and the context will not be scheduled out.
+ * @return MALI_FALSE if the retain failed (because the context is being/has been scheduled out).
+ */
+mali_bool kbasep_js_runpool_retain_ctx_nolock(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Lookup a context in the Run Pool based upon its current address space
+ * and ensure that is stays scheduled in.
+ *
+ * The context is refcounted as being busy to prevent it from scheduling
+ * out. It must be released with kbasep_js_runpool_release_ctx() when it is no
+ * longer required to stay scheduled in.
+ *
+ * @note This function can safely be called from IRQ context.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpoool_irq::lock, because
+ * it will be used internally.
+ *
+ * @return a valid kbase_context on success, which has been refcounted as being busy.
+ * @return NULL on failure, indicating that no context was found in \a as_nr
+ */
+kbase_context *kbasep_js_runpool_lookup_ctx(kbase_device *kbdev, int as_nr);
+
+/**
+ * @brief Handling the requeuing/killing of a context that was evicted from the
+ * policy queue or runpool.
+ *
+ * This should be used whenever handing off a context that has been evicted
+ * from the policy queue or the runpool:
+ * - If the context is not dying and has jobs, it gets re-added to the policy
+ * queue
+ * - Otherwise, it is not added
+ *
+ * In addition, if the context is dying the jobs are killed asynchronously.
+ *
+ * In all cases, the Power Manager active reference is released
+ * (kbase_pm_context_idle()) whenever the has_pm_ref parameter is true.  \a
+ * has_pm_ref must be set to false whenever the context was not previously in
+ * the runpool and does not hold a Power Manager active refcount. Note that
+ * contexts in a rollback of kbasep_js_try_schedule_head_ctx() might have an
+ * active refcount even though they weren't in the runpool.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (as this will be
+ * obtained internally)
+ */
+void kbasep_js_runpool_requeue_or_kill_ctx(kbase_device *kbdev, kbase_context *kctx, mali_bool has_pm_ref);
+
+/**
+ * @brief Release a refcount of a context being busy, allowing it to be
+ * scheduled out.
+ *
+ * When the refcount reaches zero and the context \em might be scheduled out
+ * (depending on whether the Scheudling Policy has deemed it so, or if it has run
+ * out of jobs).
+ *
+ * If the context does get scheduled out, then The following actions will be
+ * taken as part of deschduling a context:
+ * - For the context being descheduled:
+ *  - If the context is in the processing of dying (all the jobs are being
+ * removed from it), then descheduling also kills off any jobs remaining in the
+ * context.
+ *  - If the context is not dying, and any jobs remain after descheduling the
+ * context then it is re-enqueued to the Policy's Queue.
+ *  - Otherwise, the context is still known to the scheduler, but remains absent
+ * from the Policy Queue until a job is next added to it.
+ *  - In all descheduling cases, the Power Manager active reference (obtained
+ * during kbasep_js_try_schedule_head_ctx()) is released (kbase_pm_context_idle()).
+ *
+ * Whilst the context is being descheduled, this also handles actions that
+ * cause more atoms to be run:
+ * - Attempt submitting atoms when the Context Attributes on the Runpool have
+ * changed. This is because the context being scheduled out could mean that
+ * there are more opportunities to run atoms.
+ * - Attempt submitting to a slot that was previously blocked due to affinity
+ * restrictions. This is usually only necessary when releasing a context
+ * happens as part of completing a previous job, but is harmless nonetheless.
+ * - Attempt scheduling in a new context (if one is available), and if necessary,
+ * running a job from that new context.
+ *
+ * Unlike retaining a context in the runpool, this function \b cannot be called
+ * from IRQ context.
+ *
+ * It is a programming error to call this on a \a kctx that is not currently
+ * scheduled, or that already has a zero refcount.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, because
+ * it will be used internally.
+ * - it must \em not hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold the kbase_device::as[n].transaction_mutex (as this will be obtained internally)
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (as this will be
+ * obtained internally)
+ *
+ */
+void kbasep_js_runpool_release_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Variant of kbasep_js_runpool_release_ctx() that handles additional
+ * actions from completing an atom.
+ *
+ * This is usually called as part of completing an atom and releasing the
+ * refcount on the context held by the atom.
+ *
+ * Therefore, the extra actions carried out are part of handling actions queued
+ * on a completed atom, namely:
+ * - Releasing the atom's context attributes
+ * - Retrying the submission on a particular slot, because we couldn't submit
+ * on that slot from an IRQ handler.
+ *
+ * The locking conditions of this function are the same as those for
+ * kbasep_js_runpool_release_ctx()
+ */
+void kbasep_js_runpool_release_ctx_and_katom_retained_state(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state);
+
+/**
+ * @brief Try to submit the next job on a \b particular slot whilst in IRQ
+ * context, and whilst the caller already holds the runpool IRQ spinlock.
+ *
+ * \a *submit_count will be checked against
+ * KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ to see whether too many jobs have
+ * been submitted. This is to prevent the IRQ handler looping over lots of GPU
+ * NULL jobs, which may complete whilst the IRQ handler is still processing. \a
+ * submit_count itself should point to kbase_device::slot_submit_count_irq[ \a js ],
+ * which is initialized to zero on entry to the IRQ handler.
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ *
+ * @return truthful (i.e. != MALI_FALSE) if too many jobs were submitted from
+ * IRQ. Therefore, this indicates that submission should be retried from a
+ * work-queue, by using
+ * kbasep_js_try_run_next_job_on_slot_nolock()/kbase_js_try_run_jobs_on_slot().
+ * @return MALI_FALSE if submission had no problems: the GPU is either already
+ * full of jobs in the HEAD and NEXT registers, or we were able to get enough
+ * jobs from the Run Pool to fill the GPU's HEAD and NEXT registers.
+ */
+mali_bool kbasep_js_try_run_next_job_on_slot_irq_nolock(kbase_device *kbdev, int js, s8 *submit_count);
+
+/**
+ * @brief Try to submit the next job on a particular slot, outside of IRQ context
+ *
+ * This obtains the Job Slot lock for the duration of the call only.
+ *
+ * Unlike kbasep_js_try_run_next_job_on_slot_irq_nolock(), there is no limit on
+ * submission, because eventually IRQ_THROTTLE will kick in to prevent us
+ * getting stuck in a loop of submitting GPU NULL jobs. This is because the IRQ
+ * handler will be delayed, and so this function will eventually fill up the
+ * space in our software 'submitted' slot (kbase_jm_slot::submitted).
+ *
+ * In addition, there's no return value - we'll run the maintenence functions
+ * on the Policy's Run Pool, but if there's nothing there after that, then the
+ * Run Pool is truely empty, and so no more action need be taken.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_mutex
+ * - it must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ * This must only be called whilst the GPU is powered - for example, when
+ * kbdev->jsdata.nr_user_contexts_running > 0.
+ *
+ * @note The caller \em might be holding one of the
+ * kbasep_js_kctx_info::ctx::jsctx_mutex locks.
+ *
+ */
+void kbasep_js_try_run_next_job_on_slot_nolock(kbase_device *kbdev, int js);
+
+/**
+ * @brief Try to submit the next job for each slot in the system, outside of IRQ context
+ *
+ * This will internally call kbasep_js_try_run_next_job_on_slot_nolock(), so similar
+ * locking conditions on the caller are required.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_mutex
+ * - it must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ * @note The caller \em might be holding one of the
+ * kbasep_js_kctx_info::ctx::jsctx_mutex locks.
+ *
+ */
+void kbasep_js_try_run_next_job_nolock(kbase_device *kbdev);
+
+/**
+ * @brief Try to schedule the next context onto the Run Pool
+ *
+ * This checks whether there's space in the Run Pool to accommodate a new
+ * context. If so, it attempts to dequeue a context from the Policy Queue, and
+ * submit this to the Run Pool.
+ *
+ * If the scheduling succeeds, then it also makes a call to
+ * kbasep_js_try_run_next_job_nolock(), in case the new context has jobs
+ * matching the job slot requirements, but no other currently scheduled context
+ * has such jobs.
+ *
+ * Whilst attempting to obtain a context from the policy queue, or add a
+ * context to the runpool, this function takes a Power Manager active
+ * reference. If for any reason a context cannot be added to the runpool, any
+ * reference obtained is released once the context is safely back in the policy
+ * queue. If no context was available on the policy queue, any reference
+ * obtained is released too.
+ *
+ * Only if the context gets placed in the runpool does the Power Manager active
+ * reference stay held (and is effectively now owned by the
+ * context/runpool). It is only released once the context is removed
+ * completely, or added back to the policy queue
+ * (e.g. kbasep_js_runpool_release_ctx(),
+ * kbasep_js_runpool_requeue_or_kill_ctx(), etc)
+ *
+ * If any of these actions fail (Run Pool Full, Policy Queue empty, can't get
+ * PM active reference due to a suspend, etc) then any actions taken are rolled
+ * back and the function just returns normally.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, because
+ * it will be used internally.
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold the kbase_device::as[n].transaction_mutex (as this will be obtained internally)
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (again, it's used internally).
+ * - it must \em not hold kbasep_js_kctx_info::ctx::jsctx_mutex, because it will
+ * be used internally.
+ *
+ */
+void kbasep_js_try_schedule_head_ctx(kbase_device *kbdev);
+
+/**
+ * @brief Schedule in a privileged context
+ *
+ * This schedules a context in regardless of the context priority.
+ * If the runpool is full, a context will be forced out of the runpool and the function will wait
+ * for the new context to be scheduled in.
+ * The context will be kept scheduled in (and the corresponding address space reserved) until
+ * kbasep_js_release_privileged_ctx is called).
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, because
+ * it will be used internally.
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold the kbase_device::as[n].transaction_mutex (as this will be obtained internally)
+ * - it must \em not hold kbasep_jd_device_data::queue_mutex (again, it's used internally).
+ * - it must \em not hold kbasep_js_kctx_info::ctx::jsctx_mutex, because it will
+ * be used internally.
+ *
+ */
+void kbasep_js_schedule_privileged_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Release a privileged context, allowing it to be scheduled out.
+ *
+ * See kbasep_js_runpool_release_ctx for potential side effects.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpool_irq::lock, because
+ * it will be used internally.
+ * - it must \em not hold kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold the kbase_device::as[n].transaction_mutex (as this will be obtained internally)
+ *
+ */
+void kbasep_js_release_privileged_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Handle the Job Scheduler component for the IRQ of a job finishing
+ *
+ * This does the following:
+ * -# Releases resources held by the atom
+ * -# if \a end_timestamp != NULL, updates the runpool's notion of time spent by a running ctx
+ * -# determines whether a context should be marked for scheduling out
+ * -# examines done_code to determine whether to submit the next job on the slot
+ * (picking from all ctxs in the runpool)
+ *
+ * In addition, if submission didn't happen (the submit-from-IRQ function
+ * failed or done_code didn't specify to start new jobs), then this sets a
+ * message on katom that submission needs to be retried from the worker thread.
+ *
+ * Normally, the time calculated from end_timestamp is rounded up to the
+ * minimum time precision. Therefore, to ensure the job is recorded as not
+ * spending any time, then set end_timestamp to NULL. For example, this is necessary when
+ * evicting jobs from JSn_HEAD_NEXT (because they didn't actually run).
+ *
+ * NOTE: It's possible to move the steps (2) and (3) (inc calculating job's time
+ * used) into the worker (outside of IRQ context), but this may allow a context
+ * to use up to twice as much timeslice as is allowed by the policy. For
+ * policies that order by time spent, this is not a problem for overall
+ * 'fairness', but can still increase latency between contexts.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpoool_irq::lock
+ */
+void kbasep_js_job_done_slot_irq(kbase_jd_atom *katom, int slot_nr,
+                                 ktime_t *end_timestamp,
+                                 kbasep_js_atom_done_code done_code);
+
+/**
+ * @brief Try to submit the next job on each slot
+ *
+ * The following locks may be used:
+ * - kbasep_js_device_data::runpool_mutex
+ * - kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_js_try_run_jobs(kbase_device *kbdev);
+
+/**
+ * @brief Try to submit the next job on a specfic slot
+ *
+ * The following locking conditions are made on the caller:
+ *
+ * - it must \em not hold kbasep_js_device_data::runpool_mutex (as this will be
+ * obtained internally)
+ * - it must \em not hold kbasep_js_device_data::runpool_irq::lock (as this
+ * will be obtained internally)
+ *
+ */
+void kbase_js_try_run_jobs_on_slot(kbase_device *kbdev, int js);
+
+/**
+ * @brief Handle releasing cores for power management and affinity management,
+ * ensuring that cores are powered down and affinity tracking is updated.
+ *
+ * This must only be called on an atom that is not currently running, and has
+ * not been re-queued onto the context (and so does not need locking)
+ *
+ * This function enters at the following @ref kbase_atom_coreref_state states:
+ * - NO_CORES_REQUESTED
+ * - WAITING_FOR_REQUESTED_CORES
+ * - RECHECK_AFFINITY
+ * - READY
+ *
+ * It transitions the above states back to NO_CORES_REQUESTED by the end of the
+ * function call (possibly via intermediate states).
+ *
+ * No locks need be held by the caller, since this takes the necessary Power
+ * Management locks itself. The runpool_irq.lock is not taken (the work that
+ * requires it is handled by kbase_js_affinity_submit_to_blocked_slots() ).
+ *
+ * @note The corresponding kbasep_js_job_check_ref_cores() is private to the
+ * Job Scheduler, and is called automatically when running the next job.
+ */
+void kbasep_js_job_check_deref_cores(kbase_device *kbdev, struct kbase_jd_atom *katom);
+
+/**
+ * @brief Suspend the job scheduler during a Power Management Suspend event.
+ *
+ * Causes all contexts to be removed from the runpool, and prevents any
+ * contexts from (re)entering the runpool.
+ *
+ * This does not handle suspending the one privileged context: the caller must
+ * instead do this by by suspending the GPU HW Counter Instrumentation.
+ *
+ * This will eventually cause all Power Management active references held by
+ * contexts on the runpool to be released, without running any more atoms.
+ *
+ * The caller must then wait for all Power Mangement active refcount to become
+ * zero before completing the suspend.
+ *
+ * The emptying mechanism may take some time to complete, since it can wait for
+ * jobs to complete naturally instead of forcing them to end quickly. However,
+ * this is bounded by the Job Scheduling Policy's Job Timeouts. Hence, this
+ * function is guaranteed to complete in a finite time whenever the Job
+ * Scheduling Policy implements Job Timeouts (such as those done by CFS).
+ */
+void kbasep_js_suspend(kbase_device *kbdev);
+
+/**
+ * @brief Resume the Job Scheduler after a Power Management Resume event.
+ *
+ * This restores the actions from kbasep_js_suspend():
+ * - Schedules contexts back into the runpool
+ * - Resumes running atoms on the GPU
+ */
+void kbasep_js_resume(kbase_device *kbdev);
+
+
+/*
+ * Helpers follow
+ */
+
+/**
+ * @brief Check that a context is allowed to submit jobs on this policy
+ *
+ * The purpose of this abstraction is to hide the underlying data size, and wrap up
+ * the long repeated line of code.
+ *
+ * As with any mali_bool, never test the return value with MALI_TRUE.
+ *
+ * The caller must hold kbasep_js_device_data::runpool_irq::lock.
+ */
+static INLINE mali_bool kbasep_js_is_submit_allowed(kbasep_js_device_data *js_devdata, kbase_context *kctx)
+{
+	u16 test_bit;
+
+	/* Ensure context really is scheduled in */
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+	KBASE_DEBUG_ASSERT(kctx->jctx.sched_info.ctx.is_scheduled != MALI_FALSE);
+
+	test_bit = (u16) (1u << kctx->as_nr);
+
+	return (mali_bool) (js_devdata->runpool_irq.submit_allowed & test_bit);
+}
+
+/**
+ * @brief Allow a context to submit jobs on this policy
+ *
+ * The purpose of this abstraction is to hide the underlying data size, and wrap up
+ * the long repeated line of code.
+ *
+ * The caller must hold kbasep_js_device_data::runpool_irq::lock.
+ */
+static INLINE void kbasep_js_set_submit_allowed(kbasep_js_device_data *js_devdata, kbase_context *kctx)
+{
+	u16 set_bit;
+
+	/* Ensure context really is scheduled in */
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+	KBASE_DEBUG_ASSERT(kctx->jctx.sched_info.ctx.is_scheduled != MALI_FALSE);
+
+	set_bit = (u16) (1u << kctx->as_nr);
+
+	dev_dbg(kctx->kbdev->dev, "JS: Setting Submit Allowed on %p (as=%d)", kctx, kctx->as_nr);
+
+	js_devdata->runpool_irq.submit_allowed |= set_bit;
+}
+
+/**
+ * @brief Prevent a context from submitting more jobs on this policy
+ *
+ * The purpose of this abstraction is to hide the underlying data size, and wrap up
+ * the long repeated line of code.
+ *
+ * The caller must hold kbasep_js_device_data::runpool_irq::lock.
+ */
+static INLINE void kbasep_js_clear_submit_allowed(kbasep_js_device_data *js_devdata, kbase_context *kctx)
+{
+	u16 clear_bit;
+	u16 clear_mask;
+
+	/* Ensure context really is scheduled in */
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+	KBASE_DEBUG_ASSERT(kctx->jctx.sched_info.ctx.is_scheduled != MALI_FALSE);
+
+	clear_bit = (u16) (1u << kctx->as_nr);
+	clear_mask = ~clear_bit;
+
+	dev_dbg(kctx->kbdev->dev, "JS: Clearing Submit Allowed on %p (as=%d)", kctx, kctx->as_nr);
+
+	js_devdata->runpool_irq.submit_allowed &= clear_mask;
+}
+
+/**
+ * @brief Manage the 'retry_submit_on_slot' part of a kbase_jd_atom
+ */
+static INLINE void kbasep_js_clear_job_retry_submit(kbase_jd_atom *atom)
+{
+	atom->retry_submit_on_slot = KBASEP_JS_RETRY_SUBMIT_SLOT_INVALID;
+}
+
+/**
+ * Mark a slot as requiring resubmission by carrying that information on a
+ * completing atom.
+ *
+ * @note This can ASSERT in debug builds if the submit slot has been set to
+ * something other than the current value for @a js. This is because you might
+ * be unintentionally stopping more jobs being submitted on the old submit
+ * slot, and that might cause a scheduling-hang.
+ *
+ * @note If you can guarantee that the atoms for the original slot will be
+ * submitted on some other slot, then call kbasep_js_clear_job_retry_submit()
+ * first to silence the ASSERT.
+ */
+static INLINE void kbasep_js_set_job_retry_submit_slot(kbase_jd_atom *atom, int js)
+{
+	KBASE_DEBUG_ASSERT(0 <= js && js <= BASE_JM_MAX_NR_SLOTS);
+	KBASE_DEBUG_ASSERT(atom->retry_submit_on_slot == KBASEP_JS_RETRY_SUBMIT_SLOT_INVALID
+	                   || atom->retry_submit_on_slot == js);
+
+	atom->retry_submit_on_slot = js;
+}
+
+/**
+ * Create an initial 'invalid' atom retained state, that requires no
+ * atom-related work to be done on releasing with
+ * kbasep_js_runpool_release_ctx_and_katom_retained_state()
+ */
+static INLINE void kbasep_js_atom_retained_state_init_invalid(kbasep_js_atom_retained_state *retained_state)
+{
+	retained_state->event_code = BASE_JD_EVENT_NOT_STARTED;
+	retained_state->core_req = KBASEP_JS_ATOM_RETAINED_STATE_CORE_REQ_INVALID;
+	retained_state->retry_submit_on_slot = KBASEP_JS_RETRY_SUBMIT_SLOT_INVALID;
+}
+
+/**
+ * Copy atom state that can be made available after jd_done_nolock() is called
+ * on that atom.
+ */
+static INLINE void kbasep_js_atom_retained_state_copy(kbasep_js_atom_retained_state *retained_state, const kbase_jd_atom *katom)
+{
+	retained_state->event_code = katom->event_code;
+	retained_state->core_req = katom->core_req;
+	retained_state->retry_submit_on_slot = katom->retry_submit_on_slot;
+	retained_state->device_nr = katom->device_nr;
+}
+
+/**
+ * @brief Determine whether an atom has finished (given its retained state),
+ * and so should be given back to userspace/removed from the system.
+ *
+ * Reasons for an atom not finishing include:
+ * - Being soft-stopped (and so, the atom should be resubmitted sometime later)
+ *
+ * @param[in] katom_retained_state the retained state of the atom to check
+ * @return    MALI_FALSE if the atom has not finished
+ * @return    !=MALI_FALSE if the atom has finished
+ */
+static INLINE mali_bool kbasep_js_has_atom_finished(const kbasep_js_atom_retained_state *katom_retained_state)
+{
+	return (mali_bool) (katom_retained_state->event_code != BASE_JD_EVENT_STOPPED && katom_retained_state->event_code != BASE_JD_EVENT_REMOVED_FROM_NEXT);
+}
+
+/**
+ * @brief Determine whether a kbasep_js_atom_retained_state is valid
+ *
+ * An invalid kbasep_js_atom_retained_state is allowed, and indicates that the
+ * code should just ignore it.
+ *
+ * @param[in] katom_retained_state the atom's retained state to check
+ * @return    MALI_FALSE if the retained state is invalid, and can be ignored
+ * @return    !=MALI_FALSE if the retained state is valid
+ */
+static INLINE mali_bool kbasep_js_atom_retained_state_is_valid(const kbasep_js_atom_retained_state *katom_retained_state)
+{
+	return (mali_bool) (katom_retained_state->core_req != KBASEP_JS_ATOM_RETAINED_STATE_CORE_REQ_INVALID);
+}
+
+static INLINE mali_bool kbasep_js_get_atom_retry_submit_slot(const kbasep_js_atom_retained_state *katom_retained_state, int *res)
+{
+	int js = katom_retained_state->retry_submit_on_slot;
+	*res = js;
+	return (mali_bool) (js >= 0);
+}
+
+#if KBASE_DEBUG_DISABLE_ASSERTS == 0
+/**
+ * Debug Check the refcount of a context. Only use within ASSERTs
+ *
+ * Obtains kbasep_js_device_data::runpool_irq::lock
+ *
+ * @return negative value if the context is not scheduled in
+ * @return current refcount of the context if it is scheduled in. The refcount
+ * is not guarenteed to be kept constant.
+ */
+static INLINE int kbasep_js_debug_check_ctx_refcount(kbase_device *kbdev, kbase_context *kctx)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	int result = -1;
+	int as_nr;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID)
+		result = js_devdata->runpool_irq.per_as_data[as_nr].as_busy_refcount;
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return result;
+}
+#endif				/* KBASE_DEBUG_DISABLE_ASSERTS == 0 */
+
+/**
+ * @brief Variant of kbasep_js_runpool_lookup_ctx() that can be used when the
+ * context is guarenteed to be already previously retained.
+ *
+ * It is a programming error to supply the \a as_nr of a context that has not
+ * been previously retained/has a busy refcount of zero. The only exception is
+ * when there is no ctx in \a as_nr (NULL returned).
+ *
+ * The following locking conditions are made on the caller:
+ * - it must \em not hold the kbasep_js_device_data::runpoool_irq::lock, because
+ * it will be used internally.
+ *
+ * @return a valid kbase_context on success, with a refcount that is guarenteed
+ * to be non-zero and unmodified by this function.
+ * @return NULL on failure, indicating that no context was found in \a as_nr
+ */
+static INLINE kbase_context *kbasep_js_runpool_lookup_ctx_noretain(kbase_device *kbdev, int as_nr)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	kbase_context *found_kctx;
+	kbasep_js_per_as_data *js_per_as_data;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(0 <= as_nr && as_nr < BASE_MAX_NR_AS);
+	js_devdata = &kbdev->js_data;
+	js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+
+	found_kctx = js_per_as_data->kctx;
+	KBASE_DEBUG_ASSERT(found_kctx == NULL || js_per_as_data->as_busy_refcount > 0);
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return found_kctx;
+}
+
+/**
+ * This will provide a conversion from time (us) to ticks of the gpu clock
+ * based on the minimum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: when you need the number of cycles to guarantee you won't wait for
+ * longer than 'us' time (you might have a shorter wait).
+ */
+static INLINE u32 kbasep_js_convert_us_to_gpu_ticks_min_freq(kbase_device *kbdev, u32 us)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_min;
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return us * (gpu_freq / 1000);
+}
+
+/**
+ * This will provide a conversion from time (us) to ticks of the gpu clock
+ * based on the maximum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need the number of cycles to guarantee you'll wait at least
+ * 'us' amount of time (but you might wait longer).
+ */
+static INLINE u32 kbasep_js_convert_us_to_gpu_ticks_max_freq(kbase_device *kbdev, u32 us)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_max;
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return us * (u32) (gpu_freq / 1000);
+}
+
+/**
+ * This will provide a conversion from ticks of the gpu clock to time (us)
+ * based on the minimum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need to know the worst-case wait that 'ticks' cycles will
+ * take (you guarantee that you won't wait any longer than this, but it may
+ * be shorter).
+ */
+static INLINE u32 kbasep_js_convert_gpu_ticks_to_us_min_freq(kbase_device *kbdev, u32 ticks)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_min;
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return ticks / gpu_freq * 1000;
+}
+
+/**
+ * This will provide a conversion from ticks of the gpu clock to time (us)
+ * based on the maximum available gpu frequency.
+ * This is usually good to compute best/worst case (where the use of current
+ * frequency is not valid due to DVFS).
+ * e.g.: When you need to know the best-case wait for 'tick' cycles (you
+ * guarantee to be waiting for at least this long, but it may be longer).
+ */
+static INLINE u32 kbasep_js_convert_gpu_ticks_to_us_max_freq(kbase_device *kbdev, u32 ticks)
+{
+	u32 gpu_freq = kbdev->gpu_props.props.core_props.gpu_freq_khz_max;
+	KBASE_DEBUG_ASSERT(0 != gpu_freq);
+	return ticks / gpu_freq * 1000;
+}
+
+	  /** @} *//* end group kbase_js */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_JS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.c
new file mode 100644
index 0000000..6ded87d
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.c
@@ -0,0 +1,382 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_affinity.c
+ * Base kernel affinity manager APIs
+ */
+
+#include <mali_kbase.h>
+#include "mali_kbase_js_affinity.h"
+
+
+STATIC INLINE mali_bool affinity_job_uses_high_cores(kbase_device *kbdev, kbase_jd_atom *katom)
+{
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		kbase_context *kctx;
+		kbase_context_flags ctx_flags;
+
+		kctx = katom->kctx;
+		ctx_flags = kctx->jctx.sched_info.ctx.flags;
+
+		/* In this HW Workaround, compute-only jobs/contexts use the high cores
+		 * during a core-split, all other contexts use the low cores. */
+		return (mali_bool) ((katom->core_req & BASE_JD_REQ_ONLY_COMPUTE) != 0 || (ctx_flags & KBASE_CTX_FLAG_HINT_ONLY_COMPUTE) != 0);
+	}
+	return MALI_FALSE;
+}
+
+/**
+ * @brief Decide whether a split in core affinity is required across job slots
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ * @param kbdev The kbase device structure of the device
+ * @return MALI_FALSE if a core split is not required
+ * @return != MALI_FALSE if a core split is required.
+ */
+STATIC INLINE mali_bool kbase_affinity_requires_split(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		s8 nr_compute_ctxs = kbasep_js_ctx_attr_count_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_COMPUTE);
+		s8 nr_noncompute_ctxs = kbasep_js_ctx_attr_count_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_NON_COMPUTE);
+
+		/* In this case, a mix of Compute+Non-Compute determines whether a
+		 * core-split is required, to ensure jobs with different numbers of RMUs
+		 * don't use the same cores.
+		 *
+		 * When it's entirely compute, or entirely non-compute, then no split is
+		 * required.
+		 *
+		 * A context can be both Compute and Non-compute, in which case this will
+		 * correctly decide that a core-split is required. */
+
+		return (mali_bool) (nr_compute_ctxs > 0 && nr_noncompute_ctxs > 0);
+	}
+	return MALI_FALSE;
+}
+
+mali_bool kbase_js_can_run_job_on_slot_no_lock(kbase_device *kbdev, int js)
+{
+	/*
+	 * Here are the reasons for using job slot 2:
+	 * - BASE_HW_ISSUE_8987 (which is entirely used for that purpose)
+	 * - In absence of the above, then:
+	 *  - Atoms with BASE_JD_REQ_COHERENT_GROUP
+	 *  - But, only when there aren't contexts with
+	 *  KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES, because the atoms that run on
+	 *  all cores on slot 1 could be blocked by those using a coherent group
+	 *  on slot 2
+	 *  - And, only when you actually have 2 or more coregroups - if you only
+	 *  have 1 coregroup, then having jobs for slot 2 implies they'd also be
+	 *  for slot 1, meaning you'll get interference from them. Jobs able to
+	 *  run on slot 2 could also block jobs that can only run on slot 1
+	 *  (tiler jobs)
+	 */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987))
+		return MALI_TRUE;
+
+	if (js != 2)
+		return MALI_TRUE;
+
+	/* Only deal with js==2 now: */
+	if (kbdev->gpu_props.num_core_groups > 1) {
+		/* Only use slot 2 in the 2+ coregroup case */
+		if (kbasep_js_ctx_attr_is_attr_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES) == MALI_FALSE) {
+			/* ...But only when we *don't* have atoms that run on all cores */
+
+			/* No specific check for BASE_JD_REQ_COHERENT_GROUP atoms - the policy will sort that out */
+			return MALI_TRUE;
+		}
+	}
+
+	/* Above checks failed mean we shouldn't use slot 2 */
+	return MALI_FALSE;
+}
+
+/*
+ * As long as it has been decided to have a deeper modification of
+ * what job scheduler, power manager and affinity manager will
+ * implement, this function is just an intermediate step that
+ * assumes:
+ * - all working cores will be powered on when this is called.
+ * - largest current configuration is 2 core groups.
+ * - It has been decided not to have hardcoded values so the low
+ *   and high cores in a core split will be evently distributed.
+ * - Odd combinations of core requirements have been filtered out
+ *   and do not get to this function (e.g. CS+T+NSS is not
+ *   supported here).
+ * - This function is frequently called and can be optimized,
+ *   (see notes in loops), but as the functionallity will likely
+ *   be modified, optimization has not been addressed.
+*/
+mali_bool kbase_js_choose_affinity(u64 * const affinity, kbase_device *kbdev, kbase_jd_atom *katom, int js)
+{
+	base_jd_core_req core_req = katom->core_req;
+	unsigned int num_core_groups = kbdev->gpu_props.num_core_groups;
+	u64 core_availability_mask;
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	core_availability_mask = kbase_pm_ca_get_core_mask(kbdev);
+
+	/*
+	 * If no cores are currently available (core availability policy is
+	 * transitioning) then fail.
+	 */
+	if (0 == core_availability_mask)
+	{
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+		*affinity = 0;
+		return MALI_FALSE;
+	}
+
+	KBASE_DEBUG_ASSERT(js >= 0);
+
+	if ((core_req & (BASE_JD_REQ_FS | BASE_JD_REQ_CS | BASE_JD_REQ_T)) == BASE_JD_REQ_T)
+	{
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+		/* Tiler only job, bit 0 needed to enable tiler but no shader cores required */
+		*affinity = 1;
+		return MALI_TRUE;
+	}
+
+	if (1 == kbdev->gpu_props.num_cores) {
+		/* trivial case only one core, nothing to do */
+		*affinity = core_availability_mask;
+	} else if (kbase_affinity_requires_split(kbdev) == MALI_FALSE) {
+		if ((core_req & (BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP))) {
+			if (js == 0 || num_core_groups == 1) {
+				/* js[0] and single-core-group systems just get the first core group */
+				*affinity = kbdev->gpu_props.props.coherency_info.group[0].core_mask & core_availability_mask;
+			} else {
+				/* js[1], js[2] use core groups 0, 1 for dual-core-group systems */
+				u32 core_group_idx = ((u32) js) - 1;
+				KBASE_DEBUG_ASSERT(core_group_idx < num_core_groups);
+				*affinity = kbdev->gpu_props.props.coherency_info.group[core_group_idx].core_mask & core_availability_mask;
+
+				/* If the job is specifically targeting core group 1 and the core
+				 * availability policy is keeping that core group off, then fail */
+				if (*affinity == 0 && core_group_idx == 1 && kbdev->pm.cg1_disabled == MALI_TRUE)
+					katom->event_code = BASE_JD_EVENT_PM_EVENT;
+			}
+		} else {
+			/* All cores are available when no core split is required */
+			*affinity = core_availability_mask;
+		}
+	} else {
+		/* Core split required - divide cores in two non-overlapping groups */
+		u64 low_bitmap, high_bitmap;
+		int n_high_cores = kbdev->gpu_props.num_cores >> 1;
+		KBASE_DEBUG_ASSERT(1 == num_core_groups);
+		KBASE_DEBUG_ASSERT(0 != n_high_cores);
+
+		/* compute the reserved high cores bitmap */
+		high_bitmap = ~0;
+		/* note: this can take a while, optimization desirable */
+		while (n_high_cores != hweight32(high_bitmap & kbdev->shader_present_bitmap))
+			high_bitmap = high_bitmap << 1;
+
+		high_bitmap &= core_availability_mask;
+		low_bitmap = core_availability_mask ^ high_bitmap;
+
+		if (affinity_job_uses_high_cores(kbdev, katom))
+			*affinity = high_bitmap;
+		else
+			*affinity = low_bitmap;
+	}
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	/*
+	 * If no cores are currently available in the desired core group(s)
+	 * (core availability policy is transitioning) then fail.
+	 */
+	if (*affinity == 0)
+		return MALI_FALSE;
+
+	/* Enable core 0 if tiler required */
+	if (core_req & BASE_JD_REQ_T)
+		*affinity = *affinity | 1;
+
+	return MALI_TRUE;
+}
+
+STATIC INLINE mali_bool kbase_js_affinity_is_violating(kbase_device *kbdev, u64 *affinities)
+{
+	/* This implementation checks whether the two slots involved in Generic thread creation
+	 * have intersecting affinity. This is due to micro-architectural issues where a job in
+	 * slot A targetting cores used by slot B could prevent the job in slot B from making
+	 * progress until the job in slot A has completed.
+	 *
+	 * @note It just so happens that this restriction also allows
+	 * BASE_HW_ISSUE_8987 to be worked around by placing on job slot 2 the
+	 * atoms from ctxs with KBASE_CTX_FLAG_HINT_ONLY_COMPUTE flag set
+	 */
+	u64 affinity_set_left;
+	u64 affinity_set_right;
+	u64 intersection;
+	KBASE_DEBUG_ASSERT(affinities != NULL);
+
+	affinity_set_left = affinities[1];
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		/* The left set also includes those on the Fragment slot when
+		 * we are using the HW workaround for BASE_HW_ISSUE_8987 */
+		affinity_set_left |= affinities[0];
+	}
+
+	affinity_set_right = affinities[2];
+
+	/* A violation occurs when any bit in the left_set is also in the right_set */
+	intersection = affinity_set_left & affinity_set_right;
+
+	return (mali_bool) (intersection != (u64) 0u);
+}
+
+mali_bool kbase_js_affinity_would_violate(kbase_device *kbdev, int js, u64 affinity)
+{
+	kbasep_js_device_data *js_devdata;
+	u64 new_affinities[BASE_JM_MAX_NR_SLOTS];
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(js < BASE_JM_MAX_NR_SLOTS);
+	js_devdata = &kbdev->js_data;
+
+	memcpy(new_affinities, js_devdata->runpool_irq.slot_affinities, sizeof(js_devdata->runpool_irq.slot_affinities));
+
+	new_affinities[js] |= affinity;
+
+	return kbase_js_affinity_is_violating(kbdev, new_affinities);
+}
+
+void kbase_js_affinity_retain_slot_cores(kbase_device *kbdev, int js, u64 affinity)
+{
+	kbasep_js_device_data *js_devdata;
+	u64 cores;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(js < BASE_JM_MAX_NR_SLOTS);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_DEBUG_ASSERT(kbase_js_affinity_would_violate(kbdev, js, affinity) == MALI_FALSE);
+
+	cores = affinity;
+	while (cores) {
+		int bitnum = fls64(cores) - 1;
+		u64 bit = 1ULL << bitnum;
+		s8 cnt;
+
+		KBASE_DEBUG_ASSERT(js_devdata->runpool_irq.slot_affinity_refcount[js][bitnum] < BASE_JM_SUBMIT_SLOTS);
+
+		cnt = ++(js_devdata->runpool_irq.slot_affinity_refcount[js][bitnum]);
+
+		if (cnt == 1)
+			js_devdata->runpool_irq.slot_affinities[js] |= bit;
+
+		cores &= ~bit;
+	}
+
+}
+
+void kbase_js_affinity_release_slot_cores(kbase_device *kbdev, int js, u64 affinity)
+{
+	kbasep_js_device_data *js_devdata;
+	u64 cores;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(js < BASE_JM_MAX_NR_SLOTS);
+	js_devdata = &kbdev->js_data;
+
+	cores = affinity;
+	while (cores) {
+		int bitnum = fls64(cores) - 1;
+		u64 bit = 1ULL << bitnum;
+		s8 cnt;
+
+		KBASE_DEBUG_ASSERT(js_devdata->runpool_irq.slot_affinity_refcount[js][bitnum] > 0);
+
+		cnt = --(js_devdata->runpool_irq.slot_affinity_refcount[js][bitnum]);
+
+		if (0 == cnt)
+			js_devdata->runpool_irq.slot_affinities[js] &= ~bit;
+
+		cores &= ~bit;
+	}
+
+}
+
+void kbase_js_affinity_slot_blocked_an_atom(kbase_device *kbdev, int js)
+{
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(js < BASE_JM_MAX_NR_SLOTS);
+	js_devdata = &kbdev->js_data;
+
+	js_devdata->runpool_irq.slots_blocked_on_affinity |= 1u << js;
+}
+
+void kbase_js_affinity_submit_to_blocked_slots(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	u16 slots;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_DEBUG_ASSERT(js_devdata->nr_user_contexts_running != 0);
+
+	/* Must take a copy because submitting jobs will update this member. */
+	slots = js_devdata->runpool_irq.slots_blocked_on_affinity;
+
+	while (slots) {
+		int bitnum = fls(slots) - 1;
+		u16 bit = 1u << bitnum;
+		slots &= ~bit;
+
+		KBASE_TRACE_ADD_SLOT(kbdev, JS_AFFINITY_SUBMIT_TO_BLOCKED, NULL, NULL, 0u, bitnum);
+
+		/* must update this before we submit, incase it's set again */
+		js_devdata->runpool_irq.slots_blocked_on_affinity &= ~bit;
+
+		kbasep_js_try_run_next_job_on_slot_nolock(kbdev, bitnum);
+
+		/* Don't re-read slots_blocked_on_affinity after this - it could loop for a long time */
+	}
+}
+
+#if KBASE_TRACE_ENABLE != 0
+void kbase_js_debug_log_current_affinities(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	int slot_nr;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	for (slot_nr = 0; slot_nr < 3; ++slot_nr)
+		KBASE_TRACE_ADD_SLOT_INFO(kbdev, JS_AFFINITY_CURRENT, NULL, NULL, 0u, slot_nr, (u32) js_devdata->runpool_irq.slot_affinities[slot_nr]);
+}
+#endif				/* KBASE_TRACE_ENABLE != 0 */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.h
new file mode 100644
index 0000000..38de8b3
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_affinity.h
@@ -0,0 +1,157 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_affinity.h
+ * Affinity Manager internal APIs.
+ */
+
+#ifndef _KBASE_JS_AFFINITY_H_
+#define _KBASE_JS_AFFINITY_H_
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js_affinity Affinity Manager internal APIs.
+ * @{
+ *
+ */
+
+/**
+ * @brief Decide whether it is possible to submit a job to a particular job slot in the current status
+ *
+ * Will check if submitting to the given job slot is allowed in the current
+ * status.  For example using job slot 2 while in soft-stoppable state and only
+ * having 1 coregroup is not allowed by the policy. This function should be
+ * called prior to submitting a job to a slot to make sure policy rules are not
+ * violated.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ * @param kbdev The kbase device structure of the device
+ * @param js    Job slot number to check for allowance
+ */
+mali_bool kbase_js_can_run_job_on_slot_no_lock(kbase_device *kbdev, int js);
+
+/**
+ * @brief Compute affinity for a given job.
+ *
+ * Currently assumes an all-on/all-off power management policy.
+ * Also assumes there is at least one core with tiler available.
+ *
+ * Returns MALI_TRUE if a valid affinity was chosen, MALI_FALSE if
+ * no cores were available.
+ *
+ * @param[out] affinity       Affinity bitmap computed
+ * @param kbdev The kbase device structure of the device
+ * @param katom Job chain of which affinity is going to be found
+ * @param js    Slot the job chain is being submitted
+
+ */
+mali_bool kbase_js_choose_affinity(u64 * const affinity, kbase_device *kbdev, kbase_jd_atom *katom, int js);
+
+/**
+ * @brief Determine whether a proposed \a affinity on job slot \a js would
+ * cause a violation of affinity restrictions.
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ */
+mali_bool kbase_js_affinity_would_violate(kbase_device *kbdev, int js, u64 affinity);
+
+/**
+ * @brief Affinity tracking: retain cores used by a slot
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_js_affinity_retain_slot_cores(kbase_device *kbdev, int js, u64 affinity);
+
+/**
+ * @brief Affinity tracking: release cores used by a slot
+ *
+ * Cores \b must be released as soon as a job is dequeued from a slot's 'submit
+ * slots', and before another job is submitted to those slots. Otherwise, the
+ * refcount could exceed the maximum number submittable to a slot,
+ * BASE_JM_SUBMIT_SLOTS.
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_js_affinity_release_slot_cores(kbase_device *kbdev, int js, u64 affinity);
+
+/**
+ * @brief Register a slot as blocking atoms due to affinity violations
+ *
+ * Once a slot has been registered, we must check after every atom completion
+ * (including those on different slots) to see if the slot can be
+ * unblocked. This is done by calling
+ * kbase_js_affinity_submit_to_blocked_slots(), which will also deregister the
+ * slot if it no long blocks atoms due to affinity violations.
+ *
+ * The following locks must be held by the caller:
+ * - kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_js_affinity_slot_blocked_an_atom(kbase_device *kbdev, int js);
+
+/**
+ * @brief Submit to job slots that have registered that an atom was blocked on
+ * the slot previously due to affinity violations.
+ *
+ * This submits to all slots registered by
+ * kbase_js_affinity_slot_blocked_an_atom(). If submission succeeded, then the
+ * slot is deregistered as having blocked atoms due to affinity
+ * violations. Otherwise it stays registered, and the next atom to complete
+ * must attempt to submit to the blocked slots again.
+ *
+ * This must only be called whilst the GPU is powered - for example, when
+ * kbdev->jsdata.nr_user_contexts_running > 0.
+ *
+ * The following locking conditions are made on the caller:
+ * - it must hold kbasep_js_device_data::runpool_mutex
+ * - it must hold kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_js_affinity_submit_to_blocked_slots(kbase_device *kbdev);
+
+/**
+ * @brief Output to the Trace log the current tracked affinities on all slots
+ */
+#if KBASE_TRACE_ENABLE != 0
+void kbase_js_debug_log_current_affinities(kbase_device *kbdev);
+#else				/*  KBASE_TRACE_ENABLE != 0 */
+static INLINE void kbase_js_debug_log_current_affinities(kbase_device *kbdev)
+{
+}
+#endif				/*  KBASE_TRACE_ENABLE != 0 */
+
+	  /** @} *//* end group kbase_js_affinity */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+
+#endif				/* _KBASE_JS_AFFINITY_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.c
new file mode 100644
index 0000000..4da6902
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.c
@@ -0,0 +1,326 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+#include <mali_kbase.h>
+
+/*
+ * Private functions follow
+ */
+
+/**
+ * @brief Check whether a ctx has a certain attribute, and if so, retain that
+ * attribute on the runpool.
+ *
+ * Requires:
+ * - jsctx mutex
+ * - runpool_irq spinlock
+ * - ctx is scheduled on the runpool
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * In this state, the scheduler might be able to submit more jobs than
+ * previously, and so the caller should ensure kbasep_js_try_run_next_job_nolock()
+ * or similar is called sometime later.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+STATIC mali_bool kbasep_js_ctx_attr_runpool_retain_attr(kbase_device *kbdev, kbase_context *kctx, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool runpool_state_changed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled != MALI_FALSE);
+
+	if (kbasep_js_ctx_attr_is_attr_on_ctx(kctx, attribute) != MALI_FALSE) {
+		KBASE_DEBUG_ASSERT(js_devdata->runpool_irq.ctx_attr_ref_count[attribute] < S8_MAX);
+		++(js_devdata->runpool_irq.ctx_attr_ref_count[attribute]);
+
+		if (js_devdata->runpool_irq.ctx_attr_ref_count[attribute] == 1) {
+			/* First refcount indicates a state change */
+			runpool_state_changed = MALI_TRUE;
+			KBASE_TRACE_ADD(kbdev, JS_CTX_ATTR_NOW_ON_RUNPOOL, kctx, NULL, 0u, attribute);
+		}
+	}
+
+	return runpool_state_changed;
+}
+
+/**
+ * @brief Check whether a ctx has a certain attribute, and if so, release that
+ * attribute on the runpool.
+ *
+ * Requires:
+ * - jsctx mutex
+ * - runpool_irq spinlock
+ * - ctx is scheduled on the runpool
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * In this state, the scheduler might be able to submit more jobs than
+ * previously, and so the caller should ensure kbasep_js_try_run_next_job_nolock()
+ * or similar is called sometime later.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+STATIC mali_bool kbasep_js_ctx_attr_runpool_release_attr(kbase_device *kbdev, kbase_context *kctx, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool runpool_state_changed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_devdata = &kbdev->js_data;
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.is_scheduled != MALI_FALSE);
+
+	if (kbasep_js_ctx_attr_is_attr_on_ctx(kctx, attribute) != MALI_FALSE) {
+		KBASE_DEBUG_ASSERT(js_devdata->runpool_irq.ctx_attr_ref_count[attribute] > 0);
+		--(js_devdata->runpool_irq.ctx_attr_ref_count[attribute]);
+
+		if (js_devdata->runpool_irq.ctx_attr_ref_count[attribute] == 0) {
+			/* Last de-refcount indicates a state change */
+			runpool_state_changed = MALI_TRUE;
+			KBASE_TRACE_ADD(kbdev, JS_CTX_ATTR_NOW_OFF_RUNPOOL, kctx, NULL, 0u, attribute);
+		}
+	}
+
+	return runpool_state_changed;
+}
+
+/**
+ * @brief Retain a certain attribute on a ctx, also retaining it on the runpool
+ * if the context is scheduled.
+ *
+ * Requires:
+ * - jsctx mutex
+ * - If the context is scheduled, then runpool_irq spinlock must also be held
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * This may allow the scheduler to submit more jobs than previously.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+STATIC mali_bool kbasep_js_ctx_attr_ctx_retain_attr(kbase_device *kbdev, kbase_context *kctx, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool runpool_state_changed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.ctx_attr_ref_count[attribute] < U32_MAX);
+
+	++(js_kctx_info->ctx.ctx_attr_ref_count[attribute]);
+
+	if (js_kctx_info->ctx.is_scheduled != MALI_FALSE && js_kctx_info->ctx.ctx_attr_ref_count[attribute] == 1) {
+		lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+		/* Only ref-count the attribute on the runpool for the first time this contexts sees this attribute */
+		KBASE_TRACE_ADD(kbdev, JS_CTX_ATTR_NOW_ON_CTX, kctx, NULL, 0u, attribute);
+		runpool_state_changed = kbasep_js_ctx_attr_runpool_retain_attr(kbdev, kctx, attribute);
+	}
+
+	return runpool_state_changed;
+}
+
+/**
+ * @brief Release a certain attribute on a ctx, also releasign it from the runpool
+ * if the context is scheduled.
+ *
+ * Requires:
+ * - jsctx mutex
+ * - If the context is scheduled, then runpool_irq spinlock must also be held
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * This may allow the scheduler to submit more jobs than previously.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+STATIC mali_bool kbasep_js_ctx_attr_ctx_release_attr(kbase_device *kbdev, kbase_context *kctx, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool runpool_state_changed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	BUG_ON(!mutex_is_locked(&js_kctx_info->ctx.jsctx_mutex));
+	KBASE_DEBUG_ASSERT(js_kctx_info->ctx.ctx_attr_ref_count[attribute] > 0);
+
+	if (js_kctx_info->ctx.is_scheduled != MALI_FALSE && js_kctx_info->ctx.ctx_attr_ref_count[attribute] == 1) {
+		lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+		/* Only de-ref-count the attribute on the runpool when this is the last ctx-reference to it */
+		runpool_state_changed = kbasep_js_ctx_attr_runpool_release_attr(kbdev, kctx, attribute);
+		KBASE_TRACE_ADD(kbdev, JS_CTX_ATTR_NOW_OFF_CTX, kctx, NULL, 0u, attribute);
+	}
+
+	/* De-ref must happen afterwards, because kbasep_js_ctx_attr_runpool_release() needs to check it too */
+	--(js_kctx_info->ctx.ctx_attr_ref_count[attribute]);
+
+	return runpool_state_changed;
+}
+
+/*
+ * More commonly used public functions
+ */
+
+void kbasep_js_ctx_attr_set_initial_attrs(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+	mali_bool runpool_state_changed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	if ((js_kctx_info->ctx.flags & KBASE_CTX_FLAG_SUBMIT_DISABLED) != MALI_FALSE) {
+		/* This context never submits, so don't track any scheduling attributes */
+		return;
+	}
+
+	/* Transfer attributes held in the context flags for contexts that have submit enabled */
+
+	if ((js_kctx_info->ctx.flags & KBASE_CTX_FLAG_HINT_ONLY_COMPUTE) != MALI_FALSE) {
+		/* Compute context */
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_retain_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_COMPUTE);
+	}
+	/* NOTE: Whether this is a non-compute context depends on the jobs being
+	 * run, e.g. it might be submitting jobs with BASE_JD_REQ_ONLY_COMPUTE */
+
+	/* ... More attributes can be added here ... */
+
+	/* The context should not have been scheduled yet, so ASSERT if this caused
+	 * runpool state changes (note that other threads *can't* affect the value
+	 * of runpool_state_changed, due to how it's calculated) */
+	KBASE_DEBUG_ASSERT(runpool_state_changed == MALI_FALSE);
+	CSTD_UNUSED(runpool_state_changed);
+}
+
+void kbasep_js_ctx_attr_runpool_retain_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	mali_bool runpool_state_changed;
+	int i;
+
+	/* Retain any existing attributes */
+	for (i = 0; i < KBASEP_JS_CTX_ATTR_COUNT; ++i) {
+		if (kbasep_js_ctx_attr_is_attr_on_ctx(kctx, (kbasep_js_ctx_attr) i) != MALI_FALSE) {
+			/* The context is being scheduled in, so update the runpool with the new attributes */
+			runpool_state_changed = kbasep_js_ctx_attr_runpool_retain_attr(kbdev, kctx, (kbasep_js_ctx_attr) i);
+
+			/* We don't need to know about state changed, because retaining a
+			 * context occurs on scheduling it, and that itself will also try
+			 * to run new atoms */
+			CSTD_UNUSED(runpool_state_changed);
+		}
+	}
+}
+
+mali_bool kbasep_js_ctx_attr_runpool_release_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	mali_bool runpool_state_changed = MALI_FALSE;
+	int i;
+
+	/* Release any existing attributes */
+	for (i = 0; i < KBASEP_JS_CTX_ATTR_COUNT; ++i) {
+		if (kbasep_js_ctx_attr_is_attr_on_ctx(kctx, (kbasep_js_ctx_attr) i) != MALI_FALSE) {
+			/* The context is being scheduled out, so update the runpool on the removed attributes */
+			runpool_state_changed |= kbasep_js_ctx_attr_runpool_release_attr(kbdev, kctx, (kbasep_js_ctx_attr) i);
+		}
+	}
+
+	return runpool_state_changed;
+}
+
+void kbasep_js_ctx_attr_ctx_retain_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	mali_bool runpool_state_changed = MALI_FALSE;
+	base_jd_core_req core_req;
+
+	KBASE_DEBUG_ASSERT(katom);
+	core_req = katom->core_req;
+
+	if (core_req & BASE_JD_REQ_ONLY_COMPUTE)
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_retain_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_COMPUTE);
+	else
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_retain_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_NON_COMPUTE);
+
+	if ((core_req & (BASE_JD_REQ_CS | BASE_JD_REQ_ONLY_COMPUTE | BASE_JD_REQ_T)) != 0 && (core_req & (BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP)) == 0) {
+		/* Atom that can run on slot1 or slot2, and can use all cores */
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_retain_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES);
+	}
+
+	/* We don't need to know about state changed, because retaining an
+	 * atom occurs on adding it, and that itself will also try to run
+	 * new atoms */
+	CSTD_UNUSED(runpool_state_changed);
+}
+
+mali_bool kbasep_js_ctx_attr_ctx_release_atom(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state)
+{
+	mali_bool runpool_state_changed = MALI_FALSE;
+	base_jd_core_req core_req;
+
+	KBASE_DEBUG_ASSERT(katom_retained_state);
+	core_req = katom_retained_state->core_req;
+
+	/* No-op for invalid atoms */
+	if (kbasep_js_atom_retained_state_is_valid(katom_retained_state) == MALI_FALSE)
+		return MALI_FALSE;
+
+	if (core_req & BASE_JD_REQ_ONLY_COMPUTE) {
+		unsigned long flags;
+		int device_nr = (core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) ? katom_retained_state->device_nr : 0;
+		KBASE_DEBUG_ASSERT(device_nr < 2);
+
+		spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+		kbasep_pm_record_job_status(kbdev);
+		kbdev->pm.metrics.active_cl_ctx[device_nr]--;
+		spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_release_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_COMPUTE);
+	} else {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+		kbasep_pm_record_job_status(kbdev);
+		kbdev->pm.metrics.active_gl_ctx--;
+		spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_release_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_NON_COMPUTE);
+	}
+
+	if ((core_req & (BASE_JD_REQ_CS | BASE_JD_REQ_ONLY_COMPUTE | BASE_JD_REQ_T)) != 0 && (core_req & (BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP)) == 0) {
+		/* Atom that can run on slot1 or slot2, and can use all cores */
+		runpool_state_changed |= kbasep_js_ctx_attr_ctx_release_attr(kbdev, kctx, KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES);
+	}
+
+	return runpool_state_changed;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.h
new file mode 100644
index 0000000..6e72229
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_ctx_attr.h
@@ -0,0 +1,158 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_ctx_attr.h
+ * Job Scheduler Context Attribute APIs
+ */
+
+#ifndef _KBASE_JS_CTX_ATTR_H_
+#define _KBASE_JS_CTX_ATTR_H_
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js
+ * @{
+ */
+
+/**
+ * Set the initial attributes of a context (when context create flags are set)
+ *
+ * Requires:
+ * - Hold the jsctx_mutex
+ */
+void kbasep_js_ctx_attr_set_initial_attrs(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * Retain all attributes of a context
+ *
+ * This occurs on scheduling in the context on the runpool (but after
+ * is_scheduled is set)
+ *
+ * Requires:
+ * - jsctx mutex
+ * - runpool_irq spinlock
+ * - ctx->is_scheduled is true
+ */
+void kbasep_js_ctx_attr_runpool_retain_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * Release all attributes of a context
+ *
+ * This occurs on scheduling out the context from the runpool (but before
+ * is_scheduled is cleared)
+ *
+ * Requires:
+ * - jsctx mutex
+ * - runpool_irq spinlock
+ * - ctx->is_scheduled is true
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * In this state, the scheduler might be able to submit more jobs than
+ * previously, and so the caller should ensure kbasep_js_try_run_next_job_nolock()
+ * or similar is called sometime later.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+mali_bool kbasep_js_ctx_attr_runpool_release_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * Retain all attributes of an atom
+ *
+ * This occurs on adding an atom to a context
+ *
+ * Requires:
+ * - jsctx mutex
+ * - If the context is scheduled, then runpool_irq spinlock must also be held
+ */
+void kbasep_js_ctx_attr_ctx_retain_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+ * Release all attributes of an atom, given its retained state.
+ *
+ * This occurs after (permanently) removing an atom from a context
+ *
+ * Requires:
+ * - jsctx mutex
+ * - If the context is scheduled, then runpool_irq spinlock must also be held
+ *
+ * This is a no-op when \a katom_retained_state is invalid.
+ *
+ * @return MALI_TRUE indicates a change in ctx attributes state of the runpool.
+ * In this state, the scheduler might be able to submit more jobs than
+ * previously, and so the caller should ensure kbasep_js_try_run_next_job_nolock()
+ * or similar is called sometime later.
+ * @return MALI_FALSE indicates no change in ctx attributes state of the runpool.
+ */
+mali_bool kbasep_js_ctx_attr_ctx_release_atom(kbase_device *kbdev, kbase_context *kctx, kbasep_js_atom_retained_state *katom_retained_state);
+
+/**
+ * Requires:
+ * - runpool_irq spinlock
+ */
+static INLINE s8 kbasep_js_ctx_attr_count_on_runpool(kbase_device *kbdev, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_devdata = &kbdev->js_data;
+
+	return js_devdata->runpool_irq.ctx_attr_ref_count[attribute];
+}
+
+/**
+ * Requires:
+ * - runpool_irq spinlock
+ */
+static INLINE mali_bool kbasep_js_ctx_attr_is_attr_on_runpool(kbase_device *kbdev, kbasep_js_ctx_attr attribute)
+{
+	/* In general, attributes are 'on' when they have a non-zero refcount (note: the refcount will never be < 0) */
+	return (mali_bool) kbasep_js_ctx_attr_count_on_runpool(kbdev, attribute);
+}
+
+/**
+ * Requires:
+ * - jsctx mutex
+ */
+static INLINE mali_bool kbasep_js_ctx_attr_is_attr_on_ctx(kbase_context *kctx, kbasep_js_ctx_attr attribute)
+{
+	kbasep_js_kctx_info *js_kctx_info;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(attribute < KBASEP_JS_CTX_ATTR_COUNT);
+	js_kctx_info = &kctx->jctx.sched_info;
+
+	/* In general, attributes are 'on' when they have a refcount (which should never be < 0) */
+	return (mali_bool) (js_kctx_info->ctx.ctx_attr_ref_count[attribute]);
+}
+
+	  /** @} *//* end group kbase_js */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_JS_DEFS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_defs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_defs.h
new file mode 100644
index 0000000..981bb1d
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_defs.h
@@ -0,0 +1,481 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js.h
+ * Job Scheduler Type Definitions
+ */
+
+#ifndef _KBASE_JS_DEFS_H_
+#define _KBASE_JS_DEFS_H_
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js
+ * @{
+ */
+/* Forward decls */
+struct kbase_device;
+struct kbase_jd_atom;
+
+
+/* Types used by the policies must go here */
+enum {
+	/** Context will not submit any jobs */
+	KBASE_CTX_FLAG_SUBMIT_DISABLED = (1u << 0),
+
+	/** Set if the context uses an address space and should be kept scheduled in */
+	KBASE_CTX_FLAG_PRIVILEGED = (1u << 1),
+
+	/** Kernel-side equivalent of BASE_CONTEXT_HINT_ONLY_COMPUTE. Non-mutable after creation flags set */
+	KBASE_CTX_FLAG_HINT_ONLY_COMPUTE = (1u << 2)
+
+	    /* NOTE: Add flags for other things, such as 'is scheduled', and 'is dying' */
+};
+
+typedef u32 kbase_context_flags;
+
+typedef struct kbasep_atom_req {
+	base_jd_core_req core_req;
+	kbase_context_flags ctx_req;
+	u32 device_nr;
+} kbasep_atom_req;
+
+#include "mali_kbase_js_policy_cfs.h"
+
+/* Wrapper Interface - doxygen is elsewhere */
+typedef union kbasep_js_policy {
+#ifdef KBASE_JS_POLICY_AVAILABLE_FCFS
+	kbasep_js_policy_fcfs fcfs;
+#endif
+#ifdef KBASE_JS_POLICY_AVAILABLE_CFS
+	kbasep_js_policy_cfs cfs;
+#endif
+} kbasep_js_policy;
+
+/* Wrapper Interface - doxygen is elsewhere */
+typedef union kbasep_js_policy_ctx_info {
+#ifdef KBASE_JS_POLICY_AVAILABLE_FCFS
+	kbasep_js_policy_fcfs_ctx fcfs;
+#endif
+#ifdef KBASE_JS_POLICY_AVAILABLE_CFS
+	kbasep_js_policy_cfs_ctx cfs;
+#endif
+} kbasep_js_policy_ctx_info;
+
+/* Wrapper Interface - doxygen is elsewhere */
+typedef union kbasep_js_policy_job_info {
+#ifdef KBASE_JS_POLICY_AVAILABLE_FCFS
+	kbasep_js_policy_fcfs_job fcfs;
+#endif
+#ifdef KBASE_JS_POLICY_AVAILABLE_CFS
+	kbasep_js_policy_cfs_job cfs;
+#endif
+} kbasep_js_policy_job_info;
+
+
+/** Callback function run on all of a context's jobs registered with the Job
+ * Scheduler */
+typedef void (*kbasep_js_policy_ctx_job_cb)(struct kbase_device *kbdev, struct kbase_jd_atom *katom);
+
+/**
+ * @brief Maximum number of jobs that can be submitted to a job slot whilst
+ * inside the IRQ handler.
+ *
+ * This is important because GPU NULL jobs can complete whilst the IRQ handler
+ * is running. Otherwise, it potentially allows an unlimited number of GPU NULL
+ * jobs to be submitted inside the IRQ handler, which increases IRQ latency.
+ */
+#define KBASE_JS_MAX_JOB_SUBMIT_PER_SLOT_PER_IRQ 2
+
+/**
+ * @brief the IRQ_THROTTLE time in microseconds
+ *
+ * This will be converted via the GPU's clock frequency into a cycle-count.
+ *
+ * @note we can make an estimate of the GPU's frequency by periodically
+ * sampling its CYCLE_COUNT register
+ */
+#define KBASE_JS_IRQ_THROTTLE_TIME_US 20
+
+/**
+ * @brief Context attributes
+ *
+ * Each context attribute can be thought of as a boolean value that caches some
+ * state information about either the runpool, or the context:
+ * - In the case of the runpool, it is a cache of "Do any contexts owned by
+ * the runpool have attribute X?"
+ * - In the case of a context, it is a cache of "Do any atoms owned by the
+ * context have attribute X?"
+ *
+ * The boolean value of the context attributes often affect scheduling
+ * decisions, such as affinities to use and job slots to use.
+ *
+ * To accomodate changes of state in the context, each attribute is refcounted
+ * in the context, and in the runpool for all running contexts. Specifically:
+ * - The runpool holds a refcount of how many contexts in the runpool have this
+ * attribute.
+ * - The context holds a refcount of how many atoms have this attribute.
+ *
+ * Examples of use:
+ * - Finding out when there are a mix of @ref BASE_CONTEXT_HINT_ONLY_COMPUTE
+ * and ! @ref BASE_CONTEXT_HINT_ONLY_COMPUTE contexts in the runpool
+ */
+typedef enum {
+	/** Attribute indicating a context that contains Compute jobs. That is,
+	 * @ref BASE_CONTEXT_HINT_ONLY_COMPUTE is \b set and/or the context has jobs of type
+	 * @ref BASE_JD_REQ_ONLY_COMPUTE
+	 *
+	 * @note A context can be both 'Compute' and 'Non Compute' if it contains
+	 * both types of jobs.
+	 */
+	KBASEP_JS_CTX_ATTR_COMPUTE,
+
+	/** Attribute indicating a context that contains Non-Compute jobs. That is,
+	 * the context has some jobs that are \b not of type @ref
+	 * BASE_JD_REQ_ONLY_COMPUTE. The context usually has
+	 * BASE_CONTEXT_HINT_COMPUTE \b clear, but this depends on the HW
+	 * workarounds in use in the Job Scheduling Policy.
+	 *
+	 * @note A context can be both 'Compute' and 'Non Compute' if it contains
+	 * both types of jobs.
+	 */
+	KBASEP_JS_CTX_ATTR_NON_COMPUTE,
+
+	/** Attribute indicating that a context contains compute-job atoms that
+	 * aren't restricted to a coherent group, and can run on all cores.
+	 *
+	 * Specifically, this is when the atom's \a core_req satisfy:
+	 * - (\a core_req & (BASE_JD_REQ_CS | BASE_JD_REQ_ONLY_COMPUTE | BASE_JD_REQ_T) // uses slot 1 or slot 2
+	 * - && !(\a core_req & BASE_JD_REQ_COHERENT_GROUP) // not restricted to coherent groups
+	 *
+	 * Such atoms could be blocked from running if one of the coherent groups
+	 * is being used by another job slot, so tracking this context attribute
+	 * allows us to prevent such situations.
+	 *
+	 * @note This doesn't take into account the 1-coregroup case, where all
+	 * compute atoms would effectively be able to run on 'all cores', but
+	 * contexts will still not always get marked with this attribute. Instead,
+	 * it is the caller's responsibility to take into account the number of
+	 * coregroups when interpreting this attribute.
+	 *
+	 * @note Whilst Tiler atoms are normally combined with
+	 * BASE_JD_REQ_COHERENT_GROUP, it is possible to send such atoms without
+	 * BASE_JD_REQ_COHERENT_GROUP set. This is an unlikely case, but it's easy
+	 * enough to handle anyway.
+	 */
+	KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES,
+
+	/** Must be the last in the enum */
+	KBASEP_JS_CTX_ATTR_COUNT
+} kbasep_js_ctx_attr;
+
+enum {
+	/** Bit indicating that new atom should be started because this atom completed */
+	KBASE_JS_ATOM_DONE_START_NEW_ATOMS = (1u << 0),
+	/** Bit indicating that the atom was evicted from the JSn_NEXT registers */
+	KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT = (1u << 1)
+};
+
+/** Combination of KBASE_JS_ATOM_DONE_<...> bits */
+typedef u32 kbasep_js_atom_done_code;
+
+/**
+ * Data used by the scheduler that is unique for each Address Space.
+ *
+ * This is used in IRQ context and kbasep_js_device_data::runpoool_irq::lock
+ * must be held whilst accessing this data (inculding reads and atomic
+ * decisions based on the read).
+ */
+typedef struct kbasep_js_per_as_data {
+	/**
+	 * Ref count of whether this AS is busy, and must not be scheduled out
+	 *
+	 * When jobs are running this is always positive. However, it can still be
+	 * positive when no jobs are running. If all you need is a heuristic to
+	 * tell you whether jobs might be running, this should be sufficient.
+	 */
+	int as_busy_refcount;
+
+	/** Pointer to the current context on this address space, or NULL for no context */
+	kbase_context *kctx;
+} kbasep_js_per_as_data;
+
+/**
+ * @brief KBase Device Data Job Scheduler sub-structure
+ *
+ * This encapsulates the current context of the Job Scheduler on a particular
+ * device. This context is global to the device, and is not tied to any
+ * particular kbase_context running on the device.
+ *
+ * nr_contexts_running and as_free are optimized for packing together (by making
+ * them smaller types than u32). The operations on them should rarely involve
+ * masking. The use of signed types for arithmetic indicates to the compiler that
+ * the value will not rollover (which would be undefined behavior), and so under
+ * the Total License model, it is free to make optimizations based on that (i.e.
+ * to remove masking).
+ */
+typedef struct kbasep_js_device_data {
+	/** Sub-structure to collect together Job Scheduling data used in IRQ context */
+	struct runpool_irq {
+		/**
+		 * Lock for accessing Job Scheduling data used in IRQ context
+		 *
+		 * This lock must be held whenever this data is accessed (read, or
+		 * write). Even for read-only access, memory barriers would be needed.
+		 * In any case, it is likely that decisions based on only reading must
+		 * also be atomic with respect to data held here and elsewhere in the
+		 * Job Scheduler.
+		 *
+		 * This lock must also be held for accessing:
+		 * - kbase_context::as_nr
+		 * - kbase_device::jm_slots
+		 * - Parts of the kbasep_js_policy, dependent on the policy (refer to
+		 * the policy in question for more information)
+		 * - Parts of kbasep_js_policy_ctx_info, dependent on the policy (refer to
+		 * the policy in question for more information)
+		 */
+		spinlock_t lock;
+
+		/** Bitvector indicating whether a currently scheduled context is allowed to submit jobs.
+		 * When bit 'N' is set in this, it indicates whether the context bound to address space
+		 * 'N' (per_as_data[N].kctx) is allowed to submit jobs.
+		 *
+		 * It is placed here because it's much more memory efficient than having a mali_bool8 in
+		 * kbasep_js_per_as_data to store this flag  */
+		u16 submit_allowed;
+
+		/** Context Attributes:
+		 * Each is large enough to hold a refcount of the number of contexts
+		 * that can fit into the runpool. This is currently BASE_MAX_NR_AS
+		 *
+		 * Note that when BASE_MAX_NR_AS==16 we need 5 bits (not 4) to store
+		 * the refcount. Hence, it's not worthwhile reducing this to
+		 * bit-manipulation on u32s to save space (where in contrast, 4 bit
+		 * sub-fields would be easy to do and would save space).
+		 *
+		 * Whilst this must not become negative, the sign bit is used for:
+		 * - error detection in debug builds
+		 * - Optimization: it is undefined for a signed int to overflow, and so
+		 * the compiler can optimize for that never happening (thus, no masking
+		 * is required on updating the variable) */
+		s8 ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT];
+
+		/** Data that is unique for each AS */
+		kbasep_js_per_as_data per_as_data[BASE_MAX_NR_AS];
+
+		/*
+		 * Affinity management and tracking
+		 */
+		/** Bitvector to aid affinity checking. Element 'n' bit 'i' indicates
+		 * that slot 'n' is using core i (i.e. slot_affinity_refcount[n][i] > 0) */
+		u64 slot_affinities[BASE_JM_MAX_NR_SLOTS];
+		/** Bitvector indicating which slots \em might have atoms blocked on
+		 * them because otherwise they'd violate affinity restrictions */
+		u16 slots_blocked_on_affinity;
+		/** Refcount for each core owned by each slot. Used to generate the
+		 * slot_affinities array of bitvectors
+		 *
+		 * The value of the refcount will not exceed BASE_JM_SUBMIT_SLOTS,
+		 * because it is refcounted only when a job is definitely about to be
+		 * submitted to a slot, and is de-refcounted immediately after a job
+		 * finishes */
+		s8 slot_affinity_refcount[BASE_JM_MAX_NR_SLOTS][64];
+	} runpool_irq;
+
+	/**
+	 * Run Pool mutex, for managing contexts within the runpool.
+	 * Unless otherwise specified, you must hold this lock whilst accessing any
+	 * members that follow
+	 *
+	 * In addition, this is used to access:
+	 * - the kbasep_js_kctx_info::runpool substructure
+	 */
+	struct mutex runpool_mutex;
+
+	/**
+	 * Queue Lock, used to access the Policy's queue of contexts independently
+	 * of the Run Pool.
+	 *
+	 * Of course, you don't need the Run Pool lock to access this.
+	 */
+	struct mutex queue_mutex;
+
+	u16 as_free;				/**< Bitpattern of free Address Spaces */
+
+	/** Number of currently scheduled user contexts (excluding ones that are not submitting jobs) */
+	s8 nr_user_contexts_running;
+	/** Number of currently scheduled contexts (including ones that are not submitting jobs) */
+	s8 nr_all_contexts_running;
+
+	/**
+	 * Policy-specific information.
+	 *
+	 * Refer to the structure defined by the current policy to determine which
+	 * locks must be held when accessing this.
+	 */
+	kbasep_js_policy policy;
+
+	/** Core Requirements to match up with base_js_atom's core_req memeber
+	 * @note This is a write-once member, and so no locking is required to read */
+	base_jd_core_req js_reqs[BASE_JM_MAX_NR_SLOTS];
+
+	u32 scheduling_tick_ns;		 /**< Value for KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS */
+	u32 soft_stop_ticks;		 /**< Value for KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS */
+	u32 soft_stop_ticks_cl;		 /**< Value for KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL */
+	u32 hard_stop_ticks_ss;		 /**< Value for KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS */
+	u32 hard_stop_ticks_cl;		 /**< Value for KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL */
+	u32 hard_stop_ticks_nss;	 /**< Value for KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS */
+	u32 gpu_reset_ticks_ss;		 /**< Value for KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS */
+	u32 gpu_reset_ticks_cl;		 /**< Value for KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL */
+	u32 gpu_reset_ticks_nss;	 /**< Value for KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS */
+	u32 ctx_timeslice_ns;		 /**< Value for KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS */
+	u32 cfs_ctx_runtime_init_slices; /**< Value for KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_INIT_SLICES */
+	u32 cfs_ctx_runtime_min_slices;	 /**< Value for  KBASE_CONFIG_ATTR_JS_CFS_CTX_RUNTIME_MIN_SLICES */
+
+	/** List of suspended soft jobs */
+	struct list_head suspended_soft_jobs_list;
+
+#ifdef CONFIG_MALI_DEBUG
+	/* Support soft-stop on a single context */
+	mali_bool softstop_always;
+#endif				/* CONFIG_MALI_DEBUG */
+	/** The initalized-flag is placed at the end, to avoid cache-pollution (we should
+	 * only be using this during init/term paths).
+	 * @note This is a write-once member, and so no locking is required to read */
+	int init_status;
+} kbasep_js_device_data;
+
+/**
+ * @brief KBase Context Job Scheduling information structure
+ *
+ * This is a substructure in the kbase_context that encapsulates all the
+ * scheduling information.
+ */
+typedef struct kbasep_js_kctx_info {
+	/**
+	 * Runpool substructure. This must only be accessed whilst the Run Pool
+	 * mutex ( kbasep_js_device_data::runpool_mutex ) is held.
+	 *
+	 * In addition, the kbasep_js_device_data::runpool_irq::lock may need to be
+	 * held for certain sub-members.
+	 *
+	 * @note some of the members could be moved into kbasep_js_device_data for
+	 * improved d-cache/tlb efficiency.
+	 */
+	struct {
+		kbasep_js_policy_ctx_info policy_ctx;	/**< Policy-specific context */
+	} runpool;
+
+	/**
+	 * Job Scheduler Context information sub-structure. These members are
+	 * accessed regardless of whether the context is:
+	 * - In the Policy's Run Pool
+	 * - In the Policy's Queue
+	 * - Not queued nor in the Run Pool.
+	 *
+	 * You must obtain the jsctx_mutex before accessing any other members of
+	 * this substructure.
+	 *
+	 * You may not access any of these members from IRQ context.
+	 */
+	struct {
+		struct mutex jsctx_mutex;		    /**< Job Scheduler Context lock */
+
+		/** Number of jobs <b>ready to run</b> - does \em not include the jobs waiting in
+		 * the dispatcher, and dependency-only jobs. See kbase_jd_context::job_nr
+		 * for such jobs*/
+		u32 nr_jobs;
+
+		/** Context Attributes:
+		 * Each is large enough to hold a refcount of the number of atoms on
+		 * the context. **/
+		u32 ctx_attr_ref_count[KBASEP_JS_CTX_ATTR_COUNT];
+
+		kbase_context_flags flags;
+		/* NOTE: Unify the following flags into kbase_context_flags */
+		/**
+		 * Is the context scheduled on the Run Pool?
+		 *
+		 * This is only ever updated whilst the jsctx_mutex is held.
+		 */
+		mali_bool is_scheduled;
+		/**
+		 * Wait queue to wait for is_scheduled state changes.
+		 * */
+		wait_queue_head_t is_scheduled_wait;
+
+		mali_bool is_dying;			/**< Is the context in the process of being evicted? */
+	} ctx;
+
+	/* The initalized-flag is placed at the end, to avoid cache-pollution (we should
+	 * only be using this during init/term paths) */
+	int init_status;
+} kbasep_js_kctx_info;
+
+/** Subset of atom state that can be available after jd_done_nolock() is called
+ * on that atom. A copy must be taken via kbasep_js_atom_retained_state_copy(),
+ * because the original atom could disappear. */
+typedef struct kbasep_js_atom_retained_state {
+	/** Event code - to determine whether the atom has finished */
+	base_jd_event_code event_code;
+	/** core requirements */
+	base_jd_core_req core_req;
+	/** Job Slot to retry submitting to if submission from IRQ handler failed */
+	int retry_submit_on_slot;
+	/* Core group atom was executed on */
+	u32 device_nr;
+
+} kbasep_js_atom_retained_state;
+
+/**
+ * Value signifying 'no retry on a slot required' for:
+ * - kbase_js_atom_retained_state::retry_submit_on_slot
+ * - kbase_jd_atom::retry_submit_on_slot
+ */
+#define KBASEP_JS_RETRY_SUBMIT_SLOT_INVALID (-1)
+
+/**
+ * base_jd_core_req value signifying 'invalid' for a kbase_jd_atom_retained_state.
+ *
+ * @see kbase_atom_retained_state_is_valid()
+ */
+#define KBASEP_JS_ATOM_RETAINED_STATE_CORE_REQ_INVALID BASE_JD_REQ_DEP
+
+/**
+ * @brief The JS timer resolution, in microseconds
+ *
+ * Any non-zero difference in time will be at least this size.
+ */
+#define KBASEP_JS_TICK_RESOLUTION_US 1
+
+#endif				/* _KBASE_JS_DEFS_H_ */
+
+	  /** @} *//* end group kbase_js */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy.h
new file mode 100644
index 0000000..f746f1d
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy.h
@@ -0,0 +1,767 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_policy.h
+ * Job Scheduler Policy APIs.
+ */
+
+#ifndef _KBASE_JS_POLICY_H_
+#define _KBASE_JS_POLICY_H_
+
+/**
+ * @page page_kbase_js_policy Job Scheduling Policies
+ * The Job Scheduling system is described in the following:
+ * - @subpage page_kbase_js_policy_overview
+ * - @subpage page_kbase_js_policy_operation
+ *
+ * The API details are as follows:
+ * - @ref kbase_jm
+ * - @ref kbase_js
+ * - @ref kbase_js_policy
+ */
+
+/**
+ * @page page_kbase_js_policy_overview Overview of the Policy System
+ *
+ * The Job Scheduler Policy manages:
+ * - The assigning of KBase Contexts to GPU Address Spaces (\em ASs)
+ * - The choosing of Job Chains (\em Jobs) from a KBase context, to run on the
+ * GPU's Job Slots (\em JSs).
+ * - The amount of \em time a context is assigned to (<em>scheduled on</em>) an
+ * Address Space
+ * - The amount of \em time a Job spends running on the GPU
+ *
+ * The Policy implements this management via 2 components:
+ * - A Policy Queue, which manages a set of contexts that are ready to run,
+ * but not currently running.
+ * - A Policy Run Pool, which manages the currently running contexts (one per Address
+ * Space) and the jobs to run on the Job Slots.
+ *
+ * Each Graphics Process in the system has at least one KBase Context. Therefore,
+ * the Policy Queue can be seen as a queue of Processes waiting to run Jobs on
+ * the GPU.
+ *
+ * <!-- The following needs to be all on one line, due to doxygen's parser -->
+ * @dotfile policy_overview.dot "Diagram showing a very simplified overview of the Policy System. IRQ handling, soft/hard-stopping, contexts re-entering the system and Policy details are omitted"
+ *
+ * The main operations on the queue are:
+ * - Enqueuing a Context to it
+ * - Dequeuing a Context from it, to run it.
+ * - Note: requeuing a context is much the same as enqueuing a context, but
+ * occurs when a context is scheduled out of the system to allow other contexts
+ * to run.
+ *
+ * These operations have much the same meaning for the Run Pool - Jobs are
+ * dequeued to run on a Jobslot, and requeued when they are scheduled out of
+ * the GPU.
+ *
+ * @note This is an over-simplification of the Policy APIs - there are more
+ * operations than 'Enqueue'/'Dequeue', and a Dequeue from the Policy Queue
+ * takes at least two function calls: one to Dequeue from the Queue, one to add
+ * to the Run Pool.
+ *
+ * As indicated on the diagram, Jobs permanently leave the scheduling system
+ * when they are completed, otherwise they get dequeued/requeued until this
+ * happens. Similarly, Contexts leave the scheduling system when their jobs
+ * have all completed. However, Contexts may later return to the scheduling
+ * system (not shown on the diagram) if more Bags of Jobs are submitted to
+ * them.
+ */
+
+/**
+ * @page page_kbase_js_policy_operation Policy Operation
+ *
+ * We describe the actions that the Job Scheduler Core takes on the Policy in
+ * the following cases:
+ * - The IRQ Path
+ * - The Job Submission Path
+ * - The High Priority Job Submission Path
+ *
+ * This shows how the Policy APIs will be used by the Job Scheduler core.
+ *
+ * The following diagram shows an example Policy that contains a Low Priority
+ * queue, and a Real-time (High Priority) Queue. The RT queue is examined
+ * before the LowP one on dequeuing from the head. The Low Priority Queue is
+ * ordered by time, and the RT queue is ordered by RT-priority, and then by
+ * time. In addition, it shows that the Job Scheduler Core will start a
+ * Soft-Stop Timer (SS-Timer) when it dequeue's and submits a job. The
+ * Soft-Stop time is set by a global configuration value, and must be a value
+ * appropriate for the policy. For example, this could include "don't run a
+ * soft-stop timer" for a First-Come-First-Served (FCFS) policy.
+ *
+ * <!-- The following needs to be all on one line, due to doxygen's parser -->
+ * @dotfile policy_operation_diagram.dot "Diagram showing the objects managed by an Example Policy, and the operations made upon these objects by the Job Scheduler Core."
+ *
+ * @section sec_kbase_js_policy_operation_prio Dealing with Priority
+ *
+ * Priority applies both to a context as a whole, and to the jobs within a
+ * context. The jobs specify a priority in the base_jd_atom::prio member, which
+ * is relative to that of the context. A positive setting indicates a reduction
+ * in priority, whereas a negative setting indicates a boost in priority. Of
+ * course, the boost in priority should only be honoured when the originating
+ * process has sufficient priviledges, and should be ignored for unpriviledged
+ * processes. The meaning of the combined priority value is up to the policy
+ * itself, and could be a logarithmic scale instead of a linear scale (e.g. the
+ * policy could implement an increase/decrease in priority by 1 results in an
+ * increase/decrease in \em proportion of time spent scheduled in by 25%, an
+ * effective change in timeslice by 11%).
+ *
+ * It is up to the policy whether a boost in priority boosts the priority of
+ * the entire context (e.g. to such an extent where it may pre-empt other
+ * running contexts). If it chooses to do this, the Policy must make sure that
+ * only the high-priority jobs are run, and that the context is scheduled out
+ * once only low priority jobs remain. This ensures that the low priority jobs
+ * within the context do not gain from the priority boost, yet they still get
+ * scheduled correctly with respect to other low priority contexts.
+ *
+ *
+ * @section sec_kbase_js_policy_operation_irq IRQ Path
+ *
+ * The following happens on the IRQ path from the Job Scheduler Core:
+ * - Note the slot that completed (for later)
+ * - Log the time spent by the job (and implicitly, the time spent by the
+ * context)
+ *  - call kbasep_js_policy_log_job_result() <em>in the context of the irq
+ * handler.</em>
+ *  - This must happen regardless of whether the job completed successfully or
+ * not (otherwise the context gets away with DoS'ing the system with faulty jobs)
+ * - What was the result of the job?
+ *  - If Completed: job is just removed from the system
+ *  - If Hard-stop or failure: job is removed from the system
+ *  - If Soft-stop: queue the book-keeping work onto a work-queue: have a
+ * work-queue call kbasep_js_policy_enqueue_job()
+ * - Check the timeslice used by the owning context
+ *  - call kbasep_js_policy_should_remove_ctx() <em>in the context of the irq
+ * handler.</em>
+ *  - If this returns true, clear the "allowed" flag.
+ * - Check the ctx's flags for "allowed", "has jobs to run" and "is running
+ * jobs"
+ * - And so, should the context stay scheduled in?
+ *  - If No, push onto a work-queue the work of scheduling out the old context,
+ * and getting a new one. That is:
+ *   - kbasep_js_policy_runpool_remove_ctx() on old_ctx
+ *   - kbasep_js_policy_enqueue_ctx() on old_ctx
+ *   - kbasep_js_policy_dequeue_head_ctx() to get new_ctx
+ *   - kbasep_js_policy_runpool_add_ctx() on new_ctx
+ *   - (all of this work is deferred on a work-queue to keep the IRQ handler quick)
+ * - If there is space in the completed job slots' HEAD/NEXT registers, run the next job:
+ *  - kbasep_js_policy_dequeue_job() <em>in the context of the irq
+ * handler</em> with core_req set to that of the completing slot
+ *  - if this returned MALI_TRUE, submit the job to the completed slot.
+ *  - This is repeated until kbasep_js_policy_dequeue_job() returns
+ * MALI_FALSE, or the job slot has a job queued on both the HEAD and NEXT registers.
+ *  - If kbasep_js_policy_dequeue_job() returned false, submit some work to
+ * the work-queue to retry from outside of IRQ context (calling
+ * kbasep_js_policy_dequeue_job() from a work-queue).
+ *
+ * Since the IRQ handler submits new jobs \em and re-checks the IRQ_RAWSTAT,
+ * this sequence could loop a large number of times: this could happen if
+ * the jobs submitted completed on the GPU very quickly (in a few cycles), such
+ * as GPU NULL jobs. Then, the HEAD/NEXT registers will always be free to take
+ * more jobs, causing us to loop until we run out of jobs.
+ *
+ * To mitigate this, we must limit the number of jobs submitted per slot during
+ * the IRQ handler - for example, no more than 2 jobs per slot per IRQ should
+ * be sufficient (to fill up the HEAD + NEXT registers in normal cases). For
+ * Mali-T600 with 3 job slots, this means that up to 6 jobs could be submitted per
+ * slot. Note that IRQ Throttling can make this situation commonplace: 6 jobs
+ * could complete but the IRQ for each of them is delayed by the throttling. By
+ * the time you get the IRQ, all 6 jobs could've completed, meaning you can
+ * submit jobs to fill all 6 HEAD+NEXT registers again.
+ *
+ * @note As much work is deferred as possible, which includes the scheduling
+ * out of a context and scheduling in a new context. However, we can still make
+ * starting a single high-priorty context quick despite this:
+ * - On Mali-T600 family, there is one more AS than JSs.
+ * - This means we can very quickly schedule out one AS, no matter what the
+ * situation (because there will always be one AS that's not currently running
+ * on the job slot - it can only have a job in the NEXT register).
+ *  - Even with this scheduling out, fair-share can still be guaranteed e.g. by
+ * a timeline-based Completely Fair Scheduler.
+ * - When our high-priority context comes in, we can do this quick-scheduling
+ * out immediately, and then schedule in the high-priority context without having to block.
+ * - This all assumes that the context to schedule out is of lower
+ * priority. Otherwise, we will have to block waiting for some other low
+ * priority context to finish its jobs. Note that it's likely (but not
+ * impossible) that the high-priority context \b is running jobs, by virtue of
+ * it being high priority.
+ * - Therefore, we can give a high liklihood that on Mali-T600 at least one
+ * high-priority context can be started very quickly. For the general case, we
+ * can guarantee starting (no. ASs) - (no. JSs) high priority contexts
+ * quickly. In any case, there is a high likelihood that we're able to start
+ * more than one high priority context quickly.
+ *
+ * In terms of the functions used in the IRQ handler directly, these are the
+ * perfomance considerations:
+ * - kbase_js_policy_log_job_result():
+ *  - This is just adding to a 64-bit value (possibly even a 32-bit value if we
+ * only store the time the job's recently spent - see below on 'priority weighting')
+ *  - For priority weighting, a divide operation ('div') could happen, but
+ * this can happen in a deferred context (outside of IRQ) when scheduling out
+ * the ctx; as per our Engineering Specification, the contexts of different
+ * priority still stay scheduled in for the same timeslice, but higher priority
+ * ones scheduled back in more often.
+ *  - That is, the weighted and unweighted times must be stored separately, and
+ * the weighted time is only updated \em outside of IRQ context.
+ *  - Of course, this divide is more likely to be a 'multiply by inverse of the
+ * weight', assuming that the weight (priority) doesn't change.
+ * - kbasep_js_policy_should_remove_ctx():
+ *  - This is usually just a comparison of the stored time value against some
+ * maximum value.
+ *
+ * @note all deferred work can be wrapped up into one call - we usually need to
+ * indicate that a job/bag is done outside of IRQ context anyway.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_submit Submission path
+ *
+ * Start with a Context with no jobs present, and assume equal priority of all
+ * contexts in the system. The following work all happens outside of IRQ
+ * Context :
+ * - As soon as job is made 'ready to 'run', then is must be registerd with the Job
+ * Scheduler Policy:
+ *  - 'Ready to run' means they've satisified their dependencies in the
+ * Kernel-side Job Dispatch system.
+ *  - Call kbasep_js_policy_enqueue_job()
+ *  - This indicates that the job should be scheduled (it is ready to run).
+ * - As soon as a ctx changes from having 0 jobs 'ready to run' to >0 jobs
+ * 'ready to run', we enqueue the context on the policy queue:
+ *  - Call kbasep_js_policy_enqueue_ctx()
+ *  - This indicates that the \em ctx should be scheduled (it is ready to run)
+ *
+ * Next, we need to handle adding a context to the Run Pool - if it's sensible
+ * to do so. This can happen due to two reasons:
+ * -# A context is enqueued as above, and there are ASs free for it to run on
+ * (e.g. it is the first context to be run, in which case it can be added to
+ * the Run Pool immediately after enqueuing on the Policy Queue)
+ * -# A previous IRQ caused another ctx to be scheduled out, requiring that the
+ * context at the head of the queue be scheduled in. Such steps would happen in
+ * a work queue (work deferred from the IRQ context).
+ *
+ * In both cases, we'd handle it as follows:
+ * - Get the context at the Head of the Policy Queue:
+ *  - Call kbasep_js_policy_dequeue_head_ctx()
+ * - Assign the Context an Address Space (Assert that there will be one free,
+ * given the above two reasons)
+ * - Add this context to the Run Pool:
+ *  - Call kbasep_js_policy_runpool_add_ctx()
+ * - Now see if a job should be run:
+ *  - Mostly, this will be done in the IRQ handler at the completion of a
+ * previous job.
+ *  - However, there are two cases where this cannot be done: a) The first job
+ * enqueued to the system (there is no previous IRQ to act upon) b) When jobs
+ * are submitted at a low enough rate to not fill up all Job Slots (or, not to
+ * fill both the 'HEAD' and 'NEXT' registers in the job-slots)
+ *  - Hence, on each ctx <b>and job</b> submission we should try to see if we
+ * can run a job:
+ *  - For each job slot that has free space (in NEXT or HEAD+NEXT registers):
+ *   - Call kbasep_js_policy_dequeue_job() with core_req set to that of the
+ * slot
+ *   - if we got one, submit it to the job slot.
+ *   - This is repeated until kbasep_js_policy_dequeue_job() returns
+ * MALI_FALSE, or the job slot has a job queued on both the HEAD and NEXT registers.
+ *
+ * The above case shows that we should attempt to run jobs in cases where a) a ctx
+ * has been added to the Run Pool, and b) new jobs have been added to a context
+ * in the Run Pool:
+ * - In the latter case, the context is in the runpool because it's got a job
+ * ready to run, or is already running a job
+ * - We could just wait until the IRQ handler fires, but for certain types of
+ * jobs this can take comparatively a long time to complete, e.g. GLES FS jobs
+ * generally take much longer to run that GLES CS jobs, which are vertex shader
+ * jobs.
+ * - Therefore, when a new job appears in the ctx, we must check the job-slots
+ * to see if they're free, and run the jobs as before.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_submit_hipri Submission path for High Priority Contexts
+ *
+ * For High Priority Contexts on Mali-T600, we can make sure that at least 1 of
+ * them can be scheduled in immediately to start high prioriy jobs. In general,
+ * (no. ASs) - (no JSs) high priority contexts may be started immediately. The
+ * following describes how this happens:
+ *
+ * Similar to the previous section, consider what happens with a high-priority
+ * context (a context with a priority higher than that of any in the Run Pool)
+ * that starts out with no jobs:
+ * - A job becomes ready to run on the context, and so we enqueue the context
+ * on the Policy's Queue.
+ * - However, we'd like to schedule in this context immediately, instead of
+ * waiting for one of the Run Pool contexts' timeslice to expire
+ * - The policy's Enqueue function must detect this (because it is the policy
+ * that embodies the concept of priority), and take appropriate action
+ *  - That is, kbasep_js_policy_enqueue_ctx() should check the Policy's Run
+ * Pool to see if a lower priority context should be scheduled out, and then
+ * schedule in the High Priority context.
+ *  - For Mali-T600, we can always pick a context to schedule out immediately
+ * (because there are more ASs than JSs), and so scheduling out a victim context
+ * and scheduling in the high priority context can happen immediately.
+ *   - If a policy implements fair-sharing, then this can still ensure the
+ * victim later on gets a fair share of the GPU.
+ *   - As a note, consider whether the victim can be of equal/higher priority
+ * than the incoming context:
+ *   - Usually, higher priority contexts will be the ones currently running
+ * jobs, and so the context with the lowest priority is usually not running
+ * jobs.
+ *   - This makes it likely that the victim context is low priority, but
+ * it's not impossible for it to be a high priority one:
+ *    - Suppose 3 high priority contexts are submitting only FS jobs, and one low
+ * priority context submitting CS jobs. Then, the context not running jobs will
+ * be one of the hi priority contexts (because only 2 FS jobs can be
+ * queued/running on the GPU HW for Mali-T600).
+ *   - The problem can be mitigated by extra action, but it's questionable
+ * whether we need to: we already have a high likelihood that there's at least
+ * one high priority context - that should be good enough.
+ *   - And so, this method makes sure that at least one high priority context
+ * can be started very quickly, but more than one high priority contexts could be
+ * delayed (up to one timeslice).
+ *   - To improve this, use a GPU with a higher number of Address Spaces vs Job
+ * Slots.
+ * - At this point, let's assume this high priority context has been scheduled
+ * in immediately. The next step is to ensure it can start some jobs quickly.
+ *  - It must do this by Soft-Stopping jobs on any of the Job Slots that it can
+ * submit to.
+ *  - The rest of the logic for starting the jobs is taken care of by the IRQ
+ * handler. All the policy needs to do is ensure that
+ * kbasep_js_policy_dequeue_job() will return the jobs from the high priority
+ * context.
+ *
+ * @note in SS state, we currently only use 2 job-slots (even for T608, but
+ * this might change in future). In this case, it's always possible to schedule
+ * out 2 ASs quickly (their jobs won't be in the HEAD registers). At the same
+ * time, this maximizes usage of the job-slots (only 2 are in use), because you
+ * can guarantee starting of the jobs from the High Priority contexts immediately too.
+ *
+ *
+ *
+ * @section sec_kbase_js_policy_operation_notes Notes
+ *
+ * - In this design, a separate 'init' is needed from dequeue/requeue, so that
+ * information can be retained between the dequeue/requeue calls. For example,
+ * the total time spent for a context/job could be logged between
+ * dequeue/requeuing, to implement Fair Sharing. In this case, 'init' just
+ * initializes that information to some known state.
+ *
+ *
+ *
+ */
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @addtogroup base_kbase_api
+ * @{
+ */
+
+/**
+ * @addtogroup kbase_js_policy Job Scheduler Policy APIs
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core.</b>
+ */
+
+/**
+ * @brief Job Scheduler Policy structure
+ */
+union kbasep_js_policy;
+
+/**
+ * @brief Initialize the Job Scheduler Policy
+ */
+mali_error kbasep_js_policy_init(kbase_device *kbdev);
+
+/**
+ * @brief Terminate the Job Scheduler Policy
+ */
+void kbasep_js_policy_term(kbasep_js_policy *js_policy);
+
+/**
+ * @addtogroup kbase_js_policy_ctx Job Scheduler Policy, Context Management API
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core.</b>
+ */
+
+/**
+ * @brief Job Scheduler Policy Ctx Info structure
+ *
+ * This structure is embedded in the kbase_context structure. It is used to:
+ * - track information needed for the policy to schedule the context (e.g. time
+ * used, OS priority etc.)
+ * - link together kbase_contexts into a queue, so that a kbase_context can be
+ * obtained as the container of the policy ctx info. This allows the API to
+ * return what "the next context" should be.
+ * - obtain other information already stored in the kbase_context for
+ * scheduling purposes (e.g process ID to get the priority of the originating
+ * process)
+ */
+union kbasep_js_policy_ctx_info;
+
+/**
+ * @brief Initialize a ctx for use with the Job Scheduler Policy
+ *
+ * This effectively initializes the kbasep_js_policy_ctx_info structure within
+ * the kbase_context (itself located within the kctx->jctx.sched_info structure).
+ */
+mali_error kbasep_js_policy_init_ctx(kbase_device *kbdev, kbase_context *kctx);
+
+/**
+ * @brief Terminate resources associated with using a ctx in the Job Scheduler
+ * Policy.
+ */
+void kbasep_js_policy_term_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Enqueue a context onto the Job Scheduler Policy Queue
+ *
+ * If the context enqueued has a priority higher than any in the Run Pool, then
+ * it is the Policy's responsibility to decide whether to schedule out a low
+ * priority context from the Run Pool to allow the high priority context to be
+ * scheduled in.
+ *
+ * If the context has the privileged flag set, it will always be kept at the
+ * head of the queue.
+ *
+ * The caller will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ */
+void kbasep_js_policy_enqueue_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Dequeue a context from the Head of the Job Scheduler Policy Queue
+ *
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ *
+ * @return MALI_TRUE if a context was available, and *kctx_ptr points to
+ * the kctx dequeued.
+ * @return MALI_FALSE if no contexts were available.
+ */
+mali_bool kbasep_js_policy_dequeue_head_ctx(kbasep_js_policy *js_policy, kbase_context ** const kctx_ptr);
+
+/**
+ * @brief Evict a context from the Job Scheduler Policy Queue
+ *
+ * This is only called as part of destroying a kbase_context.
+ *
+ * There are many reasons why this might fail during the lifetime of a
+ * context. For example, the context is in the process of being scheduled. In
+ * that case a thread doing the scheduling might have a pointer to it, but the
+ * context is neither in the Policy Queue, nor is it in the Run
+ * Pool. Crucially, neither the Policy Queue, Run Pool, or the Context itself
+ * are locked.
+ *
+ * Hence to find out where in the system the context is, it is important to do
+ * more than just check the kbasep_js_kctx_info::ctx::is_scheduled member.
+ *
+ * The caller will be holding kbasep_js_device_data::queue_mutex.
+ *
+ * @return MALI_TRUE if the context was evicted from the Policy Queue
+ * @return MALI_FALSE if the context was not found in the Policy Queue
+ */
+mali_bool kbasep_js_policy_try_evict_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Call a function on all jobs belonging to a non-queued, non-running
+ * context, optionally detaching the jobs from the context as it goes.
+ *
+ * At the time of the call, the context is guarenteed to be not-currently
+ * scheduled on the Run Pool (is_scheduled == MALI_FALSE), and not present in
+ * the Policy Queue. This is because one of the following functions was used
+ * recently on the context:
+ * - kbasep_js_policy_evict_ctx()
+ * - kbasep_js_policy_runpool_remove_ctx()
+ *
+ * In both cases, no subsequent call was made on the context to any of:
+ * - kbasep_js_policy_runpool_add_ctx()
+ * - kbasep_js_policy_enqueue_ctx()
+ *
+ * Due to the locks that might be held at the time of the call, the callback
+ * may need to defer work on a workqueue to complete its actions (e.g. when
+ * cancelling jobs)
+ *
+ * \a detach_jobs must only be set when cancelling jobs (which occurs as part
+ * of context destruction).
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ */
+void kbasep_js_policy_foreach_ctx_job(kbasep_js_policy *js_policy, kbase_context *kctx,
+	kbasep_js_policy_ctx_job_cb callback, mali_bool detach_jobs);
+
+/**
+ * @brief Add a context to the Job Scheduler Policy's Run Pool
+ *
+ * If the context enqueued has a priority higher than any in the Run Pool, then
+ * it is the Policy's responsibility to decide whether to schedule out low
+ * priority jobs that are currently running on the GPU.
+ *
+ * The number of contexts present in the Run Pool will never be more than the
+ * number of Address Spaces.
+ *
+ * The following guarentees are made about the state of the system when this
+ * is called:
+ * - kctx->as_nr member is valid
+ * - the context has its submit_allowed flag set
+ * - kbasep_js_device_data::runpool_irq::per_as_data[kctx->as_nr] is valid
+ * - The refcount of the context is guarenteed to be zero.
+ * - kbasep_js_kctx_info::ctx::is_scheduled will be MALI_TRUE.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_irq::lock (a spinlock)
+ *
+ * Due to a spinlock being held, this function must not call any APIs that sleep.
+ */
+void kbasep_js_policy_runpool_add_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Remove a context from the Job Scheduler Policy's Run Pool
+ *
+ * The kctx->as_nr member is valid and the context has its submit_allowed flag
+ * set when this is called. The state of
+ * kbasep_js_device_data::runpool_irq::per_as_data[kctx->as_nr] is also
+ * valid. The refcount of the context is guarenteed to be zero.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_irq::lock (a spinlock)
+ *
+ * Due to a spinlock being held, this function must not call any APIs that sleep.
+ */
+void kbasep_js_policy_runpool_remove_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Indicate whether a context should be removed from the Run Pool
+ * (should be scheduled out).
+ *
+ * The kbasep_js_device_data::runpool_irq::lock will be held by the caller.
+ *
+ * @note This API is called from IRQ context.
+ */
+mali_bool kbasep_js_policy_should_remove_ctx(kbasep_js_policy *js_policy, kbase_context *kctx);
+
+/**
+ * @brief Synchronize with any timers acting upon the runpool
+ *
+ * The policy should check whether any timers it owns should be running. If
+ * they should not, the policy must cancel such timers and ensure they are not
+ * re-run by the time this function finishes.
+ *
+ * In particular, the timers must not be running when there are no more contexts
+ * on the runpool, because the GPU could be powered off soon after this call.
+ *
+ * The locking conditions on the caller are as follows:
+ * - it will be holding kbasep_js_kctx_info::ctx::jsctx_mutex.
+ * - it will be holding kbasep_js_device_data::runpool_mutex.
+ */
+void kbasep_js_policy_runpool_timers_sync(kbasep_js_policy *js_policy);
+
+
+/**
+ * @brief Indicate whether a new context has an higher priority than the current context.
+ *
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held for \a new_ctx
+ *
+ * This function must not sleep, because an IRQ spinlock might be held whilst
+ * this is called.
+ *
+ * @note There is nothing to stop the priority of \a current_ctx changing
+ * during or immediately after this function is called (because its jsctx_mutex
+ * cannot be held). Therefore, this function should only be seen as a heuristic
+ * guide as to whether \a new_ctx is higher priority than \a current_ctx
+ */
+mali_bool kbasep_js_policy_ctx_has_priority(kbasep_js_policy *js_policy, kbase_context *current_ctx, kbase_context *new_ctx);
+
+	  /** @} *//* end group kbase_js_policy_ctx */
+
+/**
+ * @addtogroup kbase_js_policy_job Job Scheduler Policy, Job Chain Management API
+ * @{
+ *
+ * <b>Refer to @ref page_kbase_js_policy for an overview and detailed operation of
+ * the Job Scheduler Policy and its use from the Job Scheduler Core.</b>
+ */
+
+/**
+ * @brief Job Scheduler Policy Job Info structure
+ *
+ * This structure is embedded in the kbase_jd_atom structure. It is used to:
+ * - track information needed for the policy to schedule the job (e.g. time
+ * used, OS priority etc.)
+ * - link together jobs into a queue/buffer, so that a kbase_jd_atom can be
+ * obtained as the container of the policy job info. This allows the API to
+ * return what "the next job" should be.
+ * - obtain other information already stored in the kbase_context for
+ * scheduling purposes (e.g user-side relative priority)
+ */
+union kbasep_js_policy_job_info;
+
+/**
+ * @brief Initialize a job for use with the Job Scheduler Policy
+ *
+ * This function initializes the kbasep_js_policy_job_info structure within the
+ * kbase_jd_atom. It will only initialize/allocate resources that are specific
+ * to the job.
+ *
+ * That is, this function makes \b no attempt to:
+ * - initialize any context/policy-wide information
+ * - enqueue the job on the policy.
+ *
+ * At some later point, the following functions must be called on the job, in this order:
+ * - kbasep_js_policy_register_job() to register the job and initialize policy/context wide data.
+ * - kbasep_js_policy_enqueue_job() to enqueue the job
+ *
+ * A job must only ever be initialized on the Policy once, and must be
+ * terminated on the Policy before the job is freed.
+ *
+ * The caller will not be holding any locks, and so this function will not
+ * modify any information in \a kctx or \a js_policy.
+ *
+ * @return MALI_ERROR_NONE if initialization was correct.
+ */
+mali_error kbasep_js_policy_init_job(const kbasep_js_policy *js_policy, const kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+ * @brief Register context/policy-wide information for a job on the Job Scheduler Policy.
+ *
+ * Registers the job with the policy. This is used to track the job before it
+ * has been enqueued/requeued by kbasep_js_policy_enqueue_job(). Specifically,
+ * it is used to update information under a lock that could not be updated at
+ * kbasep_js_policy_init_job() time (such as context/policy-wide data).
+ *
+ * @note This function will not fail, and hence does not allocate any
+ * resources. Any failures that could occur on registration will be caught
+ * during kbasep_js_policy_init_job() instead.
+ *
+ * A job must only ever be registerd on the Policy once, and must be
+ * deregistered on the Policy on completion (whether or not that completion was
+ * success/failure).
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_register_job(kbasep_js_policy *js_policy, kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+ * @brief De-register context/policy-wide information for a on the Job Scheduler Policy.
+ *
+ * This must be used before terminating the resources associated with using a
+ * job in the Job Scheduler Policy. This function does not itself terminate any
+ * resources, at most it just updates information in the policy and context.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_deregister_job(kbasep_js_policy *js_policy, kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+ * @brief Dequeue a Job for a job slot from the Job Scheduler Policy Run Pool
+ *
+ * The job returned by the policy will match at least one of the bits in the
+ * job slot's core requirements (but it may match more than one, or all @ref
+ * base_jd_core_req bits supported by the job slot).
+ *
+ * In addition, the requirements of the job returned will be a subset of those
+ * requested - the job returned will not have requirements that \a job_slot_idx
+ * cannot satisfy.
+ *
+ * The caller will submit the job to the GPU as soon as the GPU's NEXT register
+ * for the corresponding slot is empty. Of course, the GPU will then only run
+ * this new job when the currently executing job (in the jobslot's HEAD
+ * register) has completed.
+ *
+ * @return MALI_TRUE if a job was available, and *kctx_ptr points to
+ * the kctx dequeued.
+ * @return MALI_FALSE if no jobs were available among all ctxs in the Run Pool.
+ *
+ * @note base_jd_core_req is currently a u8 - beware of type conversion.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_device_data::runpool_lock::irq will be held.
+ * - kbasep_js_device_data::runpool_mutex will be held.
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex. will be held
+ */
+mali_bool kbasep_js_policy_dequeue_job(kbase_device *kbdev, int job_slot_idx, kbase_jd_atom ** const katom_ptr);
+
+/**
+ * @brief Requeue a Job back into the the Job Scheduler Policy Run Pool
+ *
+ * This will be used to enqueue a job after its creation and also to requeue
+ * a job into the Run Pool that was previously dequeued (running). It notifies
+ * the policy that the job should be run again at some point later.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_device_data::runpool_irq::lock (a spinlock) will be held.
+ * - kbasep_js_device_data::runpool_mutex will be held.
+ * - kbasep_js_kctx_info::ctx::jsctx_mutex will be held.
+ */
+void kbasep_js_policy_enqueue_job(kbasep_js_policy *js_policy, kbase_jd_atom *katom);
+
+/**
+ * @brief Log the result of a job: the time spent on a job/context, and whether
+ * the job failed or not.
+ *
+ * Since a kbase_jd_atom contains a pointer to the kbase_context owning it,
+ * then this can also be used to log time on either/both the job and the
+ * containing context.
+ *
+ * The completion state of the job can be found by examining \a katom->event.event_code
+ *
+ * If the Job failed and the policy is implementing fair-sharing, then the
+ * policy must penalize the failing job/context:
+ * - At the very least, it should penalize the time taken by the amount of
+ * time spent processing the IRQ in SW. This because a job in the NEXT slot
+ * waiting to run will be delayed until the failing job has had the IRQ
+ * cleared.
+ * - \b Optionally, the policy could apply other penalties. For example, based
+ * on a threshold of a number of failing jobs, after which a large penalty is
+ * applied.
+ *
+ * The kbasep_js_device_data::runpool_mutex will be held by the caller.
+ *
+ * @note This API is called from IRQ context.
+ *
+ * The caller has the following conditions on locking:
+ * - kbasep_js_device_data::runpool_irq::lock will be held.
+ *
+ * @param js_policy     job scheduler policy
+ * @param katom         job dispatch atom
+ * @param time_spent_us the time spent by the job, in microseconds (10^-6 seconds).
+ */
+void kbasep_js_policy_log_job_result(kbasep_js_policy *js_policy, kbase_jd_atom *katom, u64 time_spent_us);
+
+	  /** @} *//* end group kbase_js_policy_job */
+
+	  /** @} *//* end group kbase_js_policy */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif				/* _KBASE_JS_POLICY_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.c
new file mode 100644
index 0000000..046acb1
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.c
@@ -0,0 +1,1448 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/*
+ * Job Scheduler: Completely Fair Policy Implementation
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_jm.h>
+#include <mali_kbase_js.h>
+#include <mali_kbase_js_policy_cfs.h>
+#include <linux/sched/rt.h>
+
+/**
+ * Define for when dumping is enabled.
+ * This should not be based on the instrumentation level as whether dumping is enabled for a particular level is down to the integrator.
+ * However this is being used for now as otherwise the cinstr headers would be needed.
+ */
+#define CINSTR_DUMPING_ENABLED (2 == MALI_INSTRUMENTATION_LEVEL)
+
+/** Fixed point constants used for runtime weight calculations */
+#define WEIGHT_FIXEDPOINT_SHIFT 10
+#define WEIGHT_TABLE_SIZE       40
+#define WEIGHT_0_NICE           (WEIGHT_TABLE_SIZE/2)
+#define WEIGHT_0_VAL            (1 << WEIGHT_FIXEDPOINT_SHIFT)
+
+#define LOOKUP_VARIANT_MASK ((1u<<KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS) - 1u)
+
+#define PROCESS_PRIORITY_MIN (-20)
+#define PROCESS_PRIORITY_MAX  (19)
+
+/** Core requirements that all the variants support */
+#define JS_CORE_REQ_ALL_OTHERS \
+	(BASE_JD_REQ_CF | BASE_JD_REQ_V | BASE_JD_REQ_PERMON | BASE_JD_REQ_EXTERNAL_RESOURCES | BASEP_JD_REQ_EVENT_NEVER)
+
+/** Context requirements the all the variants support */
+
+/* In HW issue 8987 workaround, restrict Compute-only contexts and Compute jobs onto job slot[2],
+ * which will ensure their affinity does not intersect GLES jobs */
+#define JS_CTX_REQ_ALL_OTHERS_8987 \
+	(KBASE_CTX_FLAG_PRIVILEGED)
+#define JS_CORE_REQ_COMPUTE_SLOT_8987 \
+	(BASE_JD_REQ_CS)
+#define JS_CORE_REQ_ONLY_COMPUTE_SLOT_8987 \
+	(BASE_JD_REQ_ONLY_COMPUTE)
+
+/* Otherwise, compute-only contexts/compute jobs can use any job slot */
+#define JS_CTX_REQ_ALL_OTHERS \
+	(KBASE_CTX_FLAG_PRIVILEGED | KBASE_CTX_FLAG_HINT_ONLY_COMPUTE)
+#define JS_CORE_REQ_COMPUTE_SLOT \
+	(BASE_JD_REQ_CS | BASE_JD_REQ_ONLY_COMPUTE)
+
+/* core_req variants are ordered by least restrictive first, so that our
+ * algorithm in cached_variant_idx_init picks the least restrictive variant for
+ * each job . Note that coherent_group requirement is added to all CS variants as the
+ * selection of job-slot does not depend on the coherency requirement. */
+static const kbasep_atom_req core_req_variants[] = {
+	{
+	 /* 0: Fragment variant */
+	 (JS_CORE_REQ_ALL_OTHERS | BASE_JD_REQ_FS | BASE_JD_REQ_FS_AFBC |
+						BASE_JD_REQ_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 0},
+	{
+	 /* 1: Compute variant, can use all coregroups */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 0},
+	{
+	 /* 2: Compute variant, uses only coherent coregroups */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT | BASE_JD_REQ_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 0},
+	{
+	 /* 3: Compute variant, might only use coherent coregroup, and must use tiling */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_T),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 0},
+
+	{
+	 /* 4: Unused */
+	 0,
+	 0,
+	 0},
+
+	{
+	 /* 5: Compute variant for specific-coherent-group targetting CoreGroup 0 */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 0			/* device_nr */
+	 },
+	{
+	 /* 6: Compute variant for specific-coherent-group targetting CoreGroup 1 */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS),
+	 1			/* device_nr */
+	 },
+
+	/* Unused core_req variants, to bring the total up to a power of 2 */
+	{
+	 /* 7 */
+	 0,
+	 0,
+	 0},
+};
+
+static const kbasep_atom_req core_req_variants_8987[] = {
+	{
+	 /* 0: Fragment variant */
+	 (JS_CORE_REQ_ALL_OTHERS | BASE_JD_REQ_FS | BASE_JD_REQ_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS_8987),
+	 0},
+	{
+	 /* 1: Compute variant, can use all coregroups */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT_8987),
+	 (JS_CTX_REQ_ALL_OTHERS_8987),
+	 0},
+	{
+	 /* 2: Compute variant, uses only coherent coregroups */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT_8987 | BASE_JD_REQ_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS_8987),
+	 0},
+	{
+	 /* 3: Compute variant, might only use coherent coregroup, and must use tiling */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_COMPUTE_SLOT_8987 | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_T),
+	 (JS_CTX_REQ_ALL_OTHERS_8987),
+	 0},
+
+	{
+	 /* 4: Variant guarenteed to support Compute contexts/atoms
+	  *
+	  * In the case of a context that's specified as 'Only Compute', it'll
+	  * not allow Tiler or Fragment atoms, and so those get rejected */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_ONLY_COMPUTE_SLOT_8987 | BASE_JD_REQ_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS_8987 | KBASE_CTX_FLAG_HINT_ONLY_COMPUTE),
+	 0},
+
+	{
+	 /* 5: Compute variant for specific-coherent-group targetting CoreGroup 0
+	  * Specifically, this only allows 'Only Compute' contexts/atoms */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_ONLY_COMPUTE_SLOT_8987 | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS_8987 | KBASE_CTX_FLAG_HINT_ONLY_COMPUTE),
+	 0			/* device_nr */
+	 },
+	{
+	 /* 6: Compute variant for specific-coherent-group targetting CoreGroup 1
+	  * Specifically, this only allows 'Only Compute' contexts/atoms */
+	 (JS_CORE_REQ_ALL_OTHERS | JS_CORE_REQ_ONLY_COMPUTE_SLOT_8987 | BASE_JD_REQ_COHERENT_GROUP | BASE_JD_REQ_SPECIFIC_COHERENT_GROUP),
+	 (JS_CTX_REQ_ALL_OTHERS_8987 | KBASE_CTX_FLAG_HINT_ONLY_COMPUTE),
+	 1			/* device_nr */
+	 },
+	/* Unused core_req variants, to bring the total up to a power of 2 */
+	{
+	 /* 7 */
+	 0,
+	 0,
+	 0},
+};
+
+#define CORE_REQ_VARIANT_FRAGMENT                    0
+#define CORE_REQ_VARIANT_COMPUTE_ALL_CORES           1
+#define CORE_REQ_VARIANT_COMPUTE_ONLY_COHERENT_GROUP 2
+#define CORE_REQ_VARIANT_COMPUTE_OR_TILING           3
+#define CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_0 5
+#define CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_1 6
+
+#define CORE_REQ_VARIANT_ONLY_COMPUTE_8987                     4
+#define CORE_REQ_VARIANT_ONLY_COMPUTE_8987_SPECIFIC_COHERENT_0 5
+#define CORE_REQ_VARIANT_ONLY_COMPUTE_8987_SPECIFIC_COHERENT_1 6
+
+#define NUM_CORE_REQ_VARIANTS NELEMS(core_req_variants)
+#define NUM_CORE_REQ_VARIANTS_8987 NELEMS(core_req_variants_8987)
+
+/** Mappings between job slot and variant lists for Soft-Stoppable State */
+static const u32 variants_supported_ss_state[] = {
+	/* js[0] uses Fragment only */
+	(1u << CORE_REQ_VARIANT_FRAGMENT),
+
+	/* js[1] uses: Compute-all-cores, Compute-only-coherent, Compute-or-Tiling,
+	 * compute-specific-coregroup-0 */
+	(1u << CORE_REQ_VARIANT_COMPUTE_ALL_CORES)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_ONLY_COHERENT_GROUP)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_OR_TILING)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_0),
+
+	/* js[2] uses: Compute-only-coherent, compute-specific-coregroup-1 */
+	(1u << CORE_REQ_VARIANT_COMPUTE_ONLY_COHERENT_GROUP)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_1)
+};
+
+/** Mappings between job slot and variant lists for Soft-Stoppable State, when
+ * we have atoms that can use all the cores (KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES)
+ * and there's more than one coregroup */
+static const u32 variants_supported_ss_allcore_state[] = {
+	/* js[0] uses Fragment only */
+	(1u << CORE_REQ_VARIANT_FRAGMENT),
+
+	/* js[1] uses: Compute-all-cores, Compute-only-coherent, Compute-or-Tiling,
+	 * compute-specific-coregroup-0, compute-specific-coregroup-1 */
+	(1u << CORE_REQ_VARIANT_COMPUTE_ALL_CORES)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_ONLY_COHERENT_GROUP)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_OR_TILING)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_0)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_SPECIFIC_COHERENT_1),
+
+	/* js[2] not used */
+	0
+};
+
+/** Mappings between job slot and variant lists for Soft-Stoppable State for
+ * BASE_HW_ISSUE_8987
+ *
+ * @note There is no 'allcores' variant of this, because this HW issue forces all
+ * atoms with BASE_JD_CORE_REQ_SPECIFIC_COHERENT_GROUP to use slot 2 anyway -
+ * hence regardless of whether a specific coregroup is targetted, those atoms
+ * still make progress. */
+static const u32 variants_supported_ss_state_8987[] = {
+	/* js[0] uses Fragment only */
+	(1u << CORE_REQ_VARIANT_FRAGMENT),
+
+	/* js[1] uses: Compute-all-cores, Compute-only-coherent, Compute-or-Tiling */
+	(1u << CORE_REQ_VARIANT_COMPUTE_ALL_CORES)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_ONLY_COHERENT_GROUP)
+	    | (1u << CORE_REQ_VARIANT_COMPUTE_OR_TILING),
+
+	/* js[2] uses: All Only-compute atoms (including those targetting a
+	 * specific coregroup), and nothing else. This is because their affinity
+	 * must not intersect with non-only-compute atoms.
+	 *
+	 * As a side effect, this causes the 'device_nr' for atoms targetting a
+	 * specific coregroup to be ignored */
+	(1u << CORE_REQ_VARIANT_ONLY_COMPUTE_8987)
+	    | (1u << CORE_REQ_VARIANT_ONLY_COMPUTE_8987_SPECIFIC_COHERENT_0)
+	    | (1u << CORE_REQ_VARIANT_ONLY_COMPUTE_8987_SPECIFIC_COHERENT_1)
+};
+
+/* Defines for easy asserts 'is scheduled'/'is queued'/'is neither queued norscheduled' */
+#define KBASEP_JS_CHECKFLAG_QUEUED       (1u << 0) /**< Check the queued state */
+#define KBASEP_JS_CHECKFLAG_SCHEDULED    (1u << 1) /**< Check the scheduled state */
+#define KBASEP_JS_CHECKFLAG_IS_QUEUED    (1u << 2) /**< Expect queued state to be set */
+#define KBASEP_JS_CHECKFLAG_IS_SCHEDULED (1u << 3) /**< Expect scheduled state to be set */
+
+enum {
+	KBASEP_JS_CHECK_NOTQUEUED = KBASEP_JS_CHECKFLAG_QUEUED,
+	KBASEP_JS_CHECK_NOTSCHEDULED = KBASEP_JS_CHECKFLAG_SCHEDULED,
+	KBASEP_JS_CHECK_QUEUED = KBASEP_JS_CHECKFLAG_QUEUED | KBASEP_JS_CHECKFLAG_IS_QUEUED,
+	KBASEP_JS_CHECK_SCHEDULED = KBASEP_JS_CHECKFLAG_SCHEDULED | KBASEP_JS_CHECKFLAG_IS_SCHEDULED
+};
+
+typedef u32 kbasep_js_check;
+
+/*
+ * Private Functions
+ */
+
+/* Table autogenerated using util built from: midgard/scripts/gen_cfs_weight_of_prio.c */
+
+/* weight = 1.25 */
+static const int weight_of_priority[] = {
+	/*  -20 */ 11, 14, 18, 23,
+	/*  -16 */ 29, 36, 45, 56,
+	/*  -12 */ 70, 88, 110, 137,
+	/*   -8 */ 171, 214, 268, 335,
+	/*   -4 */ 419, 524, 655, 819,
+	/*    0 */ 1024, 1280, 1600, 2000,
+	/*    4 */ 2500, 3125, 3906, 4883,
+	/*    8 */ 6104, 7630, 9538, 11923,
+	/*   12 */ 14904, 18630, 23288, 29110,
+	/*   16 */ 36388, 45485, 56856, 71070
+};
+
+/**
+ * @note There is nothing to stop the priority of the ctx containing \a
+ * ctx_info changing during or immediately after this function is called
+ * (because its jsctx_mutex cannot be held during IRQ). Therefore, this
+ * function should only be seen as a heuristic guide as to the priority weight
+ * of the context.
+ */
+STATIC u64 priority_weight(kbasep_js_policy_cfs_ctx *ctx_info, u64 time_us)
+{
+	u64 time_delta_us;
+	int priority;
+	priority = ctx_info->process_priority + ctx_info->bag_priority;
+
+	/* Adjust runtime_us using priority weight if required */
+	if (priority != 0 && time_us != 0) {
+		int clamped_priority;
+
+		/* Clamp values to min..max weights */
+		if (priority > PROCESS_PRIORITY_MAX)
+			clamped_priority = PROCESS_PRIORITY_MAX;
+		else if (priority < PROCESS_PRIORITY_MIN)
+			clamped_priority = PROCESS_PRIORITY_MIN;
+		else
+			clamped_priority = priority;
+
+		/* Fixed point multiplication */
+		time_delta_us = (time_us * weight_of_priority[WEIGHT_0_NICE + clamped_priority]);
+		/* Remove fraction */
+		time_delta_us = time_delta_us >> WEIGHT_FIXEDPOINT_SHIFT;
+		/* Make sure the time always increases */
+		if (0 == time_delta_us)
+			time_delta_us++;
+	} else {
+		time_delta_us = time_us;
+	}
+
+	return time_delta_us;
+}
+
+#if KBASE_TRACE_ENABLE != 0
+STATIC int kbasep_js_policy_trace_get_refcnt_nolock(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	int as_nr;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	as_nr = kctx->as_nr;
+	if (as_nr != KBASEP_AS_NR_INVALID) {
+		kbasep_js_per_as_data *js_per_as_data;
+		js_per_as_data = &js_devdata->runpool_irq.per_as_data[as_nr];
+
+		refcnt = js_per_as_data->as_busy_refcount;
+	}
+
+	return refcnt;
+}
+
+STATIC INLINE int kbasep_js_policy_trace_get_refcnt(kbase_device *kbdev, kbase_context *kctx)
+{
+	unsigned long flags;
+	kbasep_js_device_data *js_devdata;
+	int refcnt = 0;
+
+	js_devdata = &kbdev->js_data;
+
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	refcnt = kbasep_js_policy_trace_get_refcnt_nolock(kbdev, kctx);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return refcnt;
+}
+#else				/* KBASE_TRACE_ENABLE != 0 */
+STATIC int kbasep_js_policy_trace_get_refcnt_nolock(kbase_device *kbdev, kbase_context *kctx)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(kctx);
+	return 0;
+}
+
+STATIC INLINE int kbasep_js_policy_trace_get_refcnt(kbase_device *kbdev, kbase_context *kctx)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(kctx);
+	return 0;
+}
+#endif				/* KBASE_TRACE_ENABLE != 0 */
+
+#ifdef CONFIG_MALI_DEBUG
+STATIC void kbasep_js_debug_check(kbasep_js_policy_cfs *policy_info, kbase_context *kctx, kbasep_js_check check_flag)
+{
+	/* This function uses the ternary operator and non-explicit comparisons,
+	 * because it makes for much shorter, easier to read code */
+
+	if (check_flag & KBASEP_JS_CHECKFLAG_QUEUED) {
+		mali_bool is_queued;
+		mali_bool expect_queued;
+		is_queued = (kbasep_list_member_of(&policy_info->ctx_queue_head, &kctx->jctx.sched_info.runpool.policy_ctx.cfs.list)) ? MALI_TRUE : MALI_FALSE;
+
+		if (!is_queued)
+			is_queued = (kbasep_list_member_of(&policy_info->ctx_rt_queue_head, &kctx->jctx.sched_info.runpool.policy_ctx.cfs.list)) ? MALI_TRUE : MALI_FALSE;
+
+		expect_queued = (check_flag & KBASEP_JS_CHECKFLAG_IS_QUEUED) ? MALI_TRUE : MALI_FALSE;
+
+		KBASE_DEBUG_ASSERT_MSG(expect_queued == is_queued, "Expected context %p to be %s but it was %s\n", kctx, (expect_queued) ? "queued" : "not queued", (is_queued) ? "queued" : "not queued");
+
+	}
+
+	if (check_flag & KBASEP_JS_CHECKFLAG_SCHEDULED) {
+		mali_bool is_scheduled;
+		mali_bool expect_scheduled;
+		is_scheduled = (kbasep_list_member_of(&policy_info->scheduled_ctxs_head, &kctx->jctx.sched_info.runpool.policy_ctx.cfs.list)) ? MALI_TRUE : MALI_FALSE;
+
+		expect_scheduled = (check_flag & KBASEP_JS_CHECKFLAG_IS_SCHEDULED) ? MALI_TRUE : MALI_FALSE;
+		KBASE_DEBUG_ASSERT_MSG(expect_scheduled == is_scheduled, "Expected context %p to be %s but it was %s\n", kctx, (expect_scheduled) ? "scheduled" : "not scheduled", (is_scheduled) ? "scheduled" : "not scheduled");
+
+	}
+
+}
+#else				/* CONFIG_MALI_DEBUG */
+STATIC void kbasep_js_debug_check(kbasep_js_policy_cfs *policy_info, kbase_context *kctx, kbasep_js_check check_flag)
+{
+	CSTD_UNUSED(policy_info);
+	CSTD_UNUSED(kctx);
+	CSTD_UNUSED(check_flag);
+	return;
+}
+#endif				/* CONFIG_MALI_DEBUG */
+
+STATIC INLINE void set_slot_to_variant_lookup(u32 *bit_array, u32 slot_idx, u32 variants_supported)
+{
+	u32 overall_bit_idx = slot_idx * KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS;
+	u32 word_idx = overall_bit_idx / 32;
+	u32 bit_idx = overall_bit_idx % 32;
+
+	KBASE_DEBUG_ASSERT(slot_idx < BASE_JM_MAX_NR_SLOTS);
+	KBASE_DEBUG_ASSERT((variants_supported & ~LOOKUP_VARIANT_MASK) == 0);
+
+	bit_array[word_idx] |= variants_supported << bit_idx;
+}
+
+STATIC INLINE u32 get_slot_to_variant_lookup(u32 *bit_array, u32 slot_idx)
+{
+	u32 overall_bit_idx = slot_idx * KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS;
+	u32 word_idx = overall_bit_idx / 32;
+	u32 bit_idx = overall_bit_idx % 32;
+
+	u32 res;
+
+	KBASE_DEBUG_ASSERT(slot_idx < BASE_JM_MAX_NR_SLOTS);
+
+	res = bit_array[word_idx] >> bit_idx;
+	res &= LOOKUP_VARIANT_MASK;
+
+	return res;
+}
+
+/* Check the core_req_variants: make sure that every job slot is satisifed by
+ * one of the variants. This checks that cached_variant_idx_init will produce a
+ * valid result for jobs that make maximum use of the job slots.
+ *
+ * @note The checks are limited to the job slots - this does not check that
+ * every context requirement is covered (because some are intentionally not
+ * supported, such as KBASE_CTX_FLAG_SUBMIT_DISABLED) */
+#ifdef CONFIG_MALI_DEBUG
+STATIC void debug_check_core_req_variants(kbase_device *kbdev, kbasep_js_policy_cfs *policy_info)
+{
+	kbasep_js_device_data *js_devdata;
+	u32 i;
+	int j;
+
+	js_devdata = &kbdev->js_data;
+
+	for (j = 0; j < kbdev->gpu_props.num_job_slots; ++j) {
+		base_jd_core_req job_core_req;
+		mali_bool found = MALI_FALSE;
+
+		job_core_req = js_devdata->js_reqs[j];
+		for (i = 0; i < policy_info->num_core_req_variants; ++i) {
+			base_jd_core_req var_core_req;
+			var_core_req = policy_info->core_req_variants[i].core_req;
+
+			if ((var_core_req & job_core_req) == job_core_req) {
+				found = MALI_TRUE;
+				break;
+			}
+		}
+
+		/* Early-out on any failure */
+		KBASE_DEBUG_ASSERT_MSG(found != MALI_FALSE, "Job slot %d features 0x%x not matched by core_req_variants. " "Rework core_req_variants and vairants_supported_<...>_state[] to match\n", j, job_core_req);
+	}
+}
+#endif
+
+STATIC void build_core_req_variants(kbase_device *kbdev, kbasep_js_policy_cfs *policy_info)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(policy_info != NULL);
+	CSTD_UNUSED(kbdev);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987)) {
+		KBASE_DEBUG_ASSERT(NUM_CORE_REQ_VARIANTS_8987 <= KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS);
+
+		/* Assume a static set of variants */
+		memcpy(policy_info->core_req_variants, core_req_variants_8987, sizeof(core_req_variants_8987));
+
+		policy_info->num_core_req_variants = NUM_CORE_REQ_VARIANTS_8987;
+	} else {
+		KBASE_DEBUG_ASSERT(NUM_CORE_REQ_VARIANTS <= KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS);
+
+		/* Assume a static set of variants */
+		memcpy(policy_info->core_req_variants, core_req_variants, sizeof(core_req_variants));
+
+		policy_info->num_core_req_variants = NUM_CORE_REQ_VARIANTS;
+	}
+
+	KBASE_DEBUG_CODE(debug_check_core_req_variants(kbdev, policy_info));
+}
+
+STATIC void build_slot_lookups(kbase_device *kbdev, kbasep_js_policy_cfs *policy_info)
+{
+	u8 i;
+	const u32 *variants_supported_ss_for_this_hw = variants_supported_ss_state;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(policy_info != NULL);
+
+	KBASE_DEBUG_ASSERT(kbdev->gpu_props.num_job_slots <= NELEMS(variants_supported_ss_state));
+	KBASE_DEBUG_ASSERT(kbdev->gpu_props.num_job_slots <= NELEMS(variants_supported_ss_allcore_state));
+	KBASE_DEBUG_ASSERT(kbdev->gpu_props.num_job_slots <= NELEMS(variants_supported_ss_state_8987));
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8987))
+		variants_supported_ss_for_this_hw = variants_supported_ss_state_8987;
+
+	/* Given the static set of variants, provide a static set of lookups */
+	for (i = 0; i < kbdev->gpu_props.num_job_slots; ++i) {
+		set_slot_to_variant_lookup(policy_info->slot_to_variant_lookup_ss_state, i, variants_supported_ss_for_this_hw[i]);
+
+		set_slot_to_variant_lookup(policy_info->slot_to_variant_lookup_ss_allcore_state, i, variants_supported_ss_allcore_state[i]);
+	}
+
+}
+
+STATIC mali_error cached_variant_idx_init(const kbasep_js_policy_cfs *policy_info, const kbase_context *kctx, kbase_jd_atom *atom)
+{
+	kbasep_js_policy_cfs_job *job_info;
+	u32 i;
+	base_jd_core_req job_core_req;
+	u32 job_device_nr;
+	kbase_context_flags ctx_flags;
+	const kbasep_js_kctx_info *js_kctx_info;
+	const kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(policy_info != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(atom != NULL);
+
+	kbdev = container_of(policy_info, const kbase_device, js_data.policy.cfs);
+	job_info = &atom->sched_info.cfs;
+	job_core_req = atom->core_req;
+	job_device_nr = atom->device_nr;
+	js_kctx_info = &kctx->jctx.sched_info;
+	ctx_flags = js_kctx_info->ctx.flags;
+
+	/* Initial check for atoms targetting a specific coregroup */
+	if ((job_core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) != MALI_FALSE && job_device_nr >= kbdev->gpu_props.num_core_groups) {
+		/* device_nr exceeds the number of coregroups - not allowed by
+		 * @ref base_jd_atom API contract */
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	/* Pick a core_req variant that matches us. Since they're ordered by least
+	 * restrictive first, it picks the least restrictive variant */
+	for (i = 0; i < policy_info->num_core_req_variants; ++i) {
+		base_jd_core_req var_core_req;
+		kbase_context_flags var_ctx_req;
+		u32 var_device_nr;
+		var_core_req = policy_info->core_req_variants[i].core_req;
+		var_ctx_req = policy_info->core_req_variants[i].ctx_req;
+		var_device_nr = policy_info->core_req_variants[i].device_nr;
+
+		if ((var_core_req & job_core_req) == job_core_req && (var_ctx_req & ctx_flags) == ctx_flags && ((var_core_req & BASE_JD_REQ_SPECIFIC_COHERENT_GROUP) == MALI_FALSE || var_device_nr == job_device_nr)) {
+			job_info->cached_variant_idx = i;
+			return MALI_ERROR_NONE;
+		}
+	}
+
+	/* Could not find a matching requirement, this should only be caused by an
+	 * attempt to attack the driver. */
+	return MALI_ERROR_FUNCTION_FAILED;
+}
+
+STATIC mali_bool dequeue_job(kbase_device *kbdev,
+			     kbase_context *kctx,
+			     u32 variants_supported,
+			     kbase_jd_atom ** const katom_ptr,
+			     int job_slot_idx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy_cfs *policy_info;
+	kbasep_js_policy_cfs_ctx *ctx_info;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(katom_ptr != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	policy_info = &js_devdata->policy.cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	/* Only submit jobs from contexts that are allowed */
+	if (kbasep_js_is_submit_allowed(js_devdata, kctx) != MALI_FALSE) {
+		/* Check each variant in turn */
+		while (variants_supported != 0) {
+			long variant_idx;
+			struct list_head *job_list;
+			variant_idx = ffs(variants_supported) - 1;
+			job_list = &ctx_info->job_list_head[variant_idx];
+
+			if (!list_empty(job_list)) {
+				/* Found a context with a matching job */
+				{
+					kbase_jd_atom *front_atom = list_entry(job_list->next, kbase_jd_atom, sched_info.cfs.list);
+					KBASE_TRACE_ADD_SLOT(kbdev, JS_POLICY_DEQUEUE_JOB, front_atom->kctx, front_atom, front_atom->jc, job_slot_idx);
+				}
+				*katom_ptr = list_entry(job_list->next, kbase_jd_atom, sched_info.cfs.list);
+				list_del(job_list->next);
+
+				(*katom_ptr)->sched_info.cfs.ticks = 0;
+
+				/* Put this context at the back of the Run Pool */
+				list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+				list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+
+				return MALI_TRUE;
+			}
+
+			variants_supported &= ~(1u << variant_idx);
+		}
+		/* All variants checked by here */
+	}
+
+	/* The context does not have a  matching job */
+
+	return MALI_FALSE;
+}
+
+/**
+ * Hold the runpool_irq spinlock for this
+ */
+STATIC INLINE mali_bool timer_callback_should_run(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	s8 nr_running_ctxs;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+
+	/* nr_user_contexts_running is updated with the runpool_mutex. However, the
+	 * locking in the caller gives us a barrier that ensures nr_user_contexts is
+	 * up-to-date for reading */
+	nr_running_ctxs = js_devdata->nr_user_contexts_running;
+
+#ifdef CONFIG_MALI_DEBUG
+	if (js_devdata->softstop_always && nr_running_ctxs > 0) {
+		/* Debug support for allowing soft-stop on a single context */
+		return MALI_TRUE;
+	}
+#endif				/* CONFIG_MALI_DEBUG */
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_9435)) {
+		/* Timeouts would have to be 4x longer (due to micro-architectural design)
+		 * to support OpenCL conformance tests, so only run the timer when there's:
+		 * - 2 or more CL contexts
+		 * - 1 or more GLES contexts
+		 *
+		 * NOTE: We will treat a context that has both Compute and Non-Compute jobs
+		 * will be treated as an OpenCL context (hence, we don't check
+		 * KBASEP_JS_CTX_ATTR_NON_COMPUTE).
+		 */
+		{
+			s8 nr_compute_ctxs = kbasep_js_ctx_attr_count_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_COMPUTE);
+			s8 nr_noncompute_ctxs = nr_running_ctxs - nr_compute_ctxs;
+
+			return (mali_bool) (nr_compute_ctxs >= 2 || nr_noncompute_ctxs > 0);
+		}
+	} else {
+		/* Run the timer callback whenever you have at least 1 context */
+		return (mali_bool) (nr_running_ctxs > 0);
+	}
+}
+
+static enum hrtimer_restart timer_callback(struct hrtimer *timer)
+{
+	unsigned long flags;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy_cfs *policy_info;
+	int s;
+	mali_bool reset_needed = MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(timer != NULL);
+
+	policy_info = container_of(timer, kbasep_js_policy_cfs, scheduling_timer);
+	kbdev = container_of(policy_info, kbase_device, js_data.policy.cfs);
+	js_devdata = &kbdev->js_data;
+
+	/* Loop through the slots */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	for (s = 0; s < kbdev->gpu_props.num_job_slots; s++) {
+		kbase_jm_slot *slot = &kbdev->jm_slots[s];
+		kbase_jd_atom *atom = NULL;
+
+		if (kbasep_jm_nr_jobs_submitted(slot) > 0) {
+			atom = kbasep_jm_peek_idx_submit_slot(slot, 0);
+			KBASE_DEBUG_ASSERT(atom != NULL);
+
+			if (kbasep_jm_is_dummy_workaround_job(kbdev, atom) != MALI_FALSE) {
+				/* Prevent further use of the atom - never cause a soft-stop, hard-stop, or a GPU reset due to it. */
+				atom = NULL;
+			}
+		}
+
+		if (atom != NULL) {
+			/* The current version of the model doesn't support Soft-Stop */
+			if (!kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_5736)) {
+				u32 ticks = atom->sched_info.cfs.ticks++;
+
+#if !CINSTR_DUMPING_ENABLED
+				u32 soft_stop_ticks, hard_stop_ticks, gpu_reset_ticks;
+				if (atom->core_req & BASE_JD_REQ_ONLY_COMPUTE) {
+					soft_stop_ticks = js_devdata->soft_stop_ticks_cl;
+					hard_stop_ticks = js_devdata->hard_stop_ticks_cl;
+					gpu_reset_ticks = js_devdata->gpu_reset_ticks_cl;
+				} else {
+					soft_stop_ticks = js_devdata->soft_stop_ticks;
+					hard_stop_ticks = js_devdata->hard_stop_ticks_ss;
+					gpu_reset_ticks = js_devdata->gpu_reset_ticks_ss;
+				}
+
+				/* Job is Soft-Stoppable */
+				if (ticks == soft_stop_ticks) {
+					/* Job has been scheduled for at least js_devdata->soft_stop_ticks ticks.
+					 * Soft stop the slot so we can run other jobs.
+					 */
+					dev_dbg(kbdev->dev, "Soft-stop");
+
+#if KBASE_DISABLE_SCHEDULING_SOFT_STOPS == 0
+					kbase_job_slot_softstop(kbdev, s, atom);
+#endif
+				} else if (ticks == hard_stop_ticks) {
+					/* Job has been scheduled for at least js_devdata->hard_stop_ticks_ss ticks.
+					 * It should have been soft-stopped by now. Hard stop the slot.
+					 */
+#if KBASE_DISABLE_SCHEDULING_HARD_STOPS == 0
+					dev_warn(kbdev->dev, "JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)", (unsigned long)ticks, (unsigned long)(js_devdata->scheduling_tick_ns / 1000000u));
+					kbase_job_slot_hardstop(atom->kctx, s, atom);
+#endif
+				} else if (ticks == gpu_reset_ticks) {
+					/* Job has been scheduled for at least js_devdata->gpu_reset_ticks_ss ticks.
+					 * It should have left the GPU by now. Signal that the GPU needs to be reset.
+					 */
+					reset_needed = MALI_TRUE;
+				}
+#else 				/* !CINSTR_DUMPING_ENABLED */
+				/* NOTE: During CINSTR_DUMPING_ENABLED, we use the alternate timeouts, which
+				 * makes the hard-stop and GPU reset timeout much longer. We also ensure that
+				 * we don't soft-stop at all. */
+				if (ticks == js_devdata->soft_stop_ticks) {
+					/* Job has been scheduled for at least js_devdata->soft_stop_ticks.
+					 * We do not soft-stop during CINSTR_DUMPING_ENABLED, however.
+					 */
+					dev_dbg(kbdev->dev, "Soft-stop");
+				} else if (ticks == js_devdata->hard_stop_ticks_nss) {
+					/* Job has been scheduled for at least js_devdata->hard_stop_ticks_nss ticks.
+					 * Hard stop the slot.
+					 */
+#if KBASE_DISABLE_SCHEDULING_HARD_STOPS == 0
+					dev_warn(kbdev->dev, "JS: Job Hard-Stopped (took more than %lu ticks at %lu ms/tick)", (unsigned long)ticks, (unsigned long)(js_devdata->scheduling_tick_ns / 1000000u));
+					kbase_job_slot_hardstop(atom->kctx, s, atom);
+#endif
+				} else if (ticks == js_devdata->gpu_reset_ticks_nss) {
+					/* Job has been scheduled for at least js_devdata->gpu_reset_ticks_nss ticks.
+					 * It should have left the GPU by now. Signal that the GPU needs to be reset.
+					 */
+					reset_needed = MALI_TRUE;
+				}
+#endif				/* !CINSTR_DUMPING_ENABLED */
+			}
+		}
+	}
+
+	if (reset_needed) {
+		dev_err(kbdev->dev, "JS: Job has been on the GPU for too long (KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS/NSS timeout hit). Issueing GPU soft-reset to resolve.");
+
+		if (kbase_prepare_to_reset_gpu_locked(kbdev))
+			kbase_reset_gpu_locked(kbdev);
+	}
+
+	/* the timer is re-issued if there is contexts in the run-pool */
+
+	if (timer_callback_should_run(kbdev) != MALI_FALSE) {
+		hrtimer_start(&policy_info->scheduling_timer, HR_TIMER_DELAY_NSEC(js_devdata->scheduling_tick_ns), HRTIMER_MODE_REL);
+	} else {
+		KBASE_TRACE_ADD(kbdev, JS_POLICY_TIMER_END, NULL, NULL, 0u, 0u);
+		/* timer_running state is updated by kbasep_js_policy_runpool_timers_sync() */
+	}
+
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	return HRTIMER_NORESTART;
+}
+
+/*
+ * Non-private functions
+ */
+
+mali_error kbasep_js_policy_init(kbase_device *kbdev)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy_cfs *policy_info;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	js_devdata = &kbdev->js_data;
+	policy_info = &js_devdata->policy.cfs;
+
+	INIT_LIST_HEAD(&policy_info->ctx_queue_head);
+	INIT_LIST_HEAD(&policy_info->scheduled_ctxs_head);
+	INIT_LIST_HEAD(&policy_info->ctx_rt_queue_head);
+
+	atomic64_set(&policy_info->least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+	atomic64_set(&policy_info->rt_least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+
+	hrtimer_init(&policy_info->scheduling_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	policy_info->scheduling_timer.function = timer_callback;
+
+	policy_info->timer_running = MALI_FALSE;
+	policy_info->head_runtime_us = 0;
+
+	/* Build up the core_req variants */
+	build_core_req_variants(kbdev, policy_info);
+	/* Build the slot to variant lookups */
+	build_slot_lookups(kbdev, policy_info);
+
+	return MALI_ERROR_NONE;
+}
+
+void kbasep_js_policy_term(kbasep_js_policy *js_policy)
+{
+	kbasep_js_policy_cfs *policy_info;
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	policy_info = &js_policy->cfs;
+
+	/* ASSERT that there are no contexts queued */
+	KBASE_DEBUG_ASSERT(list_empty(&policy_info->ctx_queue_head));
+	KBASE_DEBUG_ASSERT(KBASEP_JS_RUNTIME_EMPTY == atomic64_read(&policy_info->least_runtime_us));
+
+	/* ASSERT that there are no contexts scheduled */
+	KBASE_DEBUG_ASSERT(list_empty(&policy_info->scheduled_ctxs_head));
+
+	/* ASSERT that there are no contexts queued */
+	KBASE_DEBUG_ASSERT(list_empty(&policy_info->ctx_rt_queue_head));
+	KBASE_DEBUG_ASSERT(KBASEP_JS_RUNTIME_EMPTY == atomic64_read(&policy_info->rt_least_runtime_us));
+
+	hrtimer_cancel(&policy_info->scheduling_timer);
+}
+
+mali_error kbasep_js_policy_init_ctx(kbase_device *kbdev, kbase_context *kctx)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbasep_js_policy_cfs *policy_info;
+	u32 i;
+	int policy;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	js_devdata = &kbdev->js_data;
+	policy_info = &kbdev->js_data.policy.cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_INIT_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+
+	for (i = 0; i < policy_info->num_core_req_variants; ++i)
+		INIT_LIST_HEAD(&ctx_info->job_list_head[i]);
+
+	policy = current->policy;
+	if (policy == SCHED_FIFO || policy == SCHED_RR) {
+		ctx_info->process_rt_policy = MALI_TRUE;
+		ctx_info->process_priority = (((MAX_RT_PRIO - 1) - current->rt_priority) / 5) - 20;
+	} else {
+		ctx_info->process_rt_policy = MALI_FALSE;
+		ctx_info->process_priority = (current->static_prio - MAX_RT_PRIO) - 20;
+	}
+
+	ctx_info->bag_total_priority = 0;
+	ctx_info->bag_total_nr_atoms = 0;
+
+	/* Initial runtime (relative to least-run context runtime)
+	 *
+	 * This uses the Policy Queue's most up-to-date head_runtime_us by using the
+	 * queue mutex to issue memory barriers - also ensure future updates to
+	 * head_runtime_us occur strictly after this context is initialized */
+	mutex_lock(&js_devdata->queue_mutex);
+
+	/* No need to hold the the runpool_irq.lock here, because we're initializing
+	 * the value, and the context is definitely not being updated in the
+	 * runpool at this point. The queue_mutex ensures the memory barrier. */
+	ctx_info->runtime_us = policy_info->head_runtime_us + priority_weight(ctx_info, (u64) js_devdata->cfs_ctx_runtime_init_slices * (u64) (js_devdata->ctx_timeslice_ns / 1000u));
+
+	mutex_unlock(&js_devdata->queue_mutex);
+
+	return MALI_ERROR_NONE;
+}
+
+void kbasep_js_policy_term_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbasep_js_policy_cfs *policy_info;
+	u32 i;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	{
+		kbase_device *kbdev = container_of(js_policy, kbase_device, js_data.policy);
+		KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_TERM_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+	}
+
+	/* ASSERT that no jobs are present */
+	for (i = 0; i < policy_info->num_core_req_variants; ++i)
+		KBASE_DEBUG_ASSERT(list_empty(&ctx_info->job_list_head[i]));
+
+	/* No work to do */
+}
+
+/*
+ * Context Management
+ */
+
+void kbasep_js_policy_enqueue_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs *policy_info;
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbase_context *head_ctx;
+	kbase_context *list_kctx = NULL;
+	kbasep_js_device_data *js_devdata;
+	struct list_head *queue_head;
+	struct list_head *pos;
+	kbase_device *kbdev;
+	atomic64_t *least_runtime_us;
+	u64 head_runtime;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+	kbdev = container_of(js_policy, kbase_device, js_data.policy);
+	js_devdata = &kbdev->js_data;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_ENQUEUE_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+
+	/* ASSERT about scheduled-ness/queued-ness */
+	kbasep_js_debug_check(policy_info, kctx, KBASEP_JS_CHECK_NOTQUEUED);
+
+	/* Clamp the runtime to prevent DoS attacks through "stored-up" runtime */
+	if (policy_info->head_runtime_us > ctx_info->runtime_us + (u64) js_devdata->cfs_ctx_runtime_min_slices * (u64) (js_devdata->ctx_timeslice_ns / 1000u)) {
+		/* No need to hold the the runpool_irq.lock here, because we're essentially
+		 * initializing the value, and the context is definitely not being updated in the
+		 * runpool at this point. The queue_mutex held by the caller ensures the memory
+		 * barrier. */
+		ctx_info->runtime_us = policy_info->head_runtime_us - (u64) js_devdata->cfs_ctx_runtime_min_slices * (u64) (js_devdata->ctx_timeslice_ns / 1000u);
+	}
+
+	/* Find the position where the context should be enqueued */
+	if (ctx_info->process_rt_policy) {
+		queue_head = &policy_info->ctx_rt_queue_head;
+		least_runtime_us = &policy_info->rt_least_runtime_us;
+	} else {
+		queue_head = &policy_info->ctx_queue_head;
+		least_runtime_us = &policy_info->least_runtime_us;
+	}
+
+	if (list_empty(queue_head)) {
+		list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, queue_head);
+	} else {
+		list_for_each(pos, queue_head) {
+			kbasep_js_policy_cfs_ctx *list_ctx_info;
+
+			list_kctx = list_entry(pos, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+			list_ctx_info = &list_kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+			if ((kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED) != 0)
+				break;
+
+			if ((list_ctx_info->runtime_us > ctx_info->runtime_us) && ((list_kctx->jctx.sched_info.ctx.flags & KBASE_CTX_FLAG_PRIVILEGED) == 0))
+				break;
+		}
+
+		/* Add the context to the queue */
+		list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &list_kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+	}
+
+	/* Ensure least_runtime_us is up to date*/
+	head_ctx = list_entry(queue_head->next, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+	head_runtime = head_ctx->jctx.sched_info.runpool.policy_ctx.cfs.runtime_us;
+	atomic64_set(least_runtime_us, head_runtime);
+}
+
+mali_bool kbasep_js_policy_dequeue_head_ctx(kbasep_js_policy *js_policy, kbase_context ** const kctx_ptr)
+{
+	kbasep_js_policy_cfs *policy_info;
+	kbase_context *head_ctx;
+	struct list_head *queue_head;
+	atomic64_t *least_runtime_us;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx_ptr != NULL);
+
+	policy_info = &js_policy->cfs;
+	kbdev = container_of(js_policy, kbase_device, js_data.policy);
+
+	/* attempt to dequeue from the 'realttime' queue first */
+	if (list_empty(&policy_info->ctx_rt_queue_head)) {
+		if (list_empty(&policy_info->ctx_queue_head)) {
+			/* Nothing to dequeue */
+			return MALI_FALSE;
+		} else {
+			queue_head = &policy_info->ctx_queue_head;
+			least_runtime_us = &policy_info->least_runtime_us;
+		}
+	} else {
+		queue_head = &policy_info->ctx_rt_queue_head;
+		least_runtime_us = &policy_info->rt_least_runtime_us;
+	}
+
+	/* Contexts are dequeued from the front of the queue */
+	*kctx_ptr = list_entry(queue_head->next, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+	/* If dequeuing will empty the list, then set least_runtime_us prior to deletion */
+	if (queue_head->next->next == queue_head)
+		atomic64_set(least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+	list_del(queue_head->next);
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_DEQUEUE_HEAD_CTX, *kctx_ptr, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, *kctx_ptr));
+
+	/* Update the head runtime */
+	if (!list_empty(queue_head)) {
+		u64 head_runtime;
+
+		head_ctx = list_entry(queue_head->next, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+
+		/* No need to hold the the runpool_irq.lock here for reading - the
+		 * context is definitely not being updated in the runpool at this
+		 * point. The queue_mutex held by the caller ensures the memory barrier. */
+		head_runtime = head_ctx->jctx.sched_info.runpool.policy_ctx.cfs.runtime_us;
+
+		if (head_runtime > policy_info->head_runtime_us)
+			policy_info->head_runtime_us = head_runtime;
+
+		atomic64_set(least_runtime_us, head_runtime);
+	}
+
+	return MALI_TRUE;
+}
+
+mali_bool kbasep_js_policy_try_evict_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbasep_js_policy_cfs *policy_info;
+	mali_bool is_present;
+	struct list_head *queue_head;
+	atomic64_t *least_runtime_us;
+	struct list_head *qhead;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+	kbdev = container_of(js_policy, kbase_device, js_data.policy);
+
+	if (ctx_info->process_rt_policy) {
+		queue_head = &policy_info->ctx_rt_queue_head;
+		least_runtime_us = &policy_info->rt_least_runtime_us;
+	} else {
+		queue_head = &policy_info->ctx_queue_head;
+		least_runtime_us = &policy_info->least_runtime_us;
+	}
+
+	qhead = queue_head;
+
+	is_present = kbasep_list_member_of(qhead, &kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+
+	KBASE_TRACE_ADD_REFCOUNT_INFO(kbdev, JS_POLICY_TRY_EVICT_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx), is_present);
+
+	if (is_present != MALI_FALSE) {
+		kbase_context *head_ctx;
+		qhead = queue_head;
+
+		/* If dequeuing will empty the list, then set least_runtime_us prior to deletion */
+		if (queue_head->next->next == queue_head)
+			atomic64_set(least_runtime_us, KBASEP_JS_RUNTIME_EMPTY);
+
+		/* Remove the context */
+		list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+
+		qhead = queue_head;
+		/* Update the head runtime */
+		if (!list_empty(qhead)) {
+			u64 head_runtime;
+
+			head_ctx = list_entry(qhead->next, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+
+			/* No need to hold the the runpool_irq.lock here for reading - the
+			 * context is definitely not being updated in the runpool at this
+			 * point. The queue_mutex held by the caller ensures the memory barrier. */
+			head_runtime = head_ctx->jctx.sched_info.runpool.policy_ctx.cfs.runtime_us;
+
+			if (head_runtime > policy_info->head_runtime_us)
+				policy_info->head_runtime_us = head_runtime;
+
+			atomic64_set(least_runtime_us, head_runtime);
+		}
+	}
+
+	return is_present;
+}
+
+void kbasep_js_policy_foreach_ctx_job(kbasep_js_policy *js_policy, kbase_context *kctx,
+	kbasep_js_policy_ctx_job_cb callback, mali_bool detach_jobs)
+{
+	kbasep_js_policy_cfs *policy_info;
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbase_device *kbdev;
+	u32 i;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbdev = container_of(js_policy, kbase_device, js_data.policy);
+	policy_info = &js_policy->cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_FOREACH_CTX_JOBS, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt(kbdev, kctx));
+
+	/* Invoke callback on jobs on each variant in turn */
+	for (i = 0; i < policy_info->num_core_req_variants; ++i) {
+		struct list_head *job_list;
+		struct kbase_jd_atom *atom;
+		struct kbase_jd_atom *tmp_iter;
+		job_list = &ctx_info->job_list_head[i];
+		/* Invoke callback on all kbase_jd_atoms in this list, optionally
+		 * removing them from the list */
+		list_for_each_entry_safe(atom, tmp_iter, job_list, sched_info.cfs.list) {
+			if (detach_jobs)
+				list_del(&atom->sched_info.cfs.list);
+			callback(kbdev, atom);
+		}
+	}
+
+}
+
+void kbasep_js_policy_runpool_add_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs *policy_info;
+	kbasep_js_device_data *js_devdata;
+	kbase_device *kbdev;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+	js_devdata = container_of(js_policy, kbasep_js_device_data, policy);
+
+	kbdev = kctx->kbdev;
+
+	{
+		KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_RUNPOOL_ADD_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt_nolock(kbdev, kctx));
+	}
+
+	/* ASSERT about scheduled-ness/queued-ness */
+	kbasep_js_debug_check(policy_info, kctx, KBASEP_JS_CHECK_NOTSCHEDULED);
+
+	/* All enqueued contexts go to the back of the runpool */
+	list_add_tail(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list, &policy_info->scheduled_ctxs_head);
+
+	if (timer_callback_should_run(kbdev) != MALI_FALSE && policy_info->timer_running == MALI_FALSE) {
+		hrtimer_start(&policy_info->scheduling_timer, HR_TIMER_DELAY_NSEC(js_devdata->scheduling_tick_ns), HRTIMER_MODE_REL);
+
+		KBASE_TRACE_ADD(kbdev, JS_POLICY_TIMER_START, NULL, NULL, 0u, 0u);
+		policy_info->timer_running = MALI_TRUE;
+	}
+}
+
+void kbasep_js_policy_runpool_remove_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs *policy_info;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+
+	{
+		kbase_device *kbdev = container_of(js_policy, kbase_device, js_data.policy);
+		KBASE_TRACE_ADD_REFCOUNT(kbdev, JS_POLICY_RUNPOOL_REMOVE_CTX, kctx, NULL, 0u, kbasep_js_policy_trace_get_refcnt_nolock(kbdev, kctx));
+	}
+
+	/* ASSERT about scheduled-ness/queued-ness */
+	kbasep_js_debug_check(policy_info, kctx, KBASEP_JS_CHECK_SCHEDULED);
+
+	/* No searching or significant list maintenance required to remove this context */
+	list_del(&kctx->jctx.sched_info.runpool.policy_ctx.cfs.list);
+
+}
+
+mali_bool kbasep_js_policy_should_remove_ctx(kbasep_js_policy *js_policy, kbase_context *kctx)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbasep_js_policy_cfs *policy_info;
+	kbasep_js_device_data *js_devdata;
+	u64 least_runtime_us;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+	js_devdata = container_of(js_policy, kbasep_js_device_data, policy);
+
+	if (ctx_info->process_rt_policy)
+		least_runtime_us = atomic64_read(&policy_info->rt_least_runtime_us);
+	else
+		least_runtime_us = atomic64_read(&policy_info->least_runtime_us);
+
+	if (KBASEP_JS_RUNTIME_EMPTY == least_runtime_us) {
+		/* Queue is empty */
+		return MALI_FALSE;
+	}
+
+	if ((least_runtime_us + priority_weight(ctx_info, (u64) (js_devdata->ctx_timeslice_ns / 1000u)))
+	    < ctx_info->runtime_us) {
+		/* The context is scheduled out if it's not the least-run context anymore.
+		 * The "real" head runtime is used instead of the cached runtime so the current
+		 * context is not scheduled out when there is less contexts than address spaces.
+		 */
+		return MALI_TRUE;
+	}
+
+	return MALI_FALSE;
+}
+
+void kbasep_js_policy_runpool_timers_sync(kbasep_js_policy *js_policy)
+{
+	kbasep_js_policy_cfs *policy_info;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+
+	policy_info = &js_policy->cfs;
+	kbdev = container_of(js_policy, kbase_device, js_data.policy);
+	js_devdata = &kbdev->js_data;
+
+	if (!timer_callback_should_run(kbdev)) {
+		unsigned long flags;
+
+		/* If the timer is running now, synchronize with it by
+		 * locking/unlocking its spinlock, to ensure it's not using an old value
+		 * from timer_callback_should_run() */
+		spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+		spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+		/* From now on, return value of timer_callback_should_run() will also
+		 * cause the timer to not requeue itself. Its return value cannot
+		 * change, because it depends on variables updated with the
+		 * runpool_mutex held, which the caller of this must also hold */
+		hrtimer_cancel(&policy_info->scheduling_timer);
+
+		policy_info->timer_running = MALI_FALSE;
+	}
+}
+
+/*
+ * Job Chain Management
+ */
+
+mali_error kbasep_js_policy_init_job(const kbasep_js_policy *js_policy, const kbase_context *kctx, kbase_jd_atom *katom)
+{
+	const kbasep_js_policy_cfs *policy_info;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	policy_info = &js_policy->cfs;
+
+	/* Determine the job's index into the job list head, will return error if the
+	 * atom is malformed and so is reported. */
+	return cached_variant_idx_init(policy_info, kctx, katom);
+}
+
+void kbasep_js_policy_register_job(kbasep_js_policy *js_policy, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	/* Adjust context priority to include the new job */
+	ctx_info->bag_total_nr_atoms++;
+	ctx_info->bag_total_priority += katom->nice_prio;
+
+	/* Get average priority and convert to NICE range -20..19 */
+	if (ctx_info->bag_total_nr_atoms)
+		ctx_info->bag_priority = (ctx_info->bag_total_priority / ctx_info->bag_total_nr_atoms) - 20;
+}
+
+void kbasep_js_policy_deregister_job(kbasep_js_policy *js_policy, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	CSTD_UNUSED(js_policy);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	ctx_info = &kctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	/* Adjust context priority to no longer include removed job */
+	KBASE_DEBUG_ASSERT(ctx_info->bag_total_nr_atoms > 0);
+	ctx_info->bag_total_nr_atoms--;
+	ctx_info->bag_total_priority -= katom->nice_prio;
+	KBASE_DEBUG_ASSERT(ctx_info->bag_total_priority >= 0);
+
+	/* Get average priority and convert to NICE range -20..19 */
+	if (ctx_info->bag_total_nr_atoms)
+		ctx_info->bag_priority = (ctx_info->bag_total_priority / ctx_info->bag_total_nr_atoms) - 20;
+}
+KBASE_EXPORT_TEST_API(kbasep_js_policy_deregister_job)
+
+mali_bool kbasep_js_policy_dequeue_job(kbase_device *kbdev,
+				       int job_slot_idx,
+				       kbase_jd_atom ** const katom_ptr)
+{
+	kbasep_js_device_data *js_devdata;
+	kbasep_js_policy_cfs *policy_info;
+	kbase_context *kctx;
+	u32 variants_supported;
+	struct list_head *pos;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(katom_ptr != NULL);
+	KBASE_DEBUG_ASSERT(job_slot_idx < BASE_JM_MAX_NR_SLOTS);
+
+	js_devdata = &kbdev->js_data;
+	policy_info = &js_devdata->policy.cfs;
+
+	/* Get the variants for this slot */
+	if (kbdev->gpu_props.num_core_groups > 1 && kbasep_js_ctx_attr_is_attr_on_runpool(kbdev, KBASEP_JS_CTX_ATTR_COMPUTE_ALL_CORES) != MALI_FALSE) {
+		/* SS-allcore state, and there's more than one coregroup */
+		variants_supported = get_slot_to_variant_lookup(policy_info->slot_to_variant_lookup_ss_allcore_state, job_slot_idx);
+	} else {
+		/* SS-state */
+		variants_supported = get_slot_to_variant_lookup(policy_info->slot_to_variant_lookup_ss_state, job_slot_idx);
+	}
+
+	/* First pass through the runpool we consider the realtime priority jobs */
+	list_for_each(pos, &policy_info->scheduled_ctxs_head) {
+		kctx = list_entry(pos, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+		if (kctx->jctx.sched_info.runpool.policy_ctx.cfs.process_rt_policy) {
+			if (dequeue_job(kbdev, kctx, variants_supported, katom_ptr, job_slot_idx)) {
+				/* Realtime policy job matched */
+				return MALI_TRUE;
+			}
+		}
+	}
+
+	/* Second pass through the runpool we consider the non-realtime priority jobs */
+	list_for_each(pos, &policy_info->scheduled_ctxs_head) {
+		kctx = list_entry(pos, kbase_context, jctx.sched_info.runpool.policy_ctx.cfs.list);
+		if (kctx->jctx.sched_info.runpool.policy_ctx.cfs.process_rt_policy == MALI_FALSE) {
+			if (dequeue_job(kbdev, kctx, variants_supported, katom_ptr, job_slot_idx)) {
+				/* Non-realtime policy job matched */
+				return MALI_TRUE;
+			}
+		}
+	}
+
+	/* By this point, no contexts had a matching job */
+	return MALI_FALSE;
+}
+
+void kbasep_js_policy_enqueue_job(kbasep_js_policy *js_policy, kbase_jd_atom *katom)
+{
+	kbasep_js_policy_cfs_job *job_info;
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbase_context *parent_ctx;
+
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	parent_ctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(parent_ctx != NULL);
+
+	job_info = &katom->sched_info.cfs;
+	ctx_info = &parent_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	{
+		kbase_device *kbdev = container_of(js_policy, kbase_device, js_data.policy);
+		KBASE_TRACE_ADD(kbdev, JS_POLICY_ENQUEUE_JOB, katom->kctx, katom, katom->jc, 0);
+	}
+	list_add_tail(&katom->sched_info.cfs.list, &ctx_info->job_list_head[job_info->cached_variant_idx]);
+}
+
+void kbasep_js_policy_log_job_result(kbasep_js_policy *js_policy, kbase_jd_atom *katom, u64 time_spent_us)
+{
+	kbasep_js_policy_cfs_ctx *ctx_info;
+	kbase_context *parent_ctx;
+	KBASE_DEBUG_ASSERT(js_policy != NULL);
+	KBASE_DEBUG_ASSERT(katom != NULL);
+	CSTD_UNUSED(js_policy);
+
+	parent_ctx = katom->kctx;
+	KBASE_DEBUG_ASSERT(parent_ctx != NULL);
+
+	ctx_info = &parent_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	ctx_info->runtime_us += priority_weight(ctx_info, time_spent_us);
+}
+
+mali_bool kbasep_js_policy_ctx_has_priority(kbasep_js_policy *js_policy, kbase_context *current_ctx, kbase_context *new_ctx)
+{
+	kbasep_js_policy_cfs_ctx *current_ctx_info;
+	kbasep_js_policy_cfs_ctx *new_ctx_info;
+
+	KBASE_DEBUG_ASSERT(current_ctx != NULL);
+	KBASE_DEBUG_ASSERT(new_ctx != NULL);
+	CSTD_UNUSED(js_policy);
+
+	current_ctx_info = &current_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+	new_ctx_info = &new_ctx->jctx.sched_info.runpool.policy_ctx.cfs;
+
+	if ((current_ctx_info->process_rt_policy == MALI_FALSE) && (new_ctx_info->process_rt_policy == MALI_TRUE))
+		return MALI_TRUE;
+
+	if ((current_ctx_info->process_rt_policy == new_ctx_info->process_rt_policy) && (current_ctx_info->bag_priority > new_ctx_info->bag_priority))
+		return MALI_TRUE;
+
+	return MALI_FALSE;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.h
new file mode 100644
index 0000000..9c4f3c6
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_js_policy_cfs.h
@@ -0,0 +1,167 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_js_policy_cfs.h
+ * Completely Fair Job Scheduler Policy structure definitions
+ */
+
+#ifndef _KBASE_JS_POLICY_CFS_H_
+#define _KBASE_JS_POLICY_CFS_H_
+
+#define KBASE_JS_POLICY_AVAILABLE_CFS
+
+/** @addtogroup base_api
+ * @{ */
+/** @addtogroup base_kbase_api
+ * @{ */
+/** @addtogroup kbase_js_policy
+ * @{ */
+
+/**
+ * Internally, this policy keeps a few internal queues for different variants
+ * of core requirements, which are used to decide how to schedule onto the
+ * different job slots.
+ *
+ * Must be a power of 2 to keep the lookup math simple
+ */
+#define KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS_LOG2 3
+#define KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS      (1u << KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS_LOG2)
+
+/** Bits needed in the lookup to support all slots */
+#define KBASEP_JS_VARIANT_LOOKUP_BITS_NEEDED (BASE_JM_MAX_NR_SLOTS * KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS)
+/** Number of u32s needed in the lookup array to support all slots */
+#define KBASEP_JS_VARIANT_LOOKUP_WORDS_NEEDED ((KBASEP_JS_VARIANT_LOOKUP_BITS_NEEDED + 31) / 32)
+
+#define KBASEP_JS_RUNTIME_EMPTY 	((u64)-1)
+
+typedef struct kbasep_js_policy_cfs {
+	/** List of all contexts in the context queue. Hold
+	 * kbasep_js_device_data::queue_mutex whilst accessing. */
+	struct list_head ctx_queue_head;
+
+	/** List of all contexts in the realtime (priority) context queue */
+	struct list_head ctx_rt_queue_head;
+
+	/** List of scheduled contexts. Hold kbasep_jd_device_data::runpool_irq::lock
+	 * whilst accessing, which is a spinlock */
+	struct list_head scheduled_ctxs_head;
+
+	/** Number of valid elements in the core_req_variants member, and the
+	 * kbasep_js_policy_rr_ctx::job_list_head array */
+	u32 num_core_req_variants;
+
+	/** Variants of the core requirements */
+	kbasep_atom_req core_req_variants[KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS];
+
+	/* Lookups per job slot against which core_req_variants match it */
+	u32 slot_to_variant_lookup_ss_state[KBASEP_JS_VARIANT_LOOKUP_WORDS_NEEDED];
+	u32 slot_to_variant_lookup_ss_allcore_state[KBASEP_JS_VARIANT_LOOKUP_WORDS_NEEDED];
+
+	/* The timer tick used for rescheduling jobs */
+	struct hrtimer scheduling_timer;
+
+	/* Is the timer running?
+	 *
+	 * The kbasep_js_device_data::runpool_mutex must be held whilst modifying this.  */
+	mali_bool timer_running;
+
+	/* Number of us the least-run context has been running for
+	 *
+	 * The kbasep_js_device_data::queue_mutex must be held whilst updating this
+	 * Reads are possible without this mutex, but an older value might be read
+	 * if no memory barriers are issued beforehand. */
+	u64 head_runtime_us;
+
+	/* Number of us the least-run context in the context queue has been running for.
+	 * -1 if context queue is empty. */
+	atomic64_t least_runtime_us;
+
+	/* Number of us the least-run context in the realtime (priority) context queue
+	 * has been running for. -1 if realtime context queue is empty. */
+	atomic64_t rt_least_runtime_us;
+} kbasep_js_policy_cfs;
+
+/**
+ * This policy contains a single linked list of all contexts.
+ */
+typedef struct kbasep_js_policy_cfs_ctx {
+	/** Link implementing the Policy's Queue, and Currently Scheduled list */
+	struct list_head list;
+
+	/** Job lists for use when in the Run Pool - only using
+	 * kbasep_js_policy_fcfs::num_unique_slots of them. We still need to track
+	 * the jobs when we're not in the runpool, so this member is accessed from
+	 * outside the policy queue (for the first job), inside the policy queue,
+	 * and inside the runpool.
+	 *
+	 * If the context is in the runpool, then this must only be accessed with
+	 * kbasep_js_device_data::runpool_irq::lock held
+	 *
+	 * Jobs are still added to this list even when the context is not in the
+	 * runpool. In that case, the kbasep_js_kctx_info::ctx::jsctx_mutex must be
+	 * held before accessing this. */
+	struct list_head job_list_head[KBASEP_JS_MAX_NR_CORE_REQ_VARIANTS];
+
+	/** Number of us this context has been running for
+	 *
+	 * The kbasep_js_device_data::runpool_irq::lock (a spinlock) must be held
+	 * whilst updating this. Initializing will occur on context init and
+	 * context enqueue (which can only occur in one thread at a time), but
+	 * multi-thread access only occurs while the context is in the runpool.
+	 *
+	 * Reads are possible without this spinlock, but an older value might be read
+	 * if no memory barriers are issued beforehand */
+	u64 runtime_us;
+
+	/* Calling process policy scheme is a realtime scheduler and will use the priority queue
+	 * Non-mutable after ctx init */
+	mali_bool process_rt_policy;
+	/* Calling process NICE priority */
+	int process_priority;
+	/* Average NICE priority of all atoms in bag:
+	 * Hold the kbasep_js_kctx_info::ctx::jsctx_mutex when accessing  */
+	int bag_priority;
+	/* Total NICE priority of all atoms in bag
+	 * Hold the kbasep_js_kctx_info::ctx::jsctx_mutex when accessing  */
+	int bag_total_priority;
+	/* Total number of atoms in the bag
+	 * Hold the kbasep_js_kctx_info::ctx::jsctx_mutex when accessing  */
+	int bag_total_nr_atoms;
+
+} kbasep_js_policy_cfs_ctx;
+
+/**
+ * In this policy, each Job is part of at most one of the per_corereq lists
+ */
+typedef struct kbasep_js_policy_cfs_job {
+	struct list_head list;	    /**< Link implementing the Run Pool list/Jobs owned by the ctx */
+	u32 cached_variant_idx;	  /**< Cached index of the list this should be entered into on re-queue */
+
+	/** Number of ticks that this job has been executing for
+	 *
+	 * To access this, the kbasep_js_device_data::runpool_irq::lock must be held */
+	u32 ticks;
+} kbasep_js_policy_cfs_job;
+
+	  /** @} *//* end group kbase_js_policy */
+	  /** @} *//* end group base_kbase_api */
+	  /** @} *//* end group base_api */
+
+#endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_linux.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_linux.h
new file mode 100644
index 0000000..0d024f2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_linux.h
@@ -0,0 +1,47 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_linux.h
+ * Base kernel APIs, Linux implementation.
+ */
+
+#ifndef _KBASE_LINUX_H_
+#define _KBASE_LINUX_H_
+
+/* All things that are needed for the Linux port. */
+#include <linux/platform_device.h>
+#include <linux/miscdevice.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/atomic.h>
+
+#if defined(MALI_KERNEL_TEST_API)
+#if (1 == MALI_KERNEL_TEST_API)
+#define KBASE_EXPORT_TEST_API(func)		EXPORT_SYMBOL(func);
+#else
+#define KBASE_EXPORT_TEST_API(func)
+#endif
+#else
+#define KBASE_EXPORT_TEST_API(func)
+#endif
+
+#define KBASE_EXPORT_SYMBOL(func)		EXPORT_SYMBOL(func);
+
+#endif				/* _KBASE_LINUX_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.c
new file mode 100644
index 0000000..f1f4476
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.c
@@ -0,0 +1,1293 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mem.c
+ * Base kernel memory APIs
+ */
+#ifdef CONFIG_DMA_SHARED_BUFFER
+#include <linux/dma-buf.h>
+#endif				/* CONFIG_DMA_SHARED_BUFFER */
+
+#include <linux/bug.h>
+#include <linux/compat.h>
+
+#include <mali_kbase_config.h>
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_cache_policy.h>
+#include <mali_kbase_hw.h>
+#include <mali_kbase_gator.h>
+
+/**
+ * @brief Check the zone compatibility of two regions.
+ */
+STATIC int kbase_region_tracker_match_zone(struct kbase_va_region *reg1, struct kbase_va_region *reg2)
+{
+	return ((reg1->flags & KBASE_REG_ZONE_MASK) == (reg2->flags & KBASE_REG_ZONE_MASK));
+}
+
+KBASE_EXPORT_TEST_API(kbase_region_tracker_match_zone)
+
+/* This function inserts a region into the tree. */
+static void kbase_region_tracker_insert(struct kbase_context *kctx, struct kbase_va_region *new_reg)
+{
+	u64 start_pfn = new_reg->start_pfn;
+	struct rb_node **link = &(kctx->reg_rbtree.rb_node);
+	struct rb_node *parent = NULL;
+
+	/* Find the right place in the tree using tree search */
+	while (*link) {
+		struct kbase_va_region *old_reg;
+
+		parent = *link;
+		old_reg = rb_entry(parent, struct kbase_va_region, rblink);
+
+		/* RBTree requires no duplicate entries. */
+		KBASE_DEBUG_ASSERT(old_reg->start_pfn != start_pfn);
+
+		if (old_reg->start_pfn > start_pfn)
+			link = &(*link)->rb_left;
+		else
+			link = &(*link)->rb_right;
+	}
+
+	/* Put the new node there, and rebalance tree */
+	rb_link_node(&(new_reg->rblink), parent, link);
+	rb_insert_color(&(new_reg->rblink), &(kctx->reg_rbtree));
+}
+
+/* Find allocated region enclosing range. */
+struct kbase_va_region *kbase_region_tracker_find_region_enclosing_range(kbase_context *kctx, u64 start_pfn, size_t nr_pages)
+{
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+	u64 end_pfn = start_pfn + nr_pages;
+
+	rbnode = kctx->reg_rbtree.rb_node;
+
+	while (rbnode) {
+		u64 tmp_start_pfn, tmp_end_pfn;
+		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+		tmp_start_pfn = reg->start_pfn;
+		tmp_end_pfn = reg->start_pfn + kbase_reg_current_backed_size(reg);
+
+		/* If start is lower than this, go left. */
+		if (start_pfn < tmp_start_pfn)
+			rbnode = rbnode->rb_left;
+		/* If end is higher than this, then go right. */
+		else if (end_pfn > tmp_end_pfn)
+			rbnode = rbnode->rb_right;
+		else	/* Enclosing */
+			return reg;
+	}
+
+	return NULL;
+}
+
+/* Find allocated region enclosing free range. */
+struct kbase_va_region *kbase_region_tracker_find_region_enclosing_range_free(kbase_context *kctx, u64 start_pfn, size_t nr_pages)
+{
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+	u64 end_pfn = start_pfn + nr_pages;
+
+	rbnode = kctx->reg_rbtree.rb_node;
+	while (rbnode) {
+		u64 tmp_start_pfn, tmp_end_pfn;
+		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+		tmp_start_pfn = reg->start_pfn;
+		tmp_end_pfn = reg->start_pfn + reg->nr_pages;
+
+		/* If start is lower than this, go left. */
+		if (start_pfn < tmp_start_pfn)
+			rbnode = rbnode->rb_left;
+		/* If end is higher than this, then go right. */
+		else if (end_pfn > tmp_end_pfn)
+			rbnode = rbnode->rb_right;
+		else	/* Enclosing */
+			return reg;
+	}
+
+	return NULL;
+}
+
+/* Find region enclosing given address. */
+kbase_va_region *kbase_region_tracker_find_region_enclosing_address(kbase_context *kctx, mali_addr64 gpu_addr)
+{
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+	u64 gpu_pfn = gpu_addr >> PAGE_SHIFT;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	rbnode = kctx->reg_rbtree.rb_node;
+	while (rbnode) {
+		u64 tmp_start_pfn, tmp_end_pfn;
+		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+		tmp_start_pfn = reg->start_pfn;
+		tmp_end_pfn = reg->start_pfn + reg->nr_pages;
+
+		/* If start is lower than this, go left. */
+		if (gpu_pfn < tmp_start_pfn)
+			rbnode = rbnode->rb_left;
+		/* If end is higher than this, then go right. */
+		else if (gpu_pfn >= tmp_end_pfn)
+			rbnode = rbnode->rb_right;
+		else	/* Enclosing */
+			return reg;
+	}
+
+	return NULL;
+}
+
+KBASE_EXPORT_TEST_API(kbase_region_tracker_find_region_enclosing_address)
+
+/* Find region with given base address */
+kbase_va_region *kbase_region_tracker_find_region_base_address(kbase_context *kctx, mali_addr64 gpu_addr)
+{
+	u64 gpu_pfn = gpu_addr >> PAGE_SHIFT;
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	rbnode = kctx->reg_rbtree.rb_node;
+	while (rbnode) {
+		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+		if (reg->start_pfn > gpu_pfn)
+			rbnode = rbnode->rb_left;
+		else if (reg->start_pfn < gpu_pfn)
+			rbnode = rbnode->rb_right;
+		else if (gpu_pfn == reg->start_pfn)
+			return reg;
+		else
+			rbnode = NULL;
+	}
+
+	return NULL;
+}
+
+KBASE_EXPORT_TEST_API(kbase_region_tracker_find_region_base_address)
+
+/* Find region meeting given requirements */
+static struct kbase_va_region *kbase_region_tracker_find_region_meeting_reqs(kbase_context *kctx, struct kbase_va_region *reg_reqs, size_t nr_pages, size_t align)
+{
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+
+	/* Note that this search is a linear search, as we do not have a target
+	   address in mind, so does not benefit from the rbtree search */
+	rbnode = rb_first(&(kctx->reg_rbtree));
+	while (rbnode) {
+		reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+		if ((reg->nr_pages >= nr_pages) && (reg->flags & KBASE_REG_FREE) && kbase_region_tracker_match_zone(reg, reg_reqs)) {
+
+			/* Check alignment */
+			u64 start_pfn = (reg->start_pfn + align - 1) & ~(align - 1);
+			if ((start_pfn >= reg->start_pfn) && (start_pfn <= (reg->start_pfn + reg->nr_pages - 1)) && ((start_pfn + nr_pages - 1) <= (reg->start_pfn + reg->nr_pages - 1)))
+				return reg;
+		}
+		rbnode = rb_next(rbnode);
+	}
+
+	return NULL;
+}
+
+/**
+ * @brief Remove a region object from the global list.
+ *
+ * The region reg is removed, possibly by merging with other free and
+ * compatible adjacent regions.  It must be called with the context
+ * region lock held. The associated memory is not released (see
+ * kbase_free_alloced_region). Internal use only.
+ */
+STATIC mali_error kbase_remove_va_region(kbase_context *kctx, struct kbase_va_region *reg)
+{
+	struct rb_node *rbprev;
+	struct kbase_va_region *prev = NULL;
+	struct rb_node *rbnext;
+	struct kbase_va_region *next = NULL;
+
+	int merged_front = 0;
+	int merged_back = 0;
+	mali_error err = MALI_ERROR_NONE;
+
+	/* Try to merge with the previous block first */
+	rbprev = rb_prev(&(reg->rblink));
+	if (rbprev) {
+		prev = rb_entry(rbprev, struct kbase_va_region, rblink);
+		if ((prev->flags & KBASE_REG_FREE) && kbase_region_tracker_match_zone(prev, reg)) {
+			/* We're compatible with the previous VMA, merge with it */
+			prev->nr_pages += reg->nr_pages;
+			rb_erase(&(reg->rblink), &kctx->reg_rbtree);
+			reg = prev;
+			merged_front = 1;
+		}
+	}
+
+	/* Try to merge with the next block second */
+	/* Note we do the lookup here as the tree may have been rebalanced. */
+	rbnext = rb_next(&(reg->rblink));
+	if (rbnext) {
+		/* We're compatible with the next VMA, merge with it */
+		next = rb_entry(rbnext, struct kbase_va_region, rblink);
+		if ((next->flags & KBASE_REG_FREE) && kbase_region_tracker_match_zone(next, reg)) {
+			next->start_pfn = reg->start_pfn;
+			next->nr_pages += reg->nr_pages;
+			rb_erase(&(reg->rblink), &kctx->reg_rbtree);
+			merged_back = 1;
+			if (merged_front) {
+				/* We already merged with prev, free it */
+				kbase_free_alloced_region(reg);
+			}
+		}
+	}
+
+	/* If we failed to merge then we need to add a new block */
+	if (!(merged_front || merged_back)) {
+		/*
+		 * We didn't merge anything. Add a new free
+		 * placeholder and remove the original one.
+		 */
+		struct kbase_va_region *free_reg;
+
+		free_reg = kbase_alloc_free_region(kctx, reg->start_pfn, reg->nr_pages, reg->flags & KBASE_REG_ZONE_MASK);
+		if (!free_reg) {
+			err = MALI_ERROR_OUT_OF_MEMORY;
+			goto out;
+		}
+
+		rb_replace_node(&(reg->rblink), &(free_reg->rblink), &(kctx->reg_rbtree));
+	}
+
+ out:
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_remove_va_region)
+
+/**
+ * @brief Insert a VA region to the list, replacing the current at_reg.
+ */
+static mali_error kbase_insert_va_region_nolock(kbase_context *kctx, struct kbase_va_region *new_reg, struct kbase_va_region *at_reg, u64 start_pfn, size_t nr_pages)
+{
+	mali_error err = MALI_ERROR_NONE;
+
+	/* Must be a free region */
+	KBASE_DEBUG_ASSERT((at_reg->flags & KBASE_REG_FREE) != 0);
+	/* start_pfn should be contained within at_reg */
+	KBASE_DEBUG_ASSERT((start_pfn >= at_reg->start_pfn) && (start_pfn < at_reg->start_pfn + at_reg->nr_pages));
+	/* at least nr_pages from start_pfn should be contained within at_reg */
+	KBASE_DEBUG_ASSERT(start_pfn + nr_pages <= at_reg->start_pfn + at_reg->nr_pages);
+
+	new_reg->start_pfn = start_pfn;
+	new_reg->nr_pages = nr_pages;
+
+	/* Regions are a whole use, so swap and delete old one. */
+	if (at_reg->start_pfn == start_pfn && at_reg->nr_pages == nr_pages) {
+		rb_replace_node(&(at_reg->rblink), &(new_reg->rblink), &(kctx->reg_rbtree));
+		kbase_free_alloced_region(at_reg);
+	}
+	/* New region replaces the start of the old one, so insert before. */
+	else if (at_reg->start_pfn == start_pfn) {
+		at_reg->start_pfn += nr_pages;
+		KBASE_DEBUG_ASSERT(at_reg->nr_pages >= nr_pages);
+		at_reg->nr_pages -= nr_pages;
+
+		kbase_region_tracker_insert(kctx, new_reg);
+	}
+	/* New region replaces the end of the old one, so insert after. */
+	else if ((at_reg->start_pfn + at_reg->nr_pages) == (start_pfn + nr_pages)) {
+		at_reg->nr_pages -= nr_pages;
+
+		kbase_region_tracker_insert(kctx, new_reg);
+	}
+	/* New region splits the old one, so insert and create new */
+	else {
+		struct kbase_va_region *new_front_reg = kbase_alloc_free_region(kctx, at_reg->start_pfn, start_pfn - at_reg->start_pfn, at_reg->flags & KBASE_REG_ZONE_MASK);
+		if (new_front_reg) {
+			at_reg->nr_pages -= nr_pages + new_front_reg->nr_pages;
+			at_reg->start_pfn = start_pfn + nr_pages;
+
+			kbase_region_tracker_insert(kctx, new_front_reg);
+			kbase_region_tracker_insert(kctx, new_reg);
+		} else {
+			err = MALI_ERROR_OUT_OF_MEMORY;
+		}
+	}
+
+	return err;
+}
+
+/**
+ * @brief Add a VA region to the list.
+ */
+mali_error kbase_add_va_region(kbase_context *kctx, struct kbase_va_region *reg, mali_addr64 addr, size_t nr_pages, size_t align)
+{
+	struct kbase_va_region *tmp;
+	u64 gpu_pfn = addr >> PAGE_SHIFT;
+	mali_error err = MALI_ERROR_NONE;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != reg);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	if (!align)
+		align = 1;
+
+	/* must be a power of 2 */
+	KBASE_DEBUG_ASSERT((align & (align - 1)) == 0);
+	KBASE_DEBUG_ASSERT(nr_pages > 0);
+
+	/* Path 1: Map a specific address. Find the enclosing region, which *must* be free. */
+	if (gpu_pfn) {
+		struct device *dev = kctx->kbdev->dev;
+		KBASE_DEBUG_ASSERT(!(gpu_pfn & (align - 1)));
+
+		tmp = kbase_region_tracker_find_region_enclosing_range_free(kctx, gpu_pfn, nr_pages);
+		if (!tmp) {
+			dev_warn(dev, "Enclosing region not found: 0x%08llx gpu_pfn, %zu nr_pages", gpu_pfn, nr_pages);
+			err = MALI_ERROR_OUT_OF_GPU_MEMORY;
+			goto exit;
+		}
+
+		if ((!kbase_region_tracker_match_zone(tmp, reg)) || (!(tmp->flags & KBASE_REG_FREE))) {
+			dev_warn(dev, "Zone mismatch: %lu != %lu", tmp->flags & KBASE_REG_ZONE_MASK, reg->flags & KBASE_REG_ZONE_MASK);
+			dev_warn(dev, "!(tmp->flags & KBASE_REG_FREE): tmp->start_pfn=0x%llx tmp->flags=0x%lx tmp->nr_pages=0x%zx gpu_pfn=0x%llx nr_pages=0x%zx\n", tmp->start_pfn, tmp->flags, tmp->nr_pages, gpu_pfn, nr_pages);
+			dev_warn(dev, "in function %s (%p, %p, 0x%llx, 0x%zx, 0x%zx)\n", __func__, kctx, reg, addr, nr_pages, align);
+			err = MALI_ERROR_OUT_OF_GPU_MEMORY;
+			goto exit;
+		}
+
+		err = kbase_insert_va_region_nolock(kctx, reg, tmp, gpu_pfn, nr_pages);
+		if (err) {
+			dev_warn(dev, "Failed to insert va region");
+			err = MALI_ERROR_OUT_OF_GPU_MEMORY;
+			goto exit;
+		}
+
+		goto exit;
+	}
+
+	/* Path 2: Map any free address which meets the requirements.  */
+	{
+		u64 start_pfn;
+		tmp = kbase_region_tracker_find_region_meeting_reqs(kctx, reg, nr_pages, align);
+		if (!tmp) {
+			err = MALI_ERROR_OUT_OF_GPU_MEMORY;
+			goto exit;
+		}
+		start_pfn = (tmp->start_pfn + align - 1) & ~(align - 1);
+		err = kbase_insert_va_region_nolock(kctx, reg, tmp, start_pfn, nr_pages);
+	}
+
+ exit:
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_add_va_region)
+
+/**
+ * @brief Initialize the internal region tracker data structure.
+ */
+static void kbase_region_tracker_ds_init(kbase_context *kctx, struct kbase_va_region *same_va_reg, struct kbase_va_region *exec_reg, struct kbase_va_region *custom_va_reg)
+{
+	kctx->reg_rbtree = RB_ROOT;
+	kbase_region_tracker_insert(kctx, same_va_reg);
+
+	/* exec and custom_va_reg doesn't always exist */
+	if (exec_reg && custom_va_reg) {
+		kbase_region_tracker_insert(kctx, exec_reg);
+		kbase_region_tracker_insert(kctx, custom_va_reg);
+	}
+}
+
+void kbase_region_tracker_term(kbase_context *kctx)
+{
+	struct rb_node *rbnode;
+	struct kbase_va_region *reg;
+	do {
+		rbnode = rb_first(&(kctx->reg_rbtree));
+		if (rbnode) {
+			rb_erase(rbnode, &(kctx->reg_rbtree));
+			reg = rb_entry(rbnode, struct kbase_va_region, rblink);
+			kbase_free_alloced_region(reg);
+		}
+	} while (rbnode);
+}
+
+/**
+ * Initialize the region tracker data structure.
+ */
+mali_error kbase_region_tracker_init(kbase_context *kctx)
+{
+	struct kbase_va_region *same_va_reg;
+	struct kbase_va_region *exec_reg = NULL;
+	struct kbase_va_region *custom_va_reg = NULL;
+	size_t same_va_bits = sizeof(void *) * BITS_PER_BYTE;
+	u64 custom_va_size = KBASE_REG_ZONE_CUSTOM_VA_SIZE;
+	u64 gpu_va_limit = (1ULL << kctx->kbdev->gpu_props.mmu.va_bits) >> PAGE_SHIFT;
+
+#if defined(CONFIG_ARM64)
+	same_va_bits = VA_BITS;
+#elif defined(CONFIG_X86_64)
+	same_va_bits = 47;
+#elif defined(CONFIG_64BIT)
+#error Unsupported 64-bit architecture
+#endif
+
+#ifdef CONFIG_64BIT
+	if (is_compat_task())
+		same_va_bits = 32;
+#endif
+
+	if (kctx->kbdev->gpu_props.mmu.va_bits < same_va_bits)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	/* all have SAME_VA */
+	same_va_reg = kbase_alloc_free_region(kctx, 1, (1ULL << (same_va_bits - PAGE_SHIFT)) - 2, KBASE_REG_ZONE_SAME_VA);
+	if (!same_va_reg)
+		return MALI_ERROR_OUT_OF_MEMORY;
+
+#ifdef CONFIG_64BIT
+	/* only 32-bit clients have the other two zones */
+	if (is_compat_task()) {
+#endif
+		if (gpu_va_limit <= KBASE_REG_ZONE_CUSTOM_VA_BASE) {
+			kbase_free_alloced_region(same_va_reg);
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+		/* If the current size of TMEM is out of range of the 
+		 * virtual address space addressable by the MMU then
+		 * we should shrink it to fit
+		 */
+		if( (KBASE_REG_ZONE_CUSTOM_VA_BASE + KBASE_REG_ZONE_CUSTOM_VA_SIZE) >= gpu_va_limit )
+			custom_va_size = gpu_va_limit - KBASE_REG_ZONE_CUSTOM_VA_BASE;
+
+		exec_reg = kbase_alloc_free_region(kctx, KBASE_REG_ZONE_EXEC_BASE, KBASE_REG_ZONE_EXEC_SIZE, KBASE_REG_ZONE_EXEC);
+		if (!exec_reg) {
+			kbase_free_alloced_region(same_va_reg);
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+
+		custom_va_reg = kbase_alloc_free_region(kctx, KBASE_REG_ZONE_CUSTOM_VA_BASE, custom_va_size, KBASE_REG_ZONE_CUSTOM_VA);
+		if (!custom_va_reg) {
+			kbase_free_alloced_region(same_va_reg);
+			kbase_free_alloced_region(exec_reg);
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+#ifdef CONFIG_64BIT
+	}
+#endif
+
+	kbase_region_tracker_ds_init(kctx, same_va_reg, exec_reg, custom_va_reg);
+
+	return MALI_ERROR_NONE;
+}
+
+mali_error kbase_mem_init(struct kbase_device *kbdev)
+{
+	kbasep_mem_device *memdev;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	memdev = &kbdev->memdev;
+
+	/* Initialize memory usage */
+	atomic_set(&memdev->used_pages, 0);
+
+	/* nothing to do, zero-inited when kbase_device was created */
+	return MALI_ERROR_NONE;
+}
+
+void kbase_mem_halt(kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+void kbase_mem_term(kbase_device *kbdev)
+{
+	kbasep_mem_device *memdev;
+	int pages;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	memdev = &kbdev->memdev;
+
+	pages = atomic_read(&memdev->used_pages);
+	if (pages != 0)
+		dev_warn(kbdev->dev, "%s: %d pages in use!\n", __func__, pages);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mem_term)
+
+/**
+ * @brief Wait for GPU write flush - only in use for BASE_HW_ISSUE_6367
+ *
+ * Wait 1000 GPU clock cycles. This delay is known to give the GPU time to flush its write buffer.
+ * @note If GPU resets occur then the counters are reset to zero, the delay may not be as expected.
+ */
+#ifndef CONFIG_MALI_NO_MALI
+void kbase_wait_write_flush(kbase_context *kctx)
+{
+	u32 base_count = 0;
+	/* A suspend won't happen here, because we're in a syscall from a userspace thread */
+	kbase_pm_context_active(kctx->kbdev);
+	kbase_pm_request_gpu_cycle_counter(kctx->kbdev);
+	while (MALI_TRUE) {
+		u32 new_count;
+		new_count = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(CYCLE_COUNT_LO), NULL);
+		/* First time around, just store the count. */
+		if (base_count == 0) {
+			base_count = new_count;
+			continue;
+		}
+
+		/* No need to handle wrapping, unsigned maths works for this. */
+		if ((new_count - base_count) > 1000)
+			break;
+	}
+	kbase_pm_release_gpu_cycle_counter(kctx->kbdev);
+	kbase_pm_context_idle(kctx->kbdev);
+}
+#endif				/* CONFIG_MALI_NO_MALI */
+
+
+
+/**
+ * @brief Allocate a free region object.
+ *
+ * The allocated object is not part of any list yet, and is flagged as
+ * KBASE_REG_FREE. No mapping is allocated yet.
+ *
+ * zone is KBASE_REG_ZONE_CUSTOM_VA, KBASE_REG_ZONE_SAME_VA, or KBASE_REG_ZONE_EXEC
+ *
+ */
+struct kbase_va_region *kbase_alloc_free_region(kbase_context *kctx, u64 start_pfn, size_t nr_pages, int zone)
+{
+	struct kbase_va_region *new_reg;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	/* zone argument should only contain zone related region flags */
+	KBASE_DEBUG_ASSERT((zone & ~KBASE_REG_ZONE_MASK) == 0);
+	KBASE_DEBUG_ASSERT(nr_pages > 0);
+	KBASE_DEBUG_ASSERT(start_pfn + nr_pages <= (UINT64_MAX / PAGE_SIZE));	/* 64-bit address range is the max */
+
+	new_reg = kzalloc(sizeof(*new_reg), GFP_KERNEL);
+
+	if (!new_reg) {
+		dev_warn(kctx->kbdev->dev, "kzalloc failed");
+		return NULL;
+	}
+
+	new_reg->alloc = NULL; /* no alloc bound yet */
+	new_reg->kctx = kctx;
+	new_reg->flags = zone | KBASE_REG_FREE;
+
+	new_reg->flags |= KBASE_REG_GROWABLE;
+
+	/* Set up default MEMATTR usage */
+	new_reg->flags |= KBASE_REG_MEMATTR_INDEX(ASn_MEMATTR_INDEX_DEFAULT);
+
+	new_reg->start_pfn = start_pfn;
+	new_reg->nr_pages = nr_pages;
+
+	return new_reg;
+}
+
+KBASE_EXPORT_TEST_API(kbase_alloc_free_region)
+
+/**
+ * @brief Free a region object.
+ *
+ * The described region must be freed of any mapping.
+ *
+ * If the region is not flagged as KBASE_REG_FREE, the region's
+ * alloc object will be released. 
+ * It is a bug if no alloc object exists for non-free regions.
+ *
+ */
+void kbase_free_alloced_region(struct kbase_va_region *reg)
+{
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	if (!(reg->flags & KBASE_REG_FREE)) {
+		kbase_mem_phy_alloc_put(reg->alloc);
+		KBASE_DEBUG_CODE(
+					/* To detect use-after-free in debug builds */
+					reg->flags |= KBASE_REG_FREE);
+	}
+	kfree(reg);
+}
+
+KBASE_EXPORT_TEST_API(kbase_free_alloced_region)
+
+void kbase_mmu_update(kbase_context *kctx)
+{
+	/* Use GPU implementation-defined caching policy. */
+	u64 mem_attrs;
+	u32 pgd_high;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	mem_attrs = kctx->mem_attrs;
+	/* ASSERT that the context has a valid as_nr, which is only the case
+	 * when it's scheduled in.
+	 *
+	 * as_nr won't change because the caller has the runpool_irq lock */
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+
+	pgd_high = sizeof(kctx->pgd) > 4 ? (kctx->pgd >> 32) : 0;
+
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_TRANSTAB_LO),
+			(kctx->pgd & ASn_TRANSTAB_ADDR_SPACE_MASK) |
+			ASn_TRANSTAB_READ_INNER | ASn_TRANSTAB_ADRMODE_TABLE,
+			kctx);
+
+	/* Need to use a conditional expression to avoid
+	 * "right shift count >= width of type" error when using an if statement
+	 * - although the size_of condition is evaluated at compile time the
+	 * unused branch is not removed until after it is type-checked and the
+	 * error produced.
+	 */
+	pgd_high = sizeof(kctx->pgd) > 4 ? (kctx->pgd >> 32) : 0;
+
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_TRANSTAB_HI),
+			pgd_high, kctx);
+
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_MEMATTR_LO),
+			mem_attrs        & 0xFFFFFFFFUL, kctx);
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_MEMATTR_HI),
+			(mem_attrs >> 32) & 0xFFFFFFFFUL, kctx);
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND),
+			ASn_COMMAND_UPDATE, kctx);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_update)
+
+void kbase_mmu_disable(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	/* ASSERT that the context has a valid as_nr, which is only the case
+	 * when it's scheduled in.
+	 *
+	 * as_nr won't change because the caller has the runpool_irq lock */
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_TRANSTAB_LO), 0, kctx);
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_TRANSTAB_HI), 0, kctx);
+	kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_UPDATE, kctx);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_disable)
+
+mali_error kbase_gpu_mmap(kbase_context *kctx, struct kbase_va_region *reg, mali_addr64 addr, size_t nr_pages, size_t align)
+{
+	mali_error err;
+	size_t i = 0;
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != reg);
+
+	err = kbase_add_va_region(kctx, reg, addr, nr_pages, align);
+	if (MALI_ERROR_NONE != err)
+		return err;
+
+	if (reg->alloc->type == KBASE_MEM_TYPE_ALIAS) {
+		u64 stride;
+		stride = reg->alloc->imported.alias.stride;
+		KBASE_DEBUG_ASSERT(reg->alloc->imported.alias.aliased);
+		for (i = 0; i < reg->alloc->imported.alias.nents; i++) {
+			if (reg->alloc->imported.alias.aliased[i].alloc) {
+				err = kbase_mmu_insert_pages(kctx,
+						reg->start_pfn + (i * stride),
+						reg->alloc->imported.alias.aliased[i].alloc->pages + reg->alloc->imported.alias.aliased[i].offset,
+						reg->alloc->imported.alias.aliased[i].length,
+						reg->flags);
+				if (MALI_ERROR_NONE != err)
+					goto bad_insert;
+
+				kbase_mem_phy_alloc_gpu_mapped(reg->alloc->imported.alias.aliased[i].alloc);
+			} else {
+				err = kbase_mmu_insert_single_page(kctx,
+						reg->start_pfn + i * stride,
+						kctx->aliasing_sink_page,
+						reg->alloc->imported.alias.aliased[i].length,
+						(reg->flags & ~KBASE_REG_MEMATTR_MASK) | KBASE_REG_MEMATTR_INDEX(ASn_MEMATTR_INDEX_WRITE_ALLOC)
+						);
+				if (MALI_ERROR_NONE != err)
+					goto bad_insert;
+			}
+		}
+	} else {
+		err = kbase_mmu_insert_pages(kctx, reg->start_pfn,
+				kbase_get_phy_pages(reg),
+				kbase_reg_current_backed_size(reg),
+				reg->flags);
+		if (MALI_ERROR_NONE != err)
+			goto bad_insert;
+		kbase_mem_phy_alloc_gpu_mapped(reg->alloc);
+	}
+
+	return err;
+
+bad_insert:
+	if (reg->alloc->type == KBASE_MEM_TYPE_ALIAS) {
+		u64 stride;
+		stride = reg->alloc->imported.alias.stride;
+		KBASE_DEBUG_ASSERT(reg->alloc->imported.alias.aliased);
+		while (i--)
+			if (reg->alloc->imported.alias.aliased[i].alloc) {
+				kbase_mmu_teardown_pages(kctx, reg->start_pfn + (i * stride), reg->alloc->imported.alias.aliased[i].length);
+				kbase_mem_phy_alloc_gpu_unmapped(reg->alloc->imported.alias.aliased[i].alloc);
+			}
+	}
+
+	kbase_remove_va_region(kctx, reg);
+
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_gpu_mmap)
+
+mali_error kbase_gpu_munmap(kbase_context *kctx, struct kbase_va_region *reg)
+{
+	mali_error err;
+
+	if (reg->start_pfn == 0)
+		return MALI_ERROR_NONE;
+
+	if (reg->alloc && reg->alloc->type == KBASE_MEM_TYPE_ALIAS) {
+		size_t i;
+		err = kbase_mmu_teardown_pages(kctx, reg->start_pfn, reg->nr_pages);
+		KBASE_DEBUG_ASSERT(reg->alloc->imported.alias.aliased);
+		for (i = 0; i < reg->alloc->imported.alias.nents; i++)
+			if (reg->alloc->imported.alias.aliased[i].alloc)
+				kbase_mem_phy_alloc_gpu_unmapped(reg->alloc->imported.alias.aliased[i].alloc);
+	} else {
+		err = kbase_mmu_teardown_pages(kctx, reg->start_pfn, kbase_reg_current_backed_size(reg));
+		kbase_mem_phy_alloc_gpu_unmapped(reg->alloc);
+	}
+
+	if (MALI_ERROR_NONE != err)
+		return err;
+
+	err = kbase_remove_va_region(kctx, reg);
+	return err;
+}
+
+STATIC struct kbase_cpu_mapping *kbasep_find_enclosing_cpu_mapping_of_region(const struct kbase_va_region *reg, unsigned long uaddr, size_t size)
+{
+	struct kbase_cpu_mapping *map;
+	struct list_head *pos;
+
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	KBASE_DEBUG_ASSERT(reg->alloc);
+
+	if ((uintptr_t) uaddr + size < (uintptr_t) uaddr) /* overflow check */
+		return NULL;
+
+	list_for_each(pos, &reg->alloc->mappings) {
+		map = list_entry(pos, kbase_cpu_mapping, mappings_list);
+		if (map->vm_start <= uaddr && map->vm_end >= uaddr + size)
+			return map;
+	}
+
+	return NULL;
+}
+
+KBASE_EXPORT_TEST_API(kbasep_find_enclosing_cpu_mapping_of_region)
+
+mali_error kbasep_find_enclosing_cpu_mapping_offset(kbase_context *kctx,
+							  mali_addr64 gpu_addr,
+							   unsigned long uaddr,
+								   size_t size,
+							   mali_size64 *offset)
+{
+	struct kbase_cpu_mapping *map = NULL;
+	const struct kbase_va_region *reg;
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	kbase_gpu_vm_lock(kctx);
+
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx,
+								     gpu_addr);
+	if (reg) {
+		map = kbasep_find_enclosing_cpu_mapping_of_region(reg, uaddr,
+									 size);
+		if (map) {
+			*offset = (uaddr - PTR_TO_U64(map->vm_start)) +
+						 (map->page_off << PAGE_SHIFT);
+			err = MALI_ERROR_NONE;
+		}
+	}
+
+	kbase_gpu_vm_unlock(kctx);
+
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbasep_find_enclosing_cpu_mapping_offset)
+
+static mali_error kbase_do_syncset(kbase_context *kctx, struct base_syncset *set, kbase_sync_kmem_fn sync_fn)
+{
+	mali_error err = MALI_ERROR_NONE;
+	struct basep_syncset *sset = &set->basep_sset;
+	struct kbase_va_region *reg;
+	struct kbase_cpu_mapping *map;
+	unsigned long start;
+	size_t size;
+	phys_addr_t base_phy_addr = 0;
+	phys_addr_t *pa;
+	u64 page_off, page_count;
+	u64 i;
+	unsigned int offset_within_page;
+	void *base_virt_addr = 0;
+	size_t area_size = 0;
+
+	kbase_os_mem_map_lock(kctx);
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* find the region where the virtual address is contained */
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx, sset->mem_handle);
+	if (!reg) {
+		dev_warn(kctx->kbdev->dev, "Can't find region at VA 0x%016llX", sset->mem_handle);
+		err = MALI_ERROR_FUNCTION_FAILED;
+		goto out_unlock;
+	}
+
+	if (!(reg->flags & KBASE_REG_CPU_CACHED))
+		goto out_unlock;
+
+	start = (uintptr_t)sset->user_addr;
+	size = (size_t)sset->size;
+
+	map = kbasep_find_enclosing_cpu_mapping_of_region(reg, start, size);
+	if (!map) {
+		dev_warn(kctx->kbdev->dev, "Can't find CPU mapping 0x%016lX for VA 0x%016llX", start, sset->mem_handle);
+		err = MALI_ERROR_FUNCTION_FAILED;
+		goto out_unlock;
+	}
+
+	offset_within_page = start & (PAGE_SIZE - 1);
+	page_off = map->page_off + ((start - map->vm_start) >> PAGE_SHIFT);
+	page_count = ((size + offset_within_page + (PAGE_SIZE - 1)) & PAGE_MASK) >> PAGE_SHIFT;
+	pa = kbase_get_phy_pages(reg);
+
+	pagefault_disable();
+
+	for (i = 0; i < page_count; i++) {
+		u32 offset = start & (PAGE_SIZE - 1);
+		phys_addr_t paddr = pa[page_off + i] + offset;
+		size_t sz = MIN(((size_t) PAGE_SIZE - offset), size);
+		u8 tmp;
+
+		if (copy_from_user(&tmp, (void*)(uintptr_t)start, 1)) {
+			/* Not accessible */
+			err = MALI_ERROR_FUNCTION_FAILED;
+			goto out_enable_pagefaults;
+		}
+
+		if (paddr == base_phy_addr + area_size && start == ((uintptr_t) base_virt_addr + area_size)) {
+			area_size += sz;
+		} else if (area_size > 0) {
+			sync_fn(base_phy_addr, base_virt_addr, area_size);
+			area_size = 0;
+		}
+
+		if (area_size == 0) {
+			base_phy_addr = paddr;
+			base_virt_addr = (void *)(uintptr_t)start;
+			area_size = sz;
+		}
+
+		start += sz;
+		size -= sz;
+	}
+
+	if (area_size > 0)
+		sync_fn(base_phy_addr, base_virt_addr, area_size);
+
+	KBASE_DEBUG_ASSERT(size == 0);
+
+out_enable_pagefaults:
+	pagefault_enable();
+out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+	kbase_os_mem_map_unlock(kctx);
+	return err;
+}
+
+mali_error kbase_sync_now(kbase_context *kctx, struct base_syncset *syncset)
+{
+	mali_error err = MALI_ERROR_FUNCTION_FAILED;
+	struct basep_syncset *sset;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != syncset);
+
+	sset = &syncset->basep_sset;
+
+	switch (sset->type) {
+	case BASE_SYNCSET_OP_MSYNC:
+		err = kbase_do_syncset(kctx, syncset, kbase_sync_to_memory);
+		break;
+
+	case BASE_SYNCSET_OP_CSYNC:
+		err = kbase_do_syncset(kctx, syncset, kbase_sync_to_cpu);
+		break;
+
+	default:
+		dev_warn(kctx->kbdev->dev, "Unknown msync op %d\n", sset->type);
+		break;
+	}
+
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_sync_now)
+
+/* vm lock must be held */
+mali_error kbase_mem_free_region(kbase_context *kctx, kbase_va_region *reg)
+{
+	mali_error err;
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	BUG_ON(!mutex_is_locked(&kctx->reg_lock));
+	err = kbase_gpu_munmap(kctx, reg);
+	if (err) {
+		dev_warn(reg->kctx->kbdev->dev, "Could not unmap from the GPU...\n");
+		goto out;
+	}
+
+	if (kbase_hw_has_issue(kctx->kbdev, BASE_HW_ISSUE_6367)) {
+		/* Wait for GPU to flush write buffer before freeing physical pages */
+		kbase_wait_write_flush(kctx);
+	}
+
+	/* This will also free the physical pages */
+	kbase_free_alloced_region(reg);
+
+ out:
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mem_free_region)
+
+/**
+ * @brief Free the region from the GPU and unregister it.
+ *
+ * This function implements the free operation on a memory segment.
+ * It will loudly fail if called with outstanding mappings.
+ */
+mali_error kbase_mem_free(kbase_context *kctx, mali_addr64 gpu_addr)
+{
+	mali_error err = MALI_ERROR_NONE;
+	struct kbase_va_region *reg;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+
+	if (0 == gpu_addr) {
+		dev_warn(kctx->kbdev->dev, "gpu_addr 0 is reserved for the ringbuffer and it's an error to try to free it using kbase_mem_free\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+	kbase_gpu_vm_lock(kctx);
+
+	if (gpu_addr >= BASE_MEM_COOKIE_BASE &&
+	    gpu_addr < BASE_MEM_FIRST_FREE_ADDRESS) {
+		int cookie = PFN_DOWN(gpu_addr - BASE_MEM_COOKIE_BASE);
+		reg = kctx->pending_regions[cookie];
+		if (!reg) {
+			err = MALI_ERROR_FUNCTION_FAILED;
+			goto out_unlock;
+		}
+
+		/* ask to unlink the cookie as we'll free it */
+
+		kctx->pending_regions[cookie] = NULL;
+		kctx->cookies |= (1UL << cookie);
+
+		kbase_free_alloced_region(reg);
+	} else {
+		/* A real GPU va */
+
+		/* Validate the region */
+		reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
+		if (!reg) {
+			dev_warn(kctx->kbdev->dev,
+			    "kbase_mem_free called with nonexistent gpu_addr 0x%llX",
+			    gpu_addr);
+			err = MALI_ERROR_FUNCTION_FAILED;
+			goto out_unlock;
+		}
+
+		if ((reg->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_SAME_VA) {
+			/* SAME_VA must be freed through munmap */
+			dev_warn(kctx->kbdev->dev,
+			    "%s called on SAME_VA memory 0x%llX", __func__, gpu_addr);
+			err = MALI_ERROR_FUNCTION_FAILED;
+			goto out_unlock;
+		}
+
+		err = kbase_mem_free_region(kctx, reg);
+	}
+
+ out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mem_free)
+
+void kbase_update_region_flags(struct kbase_va_region *reg, unsigned long flags)
+{
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	KBASE_DEBUG_ASSERT((flags & ~((1 << BASE_MEM_FLAGS_NR_BITS) - 1)) == 0);
+
+	reg->flags |= kbase_cache_enabled(flags, reg->nr_pages);
+	/* all memory is now growable */
+	reg->flags |= KBASE_REG_GROWABLE;
+
+	if (flags & BASE_MEM_GROW_ON_GPF)
+		reg->flags |= KBASE_REG_PF_GROW;
+
+	if (flags & BASE_MEM_PROT_CPU_WR)
+		reg->flags |= KBASE_REG_CPU_WR;
+
+	if (flags & BASE_MEM_PROT_CPU_RD)
+		reg->flags |= KBASE_REG_CPU_RD;
+
+	if (flags & BASE_MEM_PROT_GPU_WR)
+		reg->flags |= KBASE_REG_GPU_WR;
+
+	if (flags & BASE_MEM_PROT_GPU_RD)
+		reg->flags |= KBASE_REG_GPU_RD;
+
+	if (0 == (flags & BASE_MEM_PROT_GPU_EX))
+		reg->flags |= KBASE_REG_GPU_NX;
+
+	if (flags & BASE_MEM_COHERENT_LOCAL)
+		reg->flags |= KBASE_REG_SHARE_IN;
+	else if (flags & BASE_MEM_COHERENT_SYSTEM)
+		reg->flags |= KBASE_REG_SHARE_BOTH;
+
+}
+KBASE_EXPORT_TEST_API(kbase_update_region_flags)
+
+int kbase_alloc_phy_pages_helper(
+	struct kbase_mem_phy_alloc *alloc,
+	size_t nr_pages_requested)
+{
+	KBASE_DEBUG_ASSERT(alloc);
+	KBASE_DEBUG_ASSERT(alloc->type == KBASE_MEM_TYPE_NATIVE);
+	KBASE_DEBUG_ASSERT(alloc->imported.kctx);
+
+	if (nr_pages_requested == 0)
+		goto done; /*nothing to do*/
+
+	kbase_atomic_add_pages(nr_pages_requested, &alloc->imported.kctx->used_pages);
+	kbase_atomic_add_pages(nr_pages_requested, &alloc->imported.kctx->kbdev->memdev.used_pages);
+
+	if (MALI_ERROR_NONE != kbase_mem_allocator_alloc(&alloc->imported.kctx->osalloc, nr_pages_requested, alloc->pages + alloc->nents))
+		goto no_alloc;
+
+	alloc->nents += nr_pages_requested;
+
+	kbase_process_page_usage_inc(alloc->imported.kctx, nr_pages_requested);
+done:
+	return 0;
+
+no_alloc:
+	kbase_atomic_sub_pages(nr_pages_requested, &alloc->imported.kctx->used_pages);
+	kbase_atomic_sub_pages(nr_pages_requested, &alloc->imported.kctx->kbdev->memdev.used_pages);
+
+	return -ENOMEM;
+}
+
+int kbase_free_phy_pages_helper(
+	struct kbase_mem_phy_alloc *alloc,
+	size_t nr_pages_to_free)
+{
+	mali_bool syncback;
+	phys_addr_t *start_free;
+	KBASE_DEBUG_ASSERT(alloc);
+	KBASE_DEBUG_ASSERT(alloc->type == KBASE_MEM_TYPE_NATIVE);
+	KBASE_DEBUG_ASSERT(alloc->imported.kctx);
+	KBASE_DEBUG_ASSERT(alloc->nents >= nr_pages_to_free);
+
+	/* early out if nothing to do */
+	if (0 == nr_pages_to_free)
+		return 0;
+
+	start_free = alloc->pages + alloc->nents - nr_pages_to_free;
+
+	syncback = alloc->accessed_cached ? MALI_TRUE : MALI_FALSE;
+
+	kbase_mem_allocator_free(&alloc->imported.kctx->osalloc,
+				  nr_pages_to_free,
+				  start_free,
+				  syncback);
+
+	alloc->nents -= nr_pages_to_free;
+	kbase_process_page_usage_dec(alloc->imported.kctx, nr_pages_to_free);
+	kbase_atomic_sub_pages(nr_pages_to_free, &alloc->imported.kctx->used_pages);
+	kbase_atomic_sub_pages(nr_pages_to_free, &alloc->imported.kctx->kbdev->memdev.used_pages);
+
+	return 0;
+}
+
+void kbase_mem_kref_free(struct kref *kref)
+{
+	struct kbase_mem_phy_alloc *alloc;
+	alloc = container_of(kref, struct kbase_mem_phy_alloc, kref);
+
+	switch (alloc->type) {
+	case KBASE_MEM_TYPE_NATIVE: {
+		KBASE_DEBUG_ASSERT(alloc->imported.kctx);
+		kbase_free_phy_pages_helper(alloc, alloc->nents);
+		break;
+	}
+	case KBASE_MEM_TYPE_ALIAS: {
+		/* just call put on the underlying phy allocs */
+		size_t i;
+		struct kbase_aliased *aliased;
+		aliased = alloc->imported.alias.aliased;
+		if (aliased) {
+				for (i = 0; i < alloc->imported.alias.nents; i++)
+					if (aliased[i].alloc)
+						kbase_mem_phy_alloc_put(aliased[i].alloc);
+				vfree(aliased);
+		}				
+		break;
+	}
+	case KBASE_MEM_TYPE_RAW:
+		/* raw pages, external cleanup */
+		break;
+ #ifdef CONFIG_UMP
+	case KBASE_MEM_TYPE_IMPORTED_UMP:
+		ump_dd_release(alloc->imported.ump_handle);
+		break;
+#endif
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	case KBASE_MEM_TYPE_IMPORTED_UMM:
+		dma_buf_detach(alloc->imported.umm.dma_buf,
+			       alloc->imported.umm.dma_attachment);
+		dma_buf_put(alloc->imported.umm.dma_buf);
+		break;
+#endif
+	case KBASE_MEM_TYPE_TB:{
+		void *tb;
+		tb = alloc->imported.kctx->jctx.tb;
+		kbase_device_trace_buffer_uninstall(alloc->imported.kctx);
+		vfree(tb);
+		break;
+	}
+	default:
+		WARN(1, "Unexecpted free of type %d\n", alloc->type);
+		break;
+	}
+	vfree(alloc);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mem_kref_free);
+
+int kbase_alloc_phy_pages(struct kbase_va_region *reg, size_t vsize, size_t size)
+{
+	KBASE_DEBUG_ASSERT(NULL != reg);
+	KBASE_DEBUG_ASSERT(vsize > 0);
+
+	/* validate user provided arguments */
+	if (size > vsize || vsize > reg->nr_pages)
+		goto out_term;
+
+	/* Prevent vsize*sizeof from wrapping around.
+	 * For instance, if vsize is 2**29+1, we'll allocate 1 byte and the alloc won't fail.
+	 */
+	if ((size_t) vsize > ((size_t) -1 / sizeof(*reg->alloc->pages)))
+		goto out_term;
+
+	KBASE_DEBUG_ASSERT(0 != vsize);
+
+	if (MALI_ERROR_NONE != kbase_alloc_phy_pages_helper(reg->alloc, size))
+		goto out_term;
+
+	return 0;
+
+ out_term:
+	return -1;
+}
+
+KBASE_EXPORT_TEST_API(kbase_alloc_phy_pages)
+
+mali_bool kbase_check_alloc_flags(unsigned long flags)
+{
+	/* Only known flags should be set. */
+	if (flags & ~((1 << BASE_MEM_FLAGS_NR_BITS) - 1))
+		return MALI_FALSE;
+
+	/* At least one flag should be set */
+	if (flags == 0)
+		return MALI_FALSE;
+
+	/* Either the GPU or CPU must be reading from the allocated memory */
+	if ((flags & (BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_GPU_RD)) == 0)
+		return MALI_FALSE;
+
+	/* Either the GPU or CPU must be writing to the allocated memory */
+	if ((flags & (BASE_MEM_PROT_CPU_WR | BASE_MEM_PROT_GPU_WR)) == 0)
+		return MALI_FALSE;
+
+	/* GPU cannot be writing to GPU executable memory and cannot grow the memory on page fault. */
+	if ((flags & BASE_MEM_PROT_GPU_EX) && (flags & (BASE_MEM_PROT_GPU_WR | BASE_MEM_GROW_ON_GPF)))
+		return MALI_FALSE;
+
+	/* GPU should have at least read or write access otherwise there is no
+	   reason for allocating. */
+	if ((flags & (BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR)) == 0)
+		return MALI_FALSE;
+
+	return MALI_TRUE;
+}
+
+/**
+ * @brief Acquire the per-context region list lock
+ */
+void kbase_gpu_vm_lock(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	mutex_lock(&kctx->reg_lock);
+}
+
+KBASE_EXPORT_TEST_API(kbase_gpu_vm_lock)
+
+/**
+ * @brief Release the per-context region list lock
+ */
+void kbase_gpu_vm_unlock(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	mutex_unlock(&kctx->reg_lock);
+}
+
+KBASE_EXPORT_TEST_API(kbase_gpu_vm_unlock)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.h
new file mode 100644
index 0000000..7baa6f8
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem.h
@@ -0,0 +1,615 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mem.h
+ * Base kernel memory APIs
+ */
+
+#ifndef _KBASE_MEM_H_
+#define _KBASE_MEM_H_
+
+#ifndef _KBASE_H_
+#error "Don't include this file directly, use mali_kbase.h instead"
+#endif
+
+#include <malisw/mali_malisw.h>
+#include <linux/kref.h>
+
+#ifdef CONFIG_UMP
+#include <linux/ump.h>
+#endif				/* CONFIG_UMP */
+#include "mali_base_kernel.h"
+#include <mali_kbase_hw.h>
+#include "mali_kbase_pm.h"
+#include "mali_kbase_defs.h"
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+#include "mali_kbase_gator.h"
+#endif  /*CONFIG_MALI_GATOR_SUPPORT*/
+
+/* Part of the workaround for uTLB invalid pages is to ensure we grow/shrink tmem by 4 pages at a time */
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2_HW_ISSUE_8316 (2)	/* round to 4 pages */
+
+/* Part of the workaround for PRLAM-9630 requires us to grow/shrink memory by 8 pages.
+The MMU reads in 8 page table entries from memory at a time, if we have more than one page fault within the same 8 pages and
+page tables are updated accordingly, the MMU does not re-read the page table entries from memory for the subsequent page table
+updates and generates duplicate page faults as the page table information used by the MMU is not valid.   */
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2_HW_ISSUE_9630 (3)	/* round to 8 pages */
+
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2 (0)	/* round to 1 page */
+
+/* This must always be a power of 2 */
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES (1u << KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2)
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_HW_ISSUE_8316 (1u << KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2_HW_ISSUE_8316)
+#define KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_HW_ISSUE_9630 (1u << KBASEP_TMEM_GROWABLE_BLOCKSIZE_PAGES_LOG2_HW_ISSUE_9630)
+/**
+ * A CPU mapping
+ */
+typedef struct kbase_cpu_mapping {
+	struct  list_head mappings_list;
+	struct  kbase_mem_phy_alloc *alloc;
+	struct  kbase_context *kctx;
+	struct  kbase_va_region *region;
+	pgoff_t page_off;
+	int     count;
+
+	unsigned long vm_start;
+	unsigned long vm_end;
+} kbase_cpu_mapping;
+
+enum kbase_memory_type {
+	KBASE_MEM_TYPE_NATIVE,
+	KBASE_MEM_TYPE_IMPORTED_UMP,
+	KBASE_MEM_TYPE_IMPORTED_UMM,
+	KBASE_MEM_TYPE_ALIAS,
+	KBASE_MEM_TYPE_TB,
+	KBASE_MEM_TYPE_RAW
+};
+
+/* internal structure, mirroring base_mem_aliasing_info,
+ * but with alloc instead of a gpu va (handle) */
+struct kbase_aliased {
+	struct kbase_mem_phy_alloc *alloc; /* NULL for special, non-NULL for native */
+	u64 offset; /* in pages */
+	u64 length; /* in pages */
+};
+
+/* physical pages tracking object.
+ * Set up to track N pages.
+ * N not stored here, the creator holds that info.
+ * This object only tracks how many elements are actually valid (present).
+ * Changing of nents or *pages should only happen if the kbase_mem_phy_alloc is not
+ * shared with another region or client. CPU mappings are OK to exist when changing, as
+ * long as the tracked mappings objects are updated as part of the change.
+ */
+struct kbase_mem_phy_alloc
+{
+	struct kref           kref; /* number of users of this alloc */
+	atomic_t              gpu_mappings;
+	size_t                nents; /* 0..N */
+	phys_addr_t *         pages; /* N elements, only 0..nents are valid */
+
+	/* kbase_cpu_mappings */
+	struct list_head      mappings;
+
+	/* type of buffer */
+	enum kbase_memory_type type;
+
+	int accessed_cached;
+
+	/* member in union valid based on @a type */
+	union {
+#ifdef CONFIG_UMP
+		ump_dd_handle ump_handle;
+#endif /* CONFIG_UMP */
+#if defined(CONFIG_DMA_SHARED_BUFFER)
+		struct {
+			struct dma_buf *dma_buf;
+			struct dma_buf_attachment *dma_attachment;
+			unsigned int current_mapping_usage_count;
+			struct sg_table *sgt;
+		} umm;
+#endif /* defined(CONFIG_DMA_SHARED_BUFFER) */
+		struct {
+			mali_size64 stride;
+			size_t nents;
+			struct kbase_aliased *aliased;
+		} alias;
+		/* Used by type = (KBASE_MEM_TYPE_NATIVE, KBASE_MEM_TYPE_TB) */
+		struct kbase_context *kctx;
+	} imported;
+};
+
+static inline void kbase_mem_phy_alloc_gpu_mapped(struct kbase_mem_phy_alloc *alloc)
+{
+	KBASE_DEBUG_ASSERT(alloc);
+	/* we only track mappings of NATIVE buffers */
+	if (alloc->type == KBASE_MEM_TYPE_NATIVE)
+		atomic_inc(&alloc->gpu_mappings);
+}
+
+static inline void kbase_mem_phy_alloc_gpu_unmapped(struct kbase_mem_phy_alloc *alloc)
+{
+	KBASE_DEBUG_ASSERT(alloc);
+	/* we only track mappings of NATIVE buffers */
+	if (alloc->type == KBASE_MEM_TYPE_NATIVE)
+		if (0 > atomic_dec_return(&alloc->gpu_mappings)) {
+			pr_err("Mismatched %s:\n", __func__);
+			dump_stack();
+		}
+}
+
+void kbase_mem_kref_free(struct kref * kref);
+
+mali_error kbase_mem_init(kbase_device * kbdev);
+void kbase_mem_halt(kbase_device * kbdev);
+void kbase_mem_term(kbase_device * kbdev);
+
+static inline struct kbase_mem_phy_alloc * kbase_mem_phy_alloc_get(struct kbase_mem_phy_alloc * alloc)
+{
+	kref_get(&alloc->kref);
+	return alloc;
+}
+
+static inline struct kbase_mem_phy_alloc * kbase_mem_phy_alloc_put(struct kbase_mem_phy_alloc * alloc)
+{
+	kref_put(&alloc->kref, kbase_mem_kref_free);
+	return NULL;
+}
+
+/**
+ * A GPU memory region, and attributes for CPU mappings.
+ */
+typedef struct kbase_va_region {
+	struct rb_node rblink;
+	struct list_head link;
+
+	kbase_context *kctx;	/* Backlink to base context */
+
+	u64 start_pfn;		/* The PFN in GPU space */
+	size_t nr_pages;
+
+/* Free region */
+#define KBASE_REG_FREE              (1ul << 0)
+/* CPU write access */
+#define KBASE_REG_CPU_WR            (1ul << 1)
+/* GPU write access */
+#define KBASE_REG_GPU_WR            (1ul << 2)
+/* No eXecute flag */
+#define KBASE_REG_GPU_NX            (1ul << 3)
+/* Is CPU cached? */
+#define KBASE_REG_CPU_CACHED        (1ul << 4)
+/* Is GPU cached? */
+#define KBASE_REG_GPU_CACHED        (1ul << 5)
+
+#define KBASE_REG_GROWABLE          (1ul << 6)
+/* Can grow on pf? */
+#define KBASE_REG_PF_GROW           (1ul << 7)
+
+/* VA managed by us */
+#define KBASE_REG_CUSTOM_VA         (1ul << 8)
+
+/* inner shareable coherency */
+#define KBASE_REG_SHARE_IN          (1ul << 9)
+/* inner & outer shareable coherency */
+#define KBASE_REG_SHARE_BOTH        (1ul << 10)
+
+/* Space for 4 different zones */
+#define KBASE_REG_ZONE_MASK         (3ul << 11)
+#define KBASE_REG_ZONE(x)           (((x) & 3) << 11)
+
+/* GPU read access */
+#define KBASE_REG_GPU_RD            (1ul<<13)
+/* CPU read access */
+#define KBASE_REG_CPU_RD            (1ul<<14)
+
+/* Aligned for GPU EX in SAME_VA */
+#define KBASE_REG_ALIGNED           (1ul<<15)
+
+/* Index of chosen MEMATTR for this region (0..7) */
+#define KBASE_REG_MEMATTR_MASK      (7ul << 16)
+#define KBASE_REG_MEMATTR_INDEX(x)  (((x) & 7) << 16)
+#define KBASE_REG_MEMATTR_VALUE(x)  (((x) & KBASE_REG_MEMATTR_MASK) >> 16)
+
+#define KBASE_REG_ZONE_SAME_VA      KBASE_REG_ZONE(0)
+
+/* only used with 32-bit clients */
+/*
+ * On a 32bit platform, custom VA should be wired from (4GB + shader region)
+ * to the VA limit of the GPU. Unfortunately, the Linux mmap() interface 
+ * limits us to 2^32 pages (2^44 bytes, see mmap64 man page for reference).
+ * So we put the default limit to the maximum possible on Linux and shrink
+ * it down, if required by the GPU, during initialization.
+ */
+#define KBASE_REG_ZONE_EXEC         KBASE_REG_ZONE(1)	/* Dedicated 16MB region for shader code */
+#define KBASE_REG_ZONE_EXEC_BASE    ((1ULL << 32) >> PAGE_SHIFT)
+#define KBASE_REG_ZONE_EXEC_SIZE    ((16ULL * 1024 * 1024) >> PAGE_SHIFT)
+
+#define KBASE_REG_ZONE_CUSTOM_VA         KBASE_REG_ZONE(2)
+#define KBASE_REG_ZONE_CUSTOM_VA_BASE    (KBASE_REG_ZONE_EXEC_BASE + KBASE_REG_ZONE_EXEC_SIZE) /* Starting after KBASE_REG_ZONE_EXEC */
+#define KBASE_REG_ZONE_CUSTOM_VA_SIZE    (((1ULL << 44) >> PAGE_SHIFT) - KBASE_REG_ZONE_CUSTOM_VA_BASE)
+/* end 32-bit clients only */
+
+	unsigned long flags;
+
+	size_t extent; /* nr of pages alloc'd on PF */
+
+	struct kbase_mem_phy_alloc * alloc; /* the one alloc object we mmap to the GPU and CPU when mapping this region */
+
+	/* non-NULL if this memory object is a kds_resource */
+	struct kds_resource *kds_res;
+
+} kbase_va_region;
+
+/* Common functions */
+static INLINE phys_addr_t *kbase_get_phy_pages(struct kbase_va_region *reg)
+{
+	KBASE_DEBUG_ASSERT(reg);
+	KBASE_DEBUG_ASSERT(reg->alloc);
+
+	return reg->alloc->pages;
+}
+
+static INLINE size_t kbase_reg_current_backed_size(struct kbase_va_region * reg)
+{
+	KBASE_DEBUG_ASSERT(reg);
+	/* if no alloc object the backed size naturally is 0 */
+	if (reg->alloc)
+		return reg->alloc->nents;
+	else
+		return 0;
+}
+
+static INLINE struct kbase_mem_phy_alloc * kbase_alloc_create(size_t nr_pages, enum kbase_memory_type type)
+{
+	struct kbase_mem_phy_alloc *alloc;
+
+	/* Prevent nr_pages*sizeof + sizeof(*alloc) from wrapping around. */
+	if (nr_pages > ((((size_t) -1) - sizeof(*alloc)) / sizeof(*alloc->pages)))
+		return ERR_PTR(-ENOMEM);
+
+	alloc = vzalloc(sizeof(*alloc) + sizeof(*alloc->pages) * nr_pages);
+	if (!alloc)
+		return ERR_PTR(-ENOMEM);
+
+	kref_init(&alloc->kref);
+	atomic_set(&alloc->gpu_mappings, 0);
+	alloc->nents = 0;
+	alloc->pages = (void*)(alloc + 1);
+	INIT_LIST_HEAD(&alloc->mappings);
+	alloc->type = type;
+
+	return alloc;
+}
+
+static INLINE int kbase_reg_prepare_native(struct kbase_va_region * reg, struct kbase_context * kctx)
+{
+	KBASE_DEBUG_ASSERT(reg);
+	KBASE_DEBUG_ASSERT(!reg->alloc);
+	KBASE_DEBUG_ASSERT(reg->flags & KBASE_REG_FREE);
+
+	reg->alloc = kbase_alloc_create(reg->nr_pages, KBASE_MEM_TYPE_NATIVE);
+	if (IS_ERR(reg->alloc))
+		return PTR_ERR(reg->alloc);
+	else if (!reg->alloc)
+		return -ENOMEM;
+	reg->alloc->imported.kctx = kctx;
+	reg->flags &= ~KBASE_REG_FREE;
+	return 0;
+}
+
+static inline int kbase_atomic_add_pages(int num_pages, atomic_t *used_pages)
+{
+	int new_val = atomic_add_return(num_pages, used_pages);
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	kbase_trace_mali_total_alloc_pages_change((long long int)new_val);
+#endif
+	return new_val;
+}
+
+static inline int kbase_atomic_sub_pages(int num_pages, atomic_t *used_pages)
+{
+	int new_val = atomic_sub_return(num_pages, used_pages);
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	kbase_trace_mali_total_alloc_pages_change((long long int)new_val);
+#endif
+	return new_val;
+}
+
+/**
+ * @brief Initialize an OS based memory allocator.
+ *
+ * Initializes a allocator.
+ * Must be called before any allocation is attempted.
+ * \a kbase_mem_allocator_alloc and \a kbase_mem_allocator_free is used
+ * to allocate and free memory.
+ * \a kbase_mem_allocator_term must be called to clean up the allocator.
+ * All memory obtained via \a kbase_mem_allocator_alloc must have been
+ * \a kbase_mem_allocator_free before \a kbase_mem_allocator_term is called.
+ *
+ * @param allocator Allocator object to initialize
+ * @param max_size Maximum number of pages to keep on the freelist.
+ * @return MALI_ERROR_NONE on success, an error code indicating what failed on error.
+ */
+mali_error kbase_mem_allocator_init(kbase_mem_allocator * allocator, unsigned int max_size);
+
+/**
+ * @brief Allocate memory via an OS based memory allocator.
+ *
+ * @param[in] allocator Allocator to obtain the memory from
+ * @param nr_pages Number of pages to allocate
+ * @param[out] pages Pointer to an array where the physical address of the allocated pages will be stored
+ * @return MALI_ERROR_NONE if the pages were allocated, an error code indicating what failed on error
+ */
+mali_error kbase_mem_allocator_alloc(kbase_mem_allocator * allocator, size_t nr_pages, phys_addr_t *pages);
+
+/**
+ * @brief Free memory obtained for an OS based memory allocator.
+ *
+ * @param[in] allocator Allocator to free the memory back to
+ * @param nr_pages Number of pages to free
+ * @param[in] pages Pointer to an array holding the physical address of the paghes to free.
+ * @param[in] sync_back MALI_TRUE case the memory should be synced back
+ */
+void kbase_mem_allocator_free(kbase_mem_allocator * allocator, size_t nr_pages, phys_addr_t *pages, mali_bool sync_back);
+
+/**
+ * @brief Terminate an OS based memory allocator.
+ *
+ * Frees all cached allocations and clean up internal state.
+ * All allocate pages must have been \a kbase_mem_allocator_free before
+ * this function is called.
+ *
+ * @param[in] allocator Allocator to terminate
+ */
+void kbase_mem_allocator_term(kbase_mem_allocator * allocator);
+
+
+
+mali_error kbase_region_tracker_init(kbase_context *kctx);
+void kbase_region_tracker_term(kbase_context *kctx);
+
+struct kbase_va_region *kbase_region_tracker_find_region_enclosing_range(kbase_context *kctx, u64 start_pgoff, size_t nr_pages);
+
+struct kbase_va_region *kbase_region_tracker_find_region_enclosing_address(kbase_context *kctx, mali_addr64 gpu_addr);
+
+/**
+ * @brief Check that a pointer is actually a valid region.
+ *
+ * Must be called with context lock held.
+ */
+struct kbase_va_region *kbase_region_tracker_find_region_base_address(kbase_context *kctx, mali_addr64 gpu_addr);
+
+struct kbase_va_region *kbase_alloc_free_region(kbase_context *kctx, u64 start_pfn, size_t nr_pages, int zone);
+void kbase_free_alloced_region(struct kbase_va_region *reg);
+mali_error kbase_add_va_region(kbase_context *kctx, struct kbase_va_region *reg, mali_addr64 addr, size_t nr_pages, size_t align);
+
+mali_error kbase_gpu_mmap(kbase_context *kctx, struct kbase_va_region *reg, mali_addr64 addr, size_t nr_pages, size_t align);
+mali_bool kbase_check_alloc_flags(unsigned long flags);
+void kbase_update_region_flags(struct kbase_va_region *reg, unsigned long flags);
+
+void kbase_gpu_vm_lock(kbase_context *kctx);
+void kbase_gpu_vm_unlock(kbase_context *kctx);
+
+int kbase_alloc_phy_pages(struct kbase_va_region *reg, size_t vsize, size_t size);
+
+mali_error kbase_mmu_init(kbase_context *kctx);
+void kbase_mmu_term(kbase_context *kctx);
+
+phys_addr_t kbase_mmu_alloc_pgd(kbase_context *kctx);
+void kbase_mmu_free_pgd(kbase_context *kctx);
+mali_error kbase_mmu_insert_pages(kbase_context *kctx, u64 vpfn,
+				  phys_addr_t *phys, size_t nr,
+				  unsigned long flags);
+mali_error kbase_mmu_insert_single_page(kbase_context *kctx, u64 vpfn,
+					phys_addr_t phys, size_t nr,
+					unsigned long flags);
+
+mali_error kbase_mmu_teardown_pages(kbase_context *kctx, u64 vpfn, size_t nr);
+mali_error kbase_mmu_update_pages(kbase_context* kctx, u64 vpfn, phys_addr_t* phys, size_t nr, unsigned long flags);
+
+/**
+ * @brief Register region and map it on the GPU.
+ *
+ * Call kbase_add_va_region() and map the region on the GPU.
+ */
+mali_error kbase_gpu_mmap(kbase_context *kctx, struct kbase_va_region *reg, mali_addr64 addr, size_t nr_pages, size_t align);
+
+/**
+ * @brief Remove the region from the GPU and unregister it.
+ *
+ * Must be called with context lock held.
+ */
+mali_error kbase_gpu_munmap(kbase_context *kctx, struct kbase_va_region *reg);
+
+/**
+ * The caller has the following locking conditions:
+ * - It must hold kbase_as::transaction_mutex on kctx's address space
+ * - It must hold the kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_mmu_update(kbase_context *kctx);
+
+/**
+ * The caller has the following locking conditions:
+ * - It must hold kbase_as::transaction_mutex on kctx's address space
+ * - It must hold the kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_mmu_disable(kbase_context *kctx);
+
+void kbase_mmu_interrupt(kbase_device *kbdev, u32 irq_stat);
+
+/** Dump the MMU tables to a buffer
+ *
+ * This function allocates a buffer (of @c nr_pages pages) to hold a dump of the MMU tables and fills it. If the
+ * buffer is too small then the return value will be NULL.
+ *
+ * The GPU vm lock must be held when calling this function.
+ *
+ * The buffer returned should be freed with @ref vfree when it is no longer required.
+ *
+ * @param[in]   kctx        The kbase context to dump
+ * @param[in]   nr_pages    The number of pages to allocate for the buffer.
+ *
+ * @return The address of the buffer containing the MMU dump or NULL on error (including if the @c nr_pages is too
+ * small)
+ */
+void *kbase_mmu_dump(kbase_context *kctx, int nr_pages);
+
+mali_error kbase_sync_now(kbase_context *kctx, base_syncset *syncset);
+void kbase_pre_job_sync(kbase_context *kctx, base_syncset *syncsets, size_t nr);
+void kbase_post_job_sync(kbase_context *kctx, base_syncset *syncsets, size_t nr);
+
+/**
+ * Set attributes for imported tmem region
+ *
+ * This function sets (extends with) requested attributes for given region
+ * of imported external memory
+ *
+ * @param[in]  kctx  	    The kbase context which the tmem belongs to
+ * @param[in]  gpu_adr     The base address of the tmem region
+ * @param[in]  attributes   The attributes of tmem region to be set
+ *
+ * @return MALI_ERROR_NONE on success.  Any other value indicates failure.
+ */
+mali_error kbase_tmem_set_attributes(kbase_context *kctx, mali_addr64 gpu_adr, u32  attributes );
+
+/**
+ * Get attributes of imported tmem region
+ *
+ * This function retrieves the attributes of imported external memory
+ *
+ * @param[in]  kctx  	    The kbase context which the tmem belongs to
+ * @param[in]  gpu_adr     The base address of the tmem region
+ * @param[out] attributes   The actual attributes of tmem region
+ *
+ * @return MALI_ERROR_NONE on success.  Any other value indicates failure.
+ */
+mali_error kbase_tmem_get_attributes(kbase_context *kctx, mali_addr64 gpu_adr, u32 * const attributes );
+
+/* OS specific functions */
+mali_error kbase_mem_free(kbase_context *kctx, mali_addr64 gpu_addr);
+mali_error kbase_mem_free_region(kbase_context *kctx, struct kbase_va_region *reg);
+void kbase_os_mem_map_lock(kbase_context *kctx);
+void kbase_os_mem_map_unlock(kbase_context *kctx);
+
+/**
+ * @brief Update the memory allocation counters for the current process
+ *
+ * OS specific call to updates the current memory allocation counters for the current process with
+ * the supplied delta.
+ *
+ * @param[in] kctx  The kbase context 
+ * @param[in] pages The desired delta to apply to the memory usage counters.
+ */
+
+void kbasep_os_process_page_usage_update( struct kbase_context * kctx, int pages );
+
+/**
+ * @brief Add to the memory allocation counters for the current process
+ *
+ * OS specific call to add to the current memory allocation counters for the current process by
+ * the supplied amount.
+ *
+ * @param[in] kctx  The kernel base context used for the allocation.
+ * @param[in] pages The desired delta to apply to the memory usage counters.
+ */
+
+static INLINE void kbase_process_page_usage_inc( struct kbase_context *kctx, int pages )
+{
+	kbasep_os_process_page_usage_update( kctx, pages );
+}
+
+/**
+ * @brief Subtract from the memory allocation counters for the current process
+ *
+ * OS specific call to subtract from the current memory allocation counters for the current process by
+ * the supplied amount.
+ *
+ * @param[in] kctx  The kernel base context used for the allocation.
+ * @param[in] pages The desired delta to apply to the memory usage counters.
+ */
+
+static INLINE void kbase_process_page_usage_dec( struct kbase_context *kctx, int pages )
+{
+	kbasep_os_process_page_usage_update( kctx, 0 - pages );
+}
+
+/**
+ * @brief Find the offset of the CPU mapping of a memory allocation containing
+ *        a given address range
+ *
+ * Searches for a CPU mapping of any part of the region starting at @p gpu_addr
+ * that fully encloses the CPU virtual address range specified by @p uaddr and
+ * @p size. Returns a failure indication if only part of the address range lies
+ * within a CPU mapping, or the address range lies within a CPU mapping of a
+ * different region.
+ *
+ * @param[in,out] kctx      The kernel base context used for the allocation.
+ * @param[in]     gpu_addr  GPU address of the start of the allocated region
+ *                          within which to search.
+ * @param[in]     uaddr     Start of the CPU virtual address range.
+ * @param[in]     size      Size of the CPU virtual address range (in bytes).
+ * @param[out]    offset    The offset from the start of the allocation to the
+ *                          specified CPU virtual address.
+ *
+ * @return MALI_ERROR_NONE if offset was obtained successfully. Error code
+ *         otherwise.
+ */
+mali_error kbasep_find_enclosing_cpu_mapping_offset(kbase_context *kctx,
+							mali_addr64 gpu_addr,
+							unsigned long uaddr,
+							size_t size,
+							mali_size64 *offset);
+
+enum hrtimer_restart kbasep_as_poke_timer_callback(struct hrtimer *timer);
+void kbase_as_poking_timer_retain_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom);
+void kbase_as_poking_timer_release_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom);
+
+/**
+* @brief Allocates physical pages.
+*
+* Allocates \a nr_pages_requested and updates the alloc object.
+*
+* @param[in] alloc allocation object to add pages to
+* @param[in] nr_pages_requested number of physical pages to allocate
+*
+* @return 0 if all pages have been successfully allocated. Error code otherwise
+*/
+int kbase_alloc_phy_pages_helper(struct kbase_mem_phy_alloc * alloc, size_t nr_pages_requested);
+
+/**
+* @brief Free physical pages.
+*
+* Frees \a nr_pages and updates the alloc object.
+*
+* @param[in] alloc allocation object to free pages from
+* @param[in] nr_pages_to_free number of physical pages to free
+*/
+int kbase_free_phy_pages_helper(struct kbase_mem_phy_alloc * alloc, size_t nr_pages_to_free);
+
+#ifdef CONFIG_MALI_NO_MALI
+static inline void kbase_wait_write_flush(kbase_context *kctx)
+{
+}
+#else
+void kbase_wait_write_flush(kbase_context *kctx);
+#endif
+
+
+#endif				/* _KBASE_MEM_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.c
new file mode 100644
index 0000000..919ee09
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.c
@@ -0,0 +1,253 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mem.c
+ * Base kernel memory APIs
+ */
+#include <mali_kbase.h>
+#include <linux/highmem.h>
+#include <linux/mempool.h>
+#include <linux/mm.h>
+#include <linux/atomic.h>
+
+static unsigned long kbase_mem_allocator_count(struct shrinker *s,
+						struct shrink_control *sc)
+{
+	kbase_mem_allocator *allocator;
+	allocator = container_of(s, kbase_mem_allocator, free_list_reclaimer);
+	return atomic_read(&allocator->free_list_size);
+}
+
+static unsigned long kbase_mem_allocator_scan(struct shrinker *s,
+						struct shrink_control *sc)
+{
+	kbase_mem_allocator *allocator;
+	int i;
+	int freed;
+
+	allocator = container_of(s, kbase_mem_allocator, free_list_reclaimer);
+
+	might_sleep();
+
+	mutex_lock(&allocator->free_list_lock);
+	i = MIN(atomic_read(&allocator->free_list_size), sc->nr_to_scan);
+	freed = i;
+
+	atomic_sub(i, &allocator->free_list_size);
+
+	while (i--) {
+		struct page *p;
+
+		BUG_ON(list_empty(&allocator->free_list_head));
+		p = list_first_entry(&allocator->free_list_head,
+					struct page, lru);
+		list_del(&p->lru);
+		__free_page(p);
+	}
+	mutex_unlock(&allocator->free_list_lock);
+	return atomic_read(&allocator->free_list_size);
+
+}
+
+static int kbase_mem_allocator_shrink(struct shrinker *s,
+					struct shrink_control *sc)
+{
+	if (sc->nr_to_scan == 0)
+		return kbase_mem_allocator_count(s, sc);
+	else
+		return kbase_mem_allocator_scan(s, sc);
+}
+
+mali_error kbase_mem_allocator_init(kbase_mem_allocator *const allocator,
+					unsigned int max_size)
+{
+	KBASE_DEBUG_ASSERT(NULL != allocator);
+
+	INIT_LIST_HEAD(&allocator->free_list_head);
+
+	mutex_init(&allocator->free_list_lock);
+
+	atomic_set(&allocator->free_list_size, 0);
+
+	allocator->free_list_max_size = max_size;
+	allocator->free_list_reclaimer.shrink = kbase_mem_allocator_shrink;
+	allocator->free_list_reclaimer.seeks = DEFAULT_SEEKS;
+	/* Kernel versions prior to 3.1 :
+	 * struct shrinker does not define batch */
+	allocator->free_list_reclaimer.batch = 0;
+
+	register_shrinker(&allocator->free_list_reclaimer);
+
+	return MALI_ERROR_NONE;
+}
+KBASE_EXPORT_TEST_API(kbase_mem_allocator_init)
+
+void kbase_mem_allocator_term(kbase_mem_allocator *allocator)
+{
+	KBASE_DEBUG_ASSERT(NULL != allocator);
+
+	unregister_shrinker(&allocator->free_list_reclaimer);
+	mutex_lock(&allocator->free_list_lock);
+	while (!list_empty(&allocator->free_list_head))
+	{
+		struct page * p;
+		p = list_first_entry(&allocator->free_list_head, struct page, lru);
+		list_del(&p->lru);
+		__free_page(p);
+	}
+	atomic_set(&allocator->free_list_size, 0);
+	mutex_unlock(&allocator->free_list_lock);
+	mutex_destroy(&allocator->free_list_lock);
+}
+KBASE_EXPORT_TEST_API(kbase_mem_allocator_term)
+
+mali_error kbase_mem_allocator_alloc(kbase_mem_allocator *allocator, size_t nr_pages, phys_addr_t *pages)
+{
+	struct page * p;
+	void * mp;
+	int i;
+	int num_from_free_list;
+	struct list_head from_free_list = LIST_HEAD_INIT(from_free_list);
+
+	might_sleep();
+
+	KBASE_DEBUG_ASSERT(NULL != allocator);
+
+	/* take from the free list first */
+	mutex_lock(&allocator->free_list_lock);
+	num_from_free_list = MIN(nr_pages, atomic_read(&allocator->free_list_size));
+	atomic_sub(num_from_free_list, &allocator->free_list_size);
+	for (i = 0; i < num_from_free_list; i++)
+	{
+		BUG_ON(list_empty(&allocator->free_list_head));
+		p = list_first_entry(&allocator->free_list_head, struct page, lru);
+		list_move(&p->lru, &from_free_list);
+	}
+	mutex_unlock(&allocator->free_list_lock);
+	i = 0;
+
+	/* Allocate as many pages from the pool of already allocated pages. */
+	list_for_each_entry(p, &from_free_list, lru)
+	{
+		pages[i] = PFN_PHYS(page_to_pfn(p));
+		i++;
+	}
+
+	if (i == nr_pages)
+		return MALI_ERROR_NONE;
+
+	/* If not all pages were sourced from the pool, request new ones. */
+	for (; i < nr_pages; i++)
+	{
+		p = alloc_page(GFP_HIGHUSER);
+		if (NULL == p)
+		{
+			goto err_out_roll_back;
+		}
+		mp = kmap(p);
+		if (NULL == mp)
+		{
+			__free_page(p);
+			goto err_out_roll_back;
+		}
+		memset(mp, 0x00, PAGE_SIZE); /* instead of __GFP_ZERO, so we can do cache maintenance */
+		kbase_sync_to_memory(PFN_PHYS(page_to_pfn(p)), mp, PAGE_SIZE);
+		kunmap(p);
+		pages[i] = PFN_PHYS(page_to_pfn(p));
+	}
+
+	return MALI_ERROR_NONE;
+
+err_out_roll_back:
+	while (i--)
+	{
+		struct page * p;
+		p = pfn_to_page(PFN_DOWN(pages[i]));
+		pages[i] = (phys_addr_t)0;
+		__free_page(p);
+	}
+
+	return MALI_ERROR_OUT_OF_MEMORY;
+}
+KBASE_EXPORT_TEST_API(kbase_mem_allocator_alloc)
+
+void kbase_mem_allocator_free(kbase_mem_allocator *allocator, size_t nr_pages, phys_addr_t *pages, mali_bool sync_back)
+{
+	int i = 0;
+	int page_count = 0;
+	int tofree;
+
+	LIST_HEAD(new_free_list_items);
+
+	KBASE_DEBUG_ASSERT(NULL != allocator);
+
+	might_sleep();
+
+	/* Starting by just freeing the overspill.
+	* As we do this outside of the lock we might spill too many pages
+	* or get too many on the free list, but the max_size is just a ballpark so it is ok
+	* providing that tofree doesn't exceed nr_pages
+	*/
+	tofree = MAX((int)allocator->free_list_max_size - atomic_read(&allocator->free_list_size),0);
+	tofree = nr_pages - MIN(tofree, nr_pages);
+	for (; i < tofree; i++)
+	{
+		if (likely(0 != pages[i]))
+		{
+			struct page * p;
+
+			p = pfn_to_page(PFN_DOWN(pages[i]));
+			pages[i] = (phys_addr_t)0;
+			__free_page(p);
+		}
+	}
+
+	for (; i < nr_pages; i++)
+	{
+		if (likely(0 != pages[i]))
+		{
+			struct page * p;
+
+			p = pfn_to_page(PFN_DOWN(pages[i]));
+			pages[i] = (phys_addr_t)0;
+			/* Sync back the memory to ensure that future cache invalidations
+			 * don't trample on memory.
+			 */
+			if( sync_back )
+			{
+				void* mp = kmap(p);
+				if( NULL != mp)
+				{
+					kbase_sync_to_cpu(PFN_PHYS(page_to_pfn(p)), mp, PAGE_SIZE);
+					kunmap(p);
+				}
+
+			}
+			list_add(&p->lru, &new_free_list_items);
+			page_count++;
+		}
+	}
+	mutex_lock(&allocator->free_list_lock);
+	list_splice(&new_free_list_items, &allocator->free_list_head);
+	atomic_add(page_count, &allocator->free_list_size);
+	mutex_unlock(&allocator->free_list_lock);
+}
+KBASE_EXPORT_TEST_API(kbase_mem_allocator_free)
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.h
new file mode 100644
index 0000000..5929b14
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_alloc.h
@@ -0,0 +1,33 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/atomic.h>
+#include <linux/mempool.h>
+#include <linux/slab.h>
+
+/* raw page handling */
+typedef struct kbase_mem_allocator
+{
+	atomic_t            free_list_size;
+	unsigned int        free_list_max_size;
+	struct mutex        free_list_lock;
+	struct list_head    free_list_head;
+	struct shrinker     free_list_reclaimer;
+} kbase_mem_allocator;
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.c
new file mode 100644
index 0000000..bfc9aef
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.c
@@ -0,0 +1,1633 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mem_linux.c
+ * Base kernel memory APIs, Linux implementation.
+ */
+
+#include <linux/compat.h>
+#include <linux/kernel.h>
+#include <linux/bug.h>
+#include <linux/mm.h>
+#include <linux/fs.h>
+#include <linux/version.h>
+#include <linux/dma-mapping.h>
+	#include <linux/dma-attrs.h>
+#ifdef CONFIG_DMA_SHARED_BUFFER
+#include <linux/dma-buf.h>
+#endif				/* defined(CONFIG_DMA_SHARED_BUFFER) */
+
+#include <mali_kbase.h>
+#include <mali_kbase_mem_linux.h>
+#include <mali_kbase_config_defaults.h>
+
+static int kbase_tracking_page_setup(struct kbase_context *kctx, struct vm_area_struct *vma);
+static const struct vm_operations_struct kbase_vm_ops;
+
+struct kbase_va_region *kbase_mem_alloc(kbase_context *kctx, u64 va_pages, u64 commit_pages, u64 extent, u64 *flags, u64 *gpu_va, u16 *va_alignment)
+{
+	int zone;
+	int gpu_pc_bits;
+	int cpu_va_bits;
+	struct kbase_va_region *reg;
+	struct device *dev;
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(flags);
+	KBASE_DEBUG_ASSERT(gpu_va);
+	KBASE_DEBUG_ASSERT(va_alignment);
+
+	dev = kctx->kbdev->dev;
+	*va_alignment = 0; /* no alignment by default */
+	*gpu_va = 0; /* return 0 on failure */
+
+	gpu_pc_bits = kctx->kbdev->gpu_props.props.core_props.log2_program_counter_size;
+	cpu_va_bits = BITS_PER_LONG;
+
+	if (0 == va_pages) {
+		dev_warn(dev, "kbase_mem_alloc called with 0 va_pages!");
+		goto zero_size;
+	}
+
+#if defined(CONFIG_64BIT)
+	if (is_compat_task())
+		cpu_va_bits = 32;
+	else
+		/* force SAME_VA if a 64-bit client */
+		*flags |= BASE_MEM_SAME_VA;
+#endif
+
+	if (!kbase_check_alloc_flags(*flags)) {
+		dev_warn(dev,
+				"kbase_mem_alloc called with bad flags (%llx)",
+				(unsigned long long)*flags);
+		goto bad_flags;
+	}
+
+	/* Limit GPU executable allocs to GPU PC size */
+	if ((*flags & BASE_MEM_PROT_GPU_EX) &&
+	    (va_pages > (1ULL << gpu_pc_bits >> PAGE_SHIFT)))
+		goto bad_ex_size;
+
+	/* find out which VA zone to use */
+	if (*flags & BASE_MEM_SAME_VA)
+		zone = KBASE_REG_ZONE_SAME_VA;
+	else if (*flags & BASE_MEM_PROT_GPU_EX)
+		zone = KBASE_REG_ZONE_EXEC;
+	else
+		zone = KBASE_REG_ZONE_CUSTOM_VA;
+
+	reg = kbase_alloc_free_region(kctx, 0, va_pages, zone);
+	if (!reg) {
+		dev_err(dev, "Failed to allocate free region");
+		goto no_region;
+	}
+
+	if (MALI_ERROR_NONE != kbase_reg_prepare_native(reg, kctx)) {
+		dev_err(dev, "Failed to prepare region");
+		goto prepare_failed;
+	}
+
+	kbase_update_region_flags(reg, *flags);
+
+	if (*flags & BASE_MEM_GROW_ON_GPF)
+		reg->extent = extent;
+	else
+		reg->extent = 0;
+
+	if (kbase_alloc_phy_pages(reg, va_pages, commit_pages)) {
+		dev_warn(dev, "Failed to allocate %lld pages (va_pages=%lld)", 
+		              (unsigned long long)commit_pages, (unsigned long long)va_pages);
+		goto no_mem;
+	}
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* mmap needed to setup VA? */
+	if (*flags & BASE_MEM_SAME_VA) {
+		/* Bind to a cookie */
+		if (!kctx->cookies) {
+			dev_err(dev, "No cookies available for allocation!");
+			goto no_cookie;
+		}
+		/* return a cookie */
+		*gpu_va = __ffs(kctx->cookies);
+		kctx->cookies &= ~(1UL << *gpu_va);
+		BUG_ON(kctx->pending_regions[*gpu_va]);
+		kctx->pending_regions[*gpu_va] = reg;
+
+		/* relocate to correct base */
+		*gpu_va += PFN_DOWN(BASE_MEM_COOKIE_BASE);
+		*gpu_va <<= PAGE_SHIFT;
+
+		/* See if we must align memory due to GPU PC bits vs CPU VA */
+		if ((*flags & BASE_MEM_PROT_GPU_EX) &&
+		    (cpu_va_bits > gpu_pc_bits)) {
+			*va_alignment = gpu_pc_bits;
+			reg->flags |= KBASE_REG_ALIGNED;
+		}
+	} else /* we control the VA */ {
+		if (MALI_ERROR_NONE != kbase_gpu_mmap(kctx, reg, 0, va_pages, 1)) {
+			dev_warn(dev, "Failed to map memory on GPU");
+			goto no_mmap;
+		}
+		/* return real GPU VA */
+		*gpu_va = reg->start_pfn << PAGE_SHIFT;
+	}
+
+	kbase_gpu_vm_unlock(kctx);
+	return reg;
+
+no_mmap:
+no_cookie:
+	kbase_gpu_vm_unlock(kctx);
+no_mem:
+	kbase_mem_phy_alloc_put(reg->alloc);
+prepare_failed:
+	kfree(reg);
+no_region:
+bad_ex_size:
+bad_flags:
+zero_size:
+	return NULL;
+}
+
+mali_error kbase_mem_query(kbase_context *kctx, mali_addr64 gpu_addr, int query, u64 * const out)
+{
+	kbase_va_region *reg;
+	mali_error ret = MALI_ERROR_FUNCTION_FAILED;
+
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(out);
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* Validate the region */
+	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
+	if (!reg || (reg->flags & KBASE_REG_FREE) )
+		goto out_unlock;
+
+	switch (query) {
+		case KBASE_MEM_QUERY_COMMIT_SIZE:
+			if (reg->alloc->type != KBASE_MEM_TYPE_ALIAS) {
+				*out = kbase_reg_current_backed_size(reg);
+			} else {
+				size_t i;
+				struct kbase_aliased *aliased;
+				*out = 0;
+				aliased = reg->alloc->imported.alias.aliased;
+				for (i = 0; i < reg->alloc->imported.alias.nents; i++)
+					*out += aliased[i].length;
+			}
+			break;
+		case KBASE_MEM_QUERY_VA_SIZE:
+			*out = reg->nr_pages;
+			break;
+		case KBASE_MEM_QUERY_FLAGS:
+		{
+			*out = 0;
+			if( KBASE_REG_GPU_WR & reg->flags )
+				*out |= BASE_MEM_PROT_GPU_WR;
+			if( KBASE_REG_GPU_RD & reg->flags )
+				*out |= BASE_MEM_PROT_GPU_RD;
+			if( !(KBASE_REG_GPU_NX & reg->flags) )
+				*out |= BASE_MEM_PROT_GPU_EX;
+			if( KBASE_REG_SHARE_BOTH & reg->flags )
+				*out |= BASE_MEM_COHERENT_SYSTEM;
+			if ( KBASE_REG_SHARE_IN & reg->flags )
+				*out |= BASE_MEM_COHERENT_LOCAL;
+			break;
+		}
+		default:
+			*out = 0;
+			goto out_unlock;
+	}
+
+	ret = MALI_ERROR_NONE;
+
+out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+	return ret;
+}
+
+mali_error kbase_mem_flags_change(kbase_context *kctx, mali_addr64 gpu_addr, unsigned int flags, unsigned int mask)
+{
+	kbase_va_region *reg;
+	mali_error ret = MALI_ERROR_FUNCTION_FAILED;
+	unsigned int real_flags = 0;
+	unsigned int prev_flags = 0;
+
+	KBASE_DEBUG_ASSERT(kctx);
+
+	if (!gpu_addr)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	/* nuke other bits */
+	flags &= mask;
+
+	/* check for only supported flags */
+	if (flags & ~(BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL))
+		goto out;
+
+	/* mask covers bits we don't support? */
+	if (mask & ~(BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL))
+		goto out;
+
+	/* convert flags */
+	if( BASE_MEM_COHERENT_SYSTEM & flags )
+		real_flags |= KBASE_REG_SHARE_BOTH;
+	else if ( BASE_MEM_COHERENT_LOCAL & flags )
+		real_flags |= KBASE_REG_SHARE_IN;
+
+	/* now we can lock down the context, and find the region */
+	kbase_gpu_vm_lock(kctx);
+
+	/* Validate the region */
+	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
+	if (!reg || (reg->flags & KBASE_REG_FREE) )
+		goto out_unlock;
+
+	/* limit to imported memory */
+	if ( (reg->alloc->type != KBASE_MEM_TYPE_IMPORTED_UMP) &&
+	     (reg->alloc->type != KBASE_MEM_TYPE_IMPORTED_UMM))
+		goto out_unlock;
+
+	/* no change? */
+	if (real_flags == (reg->flags & (KBASE_REG_SHARE_IN | KBASE_REG_SHARE_BOTH)))
+	{
+		ret = MALI_ERROR_NONE;
+		goto out_unlock;
+	}
+
+	/* save for roll back */
+	prev_flags = reg->flags;
+	reg->flags &= ~(KBASE_REG_SHARE_IN | KBASE_REG_SHARE_BOTH);
+	reg->flags |= real_flags;
+
+	/* Currently supporting only imported memory */
+	switch(reg->alloc->type)
+	{
+#ifdef CONFIG_UMP
+		case KBASE_MEM_TYPE_IMPORTED_UMP:
+			ret = kbase_mmu_update_pages(kctx, reg->start_pfn, kbase_get_phy_pages(reg), reg->alloc->nents, reg->flags);
+			break;
+#endif
+#ifdef CONFIG_DMA_SHARED_BUFFER
+		case KBASE_MEM_TYPE_IMPORTED_UMM:
+			/* Future use will use the new flags, existing mapping will NOT be updated
+			 * as memory should not be in use by the GPU when updating the flags.
+			 */
+			ret = MALI_ERROR_NONE;
+			WARN_ON(reg->alloc->imported.umm.current_mapping_usage_count);
+			break;
+#endif
+		default:
+			break;
+	}
+
+	/* roll back on error, i.e. not UMP */
+	if (ret != MALI_ERROR_NONE)
+		reg->flags = prev_flags;
+
+out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+out:
+	return ret;
+}
+
+#ifdef CONFIG_UMP
+static struct kbase_va_region *kbase_mem_from_ump(kbase_context *kctx, ump_secure_id id, u64 *va_pages, u64 *flags)
+{
+	struct kbase_va_region *reg;
+	ump_dd_handle umph;
+	u64 block_count;
+	const ump_dd_physical_block_64 *block_array;
+	u64 i, j;
+	int page = 0;
+	ump_alloc_flags ump_flags;
+	ump_alloc_flags cpu_flags;
+	ump_alloc_flags gpu_flags;
+
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(va_pages);
+	KBASE_DEBUG_ASSERT(flags);
+
+	umph = ump_dd_from_secure_id(id);
+	if (UMP_DD_INVALID_MEMORY_HANDLE == umph)
+		goto bad_id;
+
+	ump_flags = ump_dd_allocation_flags_get(umph);
+	cpu_flags = (ump_flags >> UMP_DEVICE_CPU_SHIFT) & UMP_DEVICE_MASK;
+	gpu_flags = (ump_flags >> DEFAULT_UMP_GPU_DEVICE_SHIFT) &
+			UMP_DEVICE_MASK;
+
+	*va_pages = ump_dd_size_get_64(umph);
+	*va_pages >>= PAGE_SHIFT;
+
+	if (!*va_pages)
+		goto bad_size;
+
+	if (*flags & BASE_MEM_SAME_VA)
+		reg = kbase_alloc_free_region(kctx, 0, *va_pages, KBASE_REG_ZONE_SAME_VA);
+	else
+		reg = kbase_alloc_free_region(kctx, 0, *va_pages, KBASE_REG_ZONE_CUSTOM_VA);
+
+	if (!reg)
+		goto no_region;
+
+	/* we've got pages to map now, and support SAME_VA */
+	*flags |= KBASE_MEM_IMPORT_HAVE_PAGES;
+
+	reg->alloc = kbase_alloc_create(*va_pages, KBASE_MEM_TYPE_IMPORTED_UMP);
+	if (IS_ERR_OR_NULL(reg->alloc))
+		goto no_alloc_obj;
+	
+	reg->alloc->imported.ump_handle = umph;
+
+	reg->flags &= ~KBASE_REG_FREE;
+	reg->flags |= KBASE_REG_GPU_NX;	/* UMP is always No eXecute */
+	reg->flags &= ~KBASE_REG_GROWABLE;	/* UMP cannot be grown */
+
+	if ((cpu_flags & (UMP_HINT_DEVICE_RD | UMP_HINT_DEVICE_WR)) ==
+	    (UMP_HINT_DEVICE_RD | UMP_HINT_DEVICE_WR)) {
+		reg->flags |= KBASE_REG_CPU_CACHED;
+		*flags |= BASE_MEM_CACHED_CPU;
+	}
+
+	if (cpu_flags & UMP_PROT_DEVICE_WR) {
+		reg->flags |= KBASE_REG_CPU_WR;
+		*flags |= BASE_MEM_PROT_CPU_WR;
+	}
+
+	if (cpu_flags & UMP_PROT_DEVICE_RD) {
+		reg->flags |= KBASE_REG_CPU_RD;
+		*flags |= BASE_MEM_PROT_CPU_RD;
+	}
+
+	if ((gpu_flags & (UMP_HINT_DEVICE_RD | UMP_HINT_DEVICE_WR)) ==
+	    (UMP_HINT_DEVICE_RD | UMP_HINT_DEVICE_WR))
+		reg->flags |= KBASE_REG_GPU_CACHED;
+
+	if (gpu_flags & UMP_PROT_DEVICE_WR) {
+		reg->flags |= KBASE_REG_GPU_WR;
+		*flags |= BASE_MEM_PROT_GPU_WR;
+	}
+
+	if (gpu_flags & UMP_PROT_DEVICE_RD) {
+		reg->flags |= KBASE_REG_GPU_RD;
+		*flags |= BASE_MEM_PROT_GPU_RD;
+	}
+
+	/* ump phys block query */
+	ump_dd_phys_blocks_get_64(umph, &block_count, &block_array);
+
+	for (i = 0; i < block_count; i++) {
+		for (j = 0; j < (block_array[i].size >> PAGE_SHIFT); j++) {
+			reg->alloc->pages[page] = block_array[i].addr + (j << PAGE_SHIFT);
+			page++;
+		}
+	}
+	reg->alloc->nents = *va_pages;
+	reg->extent = 0;
+
+	return reg;
+
+no_alloc_obj:
+	kfree(reg);
+no_region:
+bad_size:
+	ump_dd_release(umph);
+bad_id:
+	return NULL;
+
+}
+#endif				/* CONFIG_UMP */
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+static struct kbase_va_region *kbase_mem_from_umm(kbase_context *kctx, int fd, u64 *va_pages, u64 *flags)
+{
+	struct kbase_va_region *reg;
+	struct dma_buf *dma_buf;
+	struct dma_buf_attachment *dma_attachment;
+
+	dma_buf = dma_buf_get(fd);
+	if (IS_ERR_OR_NULL(dma_buf))
+		goto no_buf;
+
+	dma_attachment = dma_buf_attach(dma_buf, kctx->kbdev->dev);
+	if (!dma_attachment)
+		goto no_attachment;
+
+	*va_pages = PAGE_ALIGN(dma_buf->size) >> PAGE_SHIFT;
+	if (!*va_pages)
+		goto bad_size;
+
+	/* ignore SAME_VA */
+	*flags &= ~BASE_MEM_SAME_VA;
+
+#ifdef CONFIG_64BIT
+	if (!is_compat_task()) {
+		/* 64-bit tasks must MMAP anyway, but not expose this address to clients */
+		*flags |= KBASE_MEM_NEED_MMAP;
+		reg = kbase_alloc_free_region(kctx, 0, *va_pages, KBASE_REG_ZONE_SAME_VA);
+	} else {
+#else
+	if (1) {
+#endif
+		reg = kbase_alloc_free_region(kctx, 0, *va_pages, KBASE_REG_ZONE_CUSTOM_VA);
+	}
+
+	if (!reg)
+		goto no_region;
+
+	reg->alloc = kbase_alloc_create(*va_pages, KBASE_MEM_TYPE_IMPORTED_UMM);
+	if (IS_ERR_OR_NULL(reg->alloc))
+		goto no_alloc_obj;
+
+	/* No pages to map yet */
+	reg->alloc->nents = 0;
+
+	reg->flags &= ~KBASE_REG_FREE;
+	reg->flags |= KBASE_REG_GPU_NX;	/* UMM is always No eXecute */
+	reg->flags &= ~KBASE_REG_GROWABLE;	/* UMM cannot be grown */
+	reg->flags |= KBASE_REG_GPU_CACHED;
+
+	if (*flags & BASE_MEM_PROT_CPU_WR)
+		reg->flags |= KBASE_REG_CPU_WR;
+
+	if (*flags & BASE_MEM_PROT_CPU_RD)
+		reg->flags |= KBASE_REG_CPU_RD;
+
+	if (*flags & BASE_MEM_PROT_GPU_WR)
+		reg->flags |= KBASE_REG_GPU_WR;
+
+	if (*flags & BASE_MEM_PROT_GPU_RD)
+		reg->flags |= KBASE_REG_GPU_RD;
+
+	/* no read or write permission given on import, only on run do we give the right permissions */
+
+	reg->alloc->type = BASE_TMEM_IMPORT_TYPE_UMM;
+	reg->alloc->imported.umm.sgt = NULL;
+	reg->alloc->imported.umm.dma_buf = dma_buf;
+	reg->alloc->imported.umm.dma_attachment = dma_attachment;
+	reg->alloc->imported.umm.current_mapping_usage_count = 0;
+	reg->extent = 0;
+
+	return reg;
+
+no_alloc_obj:
+	kfree(reg);
+no_region:
+bad_size:
+	dma_buf_detach(dma_buf, dma_attachment);
+no_attachment:
+	dma_buf_put(dma_buf);
+no_buf:
+	return NULL;
+}
+#endif  /* CONFIG_DMA_SHARED_BUFFER */
+
+u64 kbase_mem_alias(kbase_context *kctx, u64 *flags, u64 stride,
+		    u64 nents, struct base_mem_aliasing_info *ai,
+		    u64 *num_pages)
+{
+	kbase_va_region *reg;
+	u64 gpu_va;
+	size_t i;
+
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(flags);
+	KBASE_DEBUG_ASSERT(ai);
+	KBASE_DEBUG_ASSERT(num_pages);
+
+	/* mask to only allowed flags */
+	*flags &= (BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR |
+		   BASE_MEM_HINT_GPU_RD | BASE_MEM_HINT_GPU_WR |
+		   BASE_MEM_COHERENT_SYSTEM | BASE_MEM_COHERENT_LOCAL);
+
+	if (!(*flags & (BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR) )) {
+		dev_warn(kctx->kbdev->dev,
+				"kbase_mem_alias called with bad flags (%llx)",
+				(unsigned long long)*flags);
+		goto bad_flags;
+	}
+
+	if (!stride)
+		goto bad_stride;
+
+	if (!nents)
+		goto bad_nents;
+
+	/* calculate the number of pages this alias will cover */
+	*num_pages = nents * stride;
+
+#ifdef CONFIG_64BIT
+	if (!is_compat_task()) {
+		/* 64-bit tasks must MMAP anyway, but not expose this address to
+		 * clients */
+		*flags |= KBASE_MEM_NEED_MMAP;
+		reg = kbase_alloc_free_region(kctx, 0, *num_pages,
+					      KBASE_REG_ZONE_SAME_VA);
+	} else {
+#else
+	if (1) {
+#endif
+		reg = kbase_alloc_free_region(kctx, 0, *num_pages,
+					      KBASE_REG_ZONE_CUSTOM_VA);
+	}
+
+	if (!reg)
+		goto no_reg;
+
+	/* zero-sized page array, as we don't need one/can support one */
+	reg->alloc = kbase_alloc_create(0, KBASE_MEM_TYPE_ALIAS);
+	if (IS_ERR_OR_NULL(reg->alloc))
+		goto no_alloc_obj;
+
+	kbase_update_region_flags(reg, *flags);
+
+	reg->alloc->imported.alias.nents = nents;
+	reg->alloc->imported.alias.stride = stride;
+	reg->alloc->imported.alias.aliased = vzalloc(sizeof(*reg->alloc->imported.alias.aliased) * nents);
+	if (!reg->alloc->imported.alias.aliased)
+		goto no_aliased_array;
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* validate and add src handles */
+	for (i = 0; i < nents; i++) {
+		if (ai[i].handle < BASE_MEM_FIRST_FREE_ADDRESS) {
+			if (ai[i].handle != BASE_MEM_WRITE_ALLOC_PAGES_HANDLE)
+				goto bad_handle; /* unsupported magic handle */
+			if (!ai[i].length)
+				goto bad_handle; /* must be > 0 */
+			if (ai[i].length > stride)
+				goto bad_handle; /* can't be larger than the
+						    stride */
+			reg->alloc->imported.alias.aliased[i].length = ai[i].length;
+		} else {
+			struct kbase_va_region *aliasing_reg;
+			struct kbase_mem_phy_alloc *alloc;
+			aliasing_reg = kbase_region_tracker_find_region_base_address(kctx, (ai[i].handle >> PAGE_SHIFT) << PAGE_SHIFT);
+
+			/* validate found region */
+			if (!aliasing_reg)
+				goto bad_handle; /* Not found */
+			if (aliasing_reg->flags & KBASE_REG_FREE)
+				goto bad_handle; /* Free region */
+			if (!aliasing_reg->alloc)
+				goto bad_handle; /* No alloc */
+			if (aliasing_reg->alloc->type != KBASE_MEM_TYPE_NATIVE)
+				goto bad_handle; /* Not a native alloc */
+
+			/* check size against stride */
+			if (!ai[i].length)
+				goto bad_handle; /* must be > 0 */
+			if (ai[i].length > stride)
+				goto bad_handle; /* can't be larger than the
+						    stride */
+
+			alloc = aliasing_reg->alloc;
+
+			/* check against the alloc's size */
+			if (ai[i].offset > alloc->nents)
+				goto bad_handle; /* beyond end */
+			if (ai[i].offset + ai[i].length > alloc->nents)
+				goto bad_handle; /* beyond end */
+
+			reg->alloc->imported.alias.aliased[i].alloc = kbase_mem_phy_alloc_get(alloc);
+			reg->alloc->imported.alias.aliased[i].length = ai[i].length;
+			reg->alloc->imported.alias.aliased[i].offset = ai[i].offset;
+		}
+	}
+
+#ifdef CONFIG_64BIT
+	if (!is_compat_task()) {
+		/* Bind to a cookie */
+		if (!kctx->cookies) {
+			dev_err(kctx->kbdev->dev, "No cookies "
+						"available for allocation!");
+			goto no_cookie;
+		}
+		/* return a cookie */
+		gpu_va = __ffs(kctx->cookies);
+		kctx->cookies &= ~(1UL << gpu_va);
+		BUG_ON(kctx->pending_regions[gpu_va]);
+		kctx->pending_regions[gpu_va] = reg;
+
+		/* relocate to correct base */
+		gpu_va += PFN_DOWN(BASE_MEM_COOKIE_BASE);
+		gpu_va <<= PAGE_SHIFT;
+	} else /* we control the VA */ {
+#else
+	if (1) {
+#endif
+		if (MALI_ERROR_NONE != kbase_gpu_mmap(kctx, reg, 0,
+						      *num_pages, 1)) {
+			dev_warn(kctx->kbdev->dev,
+					       "Failed to map memory on GPU");
+			goto no_mmap;
+		}
+		/* return real GPU VA */
+		gpu_va = reg->start_pfn << PAGE_SHIFT;
+	}
+
+	reg->flags &= ~KBASE_REG_FREE;
+	reg->flags &= ~KBASE_REG_GROWABLE;
+
+	kbase_gpu_vm_unlock(kctx);
+
+	return gpu_va;
+
+#ifdef CONFIG_64BIT
+no_cookie:
+#endif
+no_mmap:
+bad_handle:
+	kbase_gpu_vm_unlock(kctx);
+no_aliased_array:
+	kbase_mem_phy_alloc_put(reg->alloc);
+no_alloc_obj:
+	kfree(reg);
+no_reg:
+bad_nents:
+bad_stride:
+bad_flags:
+	return 0;
+}
+
+int kbase_mem_import(kbase_context *kctx, base_mem_import_type type, int handle, mali_addr64 * gpu_va, u64 * va_pages, u64 * flags)
+{
+	kbase_va_region * reg;
+
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(gpu_va);
+	KBASE_DEBUG_ASSERT(va_pages);
+	KBASE_DEBUG_ASSERT(flags);
+
+#ifdef CONFIG_64BIT
+	if (!is_compat_task())
+		*flags |= BASE_MEM_SAME_VA;
+#endif
+
+	switch (type) {
+#ifdef CONFIG_UMP
+	case BASE_MEM_IMPORT_TYPE_UMP:
+		reg = kbase_mem_from_ump(kctx, (ump_secure_id)handle, va_pages, flags);
+		break;
+#endif /* CONFIG_UMP */
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	case BASE_MEM_IMPORT_TYPE_UMM:
+		reg = kbase_mem_from_umm(kctx, handle, va_pages, flags);
+		break;
+#endif /* CONFIG_DMA_SHARED_BUFFER */
+	default:
+		reg = NULL;
+		break;
+	}
+
+	if (!reg)
+		goto no_reg;
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* mmap needed to setup VA? */
+	if (*flags & (BASE_MEM_SAME_VA | KBASE_MEM_NEED_MMAP)) {
+		/* Bind to a cookie */
+		if (!kctx->cookies)
+			goto no_cookie;
+		/* return a cookie */
+		*gpu_va = __ffs(kctx->cookies);
+		kctx->cookies &= ~(1UL << *gpu_va);
+		BUG_ON(kctx->pending_regions[*gpu_va]);
+		kctx->pending_regions[*gpu_va] = reg;
+
+		/* relocate to correct base */
+		*gpu_va += PFN_DOWN(BASE_MEM_COOKIE_BASE);
+		*gpu_va <<= PAGE_SHIFT;
+
+	} else if (*flags & KBASE_MEM_IMPORT_HAVE_PAGES)  {
+		/* we control the VA, mmap now to the GPU */
+		if (MALI_ERROR_NONE != kbase_gpu_mmap(kctx, reg, 0, *va_pages, 1))
+			goto no_gpu_va;
+		/* return real GPU VA */
+		*gpu_va = reg->start_pfn << PAGE_SHIFT;
+	} else {
+		/* we control the VA, but nothing to mmap yet */
+		if (MALI_ERROR_NONE != kbase_add_va_region(kctx, reg, 0, *va_pages, 1))
+			goto no_gpu_va;
+		/* return real GPU VA */
+		*gpu_va = reg->start_pfn << PAGE_SHIFT;
+	}
+
+	kbase_gpu_vm_unlock(kctx);
+
+	return 0;
+
+no_gpu_va:
+no_cookie:
+	kbase_gpu_vm_unlock(kctx);
+	kbase_mem_phy_alloc_put(reg->alloc);
+	kfree(reg);
+no_reg:
+	*gpu_va = 0;
+	*va_pages = 0;
+	*flags = 0;
+	return -ENOMEM;
+}
+
+
+
+static int zap_range_nolock(struct mm_struct *mm,
+		const struct vm_operations_struct *vm_ops,
+		unsigned long start, unsigned long end)
+{
+	struct vm_area_struct *vma;
+	int err = -EINVAL; /* in case end < start */
+
+	while (start < end) {
+		unsigned long local_end;
+
+		vma = find_vma_intersection(mm, start, end);
+		if (!vma)
+			break;
+
+		/* is it ours? */
+		if (vma->vm_ops != vm_ops)
+			goto try_next;
+
+		local_end = vma->vm_end;
+
+		if (end < local_end)
+			local_end = end;
+
+		err = zap_vma_ptes(vma, start, local_end - start);
+		if (unlikely(err))
+			break;
+
+try_next:
+		/* go to next vma, if any */
+		start = vma->vm_end;
+	}
+
+	return err;
+}
+
+int kbase_mem_commit(kbase_context * kctx, mali_addr64 gpu_addr, u64 new_pages, base_backing_threshold_status * failure_reason)
+{
+	u64 old_pages;
+	u64 delta;
+	int res = -EINVAL;
+	kbase_va_region *reg;
+	phys_addr_t *phy_pages;
+
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(failure_reason);
+	KBASE_DEBUG_ASSERT(gpu_addr != 0);
+
+	down_read(&current->mm->mmap_sem);
+	kbase_gpu_vm_lock(kctx);
+
+	/* Validate the region */
+	reg = kbase_region_tracker_find_region_base_address(kctx, gpu_addr);
+	if (!reg || (reg->flags & KBASE_REG_FREE)) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS;
+		goto out_unlock;
+	}
+
+	KBASE_DEBUG_ASSERT(reg->alloc);
+
+	if (reg->alloc->type != KBASE_MEM_TYPE_NATIVE) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
+		goto out_unlock;
+	}
+
+	if (0 == (reg->flags & KBASE_REG_GROWABLE)) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
+		goto out_unlock;
+	}
+
+	if (new_pages > reg->nr_pages) {
+		/* Would overflow the VA region */
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_INVALID_ARGUMENTS;
+		goto out_unlock;
+	}
+
+	/* can't be mapped more than once on the GPU */
+	if (atomic_read(&reg->alloc->gpu_mappings) > 1) {
+		*failure_reason = BASE_BACKING_THRESHOLD_ERROR_NOT_GROWABLE;
+		goto out_unlock;
+	}
+
+	if (new_pages == reg->alloc->nents) {
+		/* no change */
+		res = 0;
+		goto out_unlock;
+	}
+
+	phy_pages = kbase_get_phy_pages(reg);
+	old_pages = kbase_reg_current_backed_size(reg);
+
+	if (new_pages > old_pages) {
+		/* growing */
+		mali_error err;
+		delta = new_pages - old_pages;
+		/* Allocate some more pages */
+		if (MALI_ERROR_NONE != kbase_alloc_phy_pages_helper(reg->alloc, delta)) {
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
+			goto out_unlock;
+		}
+		err = kbase_mmu_insert_pages(kctx, reg->start_pfn + old_pages, phy_pages + old_pages, delta, reg->flags);
+		if (MALI_ERROR_NONE != err) {
+			kbase_free_phy_pages_helper(reg->alloc, delta);
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
+			goto out_unlock;
+		}
+	} else {
+		/* shrinking */
+		struct kbase_cpu_mapping * mapping;
+		mali_error err;
+
+		/* first, unmap from any mappings affected */
+		list_for_each_entry(mapping, &reg->alloc->mappings, mappings_list) {
+			unsigned long mapping_size = (mapping->vm_end - mapping->vm_start) >> PAGE_SHIFT;
+
+			/* is this mapping affected ?*/
+			if ((mapping->page_off + mapping_size) > new_pages) {
+				unsigned long first_bad = 0;
+				int zap_res;
+
+				if (new_pages > mapping->page_off)
+					first_bad = new_pages - mapping->page_off;
+
+				zap_res = zap_range_nolock(current->mm,
+						&kbase_vm_ops,
+						mapping->vm_start +
+						(first_bad << PAGE_SHIFT),
+						mapping->vm_end);
+				WARN(zap_res,
+				     "Failed to zap VA range (0x%lx - 0x%lx);\n",
+				     mapping->vm_start +
+				     (first_bad << PAGE_SHIFT),
+				     mapping->vm_end
+				     );
+			}
+		}
+
+		/* Free some pages */
+		delta = old_pages - new_pages;
+		err = kbase_mmu_teardown_pages(kctx, reg->start_pfn + new_pages, delta);
+		if (MALI_ERROR_NONE != err) {
+			*failure_reason = BASE_BACKING_THRESHOLD_ERROR_OOM;
+			goto out_unlock;
+		}
+
+		if (kbase_hw_has_issue(kctx->kbdev, BASE_HW_ISSUE_6367)) {
+			/* Wait for GPU to flush write buffer before freeing physical pages */
+			kbase_wait_write_flush(kctx);
+		}
+
+		kbase_free_phy_pages_helper(reg->alloc, delta);
+	}
+
+	res = 0;
+
+out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+	up_read(&current->mm->mmap_sem);
+
+	return res;
+
+}
+
+STATIC void kbase_cpu_vm_open(struct vm_area_struct *vma)
+{
+	struct kbase_cpu_mapping *map = vma->vm_private_data;
+	KBASE_DEBUG_ASSERT(map);
+	KBASE_DEBUG_ASSERT(map->count > 0);
+	/* non-atomic as we're under Linux' mm lock */
+	map->count++;
+}
+
+STATIC void kbase_cpu_vm_close(struct vm_area_struct *vma)
+{
+	struct kbase_cpu_mapping *map = vma->vm_private_data;
+	KBASE_DEBUG_ASSERT(map);
+	KBASE_DEBUG_ASSERT(map->count > 0);
+
+	/* non-atomic as we're under Linux' mm lock */
+	if (--map->count)
+		return;
+
+	KBASE_DEBUG_ASSERT(map->kctx);
+	KBASE_DEBUG_ASSERT(map->alloc);
+
+	kbase_gpu_vm_lock(map->kctx);
+
+	if (map->region) {
+		KBASE_DEBUG_ASSERT((map->region->flags & KBASE_REG_ZONE_MASK) == KBASE_REG_ZONE_SAME_VA);
+		kbase_mem_free_region(map->kctx, map->region);
+	}
+
+	list_del(&map->mappings_list);
+
+	kbase_gpu_vm_unlock(map->kctx);
+
+	kbase_mem_phy_alloc_put(map->alloc);
+	kfree(map);
+}
+
+KBASE_EXPORT_TEST_API(kbase_cpu_vm_close)
+
+
+STATIC int kbase_cpu_vm_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct kbase_cpu_mapping *map = vma->vm_private_data;
+	pgoff_t rel_pgoff;
+	size_t i;
+
+	KBASE_DEBUG_ASSERT(map);
+	KBASE_DEBUG_ASSERT(map->count > 0);
+	KBASE_DEBUG_ASSERT(map->kctx);
+	KBASE_DEBUG_ASSERT(map->alloc);
+
+	/* we don't use vmf->pgoff as it's affected by our mmap with offset being a GPU VA or a cookie */
+	rel_pgoff = ((unsigned long)vmf->virtual_address - map->vm_start) >> PAGE_SHIFT;
+
+	kbase_gpu_vm_lock(map->kctx);
+	if (map->page_off + rel_pgoff >= map->alloc->nents)
+		goto locked_bad_fault;
+
+	/* insert all valid pages from the fault location */
+	for (i = rel_pgoff;
+			   i < MIN((vma->vm_end - vma->vm_start) >> PAGE_SHIFT,
+				     map->alloc->nents - map->page_off); i++) {
+		int ret = vm_insert_pfn(vma, map->vm_start + (i << PAGE_SHIFT),
+			       PFN_DOWN(map->alloc->pages[map->page_off + i]));
+		if (ret < 0 && ret != -EBUSY)
+			goto locked_bad_fault;
+	}
+
+	kbase_gpu_vm_unlock(map->kctx);
+	/* we resolved it, nothing for VM to do */
+	return VM_FAULT_NOPAGE;
+
+locked_bad_fault:
+	kbase_gpu_vm_unlock(map->kctx);
+	send_sig(SIGSEGV, current, 1);
+	return VM_FAULT_NOPAGE;
+}
+
+static const struct vm_operations_struct kbase_vm_ops = {
+	.open  = kbase_cpu_vm_open,
+	.close = kbase_cpu_vm_close,
+	.fault = kbase_cpu_vm_fault
+};
+
+static int kbase_cpu_mmap(struct kbase_va_region *reg, struct vm_area_struct *vma, void *kaddr, size_t nr_pages, unsigned long aligned_offset, int free_on_close)
+{
+	struct kbase_cpu_mapping *map;
+	u64 start_off = vma->vm_pgoff - reg->start_pfn;
+	phys_addr_t *page_array;
+	int err = 0;
+	int i;
+
+	map = kzalloc(sizeof(*map), GFP_KERNEL);
+
+	if (!map) {
+		WARN_ON(1);
+		err = -ENOMEM;
+		goto out;
+	}
+
+	/*
+	 * VM_DONTCOPY - don't make this mapping available in fork'ed processes
+	 * VM_DONTEXPAND - disable mremap on this region
+	 * VM_IO - disables paging
+	 * VM_DONTDUMP - Don't include in core dumps (3.7 only)
+	 * VM_MIXEDMAP - Support mixing struct page*s and raw pfns.
+	 *               This is needed to support using the dedicated and
+	 *               the OS based memory backends together.
+	 */
+	/*
+	 * This will need updating to propagate coherency flags
+	 * See MIDBASE-1057
+	 */
+
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTDUMP | VM_DONTEXPAND | VM_IO;
+	vma->vm_ops = &kbase_vm_ops;
+	vma->vm_private_data = map;
+
+	page_array = kbase_get_phy_pages(reg);
+
+	if (!(reg->flags & KBASE_REG_CPU_CACHED) &&
+	    (reg->flags & (KBASE_REG_CPU_WR|KBASE_REG_CPU_RD))) {
+		/* We can't map vmalloc'd memory uncached.
+		 * Other memory will have been returned from
+		 * kbase_mem_allocator_alloc which would be
+		 * suitable for mapping uncached.
+		 */
+		BUG_ON(kaddr);
+		vma->vm_page_prot = pgprot_writecombine(vma->vm_page_prot);
+	}
+
+	if (!kaddr) {
+		vma->vm_flags |= VM_PFNMAP;
+		for (i = 0; i < nr_pages; i++) {
+			err = vm_insert_pfn(vma, vma->vm_start + (i << PAGE_SHIFT), page_array[i + start_off] >> PAGE_SHIFT);
+			if (WARN_ON(err))
+				break;
+		}
+	} else {
+		/* MIXEDMAP so we can vfree the kaddr early and not track it after map time */
+		vma->vm_flags |= VM_MIXEDMAP;
+		/* vmalloc remaping is easy... */
+		err = remap_vmalloc_range(vma, kaddr, 0);
+		WARN_ON(err);
+	}
+
+	if (err) {
+		kfree(map);
+		goto out;
+	}
+
+
+	map->page_off = start_off;
+	map->region = free_on_close ? reg : NULL;
+	map->kctx = reg->kctx;
+	map->vm_start = vma->vm_start + aligned_offset;
+	if (aligned_offset) {
+		KBASE_DEBUG_ASSERT(!start_off);
+		map->vm_end = map->vm_start + (reg->nr_pages << PAGE_SHIFT);
+	} else {
+		map->vm_end = vma->vm_end;
+	}
+	map->alloc = kbase_mem_phy_alloc_get(reg->alloc);
+	map->count = 1; /* start with one ref */
+
+	if (reg->flags & KBASE_REG_CPU_CACHED)
+		map->alloc->accessed_cached = 1;
+
+	list_add(&map->mappings_list, &map->alloc->mappings);
+
+ out:
+	return err;
+}
+
+static int kbase_trace_buffer_mmap(kbase_context *kctx, struct vm_area_struct *vma, struct kbase_va_region **const reg, void **const kaddr)
+{
+	struct kbase_va_region *new_reg;
+	u32 nr_pages;
+	size_t size;
+	int err = 0;
+	u32 *tb;
+	int owns_tb = 1;
+
+	dev_dbg(kctx->kbdev->dev, "in %s\n", __func__);
+	size = (vma->vm_end - vma->vm_start);
+	nr_pages = size >> PAGE_SHIFT;
+
+	if (!kctx->jctx.tb) {
+		KBASE_DEBUG_ASSERT(0 != size);
+		tb = vmalloc_user(size);
+
+		if (NULL == tb) {
+			err = -ENOMEM;
+			goto out;
+		}
+
+		kbase_device_trace_buffer_install(kctx, tb, size);
+	} else {
+		err = -EINVAL;
+		goto out;
+	}
+
+	*kaddr = kctx->jctx.tb;
+
+	new_reg = kbase_alloc_free_region(kctx, 0, nr_pages, KBASE_REG_ZONE_SAME_VA);
+	if (!new_reg) {
+		err = -ENOMEM;
+		WARN_ON(1);
+		goto out_no_region;
+	}
+
+	new_reg->alloc = kbase_alloc_create(0, KBASE_MEM_TYPE_TB);
+	if (IS_ERR_OR_NULL(new_reg->alloc)) {
+		err = -ENOMEM;
+		new_reg->alloc = NULL;
+		WARN_ON(1);
+		goto out_no_alloc;
+	}
+
+	new_reg->alloc->imported.kctx = kctx;
+	new_reg->flags &= ~KBASE_REG_FREE;
+	new_reg->flags |= KBASE_REG_CPU_CACHED;
+
+	/* alloc now owns the tb */
+	owns_tb = 0;
+
+	if (MALI_ERROR_NONE != kbase_add_va_region(kctx, new_reg, vma->vm_start, nr_pages, 1)) {
+		err = -ENOMEM;
+		WARN_ON(1);
+		goto out_no_va_region;
+	}
+
+	*reg = new_reg;
+
+	/* map read only, noexec */
+	vma->vm_flags &= ~(VM_WRITE | VM_MAYWRITE | VM_EXEC | VM_MAYEXEC);
+	/* the rest of the flags is added by the cpu_mmap handler */
+
+	dev_dbg(kctx->kbdev->dev, "%s done\n", __func__);
+	return 0;
+
+out_no_va_region:
+out_no_alloc:
+	kbase_free_alloced_region(new_reg);
+out_no_region:
+	if (owns_tb) {
+		kbase_device_trace_buffer_uninstall(kctx);
+		vfree(tb);
+	}
+out:
+	return err;
+
+}
+
+static int kbase_mmu_dump_mmap(kbase_context *kctx, struct vm_area_struct *vma, struct kbase_va_region **const reg, void **const kmap_addr)
+{
+	struct kbase_va_region *new_reg;
+	void *kaddr;
+	u32 nr_pages;
+	size_t size;
+	int err = 0;
+
+	dev_dbg(kctx->kbdev->dev, "in kbase_mmu_dump_mmap\n");
+	size = (vma->vm_end - vma->vm_start);
+	nr_pages = size >> PAGE_SHIFT;
+
+	kaddr = kbase_mmu_dump(kctx, nr_pages);
+
+	if (!kaddr) {
+		err = -ENOMEM;
+		goto out;
+	}
+
+	new_reg = kbase_alloc_free_region(kctx, 0, nr_pages, KBASE_REG_ZONE_SAME_VA);
+	if (!new_reg) {
+		err = -ENOMEM;
+		WARN_ON(1);
+		goto out;
+	}
+
+	new_reg->alloc = kbase_alloc_create(0, KBASE_MEM_TYPE_RAW);
+	if (IS_ERR_OR_NULL(new_reg->alloc)) {
+		err = -ENOMEM;
+		new_reg->alloc = NULL;
+		WARN_ON(1);
+		goto out_no_alloc;
+	}
+
+	new_reg->flags &= ~KBASE_REG_FREE;
+	new_reg->flags |= KBASE_REG_CPU_CACHED;
+	if (MALI_ERROR_NONE != kbase_add_va_region(kctx, new_reg, vma->vm_start, nr_pages, 1)) {
+		err = -ENOMEM;
+		WARN_ON(1);
+		goto out_va_region;
+	}
+
+	*kmap_addr = kaddr;
+	*reg = new_reg;
+
+	dev_dbg(kctx->kbdev->dev, "kbase_mmu_dump_mmap done\n");
+	return 0;
+
+out_no_alloc:
+out_va_region:
+	kbase_free_alloced_region(new_reg);
+out:
+	return err;
+}
+
+
+void kbase_os_mem_map_lock(kbase_context *kctx)
+{
+	struct mm_struct *mm = current->mm;
+	(void)kctx;
+	down_read(&mm->mmap_sem);
+}
+
+void kbase_os_mem_map_unlock(kbase_context *kctx)
+{
+	struct mm_struct *mm = current->mm;
+	(void)kctx;
+	up_read(&mm->mmap_sem);
+}
+
+int kbase_mmap(struct file *file, struct vm_area_struct *vma)
+{
+	kbase_context *kctx = file->private_data;
+	struct kbase_va_region *reg;
+	void *kaddr = NULL;
+	size_t nr_pages;
+	int err = 0;
+	int free_on_close = 0;
+	struct device *dev = kctx->kbdev->dev;
+	size_t aligned_offset = 0;
+
+	dev_dbg(dev, "kbase_mmap\n");
+	nr_pages = (vma->vm_end - vma->vm_start) >> PAGE_SHIFT;
+
+	/* strip away corresponding VM_MAY% flags to the VM_% flags requested */
+	vma->vm_flags &= ~((vma->vm_flags & (VM_READ | VM_WRITE)) << 4);
+
+	if (0 == nr_pages) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	if (!(vma->vm_flags & VM_SHARED)) {
+		err = -EINVAL;
+		goto out;
+	}
+
+	kbase_gpu_vm_lock(kctx);
+
+	if (vma->vm_pgoff == PFN_DOWN(BASE_MEM_MAP_TRACKING_HANDLE)) {
+		/* The non-mapped tracking helper page */
+		err = kbase_tracking_page_setup(kctx, vma);
+		goto out_unlock;
+	}
+
+	/* if not the MTP, verify that the MTP has been mapped */
+	rcu_read_lock();
+	/* catches both when the special page isn't present or
+	 * when we've forked */
+	if (rcu_dereference(kctx->process_mm) != current->mm) {
+		err = -EINVAL;
+		rcu_read_unlock();
+		goto out_unlock;
+	}
+	rcu_read_unlock();
+
+	switch (vma->vm_pgoff) {
+	case PFN_DOWN(BASE_MEM_INVALID_HANDLE):
+	case PFN_DOWN(BASE_MEM_WRITE_ALLOC_PAGES_HANDLE):
+		/* Illegal handle for direct map */
+		err = -EINVAL;
+		goto out_unlock;
+	case PFN_DOWN(BASE_MEM_TRACE_BUFFER_HANDLE):
+		err = kbase_trace_buffer_mmap(kctx, vma, &reg, &kaddr);
+		if (0 != err)
+			goto out_unlock;
+		dev_dbg(dev, "kbase_trace_buffer_mmap ok\n");
+		/* free the region on munmap */
+		free_on_close = 1;
+		goto map;
+	case PFN_DOWN(BASE_MEM_MMU_DUMP_HANDLE):
+		/* MMU dump */
+		err = kbase_mmu_dump_mmap(kctx, vma, &reg, &kaddr);
+		if (0 != err)
+			goto out_unlock;
+		/* free the region on munmap */
+		free_on_close = 1;
+		goto map;
+	case PFN_DOWN(BASE_MEM_COOKIE_BASE) ...
+	     PFN_DOWN(BASE_MEM_FIRST_FREE_ADDRESS) - 1: {
+		/* SAME_VA stuff, fetch the right region */
+		int gpu_pc_bits;
+		int cookie = vma->vm_pgoff - PFN_DOWN(BASE_MEM_COOKIE_BASE);
+		gpu_pc_bits = kctx->kbdev->gpu_props.props.core_props.log2_program_counter_size;
+		reg = kctx->pending_regions[cookie];
+		if (NULL != reg) {
+
+			if (reg->flags & KBASE_REG_ALIGNED) {
+				/* nr_pages must be able to hold alignment pages
+				 * plus actual pages */
+				if (nr_pages != ((1UL << gpu_pc_bits >>
+							PAGE_SHIFT) +
+							reg->nr_pages)) {
+					/* incorrect mmap size */
+					/* leave the cookie for a potential
+					 * later mapping, or to be reclaimed
+					 * later when the context is freed */
+					err = -ENOMEM;
+					goto out_unlock;
+				}
+
+				aligned_offset = (vma->vm_start +
+						  (1UL << gpu_pc_bits) - 1) &
+						 ~((1UL << gpu_pc_bits) - 1);
+				aligned_offset -= vma->vm_start;
+			} else if (reg->nr_pages != nr_pages) {
+				/* incorrect mmap size */
+				/* leave the cookie for a potential later
+				 * mapping, or to be reclaimed later when the
+				 * context is freed */
+				err = -ENOMEM;
+				goto out_unlock;
+			}
+
+			if ((vma->vm_flags & VM_READ &&
+			     !(reg->flags & KBASE_REG_CPU_RD)) ||
+			    (vma->vm_flags & VM_WRITE &&
+			     !(reg->flags & KBASE_REG_CPU_WR))) {
+				/* VM flags inconsistent with region flags */
+				err = -EPERM;
+				dev_err(dev, "%s:%d inconsistent VM flags\n",
+					__FILE__, __LINE__);
+				goto out_unlock;
+			}
+
+			/* adjust down nr_pages to what we have physically */
+			nr_pages = kbase_reg_current_backed_size(reg);
+
+			if (MALI_ERROR_NONE != kbase_gpu_mmap(kctx, reg,
+							      vma->vm_start +
+							      aligned_offset,
+							      reg->nr_pages,
+							      1)) {
+				dev_err(dev, "%s:%d\n", __FILE__, __LINE__);
+				/* Unable to map in GPU space. */
+				WARN_ON(1);
+				err = -ENOMEM;
+				goto out_unlock;
+			}
+
+			/* no need for the cookie anymore */
+			kctx->pending_regions[cookie] = NULL;
+			kctx->cookies |= (1UL << cookie);
+
+			/*
+			 * Overwrite the offset with the
+			 * region start_pfn, so we effectively
+			 * map from offset 0 in the region.
+			 */
+			vma->vm_pgoff = reg->start_pfn;
+
+			/* free the region on munmap */
+			free_on_close = 1;
+			goto map;
+		}
+
+		err = -ENOMEM;
+		goto out_unlock;
+	}
+	default: {
+		reg = kbase_region_tracker_find_region_enclosing_address(kctx, (u64)vma->vm_pgoff << PAGE_SHIFT);
+
+		if (reg && !(reg->flags & KBASE_REG_FREE)) {
+			/* will this mapping overflow the size of the region? */
+			if (nr_pages > (reg->nr_pages - (vma->vm_pgoff - reg->start_pfn)))
+				goto overflow;
+
+			if ((vma->vm_flags & VM_READ &&
+			     !(reg->flags & KBASE_REG_CPU_RD)) ||
+			    (vma->vm_flags & VM_WRITE &&
+			     !(reg->flags & KBASE_REG_CPU_WR))) {
+				/* VM flags inconsistent with region flags */
+				err = -EPERM;
+				dev_err(dev, "%s:%d inconsistent VM flags\n",
+					__FILE__, __LINE__);
+				goto out_unlock;
+			}
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+			if (reg->alloc->type == KBASE_MEM_TYPE_IMPORTED_UMM)
+				goto dma_map;
+#endif /* CONFIG_DMA_SHARED_BUFFER */
+
+			/* limit what we map to the amount currently backed */
+			if (reg->alloc->nents < (vma->vm_pgoff - reg->start_pfn + nr_pages)) {
+				if ((vma->vm_pgoff - reg->start_pfn) >= reg->alloc->nents)
+					nr_pages = 0;
+				else
+					nr_pages = reg->alloc->nents - (vma->vm_pgoff - reg->start_pfn);
+			}
+
+			goto map;
+		}
+
+overflow:
+		err = -ENOMEM;
+		goto out_unlock;
+	} /* default */
+	} /* switch */
+map:
+	err = kbase_cpu_mmap(reg, vma, kaddr, nr_pages, aligned_offset, free_on_close);
+
+	if (vma->vm_pgoff == PFN_DOWN(BASE_MEM_MMU_DUMP_HANDLE)) {
+		/* MMU dump - userspace should now have a reference on
+		 * the pages, so we can now free the kernel mapping */
+		vfree(kaddr);
+	}
+	goto out_unlock;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+dma_map:
+	err = dma_buf_mmap(reg->alloc->imported.umm.dma_buf, vma, vma->vm_pgoff - reg->start_pfn);
+#endif				/* CONFIG_DMA_SHARED_BUFFER */
+out_unlock:
+	kbase_gpu_vm_unlock(kctx);
+out:
+	if (err)
+		dev_err(dev, "mmap failed %d\n", err);
+
+	return err;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmap)
+
+void kbasep_os_process_page_usage_update( kbase_context *kctx, int pages )
+{
+	struct mm_struct *mm;
+
+	rcu_read_lock();
+	mm = rcu_dereference(kctx->process_mm);
+	if (mm)
+	{
+		atomic_add(pages, &kctx->nonmapped_pages);
+#ifdef SPLIT_RSS_COUNTING
+		add_mm_counter(mm, MM_FILEPAGES, pages);
+#else
+		spin_lock(&mm->page_table_lock);
+		add_mm_counter(mm, MM_FILEPAGES, pages);
+		spin_unlock(&mm->page_table_lock);
+#endif
+	}
+	rcu_read_unlock();
+}
+
+static void kbasep_os_process_page_usage_drain(kbase_context * kctx)
+{
+	int pages;
+	struct mm_struct * mm;
+
+	spin_lock(&kctx->mm_update_lock);
+	mm = rcu_dereference_protected(kctx->process_mm, lockdep_is_held(&kctx->mm_update_lock));
+	if (!mm)
+	{
+		spin_unlock(&kctx->mm_update_lock);
+		return;
+	}
+
+	rcu_assign_pointer(kctx->process_mm, NULL);
+	spin_unlock(&kctx->mm_update_lock);
+	synchronize_rcu();
+
+	pages = atomic_xchg(&kctx->nonmapped_pages, 0);
+#ifdef SPLIT_RSS_COUNTING
+	add_mm_counter(mm, MM_FILEPAGES, -pages);
+#else
+	spin_lock(&mm->page_table_lock);
+	add_mm_counter(mm, MM_FILEPAGES, -pages);
+	spin_unlock(&mm->page_table_lock);
+#endif
+}
+
+static void kbase_special_vm_close(struct vm_area_struct *vma)
+{
+	kbase_context * kctx;
+	kctx = vma->vm_private_data;
+	kbasep_os_process_page_usage_drain(kctx);
+}
+
+static const struct vm_operations_struct kbase_vm_special_ops = {
+	.close = kbase_special_vm_close,
+};
+
+static int kbase_tracking_page_setup(struct kbase_context * kctx, struct vm_area_struct * vma)
+{
+	/* check that this is the only tracking page */
+	spin_lock(&kctx->mm_update_lock);
+	if (rcu_dereference_protected(kctx->process_mm, lockdep_is_held(&kctx->mm_update_lock)))
+	{
+		spin_unlock(&kctx->mm_update_lock);
+		return -EFAULT;
+	}
+
+	rcu_assign_pointer(kctx->process_mm, current->mm);
+
+	spin_unlock(&kctx->mm_update_lock);
+
+	/* no real access */
+	vma->vm_flags &= ~(VM_READ | VM_MAYREAD | VM_WRITE | VM_MAYWRITE | VM_EXEC | VM_MAYEXEC);
+	vma->vm_flags |= VM_DONTCOPY | VM_DONTEXPAND | VM_DONTDUMP | VM_IO;
+	vma->vm_ops = &kbase_vm_special_ops;
+	vma->vm_private_data = kctx;
+
+	return 0;
+}
+void *kbase_va_alloc(kbase_context *kctx, u32 size, kbase_hwc_dma_mapping *handle)
+{
+	int i;
+	int res;
+	void *va;
+	dma_addr_t  dma_pa;
+	struct kbase_va_region *reg;
+	phys_addr_t *page_array;
+	DEFINE_DMA_ATTRS(attrs);
+
+	u32 pages = ((size - 1) >> PAGE_SHIFT) + 1;
+	u32 flags = BASE_MEM_PROT_CPU_RD | BASE_MEM_PROT_CPU_WR |
+		    BASE_MEM_PROT_GPU_RD | BASE_MEM_PROT_GPU_WR;
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(0 != size);
+	KBASE_DEBUG_ASSERT(0 != pages);
+
+	if (size == 0)
+		goto err;
+
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	va = dma_alloc_attrs(kctx->kbdev->dev, size, &dma_pa, GFP_KERNEL, &attrs);
+	if (!va)
+		goto err;
+
+	memset(va, 0x0, size);
+
+	/* Store the state so we can free it later. */
+	handle->cpu_va = va;
+	handle->dma_pa = dma_pa;
+	handle->size   = size;
+
+
+	reg = kbase_alloc_free_region(kctx, 0, pages, KBASE_REG_ZONE_SAME_VA);
+	if (!reg)
+		goto no_reg;
+
+	reg->flags &= ~KBASE_REG_FREE;
+	kbase_update_region_flags(reg, flags);
+
+	reg->alloc = kbase_alloc_create(pages, KBASE_MEM_TYPE_RAW);
+	if (IS_ERR_OR_NULL(reg->alloc))
+		goto no_alloc;
+
+	page_array = kbase_get_phy_pages(reg);
+
+	for (i = 0; i < pages; i++) {
+		page_array[i] = dma_pa + (i << PAGE_SHIFT);
+	}
+
+	reg->alloc->nents = pages;
+
+	kbase_gpu_vm_lock(kctx);
+	res = kbase_gpu_mmap(kctx, reg, (uintptr_t) va, pages, 1);
+	kbase_gpu_vm_unlock(kctx);
+	if (res)
+		goto no_mmap;
+
+	return va;
+
+no_mmap:
+	kbase_mem_phy_alloc_put(reg->alloc);
+no_alloc:
+	kfree(reg);
+no_reg:
+	dma_free_attrs(kctx->kbdev->dev, size, va, dma_pa, &attrs);
+err:
+	return NULL;
+}
+KBASE_EXPORT_SYMBOL(kbase_va_alloc);
+
+void kbase_va_free(kbase_context *kctx, kbase_hwc_dma_mapping *handle)
+{
+	struct kbase_va_region *reg;
+	mali_error err;
+	DEFINE_DMA_ATTRS(attrs);
+
+	KBASE_DEBUG_ASSERT(kctx != NULL);
+	KBASE_DEBUG_ASSERT(handle->cpu_va != NULL);
+
+	kbase_gpu_vm_lock(kctx);
+	reg = kbase_region_tracker_find_region_base_address(kctx, (uintptr_t)handle->cpu_va);
+	KBASE_DEBUG_ASSERT(reg);
+	err = kbase_gpu_munmap(kctx, reg);
+	kbase_gpu_vm_unlock(kctx);
+	KBASE_DEBUG_ASSERT(err == MALI_ERROR_NONE);
+
+	kbase_mem_phy_alloc_put(reg->alloc);
+	kfree(reg);
+
+	dma_set_attr(DMA_ATTR_WRITE_COMBINE, &attrs);
+	dma_free_attrs(kctx->kbdev->dev, handle->size,
+			handle->cpu_va, handle->dma_pa, &attrs);
+}
+KBASE_EXPORT_SYMBOL(kbase_va_free);
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.h
new file mode 100644
index 0000000..e7482a5
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_linux.h
@@ -0,0 +1,67 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mem_linux.h
+ * Base kernel memory APIs, Linux implementation.
+ */
+
+#ifndef _KBASE_MEM_LINUX_H_
+#define _KBASE_MEM_LINUX_H_
+
+/* This define is used by the gator kernel module compile to select which DDK
+ * API calling convention to use. If not defined (legacy DDK) gator assumes
+ * version 1. The version to DDK release mapping is:
+ *     Version 1 API: DDK versions r1px, r2px
+ *     Version 2 API: DDK versions r3px and newer
+ **/
+#define MALI_DDK_GATOR_API_VERSION 2
+
+/** A HWC dump mapping */
+typedef struct kbase_hwc_dma_mapping {
+	void       *cpu_va;
+	dma_addr_t  dma_pa;
+	size_t      size;
+} kbase_hwc_dma_mapping;
+
+struct kbase_va_region * kbase_mem_alloc(kbase_context * kctx, u64 va_pages, u64 commit_pages, u64 extent, u64 * flags, u64 * gpu_va, u16 * va_alignment);
+mali_error kbase_mem_query(kbase_context *kctx, mali_addr64 gpu_addr, int query, u64 * const pages);
+int kbase_mem_import(kbase_context *kctx, base_mem_import_type type, int handle, mali_addr64 * gpu_va, u64 * va_pages, u64 * flags);
+u64 kbase_mem_alias(kbase_context *kctx, u64* flags, u64 stride, u64 nents, struct base_mem_aliasing_info* ai, u64 * num_pages);
+mali_error kbase_mem_flags_change(kbase_context *kctx, mali_addr64 gpu_addr, unsigned int flags, unsigned int mask);
+int kbase_mem_commit(kbase_context * kctx, mali_addr64 gpu_addr, u64 new_pages, base_backing_threshold_status * failure_reason);
+int kbase_mmap(struct file *file, struct vm_area_struct *vma);
+
+/** @brief Allocate memory from kernel space and map it onto the GPU
+ *
+ * @param kctx   The context used for the allocation/mapping
+ * @param size   The size of the allocation in bytes
+ * @param handle An opaque structure used to contain the state needed to free the memory
+ * @return the VA for kernel space and GPU MMU
+ */
+void *kbase_va_alloc(kbase_context *kctx, u32 size, kbase_hwc_dma_mapping *handle);
+
+/** @brief Free/unmap memory allocated by kbase_va_alloc
+ *
+ * @param kctx   The context used for the allocation/mapping
+ * @param handle An opaque structure returned by the kbase_va_alloc function.
+ */
+void kbase_va_free(kbase_context *kctx, kbase_hwc_dma_mapping *handle);
+
+#endif				/* _KBASE_MEM_LINUX_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.c
new file mode 100644
index 0000000..62e5c9f
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.c
@@ -0,0 +1,62 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+
+#include <linux/io.h>
+#include <linux/mm.h>
+#include <linux/highmem.h>
+#include <linux/dma-mapping.h>
+#include <linux/mutex.h>
+#include <asm/cacheflush.h>
+
+void kbase_sync_to_memory(phys_addr_t paddr, void *vaddr, size_t sz)
+{
+#ifdef CONFIG_ARM
+	__cpuc_flush_dcache_area(vaddr, sz);
+	outer_flush_range(paddr, paddr + sz);
+#elif defined(CONFIG_ARM64)
+	/* TODO (MID64-46): There's no other suitable cache flush function for ARM64 */
+	flush_cache_all();
+#elif defined(CONFIG_X86)
+	struct scatterlist scl = { 0, };
+	sg_set_page(&scl, pfn_to_page(PFN_DOWN(paddr)), sz, paddr & (PAGE_SIZE - 1));
+	dma_sync_sg_for_cpu(NULL, &scl, 1, DMA_TO_DEVICE);
+	mb();			/* for outer_sync (if needed) */
+#else
+#error Implement cache maintenance for your architecture here
+#endif
+}
+
+void kbase_sync_to_cpu(phys_addr_t paddr, void *vaddr, size_t sz)
+{
+#ifdef CONFIG_ARM
+	__cpuc_flush_dcache_area(vaddr, sz);
+	outer_flush_range(paddr, paddr + sz);
+#elif defined(CONFIG_ARM64)
+	/* TODO (MID64-46): There's no other suitable cache flush function for ARM64 */
+	flush_cache_all();
+#elif defined(CONFIG_X86)
+	struct scatterlist scl = { 0, };
+	sg_set_page(&scl, pfn_to_page(PFN_DOWN(paddr)), sz, paddr & (PAGE_SIZE - 1));
+	dma_sync_sg_for_cpu(NULL, &scl, 1, DMA_FROM_DEVICE);
+#else
+#error Implement cache maintenance for your architecture here
+#endif
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.h
new file mode 100644
index 0000000..c88a3f1
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mem_lowlevel.h
@@ -0,0 +1,111 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_MEM_LOWLEVEL_H
+#define _KBASE_MEM_LOWLEVEL_H
+
+#ifndef _KBASE_H_
+#error "Don't include this file directly, use mali_kbase.h instead"
+#endif
+
+/**
+ * @brief Flags for kbase_phy_allocator_pages_alloc
+ */
+#define KBASE_PHY_PAGES_FLAG_DEFAULT (0)	/** Default allocation flag */
+#define KBASE_PHY_PAGES_FLAG_CLEAR   (1 << 0)	/** Clear the pages after allocation */
+#define KBASE_PHY_PAGES_FLAG_POISON  (1 << 1)	/** Fill the memory with a poison value */
+
+#define KBASE_PHY_PAGES_SUPPORTED_FLAGS (KBASE_PHY_PAGES_FLAG_DEFAULT|KBASE_PHY_PAGES_FLAG_CLEAR|KBASE_PHY_PAGES_FLAG_POISON)
+
+#define KBASE_PHY_PAGES_POISON_VALUE  0xFD /** Value to fill the memory with when KBASE_PHY_PAGES_FLAG_POISON is set */
+
+/**
+ * A pointer to a cache synchronization function, either kbase_sync_to_cpu()
+ * or kbase_sync_to_memory().
+ */
+typedef void (*kbase_sync_kmem_fn) (phys_addr_t, void *, size_t);
+
+/**
+ * @brief Synchronize a memory area for other system components usage
+ *
+ * Performs the necessary memory coherency operations on a given memory area,
+ * such that after the call, changes in memory are correctly seen by other
+ * system components. Any change made to memory after that call may not be seen
+ * by other system components.
+ *
+ * In effect:
+ * - all CPUs will perform a cache clean operation on their inner & outer data caches
+ * - any write buffers are drained (including that of outer cache controllers)
+ *
+ * This function waits until all operations have completed.
+ *
+ * The area is restricted to one page or less and must not cross a page boundary.
+ * The offset within the page is aligned to cache line size and size is ensured
+ * to be a multiple of the cache line size.
+ *
+ * Both physical and virtual address of the area need to be provided to support OS
+ * cache flushing APIs that either use the virtual or the physical address. When
+ * called from OS specific code it is allowed to only provide the address that
+ * is actually used by the specific OS and leave the other address as 0.
+ *
+ * @param[in] paddr  physical address
+ * @param[in] vaddr  CPU virtual address valid in the current user VM or the kernel VM
+ * @param[in] sz     size of the area, <= PAGE_SIZE.
+ */
+void kbase_sync_to_memory(phys_addr_t paddr, void *vaddr, size_t sz);
+
+/**
+ * @brief Synchronize a memory area for CPU usage
+ *
+ * Performs the necessary memory coherency operations on a given memory area,
+ * such that after the call, changes in memory are correctly seen by any CPU.
+ * Any change made to this area by any CPU before this call may be lost.
+ *
+ * In effect:
+ * - all CPUs will perform a cache clean & invalidate operation on their inner &
+ *   outer data caches.
+ *
+ * @note Stricly only an invalidate operation is required but by cleaning the cache
+ * too we prevent loosing changes made to the memory area due to software bugs. By
+ * having these changes cleaned from the cache it allows us to catch the memory
+ * area getting corrupted with the help of watch points. In correct operation the
+ * clean & invalidate operation would not be more expensive than an invalidate
+ * operation. Also note that for security reasons, it is dangerous to expose a
+ * cache 'invalidate only' operation to user space.
+ *
+ * - any read buffers are flushed (including that of outer cache controllers)
+ *
+ * This function waits until all operations have completed.
+ *
+ * The area is restricted to one page or less and must not cross a page boundary.
+ * The offset within the page is aligned to cache line size and size is ensured
+ * to be a multiple of the cache line size.
+ *
+ * Both physical and virtual address of the area need to be provided to support OS
+ * cache flushing APIs that either use the virtual or the physical address. When
+ * called from OS specific code it is allowed to only provide the address that
+ * is actually used by the specific OS and leave the other address as 0.
+ *
+ * @param[in] paddr  physical address
+ * @param[in] vaddr  CPU virtual address valid in the current user VM or the kernel VM
+ * @param[in] sz     size of the area, <= PAGE_SIZE.
+ */
+void kbase_sync_to_cpu(phys_addr_t paddr, void *vaddr, size_t sz);
+
+#endif				/* _KBASE_LOWLEVEL_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mmu.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mmu.c
new file mode 100644
index 0000000..17e4e08
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_mmu.c
@@ -0,0 +1,1681 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_mmu.c
+ * Base kernel MMU management.
+ */
+
+/* #define DEBUG    1 */
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_debug.h>
+
+#define beenthere(kctx, f, a...)  dev_dbg(kctx->kbdev->dev, "%s:" f, __func__, ##a)
+
+#include <mali_kbase_defs.h>
+#include <mali_kbase_hw.h>
+
+#define KBASE_MMU_PAGE_ENTRIES 512
+
+/*
+ * Definitions:
+ * - PGD: Page Directory.
+ * - PTE: Page Table Entry. A 64bit value pointing to the next
+ *        level of translation
+ * - ATE: Address Transation Entry. A 64bit value pointing to
+ *        a 4kB physical page.
+ */
+
+static void kbase_mmu_report_fault_and_kill(kbase_context *kctx, kbase_as *as);
+static u64 lock_region(kbase_device *kbdev, u64 pfn, size_t num_pages);
+
+/* Helper Function to perform assignment of page table entries, to ensure the use of
+ * strd, which is required on LPAE systems.
+ */
+
+static inline void page_table_entry_set( kbase_device * kbdev, u64 * pte, u64 phy )
+{
+#ifdef CONFIG_64BIT
+	*pte = phy;
+#elif defined(CONFIG_ARM)
+	/*
+	 *
+	 * In order to prevent the compiler keeping cached copies of memory, we have to explicitly
+	 * say that we have updated memory.
+	 *
+	 * Note: We could manually move the data ourselves into R0 and R1 by specifying
+	 * register variables that are explicitly given registers assignments, the down side of
+	 * this is that we have to assume cpu endianess.  To avoid this we can use the ldrd to read the
+	 * data from memory into R0 and R1 which will respect the cpu endianess, we then use strd to
+	 * make the 64 bit assignment to the page table entry.
+	 *
+	 */
+
+	asm	volatile("ldrd r0, r1, [%[ptemp]]\n\t"
+				"strd r0, r1, [%[pte]]\n\t"
+				: "=m" (*pte)
+				: [ptemp] "r" (&phy), [pte] "r" (pte), "m" (phy)
+				: "r0", "r1" );
+#else
+#error "64-bit atomic write must be implemented for your architecture"
+#endif
+}
+
+static void ksync_kern_vrange_gpu(phys_addr_t paddr, void *vaddr, size_t size)
+{
+	kbase_sync_to_memory(paddr, vaddr, size);
+}
+
+static size_t make_multiple(size_t minimum, size_t multiple)
+{
+	size_t remainder = minimum % multiple;
+	if (remainder == 0)
+		return minimum;
+	else
+		return minimum + multiple - remainder;
+}
+
+static void mmu_mask_reenable(kbase_device *kbdev, kbase_context *kctx, kbase_as *as)
+{
+	unsigned long flags;
+	u32 mask;
+	spin_lock_irqsave(&kbdev->mmu_mask_change, flags);
+	mask = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK), kctx);
+	mask |= ((1UL << as->number) | (1UL << (MMU_REGS_BUS_ERROR_FLAG(as->number))));
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), mask, kctx);
+	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
+}
+
+static void page_fault_worker(struct work_struct *data)
+{
+	u64 fault_pfn;
+	size_t new_pages;
+	size_t fault_rel_pfn;
+	kbase_as *faulting_as;
+	int as_no;
+	kbase_context *kctx;
+	kbase_device *kbdev;
+	kbase_va_region *region;
+	mali_error err;
+
+	faulting_as = container_of(data, kbase_as, work_pagefault);
+	fault_pfn = faulting_as->fault_addr >> PAGE_SHIFT;
+	as_no = faulting_as->number;
+
+	kbdev = container_of(faulting_as, kbase_device, as[as_no]);
+
+	/* Grab the context that was already refcounted in kbase_mmu_interrupt().
+	 * Therefore, it cannot be scheduled out of this AS until we explicitly release it
+	 *
+	 * NOTE: NULL can be returned here if we're gracefully handling a spurious interrupt */
+	kctx = kbasep_js_runpool_lookup_ctx_noretain(kbdev, as_no);
+
+	if (kctx == NULL) {
+		/* Only handle this if not already suspended */
+		if ( !kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE)) {
+			/* Address space has no context, terminate the work */
+			u32 reg;
+
+			/* AS transaction begin */
+			mutex_lock(&faulting_as->transaction_mutex);
+			reg = kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), NULL);
+			reg = (reg & (~(u32) MMU_TRANSTAB_ADRMODE_MASK)) | ASn_TRANSTAB_ADRMODE_UNMAPPED;
+			kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), reg, NULL);
+			kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_UPDATE, NULL);
+			mutex_unlock(&faulting_as->transaction_mutex);
+			/* AS transaction end */
+
+			mmu_mask_reenable(kbdev, NULL, faulting_as);
+			kbase_pm_context_idle(kbdev);
+		}
+		return;
+	}
+
+	KBASE_DEBUG_ASSERT(kctx->kbdev == kbdev);
+
+	kbase_gpu_vm_lock(kctx);
+
+	/* find the region object for this VA */
+	region = kbase_region_tracker_find_region_enclosing_address(kctx, faulting_as->fault_addr);
+	if (NULL == region || (GROWABLE_FLAGS_REQUIRED != (region->flags & GROWABLE_FLAGS_MASK))) {
+		kbase_gpu_vm_unlock(kctx);
+		/* failed to find the region or mismatch of the flags */
+		kbase_mmu_report_fault_and_kill(kctx, faulting_as);
+		goto fault_done;
+	}
+
+	if ((((faulting_as->fault_status & ASn_FAULTSTATUS_ACCESS_TYPE_MASK) == ASn_FAULTSTATUS_ACCESS_TYPE_READ) && !(region->flags & KBASE_REG_GPU_RD)) || (((faulting_as->fault_status & ASn_FAULTSTATUS_ACCESS_TYPE_MASK) == ASn_FAULTSTATUS_ACCESS_TYPE_WRITE) && !(region->flags & KBASE_REG_GPU_WR)) || (((faulting_as->fault_status & ASn_FAULTSTATUS_ACCESS_TYPE_MASK) == ASn_FAULTSTATUS_ACCESS_TYPE_EX) && (region->flags & KBASE_REG_GPU_NX))) {
+		dev_warn(kbdev->dev, "Access permissions don't match: region->flags=0x%lx", region->flags);
+		kbase_gpu_vm_unlock(kctx);
+		kbase_mmu_report_fault_and_kill(kctx, faulting_as);
+		goto fault_done;
+	}
+
+	/* find the size we need to grow it by */
+	/* we know the result fit in a size_t due to kbase_region_tracker_find_region_enclosing_address
+	 * validating the fault_adress to be within a size_t from the start_pfn */
+	fault_rel_pfn = fault_pfn - region->start_pfn;
+
+	if (fault_rel_pfn < kbase_reg_current_backed_size(region)) {
+		dev_warn(kbdev->dev, "Page fault in allocated region of growable TMEM: Ignoring");
+		mmu_mask_reenable(kbdev, kctx, faulting_as);
+		kbase_gpu_vm_unlock(kctx);
+		goto fault_done;
+	}
+
+	new_pages = make_multiple(fault_rel_pfn - kbase_reg_current_backed_size(region) + 1, region->extent);
+	if (new_pages + kbase_reg_current_backed_size(region) > region->nr_pages) {
+		/* cap to max vsize */
+		new_pages = region->nr_pages - kbase_reg_current_backed_size(region);
+	}
+
+	if (0 == new_pages) {
+		/* Duplicate of a fault we've already handled, nothing to do */
+		mmu_mask_reenable(kbdev, kctx, faulting_as);
+		kbase_gpu_vm_unlock(kctx);
+		goto fault_done;
+	}
+
+	if (MALI_ERROR_NONE == kbase_alloc_phy_pages_helper(region->alloc, new_pages)) {
+		/* alloc success */
+		mali_addr64 lock_addr;
+		KBASE_DEBUG_ASSERT(kbase_reg_current_backed_size(region) <= region->nr_pages);
+
+		/* AS transaction begin */
+		mutex_lock(&faulting_as->transaction_mutex);
+
+		/* Lock the VA region we're about to update */
+		lock_addr = lock_region(kbdev, faulting_as->fault_addr >> PAGE_SHIFT, new_pages);
+		kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_LOCKADDR_LO), lock_addr & 0xFFFFFFFFUL, kctx);
+		kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_LOCKADDR_HI), lock_addr >> 32, kctx);
+		kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_LOCK, kctx);
+
+		/* set up the new pages */
+		err = kbase_mmu_insert_pages(kctx, region->start_pfn + kbase_reg_current_backed_size(region) - new_pages, &kbase_get_phy_pages(region)[kbase_reg_current_backed_size(region) - new_pages], new_pages, region->flags);
+		if (MALI_ERROR_NONE != err) {
+			/* failed to insert pages, handle as a normal PF */
+			mutex_unlock(&faulting_as->transaction_mutex);
+			kbase_free_phy_pages_helper(region->alloc, new_pages);
+			kbase_gpu_vm_unlock(kctx);
+			/* The locked VA region will be unlocked and the cache invalidated in here */
+			kbase_mmu_report_fault_and_kill(kctx, faulting_as);
+			goto fault_done;
+		}
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+		kbase_trace_mali_page_fault_insert_pages(as_no, new_pages);
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+
+		/* flush L2 and unlock the VA (resumes the MMU) */
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_6367))
+			kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_FLUSH, kctx);
+		else
+			kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_FLUSH_PT, kctx);
+
+		/* wait for the flush to complete */
+		while (kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_STATUS), kctx) & 1)
+			;
+
+		if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_9630)) {
+			/* Issue an UNLOCK command to ensure that valid page tables are re-read by the GPU after an update.
+			   Note that, the FLUSH command should perform all the actions necessary, however the bus logs show
+			   that if multiple page faults occur within an 8 page region the MMU does not always re-read the
+			   updated page table entries for later faults or is only partially read, it subsequently raises the
+			   page fault IRQ for the same addresses, the unlock ensures that the MMU cache is flushed, so updates
+			   can be re-read.  As the region is now unlocked we need to issue 2 UNLOCK commands in order to flush the
+			   MMU/uTLB, see PRLAM-8812.
+			 */
+			kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_UNLOCK, kctx);
+			kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_UNLOCK, kctx);
+		}
+
+		mutex_unlock(&faulting_as->transaction_mutex);
+		/* AS transaction end */
+
+		/* reenable this in the mask */
+		mmu_mask_reenable(kbdev, kctx, faulting_as);
+		kbase_gpu_vm_unlock(kctx);
+	} else {
+		/* failed to extend, handle as a normal PF */
+		kbase_gpu_vm_unlock(kctx);
+		kbase_mmu_report_fault_and_kill(kctx, faulting_as);
+	}
+
+ fault_done:
+	/* By this point, the fault was handled in some way, so release the ctx refcount */
+	kbasep_js_runpool_release_ctx(kbdev, kctx);
+}
+
+phys_addr_t kbase_mmu_alloc_pgd(kbase_context *kctx)
+{
+	phys_addr_t pgd;
+	u64 *page;
+	int i;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	kbase_atomic_add_pages(1, &kctx->used_pages);
+	kbase_atomic_add_pages(1, &kctx->kbdev->memdev.used_pages);
+
+	if (MALI_ERROR_NONE != kbase_mem_allocator_alloc(kctx->pgd_allocator, 1, &pgd))
+		goto sub_pages;
+
+	page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+	if (NULL == page)
+		goto alloc_free;
+
+	kbase_process_page_usage_inc(kctx, 1);
+
+	for (i = 0; i < KBASE_MMU_PAGE_ENTRIES; i++)
+		page_table_entry_set( kctx->kbdev, &page[i], ENTRY_IS_INVAL );
+
+	/* Clean the full page */
+	ksync_kern_vrange_gpu(pgd, page, KBASE_MMU_PAGE_ENTRIES * sizeof(u64));
+	kunmap(pfn_to_page(PFN_DOWN(pgd)));
+	return pgd;
+
+alloc_free:
+	kbase_mem_allocator_free(kctx->pgd_allocator, 1, &pgd, MALI_FALSE);
+sub_pages:
+	kbase_atomic_sub_pages(1, &kctx->used_pages);
+	kbase_atomic_sub_pages(1, &kctx->kbdev->memdev.used_pages);
+
+	return 0;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_alloc_pgd)
+
+static phys_addr_t mmu_pte_to_phy_addr(u64 entry)
+{
+	if (!(entry & 1))
+		return 0;
+
+	return entry & ~0xFFF;
+}
+
+static u64 mmu_phyaddr_to_pte(phys_addr_t phy)
+{
+	return (phy & ~0xFFF) | ENTRY_IS_PTE;
+}
+
+static u64 mmu_phyaddr_to_ate(phys_addr_t phy, u64 flags)
+{
+	return (phy & ~0xFFF) | (flags & ENTRY_FLAGS_MASK) | ENTRY_IS_ATE;
+}
+
+/* Given PGD PFN for level N, return PGD PFN for level N+1 */
+static phys_addr_t mmu_get_next_pgd(kbase_context *kctx, phys_addr_t pgd, u64 vpfn, int level)
+{
+	u64 *page;
+	phys_addr_t target_pgd;
+
+	KBASE_DEBUG_ASSERT(pgd);
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	/*
+	 * Architecture spec defines level-0 as being the top-most.
+	 * This is a bit unfortunate here, but we keep the same convention.
+	 */
+	vpfn >>= (3 - level) * 9;
+	vpfn &= 0x1FF;
+
+	page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+	if (NULL == page) {
+		dev_warn(kctx->kbdev->dev, "mmu_get_next_pgd: kmap failure\n");
+		return 0;
+	}
+
+	target_pgd = mmu_pte_to_phy_addr(page[vpfn]);
+
+	if (!target_pgd) {
+		target_pgd = kbase_mmu_alloc_pgd(kctx);
+		if (!target_pgd) {
+			dev_warn(kctx->kbdev->dev, "mmu_get_next_pgd: kbase_mmu_alloc_pgd failure\n");
+			kunmap(pfn_to_page(PFN_DOWN(pgd)));
+			return 0;
+		}
+
+		page_table_entry_set( kctx->kbdev, &page[vpfn], mmu_phyaddr_to_pte(target_pgd) );
+
+		ksync_kern_vrange_gpu(pgd + (vpfn * sizeof(u64)), page + vpfn, sizeof(u64));
+		/* Rely on the caller to update the address space flags. */
+	}
+
+	kunmap(pfn_to_page(PFN_DOWN(pgd)));
+	return target_pgd;
+}
+
+static phys_addr_t mmu_get_bottom_pgd(kbase_context *kctx, u64 vpfn)
+{
+	phys_addr_t pgd;
+	int l;
+
+	pgd = kctx->pgd;
+
+	for (l = MIDGARD_MMU_TOPLEVEL; l < 3; l++) {
+		pgd = mmu_get_next_pgd(kctx, pgd, vpfn, l);
+		/* Handle failure condition */
+		if (!pgd) {
+			dev_warn(kctx->kbdev->dev, "mmu_get_bottom_pgd: mmu_get_next_pgd failure\n");
+			return 0;
+		}
+	}
+
+	return pgd;
+}
+
+static phys_addr_t mmu_insert_pages_recover_get_next_pgd(kbase_context *kctx, phys_addr_t pgd, u64 vpfn, int level)
+{
+	u64 *page;
+	phys_addr_t target_pgd;
+
+	KBASE_DEBUG_ASSERT(pgd);
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	/*
+	 * Architecture spec defines level-0 as being the top-most.
+	 * This is a bit unfortunate here, but we keep the same convention.
+	 */
+	vpfn >>= (3 - level) * 9;
+	vpfn &= 0x1FF;
+
+	page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
+	/* kmap_atomic should NEVER fail */
+	KBASE_DEBUG_ASSERT(NULL != page);
+
+	target_pgd = mmu_pte_to_phy_addr(page[vpfn]);
+	/* As we are recovering from what has already been set up, we should have a target_pgd */
+	KBASE_DEBUG_ASSERT(0 != target_pgd);
+
+	kunmap_atomic(page);
+	return target_pgd;
+}
+
+static phys_addr_t mmu_insert_pages_recover_get_bottom_pgd(kbase_context *kctx, u64 vpfn)
+{
+	phys_addr_t pgd;
+	int l;
+
+	pgd = kctx->pgd;
+
+	for (l = MIDGARD_MMU_TOPLEVEL; l < 3; l++) {
+		pgd = mmu_insert_pages_recover_get_next_pgd(kctx, pgd, vpfn, l);
+		/* Should never fail */
+		KBASE_DEBUG_ASSERT(0 != pgd);
+	}
+
+	return pgd;
+}
+
+static void mmu_insert_pages_failure_recovery(kbase_context *kctx, u64 vpfn,
+					      size_t nr)
+{
+	phys_addr_t pgd;
+	u64 *pgd_page;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(0 != vpfn);
+	/* 64-bit address range is the max */
+	KBASE_DEBUG_ASSERT(vpfn <= (UINT64_MAX / PAGE_SIZE));
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	while (nr) {
+		unsigned int i;
+		unsigned int index = vpfn & 0x1FF;
+		unsigned int count = KBASE_MMU_PAGE_ENTRIES - index;
+
+		if (count > nr)
+			count = nr;
+
+		pgd = mmu_insert_pages_recover_get_bottom_pgd(kctx, vpfn);
+		KBASE_DEBUG_ASSERT(0 != pgd);
+
+		pgd_page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
+		KBASE_DEBUG_ASSERT(NULL != pgd_page);
+
+		/* Invalidate the entries we added */
+		for (i = 0; i < count; i++)
+			page_table_entry_set(kctx->kbdev, &pgd_page[index + i],
+					     ENTRY_IS_INVAL);
+
+		vpfn += count;
+		nr -= count;
+
+		ksync_kern_vrange_gpu(pgd + (index * sizeof(u64)),
+				      pgd_page + index, count * sizeof(u64));
+
+		kunmap_atomic(pgd_page);
+	}
+}
+
+/**
+ * Map KBASE_REG flags to MMU flags
+ */
+static u64 kbase_mmu_get_mmu_flags(unsigned long flags)
+{
+	u64 mmu_flags;
+
+	/* store mem_attr index as 4:2 (macro called ensures 3 bits already) */
+	mmu_flags = KBASE_REG_MEMATTR_VALUE(flags) << 2;
+
+	/* write perm if requested */
+	mmu_flags |= (flags & KBASE_REG_GPU_WR) ? ENTRY_WR_BIT : 0;
+	/* read perm if requested */
+	mmu_flags |= (flags & KBASE_REG_GPU_RD) ? ENTRY_RD_BIT : 0;
+	/* nx if requested */
+	mmu_flags |= (flags & KBASE_REG_GPU_NX) ? ENTRY_NX_BIT : 0;
+
+	if (flags & KBASE_REG_SHARE_BOTH) {
+		/* inner and outer shareable */
+		mmu_flags |= SHARE_BOTH_BITS;
+	} else if (flags & KBASE_REG_SHARE_IN) {
+		/* inner shareable coherency */
+		mmu_flags |= SHARE_INNER_BITS;
+	}
+
+	return mmu_flags;
+}
+
+/*
+ * Map the single page 'phys' 'nr' of times, starting at GPU PFN 'vpfn'
+ */
+mali_error kbase_mmu_insert_single_page(kbase_context *kctx, u64 vpfn,
+					phys_addr_t phys, size_t nr,
+					unsigned long flags)
+{
+	phys_addr_t pgd;
+	u64 *pgd_page;
+	u64 pte_entry;
+	/* In case the insert_single_page only partially completes we need to be
+	 * able to recover */
+	mali_bool recover_required = MALI_FALSE;
+	u64 recover_vpfn = vpfn;
+	size_t recover_count = 0;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(0 != vpfn);
+	/* 64-bit address range is the max */
+	KBASE_DEBUG_ASSERT(vpfn <= (UINT64_MAX / PAGE_SIZE));
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	/* the one entry we'll populate everywhere */
+	pte_entry = mmu_phyaddr_to_ate(phys, kbase_mmu_get_mmu_flags(flags));
+
+	while (nr) {
+		unsigned int i;
+		unsigned int index = vpfn & 0x1FF;
+		unsigned int count = KBASE_MMU_PAGE_ENTRIES - index;
+
+		if (count > nr)
+			count = nr;
+
+		/*
+		 * Repeatedly calling mmu_get_bottom_pte() is clearly
+		 * suboptimal. We don't have to re-parse the whole tree
+		 * each time (just cache the l0-l2 sequence).
+		 * On the other hand, it's only a gain when we map more than
+		 * 256 pages at once (on average). Do we really care?
+		 */
+		pgd = mmu_get_bottom_pgd(kctx, vpfn);
+		if (!pgd) {
+			dev_warn(kctx->kbdev->dev,
+					       "kbase_mmu_insert_pages: "
+					       "mmu_get_bottom_pgd failure\n");
+			if (recover_required) {
+				/* Invalidate the pages we have partially
+				 * completed */
+				mmu_insert_pages_failure_recovery(kctx,
+								  recover_vpfn,
+								  recover_count);
+			}
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+		if (!pgd_page) {
+			dev_warn(kctx->kbdev->dev,
+					       "kbase_mmu_insert_pages: "
+					       "kmap failure\n");
+			if (recover_required) {
+				/* Invalidate the pages we have partially
+				 * completed */
+				mmu_insert_pages_failure_recovery(kctx,
+								  recover_vpfn,
+								  recover_count);
+			}
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+
+		for (i = 0; i < count; i++) {
+			unsigned int ofs = index + i;
+			KBASE_DEBUG_ASSERT(0 == (pgd_page[ofs] & 1UL));
+			page_table_entry_set(kctx->kbdev, &pgd_page[ofs],
+					     pte_entry);
+		}
+
+		vpfn += count;
+		nr -= count;
+
+		ksync_kern_vrange_gpu(pgd + (index * sizeof(u64)),
+				      pgd_page + index, count * sizeof(u64));
+
+		kunmap(pfn_to_page(PFN_DOWN(pgd)));
+		/* We have started modifying the page table.
+		 * If further pages need inserting and fail we need to undo what
+		 * has already taken place */
+		recover_required = MALI_TRUE;
+		recover_count += count;
+	}
+	return MALI_ERROR_NONE;
+}
+
+/*
+ * Map 'nr' pages pointed to by 'phys' at GPU PFN 'vpfn'
+ */
+mali_error kbase_mmu_insert_pages(kbase_context *kctx, u64 vpfn,
+				  phys_addr_t *phys, size_t nr,
+				  unsigned long flags)
+{
+	phys_addr_t pgd;
+	u64 *pgd_page;
+	u64 mmu_flags = 0;
+	/* In case the insert_pages only partially completes we need to be able
+	 * to recover */
+	mali_bool recover_required = MALI_FALSE;
+	u64 recover_vpfn = vpfn;
+	size_t recover_count = 0;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(0 != vpfn);
+	/* 64-bit address range is the max */
+	KBASE_DEBUG_ASSERT(vpfn <= (UINT64_MAX / PAGE_SIZE));
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	mmu_flags = kbase_mmu_get_mmu_flags(flags);
+
+	while (nr) {
+		unsigned int i;
+		unsigned int index = vpfn & 0x1FF;
+		unsigned int count = KBASE_MMU_PAGE_ENTRIES - index;
+
+		if (count > nr)
+			count = nr;
+
+		/*
+		 * Repeatedly calling mmu_get_bottom_pte() is clearly
+		 * suboptimal. We don't have to re-parse the whole tree
+		 * each time (just cache the l0-l2 sequence).
+		 * On the other hand, it's only a gain when we map more than
+		 * 256 pages at once (on average). Do we really care?
+		 */
+		pgd = mmu_get_bottom_pgd(kctx, vpfn);
+		if (!pgd) {
+			dev_warn(kctx->kbdev->dev,
+					       "kbase_mmu_insert_pages: "
+					       "mmu_get_bottom_pgd failure\n");
+			if (recover_required) {
+				/* Invalidate the pages we have partially
+				 * completed */
+				mmu_insert_pages_failure_recovery(kctx,
+								  recover_vpfn,
+								  recover_count);
+			}
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+		if (!pgd_page) {
+			dev_warn(kctx->kbdev->dev,
+					       "kbase_mmu_insert_pages: "
+					       "kmap failure\n");
+			if (recover_required) {
+				/* Invalidate the pages we have partially
+				 * completed */
+				mmu_insert_pages_failure_recovery(kctx,
+								  recover_vpfn,
+								  recover_count);
+			}
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+
+		for (i = 0; i < count; i++) {
+			unsigned int ofs = index + i;
+			KBASE_DEBUG_ASSERT(0 == (pgd_page[ofs] & 1UL));
+			page_table_entry_set(kctx->kbdev, &pgd_page[ofs],
+					     mmu_phyaddr_to_ate(phys[i],
+								mmu_flags)
+					     );
+		}
+
+		phys += count;
+		vpfn += count;
+		nr -= count;
+
+		ksync_kern_vrange_gpu(pgd + (index * sizeof(u64)),
+				      pgd_page + index, count * sizeof(u64));
+
+		kunmap(pfn_to_page(PFN_DOWN(pgd)));
+		/* We have started modifying the page table. If further pages
+		 * need inserting and fail we need to undo what has already
+		 * taken place */
+		recover_required = MALI_TRUE;
+		recover_count += count;
+	}
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_insert_pages)
+
+/**
+ * This function is responsible for validating the MMU PTs
+ * triggering reguired flushes.
+ *
+ * * IMPORTANT: This uses kbasep_js_runpool_release_ctx() when the context is
+ * currently scheduled into the runpool, and so potentially uses a lot of locks.
+ * These locks must be taken in the correct order with respect to others
+ * already held by the caller. Refer to kbasep_js_runpool_release_ctx() for more
+ * information.
+ */
+static void kbase_mmu_flush(kbase_context *kctx, u64 vpfn, size_t nr)
+{
+	kbase_device *kbdev;
+	mali_bool ctx_is_in_runpool;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	kbdev = kctx->kbdev;
+
+	/* We must flush if we're currently running jobs. At the very least, we need to retain the
+	 * context to ensure it doesn't schedule out whilst we're trying to flush it */
+	ctx_is_in_runpool = kbasep_js_runpool_retain_ctx(kbdev, kctx);
+
+	if (ctx_is_in_runpool) {
+		KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+
+		/* Second level check is to try to only do this when jobs are running. The refcount is
+		 * a heuristic for this. */
+		if (kbdev->js_data.runpool_irq.per_as_data[kctx->as_nr].as_busy_refcount >= 2) {
+			/* Lock the VA region we're about to update */
+			u64 lock_addr = lock_region(kbdev, vpfn, nr);
+			unsigned int max_loops = KBASE_AS_FLUSH_MAX_LOOPS;
+
+			/* AS transaction begin */
+			mutex_lock(&kbdev->as[kctx->as_nr].transaction_mutex);
+			kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_LOCKADDR_LO), lock_addr & 0xFFFFFFFFUL, kctx);
+			kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_LOCKADDR_HI), lock_addr >> 32, kctx);
+			kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_LOCK, kctx);
+
+			/* flush L2 and unlock the VA */
+			if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_6367))
+				kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_FLUSH, kctx);
+			else
+				kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_FLUSH_MEM, kctx);
+
+			/* wait for the flush to complete */
+			while (--max_loops && kbase_reg_read(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_STATUS), kctx) & ASn_STATUS_FLUSH_ACTIVE)
+				;
+
+			if (!max_loops) {
+				/* Flush failed to complete, assume the GPU has hung and perform a reset to recover */
+				dev_err(kbdev->dev, "Flush for GPU page table update did not complete. Issueing GPU soft-reset to recover\n");
+				if (kbase_prepare_to_reset_gpu(kbdev))
+					kbase_reset_gpu(kbdev);
+			}
+
+			if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_9630)) {
+				/* Issue an UNLOCK command to ensure that valid page tables are re-read by the GPU after an update.
+				   Note that, the FLUSH command should perform all the actions necessary, however the bus logs show
+				   that if multiple page faults occur within an 8 page region the MMU does not always re-read the
+				   updated page table entries for later faults or is only partially read, it subsequently raises the
+				   page fault IRQ for the same addresses, the unlock ensures that the MMU cache is flushed, so updates
+				   can be re-read.  As the region is now unlocked we need to issue 2 UNLOCK commands in order to flush the
+				   MMU/uTLB, see PRLAM-8812.
+				 */
+				kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_UNLOCK, kctx);
+				kbase_reg_write(kctx->kbdev, MMU_AS_REG(kctx->as_nr, ASn_COMMAND), ASn_COMMAND_UNLOCK, kctx);
+			}
+
+			mutex_unlock(&kbdev->as[kctx->as_nr].transaction_mutex);
+			/* AS transaction end */
+		}
+		kbasep_js_runpool_release_ctx(kbdev, kctx);
+	}
+}
+
+/*
+ * We actually only discard the ATE, and not the page table
+ * pages. There is a potential DoS here, as we'll leak memory by
+ * having PTEs that are potentially unused.  Will require physical
+ * page accounting, so MMU pages are part of the process allocation.
+ *
+ * IMPORTANT: This uses kbasep_js_runpool_release_ctx() when the context is
+ * currently scheduled into the runpool, and so potentially uses a lot of locks.
+ * These locks must be taken in the correct order with respect to others
+ * already held by the caller. Refer to kbasep_js_runpool_release_ctx() for more
+ * information.
+ */
+mali_error kbase_mmu_teardown_pages(kbase_context *kctx, u64 vpfn, size_t nr)
+{
+	phys_addr_t pgd;
+	u64 *pgd_page;
+	kbase_device *kbdev;
+	size_t requested_nr = nr;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	beenthere(kctx, "kctx %p vpfn %lx nr %d", (void *)kctx, (unsigned long)vpfn, nr);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	if (0 == nr) {
+		/* early out if nothing to do */
+		return MALI_ERROR_NONE;
+	}
+
+	kbdev = kctx->kbdev;
+
+	while (nr) {
+		unsigned int i;
+		unsigned int index = vpfn & 0x1FF;
+		unsigned int count = KBASE_MMU_PAGE_ENTRIES - index;
+		if (count > nr)
+			count = nr;
+
+		pgd = mmu_get_bottom_pgd(kctx, vpfn);
+		if (!pgd) {
+			dev_warn(kbdev->dev, "kbase_mmu_teardown_pages: mmu_get_bottom_pgd failure\n");
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+		if (!pgd_page) {
+			dev_warn(kbdev->dev, "kbase_mmu_teardown_pages: kmap failure\n");
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+
+		for (i = 0; i < count; i++) {
+			page_table_entry_set( kctx->kbdev, &pgd_page[index + i], ENTRY_IS_INVAL );
+		}
+
+		vpfn += count;
+		nr -= count;
+
+		ksync_kern_vrange_gpu(pgd + (index * sizeof(u64)), pgd_page + index, count * sizeof(u64));
+
+		kunmap(pfn_to_page(PFN_DOWN(pgd)));
+	}
+
+	kbase_mmu_flush(kctx,vpfn,requested_nr);
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_teardown_pages)
+
+/**
+ * Update the entries for specified number of pages pointed to by 'phys' at GPU PFN 'vpfn'.
+ * This call is being triggered as a response to the changes of the mem attributes
+ *
+ * @pre : The caller is responsible for validating the memory attributes
+ *
+ * IMPORTANT: This uses kbasep_js_runpool_release_ctx() when the context is
+ * currently scheduled into the runpool, and so potentially uses a lot of locks.
+ * These locks must be taken in the correct order with respect to others
+ * already held by the caller. Refer to kbasep_js_runpool_release_ctx() for more
+ * information.
+ */
+mali_error kbase_mmu_update_pages(kbase_context* kctx, u64 vpfn, phys_addr_t* phys, size_t nr, unsigned long flags)
+{
+	phys_addr_t pgd;
+	u64* pgd_page;
+	u64 mmu_flags = 0;
+	size_t requested_nr = nr;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(0 != vpfn);
+	KBASE_DEBUG_ASSERT(vpfn <= (UINT64_MAX / PAGE_SIZE));
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	mmu_flags = kbase_mmu_get_mmu_flags(flags);
+
+	dev_warn(kctx->kbdev->dev, "kbase_mmu_update_pages(): updating page share flags "\
+			"on GPU PFN 0x%llx from phys %p, %zu pages", 
+			vpfn, phys, nr);
+
+
+	while(nr)
+	{
+		unsigned int i;
+		unsigned int index = vpfn & 0x1FF;
+		size_t count = KBASE_MMU_PAGE_ENTRIES - index;
+		if (count > nr)
+			count = nr;
+
+		pgd = mmu_get_bottom_pgd(kctx, vpfn);
+		if (!pgd) {
+			dev_warn(kctx->kbdev->dev, "mmu_get_bottom_pgd failure\n");
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+		if (!pgd_page) {
+			dev_warn(kctx->kbdev->dev, "kmap failure\n");
+			return MALI_ERROR_OUT_OF_MEMORY;
+		}
+
+		for (i = 0; i < count; i++) {
+			page_table_entry_set( kctx->kbdev, &pgd_page[index + i],  mmu_phyaddr_to_ate(phys[i], mmu_flags)  );
+		}
+
+		phys += count;
+		vpfn += count;
+		nr -= count;
+
+		ksync_kern_vrange_gpu(pgd + (index * sizeof(u64)), pgd_page + index, count * sizeof(u64));
+
+		kunmap(pfn_to_page(PFN_DOWN(pgd)));
+	}
+
+	kbase_mmu_flush(kctx,vpfn,requested_nr);
+
+	return MALI_ERROR_NONE;
+}
+
+static int mmu_pte_is_valid(u64 pte)
+{
+	return ((pte & 3) == ENTRY_IS_ATE);
+}
+
+/* This is a debug feature only */
+static void mmu_check_unused(kbase_context *kctx, phys_addr_t pgd)
+{
+	u64 *page;
+	int i;
+
+	page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
+	/* kmap_atomic should NEVER fail. */
+	KBASE_DEBUG_ASSERT(NULL != page);
+
+	for (i = 0; i < KBASE_MMU_PAGE_ENTRIES; i++) {
+		if (mmu_pte_is_valid(page[i]))
+			beenthere(kctx, "live pte %016lx", (unsigned long)page[i]);
+	}
+	kunmap_atomic(page);
+}
+
+static void mmu_teardown_level(kbase_context *kctx, phys_addr_t pgd, int level, int zap, u64 *pgd_page_buffer)
+{
+	phys_addr_t target_pgd;
+	u64 *pgd_page;
+	int i;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	lockdep_assert_held(&kctx->reg_lock);
+
+	pgd_page = kmap_atomic(pfn_to_page(PFN_DOWN(pgd)));
+	/* kmap_atomic should NEVER fail. */
+	KBASE_DEBUG_ASSERT(NULL != pgd_page);
+	/* Copy the page to our preallocated buffer so that we can minimize kmap_atomic usage */
+	memcpy(pgd_page_buffer, pgd_page, PAGE_SIZE);
+	kunmap_atomic(pgd_page);
+	pgd_page = pgd_page_buffer;
+
+	for (i = 0; i < KBASE_MMU_PAGE_ENTRIES; i++) {
+		target_pgd = mmu_pte_to_phy_addr(pgd_page[i]);
+
+		if (target_pgd) {
+			if (level < 2) {
+				mmu_teardown_level(kctx, target_pgd, level + 1, zap, pgd_page_buffer + (PAGE_SIZE / sizeof(u64)));
+			} else {
+				/*
+				 * So target_pte is a level-3 page.
+				 * As a leaf, it is safe to free it.
+				 * Unless we have live pages attached to it!
+				 */
+				mmu_check_unused(kctx, target_pgd);
+			}
+
+			beenthere(kctx, "pte %lx level %d", (unsigned long)target_pgd, level + 1);
+			if (zap) {
+				kbase_mem_allocator_free(kctx->pgd_allocator, 1, &target_pgd, MALI_TRUE);
+				kbase_process_page_usage_dec(kctx, 1 );
+				kbase_atomic_sub_pages(1, &kctx->used_pages);
+				kbase_atomic_sub_pages(1, &kctx->kbdev->memdev.used_pages);
+			}
+		}
+	}
+}
+
+mali_error kbase_mmu_init(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL == kctx->mmu_teardown_pages);
+
+	/* Preallocate MMU depth of four pages for mmu_teardown_level to use */
+	kctx->mmu_teardown_pages = kmalloc(PAGE_SIZE * 4, GFP_KERNEL);
+
+	kctx->mem_attrs = (ASn_MEMATTR_IMPL_DEF_CACHE_POLICY <<
+			   (ASn_MEMATTR_INDEX_IMPL_DEF_CACHE_POLICY * 8)) |
+			  (ASn_MEMATTR_FORCE_TO_CACHE_ALL    <<
+			   (ASn_MEMATTR_INDEX_FORCE_TO_CACHE_ALL * 8)) |
+			  (ASn_MEMATTR_WRITE_ALLOC           <<
+			   (ASn_MEMATTR_INDEX_WRITE_ALLOC * 8)) |
+			  0; /* The other indices are unused for now */
+
+	if (NULL == kctx->mmu_teardown_pages)
+		return MALI_ERROR_OUT_OF_MEMORY;
+
+	return MALI_ERROR_NONE;
+}
+
+void kbase_mmu_term(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != kctx->mmu_teardown_pages);
+
+	kfree(kctx->mmu_teardown_pages);
+	kctx->mmu_teardown_pages = NULL;
+}
+
+void kbase_mmu_free_pgd(kbase_context *kctx)
+{
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	KBASE_DEBUG_ASSERT(NULL != kctx->mmu_teardown_pages);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	mmu_teardown_level(kctx, kctx->pgd, MIDGARD_MMU_TOPLEVEL, 1, kctx->mmu_teardown_pages);
+
+	beenthere(kctx, "pgd %lx", (unsigned long)kctx->pgd);
+	kbase_mem_allocator_free(kctx->pgd_allocator, 1, &kctx->pgd, MALI_TRUE);
+	kbase_process_page_usage_dec(kctx, 1 );
+	kbase_atomic_sub_pages(1, &kctx->used_pages);
+	kbase_atomic_sub_pages(1, &kctx->kbdev->memdev.used_pages);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_free_pgd)
+
+static size_t kbasep_mmu_dump_level(kbase_context *kctx, phys_addr_t pgd, int level, char ** const buffer, size_t *size_left)
+{
+	phys_addr_t target_pgd;
+	u64 *pgd_page;
+	int i;
+	size_t size = KBASE_MMU_PAGE_ENTRIES * sizeof(u64) + sizeof(u64);
+	size_t dump_size;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	lockdep_assert_held(&kctx->reg_lock);
+
+	pgd_page = kmap(pfn_to_page(PFN_DOWN(pgd)));
+	if (!pgd_page) {
+		dev_warn(kctx->kbdev->dev, "kbasep_mmu_dump_level: kmap failure\n");
+		return 0;
+	}
+
+	if (*size_left >= size) {
+		/* A modified physical address that contains the page table level */
+		u64 m_pgd = pgd | level;
+
+		/* Put the modified physical address in the output buffer */
+		memcpy(*buffer, &m_pgd, sizeof(m_pgd));
+		*buffer += sizeof(m_pgd);
+
+		/* Followed by the page table itself */
+		memcpy(*buffer, pgd_page, sizeof(u64) * KBASE_MMU_PAGE_ENTRIES);
+		*buffer += sizeof(u64) * KBASE_MMU_PAGE_ENTRIES;
+
+		*size_left -= size;
+	}
+
+	for (i = 0; i < KBASE_MMU_PAGE_ENTRIES; i++) {
+		if ((pgd_page[i] & ENTRY_IS_PTE) == ENTRY_IS_PTE) {
+			target_pgd = mmu_pte_to_phy_addr(pgd_page[i]);
+
+			dump_size = kbasep_mmu_dump_level(kctx, target_pgd, level + 1, buffer, size_left);
+			if (!dump_size) {
+				kunmap(pfn_to_page(PFN_DOWN(pgd)));
+				return 0;
+			}
+			size += dump_size;
+		}
+	}
+
+	kunmap(pfn_to_page(PFN_DOWN(pgd)));
+
+	return size;
+}
+
+void *kbase_mmu_dump(kbase_context *kctx, int nr_pages)
+{
+	void *kaddr;
+	size_t size_left;
+
+	KBASE_DEBUG_ASSERT(kctx);
+
+	lockdep_assert_held(&kctx->reg_lock);
+
+	if (0 == nr_pages) {
+		/* can't find in a 0 sized buffer, early out */
+		return NULL;
+	}
+
+	size_left = nr_pages * PAGE_SIZE;
+
+	KBASE_DEBUG_ASSERT(0 != size_left);
+	kaddr = vmalloc_user(size_left);
+
+	if (kaddr) {
+		u64 end_marker = 0xFFULL;
+		char *buffer = (char *)kaddr;
+
+		size_t size = kbasep_mmu_dump_level(kctx, kctx->pgd, MIDGARD_MMU_TOPLEVEL, &buffer, &size_left);
+		if (!size) {
+			vfree(kaddr);
+			return NULL;
+		}
+
+		/* Add on the size for the end marker */
+		size += sizeof(u64);
+
+		if (size > nr_pages * PAGE_SIZE || size_left < sizeof(u64)) {
+			/* The buffer isn't big enough - free the memory and return failure */
+			vfree(kaddr);
+			return NULL;
+		}
+
+		/* Add the end marker */
+		memcpy(buffer, &end_marker, sizeof(u64));
+	}
+
+	return kaddr;
+}
+KBASE_EXPORT_TEST_API(kbase_mmu_dump)
+
+static u64 lock_region(kbase_device *kbdev, u64 pfn, size_t num_pages)
+{
+	u64 region;
+
+	/* can't lock a zero sized range */
+	KBASE_DEBUG_ASSERT(num_pages);
+
+	region = pfn << PAGE_SHIFT;
+	/*
+	 * fls returns (given the ASSERT above):
+	 * 32-bit: 1 .. 32
+	 * 64-bit: 1 .. 32
+	 *
+	 * 32-bit: 10 + fls(num_pages)
+	 * results in the range (11 .. 42)
+	 * 64-bit: 10 + fls(num_pages)
+	 * results in the range (11 .. 42)
+	 */
+
+	/* gracefully handle num_pages being zero */
+	if (0 == num_pages) {
+		region |= 11;
+	} else {
+		u8 region_width;
+		region_width = 10 + fls(num_pages);
+		if (num_pages != (1ul << (region_width - 11))) {
+			/* not pow2, so must go up to the next pow2 */
+			region_width += 1;
+		}
+		KBASE_DEBUG_ASSERT(region_width <= KBASE_LOCK_REGION_MAX_SIZE);
+		KBASE_DEBUG_ASSERT(region_width >= KBASE_LOCK_REGION_MIN_SIZE);
+		region |= region_width;
+	}
+
+	return region;
+}
+
+static void bus_fault_worker(struct work_struct *data)
+{
+	kbase_as *faulting_as;
+	int as_no;
+	kbase_context *kctx;
+	kbase_device *kbdev;
+	u32 reg;
+	mali_bool reset_status = MALI_FALSE;
+
+	faulting_as = container_of(data, kbase_as, work_busfault);
+	as_no = faulting_as->number;
+
+	kbdev = container_of(faulting_as, kbase_device, as[as_no]);
+
+	/* Grab the context that was already refcounted in kbase_mmu_interrupt().
+	 * Therefore, it cannot be scheduled out of this AS until we explicitly release it
+	 *
+	 * NOTE: NULL can be returned here if we're gracefully handling a spurious interrupt */
+	kctx = kbasep_js_runpool_lookup_ctx_noretain(kbdev, as_no);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8245)) {
+		/* Due to H/W issue 8245 we need to reset the GPU after using UNMAPPED mode.
+		 * We start the reset before switching to UNMAPPED to ensure that unrelated jobs
+		 * are evicted from the GPU before the switch.
+		 */
+		dev_err(kbdev->dev, "GPU bus error occurred. For this GPU version we now soft-reset as part of bus error recovery\n");
+		reset_status = kbase_prepare_to_reset_gpu(kbdev);
+	}
+
+	/* NOTE: If GPU already powered off for suspend, we don't need to switch to unmapped */
+	if (!kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE)) {
+		/* switch to UNMAPPED mode, will abort all jobs and stop any hw counter dumping */
+		/* AS transaction begin */
+		mutex_lock(&kbdev->as[as_no].transaction_mutex);
+
+		reg = kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), kctx);
+		reg &= ~3;
+		kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), reg, kctx);
+		kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_UPDATE, kctx);
+		
+		mutex_unlock(&kbdev->as[as_no].transaction_mutex);
+		/* AS transaction end */
+
+		mmu_mask_reenable(kbdev, kctx, faulting_as);
+		kbase_pm_context_idle(kbdev);
+	}
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8245) && reset_status)
+		kbase_reset_gpu(kbdev);
+
+	/* By this point, the fault was handled in some way, so release the ctx refcount */
+	if (kctx != NULL)
+		kbasep_js_runpool_release_ctx(kbdev, kctx);
+}
+
+void kbase_mmu_interrupt(kbase_device *kbdev, u32 irq_stat)
+{
+	unsigned long flags;
+	const int num_as = 16;
+	const int busfault_shift = 16;
+	const int pf_shift = 0;
+	const unsigned long mask = (1UL << num_as) - 1;
+	kbasep_js_device_data *js_devdata;
+	u32 new_mask;
+	u32 tmp;
+	u32 bf_bits = (irq_stat >> busfault_shift) & mask;	/* bus faults */
+	/* Ignore ASes with both pf and bf */
+	u32 pf_bits = ((irq_stat >> pf_shift) & mask) & ~bf_bits;	/* page faults */
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+
+	js_devdata = &kbdev->js_data;
+
+	/* remember current mask */
+	spin_lock_irqsave(&kbdev->mmu_mask_change, flags);
+	new_mask = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK), NULL);
+	/* mask interrupts for now */
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0, NULL);
+	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
+
+	while (bf_bits) {
+		/* the while logic ensures we have a bit set, no need to check for not-found here */
+		int as_no = ffs(bf_bits) - 1;
+		kbase_as *as = &kbdev->as[as_no];
+		kbase_context *kctx;
+
+		/* Refcount the kctx ASAP - it shouldn't disappear anyway, since Bus/Page faults
+		 * _should_ only occur whilst jobs are running, and a job causing the Bus/Page fault
+		 * shouldn't complete until the MMU is updated */
+		kctx = kbasep_js_runpool_lookup_ctx(kbdev, as_no);
+
+		/* mark as handled */
+		bf_bits &= ~(1UL << as_no);
+
+		/* find faulting address & status */
+		as->fault_addr = ((u64)kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTADDRESS_HI), kctx) << 32) |
+		                       kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTADDRESS_LO), kctx);
+		as->fault_status = kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTSTATUS), kctx);
+
+		/* Clear the internal JM mask first before clearing the internal MMU mask */
+		kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), 1UL << MMU_REGS_BUS_ERROR_FLAG(as_no), kctx);
+
+		if (kctx) {
+			/* hw counters dumping in progress, signal the other thread that it failed */
+			if ((kbdev->hwcnt.kctx == kctx) && (kbdev->hwcnt.state == KBASE_INSTR_STATE_DUMPING))
+				kbdev->hwcnt.state = KBASE_INSTR_STATE_FAULT;
+
+			/* Stop the kctx from submitting more jobs and cause it to be scheduled
+			 * out/rescheduled when all references to it are released */
+			spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+			kbasep_js_clear_submit_allowed(js_devdata, kctx);
+			spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+			dev_warn(kbdev->dev, "Bus error in AS%d at 0x%016llx\n", as_no, as->fault_addr);
+		} else {
+			dev_warn(kbdev->dev, "Bus error in AS%d at 0x%016llx with no context present! " "Suprious IRQ or SW Design Error?\n", as_no, as->fault_addr);
+		}
+
+		/* remove the queued BFs from the mask */
+		new_mask &= ~(1UL << (as_no + num_as));
+
+		/* We need to switch to UNMAPPED mode - but we do this in a worker so that we can sleep */
+		KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&as->work_busfault));
+		INIT_WORK(&as->work_busfault, bus_fault_worker);
+		queue_work(as->pf_wq, &as->work_busfault);
+	}
+
+	/*
+	 * pf_bits is non-zero if we have at least one AS with a page fault and no bus fault.
+	 * Handle the PFs in our worker thread.
+	 */
+	while (pf_bits) {
+		/* the while logic ensures we have a bit set, no need to check for not-found here */
+		int as_no = ffs(pf_bits) - 1;
+		kbase_as *as = &kbdev->as[as_no];
+		kbase_context *kctx;
+
+		/* Refcount the kctx ASAP - it shouldn't disappear anyway, since Bus/Page faults
+		 * _should_ only occur whilst jobs are running, and a job causing the Bus/Page fault
+		 * shouldn't complete until the MMU is updated */
+		kctx = kbasep_js_runpool_lookup_ctx(kbdev, as_no);
+
+		/* mark as handled */
+		pf_bits &= ~(1UL << as_no);
+
+		/* find faulting address & status */
+		as->fault_addr = ((u64)kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTADDRESS_HI), kctx) << 32) |
+		                       kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTADDRESS_LO), kctx);
+		as->fault_status = kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_FAULTSTATUS), kctx);
+
+		/* Clear the internal JM mask first before clearing the internal MMU mask */
+		kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), 1UL << MMU_REGS_PAGE_FAULT_FLAG(as_no), kctx);
+
+		if (kctx == NULL)
+			dev_warn(kbdev->dev, "Page fault in AS%d at 0x%016llx with no context present! " "Suprious IRQ or SW Design Error?\n", as_no, as->fault_addr);
+
+		/* remove the queued PFs from the mask */
+		new_mask &= ~((1UL << as_no) | (1UL << (as_no + num_as)));
+
+		/* queue work pending for this AS */
+		KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&as->work_pagefault));
+		INIT_WORK(&as->work_pagefault, page_fault_worker);
+		queue_work(as->pf_wq, &as->work_pagefault);
+	}
+
+	/* reenable interrupts */
+	spin_lock_irqsave(&kbdev->mmu_mask_change, flags);
+	tmp = kbase_reg_read(kbdev, MMU_REG(MMU_IRQ_MASK), NULL);
+	new_mask |= tmp;
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), new_mask, NULL);
+	spin_unlock_irqrestore(&kbdev->mmu_mask_change, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_mmu_interrupt)
+
+const char *kbase_exception_name(u32 exception_code)
+{
+	const char *e;
+
+	switch (exception_code) {
+		/* Non-Fault Status code */
+	case 0x00:
+		e = "NOT_STARTED/IDLE/OK";
+		break;
+	case 0x01:
+		e = "DONE";
+		break;
+	case 0x02:
+		e = "INTERRUPTED";
+		break;
+	case 0x03:
+		e = "STOPPED";
+		break;
+	case 0x04:
+		e = "TERMINATED";
+		break;
+	case 0x08:
+		e = "ACTIVE";
+		break;
+		/* Job exceptions */
+	case 0x40:
+		e = "JOB_CONFIG_FAULT";
+		break;
+	case 0x41:
+		e = "JOB_POWER_FAULT";
+		break;
+	case 0x42:
+		e = "JOB_READ_FAULT";
+		break;
+	case 0x43:
+		e = "JOB_WRITE_FAULT";
+		break;
+	case 0x44:
+		e = "JOB_AFFINITY_FAULT";
+		break;
+	case 0x48:
+		e = "JOB_BUS_FAULT";
+		break;
+	case 0x50:
+		e = "INSTR_INVALID_PC";
+		break;
+	case 0x51:
+		e = "INSTR_INVALID_ENC";
+		break;
+	case 0x52:
+		e = "INSTR_TYPE_MISMATCH";
+		break;
+	case 0x53:
+		e = "INSTR_OPERAND_FAULT";
+		break;
+	case 0x54:
+		e = "INSTR_TLS_FAULT";
+		break;
+	case 0x55:
+		e = "INSTR_BARRIER_FAULT";
+		break;
+	case 0x56:
+		e = "INSTR_ALIGN_FAULT";
+		break;
+	case 0x58:
+		e = "DATA_INVALID_FAULT";
+		break;
+	case 0x59:
+		e = "TILE_RANGE_FAULT";
+		break;
+	case 0x5A:
+		e = "ADDR_RANGE_FAULT";
+		break;
+	case 0x60:
+		e = "OUT_OF_MEMORY";
+		break;
+		/* GPU exceptions */
+	case 0x80:
+		e = "DELAYED_BUS_FAULT";
+		break;
+	case 0x81:
+		e = "SHAREABILITY_FAULT";
+		break;
+		/* MMU exceptions */
+	case 0xC0:
+	case 0xC1:
+	case 0xC2:
+	case 0xC3:
+	case 0xC4:
+	case 0xC5:
+	case 0xC6:
+	case 0xC7:
+		e = "TRANSLATION_FAULT";
+		break;
+	case 0xC8:
+		e = "PERMISSION_FAULT";
+		break;
+	case 0xD0:
+	case 0xD1:
+	case 0xD2:
+	case 0xD3:
+	case 0xD4:
+	case 0xD5:
+	case 0xD6:
+	case 0xD7:
+		e = "TRANSTAB_BUS_FAULT";
+		break;
+	case 0xD8:
+		e = "ACCESS_FLAG";
+		break;
+	default:
+		e = "UNKNOWN";
+		break;
+	};
+
+	return e;
+}
+
+/**
+ * The caller must ensure it's retained the ctx to prevent it from being scheduled out whilst it's being worked on.
+ */
+static void kbase_mmu_report_fault_and_kill(kbase_context *kctx, kbase_as *as)
+{
+	unsigned long flags;
+	u32 reg;
+	int exception_type;
+	int access_type;
+	int source_id;
+	int as_no;
+	kbase_device *kbdev;
+	kbasep_js_device_data *js_devdata;
+	mali_bool reset_status = MALI_FALSE;
+	static const char * const access_type_names[] = { "RESERVED", "EXECUTE", "READ", "WRITE" };
+
+	KBASE_DEBUG_ASSERT(as);
+	KBASE_DEBUG_ASSERT(kctx);
+
+	as_no = as->number;
+	kbdev = kctx->kbdev;
+	js_devdata = &kbdev->js_data;
+
+	/* ASSERT that the context won't leave the runpool */
+	KBASE_DEBUG_ASSERT(kbasep_js_debug_check_ctx_refcount(kbdev, kctx) > 0);
+
+	/* decode the fault status */
+	exception_type = as->fault_status & 0xFF;
+	access_type = (as->fault_status >> 8) & 0x3;
+	source_id = (as->fault_status >> 16);
+
+	/* terminal fault, print info about the fault */
+	dev_err(kbdev->dev, "Unhandled Page fault in AS%d at VA 0x%016llX\n"
+	                    "raw fault status 0x%X\n"
+	                    "decoded fault status: %s\n"
+	                    "exception type 0x%X: %s\n"
+	                    "access type 0x%X: %s\n"
+	                    "source id 0x%X\n",
+	                    as_no, as->fault_addr,
+	                    as->fault_status,
+	                    (as->fault_status & (1 << 10) ? "DECODER FAULT" : "SLAVE FAULT"),
+	                    exception_type, kbase_exception_name(exception_type),
+	                    access_type, access_type_names[access_type],
+	                    source_id);
+
+	/* hardware counters dump fault handling */
+	if ((kbdev->hwcnt.kctx) && (kbdev->hwcnt.kctx->as_nr == as_no) && (kbdev->hwcnt.state == KBASE_INSTR_STATE_DUMPING)) {
+		unsigned int num_core_groups = kbdev->gpu_props.num_core_groups;
+		if ((as->fault_addr >= kbdev->hwcnt.addr) && (as->fault_addr < (kbdev->hwcnt.addr + (num_core_groups * 2048))))
+			kbdev->hwcnt.state = KBASE_INSTR_STATE_FAULT;
+	}
+
+	/* Stop the kctx from submitting more jobs and cause it to be scheduled
+	 * out/rescheduled - this will occur on releasing the context's refcount */
+	spin_lock_irqsave(&js_devdata->runpool_irq.lock, flags);
+	kbasep_js_clear_submit_allowed(js_devdata, kctx);
+	spin_unlock_irqrestore(&js_devdata->runpool_irq.lock, flags);
+
+	/* Kill any running jobs from the context. Submit is disallowed, so no more jobs from this
+	 * context can appear in the job slots from this point on */
+	kbase_job_kill_jobs_from_context(kctx);
+	/* AS transaction begin */
+	mutex_lock(&as->transaction_mutex);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8245)) {
+		/* Due to H/W issue 8245 we need to reset the GPU after using UNMAPPED mode.
+		 * We start the reset before switching to UNMAPPED to ensure that unrelated jobs
+		 * are evicted from the GPU before the switch.
+		 */
+		dev_err(kbdev->dev, "Unhandled page fault. For this GPU version we now soft-reset the GPU as part of page fault recovery.");
+		reset_status = kbase_prepare_to_reset_gpu(kbdev);
+	}
+
+	/* switch to UNMAPPED mode, will abort all jobs and stop any hw counter dumping */
+	reg = kbase_reg_read(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), kctx);
+	reg &= ~3;
+	kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_TRANSTAB_LO), reg, kctx);
+	kbase_reg_write(kbdev, MMU_AS_REG(as_no, ASn_COMMAND), ASn_COMMAND_UPDATE, kctx);
+
+	mutex_unlock(&as->transaction_mutex);
+	/* AS transaction end */
+	mmu_mask_reenable(kbdev, kctx, as);
+
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8245) && reset_status)
+		kbase_reset_gpu(kbdev);
+}
+
+void kbasep_as_do_poke(struct work_struct *work)
+{
+	kbase_as *as;
+	kbase_device *kbdev;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(work);
+	as = container_of(work, kbase_as, poke_work);
+	kbdev = container_of(as, kbase_device, as[as->number]);
+	KBASE_DEBUG_ASSERT(as->poke_state & KBASE_AS_POKE_STATE_IN_FLIGHT);
+
+	/* GPU power will already be active by virtue of the caller holding a JS
+	 * reference on the address space, and will not release it until this worker
+	 * has finished */
+
+	/* AS transaction begin */
+	mutex_lock(&as->transaction_mutex);
+	/* Force a uTLB invalidate */
+	kbase_reg_write(kbdev, MMU_AS_REG(as->number, ASn_COMMAND), ASn_COMMAND_UNLOCK, NULL);
+	mutex_unlock(&as->transaction_mutex);
+	/* AS transaction end */
+
+	spin_lock_irqsave(&kbdev->js_data.runpool_irq.lock, flags);
+	if (as->poke_refcount &&
+		!(as->poke_state & KBASE_AS_POKE_STATE_KILLING_POKE)) {
+		/* Only queue up the timer if we need it, and we're not trying to kill it */
+		hrtimer_start(&as->poke_timer, HR_TIMER_DELAY_MSEC(5), HRTIMER_MODE_REL);
+	}
+	spin_unlock_irqrestore(&kbdev->js_data.runpool_irq.lock, flags);
+
+}
+
+enum hrtimer_restart kbasep_as_poke_timer_callback(struct hrtimer *timer)
+{
+	kbase_as *as;
+	int queue_work_ret;
+
+	KBASE_DEBUG_ASSERT(NULL != timer);
+	as = container_of(timer, kbase_as, poke_timer);
+	KBASE_DEBUG_ASSERT(as->poke_state & KBASE_AS_POKE_STATE_IN_FLIGHT);
+
+	queue_work_ret = queue_work(as->poke_wq, &as->poke_work);
+	KBASE_DEBUG_ASSERT(queue_work_ret);
+	return HRTIMER_NORESTART;
+}
+
+/**
+ * Retain the poking timer on an atom's context (if the atom hasn't already
+ * done so), and start the timer (if it's not already started).
+ *
+ * This must only be called on a context that's scheduled in, and an atom
+ * that's running on the GPU.
+ *
+ * The caller must hold kbasep_js_device_data::runpool_irq::lock
+ *
+ * This can be called safely from atomic context
+ */
+void kbase_as_poking_timer_retain_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	kbase_as *as;
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(katom);
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+
+	if (katom->poking)
+		return;
+
+	katom->poking = 1;
+
+	/* It's safe to work on the as/as_nr without an explicit reference,
+	 * because the caller holds the runpool_irq lock, and the atom itself
+	 * was also running and had already taken a reference  */
+	as = &kbdev->as[kctx->as_nr];
+
+	if (++(as->poke_refcount) == 1) {
+		/* First refcount for poke needed: check if not already in flight */
+		if (!as->poke_state) {
+			/* need to start poking */
+			as->poke_state |= KBASE_AS_POKE_STATE_IN_FLIGHT;
+			queue_work(as->poke_wq, &as->poke_work);
+		}
+	}
+}
+
+/**
+ * If an atom holds a poking timer, release it and wait for it to finish
+ *
+ * This must only be called on a context that's scheduled in, and an atom
+ * that still has a JS reference on the context
+ *
+ * This must \b not be called from atomic context, since it can sleep.
+ */
+void kbase_as_poking_timer_release_atom(kbase_device *kbdev, kbase_context *kctx, kbase_jd_atom *katom)
+{
+	kbase_as *as;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev);
+	KBASE_DEBUG_ASSERT(kctx);
+	KBASE_DEBUG_ASSERT(katom);
+	KBASE_DEBUG_ASSERT(kctx->as_nr != KBASEP_AS_NR_INVALID);
+
+	if (!katom->poking)
+		return;
+
+	as = &kbdev->as[kctx->as_nr];
+
+	spin_lock_irqsave(&kbdev->js_data.runpool_irq.lock, flags);
+	KBASE_DEBUG_ASSERT(as->poke_refcount > 0);
+	KBASE_DEBUG_ASSERT(as->poke_state & KBASE_AS_POKE_STATE_IN_FLIGHT);
+
+	if (--(as->poke_refcount) == 0) {
+		as->poke_state |= KBASE_AS_POKE_STATE_KILLING_POKE;
+		spin_unlock_irqrestore(&kbdev->js_data.runpool_irq.lock, flags);
+
+		hrtimer_cancel(&as->poke_timer);
+		flush_workqueue(as->poke_wq);
+
+		spin_lock_irqsave(&kbdev->js_data.runpool_irq.lock, flags);
+
+		/* Re-check whether it's still needed */
+		if (as->poke_refcount) {
+			int queue_work_ret;
+			/* Poking still needed:
+			 * - Another retain will not be starting the timer or queueing work,
+			 * because it's still marked as in-flight
+			 * - The hrtimer has finished, and has not started a new timer or
+			 * queued work because it's been marked as killing
+			 *
+			 * So whatever happens now, just queue the work again */
+			as->poke_state &= ~((kbase_as_poke_state)KBASE_AS_POKE_STATE_KILLING_POKE);
+			queue_work_ret = queue_work(as->poke_wq, &as->poke_work);
+			KBASE_DEBUG_ASSERT(queue_work_ret);
+		} else {
+			/* It isn't - so mark it as not in flight, and not killing */
+			as->poke_state = 0u;
+
+			/* The poke associated with the atom has now finished. If this is
+			 * also the last atom on the context, then we can guarentee no more
+			 * pokes (and thus no more poking register accesses) will occur on
+			 * the context until new atoms are run */
+		}
+	}
+	spin_unlock_irqrestore(&kbdev->js_data.runpool_irq.lock, flags);
+
+	katom->poking = 0;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_platform_fake.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_platform_fake.c
new file mode 100644
index 0000000..b33f0b3
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_platform_fake.c
@@ -0,0 +1,142 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifdef CONFIG_MALI_PLATFORM_FAKE
+
+#include <linux/errno.h>
+#include <linux/export.h>
+#include <linux/ioport.h>
+#include <linux/platform_device.h>
+#include <linux/string.h>
+
+#ifdef CONFIG_MACH_MANTA
+#include <plat/devs.h>
+#endif
+
+/*
+ * This file is included only for type definitions and functions belonging to
+ * specific platform folders. Do not add dependencies with symbols that are
+ * defined somewhere else.
+ */
+#include <mali_kbase_config.h>
+
+#define PLATFORM_CONFIG_RESOURCE_COUNT 4
+#define PLATFORM_CONFIG_IRQ_RES_COUNT  3
+
+static struct platform_device *mali_device;
+
+#ifndef CONFIG_OF
+/**
+ * @brief Convert data in kbase_io_resources struct to Linux-specific resources
+ *
+ * Function converts data in kbase_io_resources struct to an array of Linux resource structures. Note that function
+ * assumes that size of linux_resource array is at least PLATFORM_CONFIG_RESOURCE_COUNT.
+ * Resources are put in fixed order: I/O memory region, job IRQ, MMU IRQ, GPU IRQ.
+ *
+ * @param[in]  io_resource      Input IO resource data
+ * @param[out] linux_resources  Pointer to output array of Linux resource structures
+ */
+static void kbasep_config_parse_io_resources(const kbase_io_resources *io_resources, struct resource *const linux_resources)
+{
+	if (!io_resources || !linux_resources) {
+		pr_err("%s: couldn't find proper resources\n", __func__);
+		return;
+	}
+
+	memset(linux_resources, 0, PLATFORM_CONFIG_RESOURCE_COUNT * sizeof(struct resource));
+
+	linux_resources[0].start = io_resources->io_memory_region.start;
+	linux_resources[0].end = io_resources->io_memory_region.end;
+	linux_resources[0].flags = IORESOURCE_MEM;
+
+	linux_resources[1].start = linux_resources[1].end = io_resources->job_irq_number;
+	linux_resources[1].flags = IORESOURCE_IRQ | IORESOURCE_IRQ_HIGHLEVEL;
+
+	linux_resources[2].start = linux_resources[2].end = io_resources->mmu_irq_number;
+	linux_resources[2].flags = IORESOURCE_IRQ | IORESOURCE_IRQ_HIGHLEVEL;
+
+	linux_resources[3].start = linux_resources[3].end = io_resources->gpu_irq_number;
+	linux_resources[3].flags = IORESOURCE_IRQ | IORESOURCE_IRQ_HIGHLEVEL;
+}
+#endif /* CONFIG_OF */
+
+int kbase_platform_fake_register(void)
+{
+	kbase_platform_config *config;
+	int attribute_count;
+#ifndef CONFIG_OF
+	struct resource resources[PLATFORM_CONFIG_RESOURCE_COUNT];
+#endif
+	int err;
+
+	config = kbase_get_platform_config(); /* declared in midgard/mali_kbase_config.h but defined in platform folder */
+	if (config == NULL)
+	{
+		pr_err("%s: couldn't get platform config\n", __func__);
+		return -ENODEV;
+	}
+
+	attribute_count = kbasep_get_config_attribute_count(config->attributes);
+#ifdef CONFIG_MACH_MANTA
+	err = platform_device_add_data(&exynos5_device_g3d, config->attributes, attribute_count * sizeof(config->attributes[0]));
+	if (err)
+		return err;
+#else
+
+	mali_device = platform_device_alloc("mali", 0);
+	if (mali_device == NULL)
+		return -ENOMEM;
+
+#ifndef CONFIG_OF
+	kbasep_config_parse_io_resources(config->io_resources, resources);
+	err = platform_device_add_resources(mali_device, resources, PLATFORM_CONFIG_RESOURCE_COUNT);
+	if (err) {
+		platform_device_put(mali_device);
+		mali_device = NULL;
+		return err;
+	}
+#endif /* CONFIG_OF */
+
+	err = platform_device_add_data(mali_device, config->attributes, attribute_count * sizeof(config->attributes[0]));
+	if (err) {
+		platform_device_unregister(mali_device);
+		mali_device = NULL;
+		return err;
+	}
+
+	err = platform_device_add(mali_device);
+	if (err) {
+		platform_device_unregister(mali_device);
+		mali_device = NULL;
+		return err;
+	}
+#endif /* CONFIG_CONFIG_MACH_MANTA */
+
+	return 0;
+}
+
+void kbase_platform_fake_unregister(void)
+{
+	if (mali_device)
+		platform_device_unregister(mali_device);
+}
+
+EXPORT_SYMBOL(kbase_platform_fake_register);
+EXPORT_SYMBOL(kbase_platform_fake_unregister);
+
+#endif /* CONFIG_MALI_PLATFORM_FAKE */
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.c
new file mode 100644
index 0000000..3fc6710
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.c
@@ -0,0 +1,450 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm.c
+ * Base kernel power management APIs
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+
+#include <mali_kbase_pm.h>
+
+void kbase_pm_register_access_enable(kbase_device *kbdev)
+{
+	kbase_pm_callback_conf *callbacks;
+
+	callbacks = (kbase_pm_callback_conf *) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS);
+
+	if (callbacks)
+		callbacks->power_on_callback(kbdev);
+}
+
+void kbase_pm_register_access_disable(kbase_device *kbdev)
+{
+	kbase_pm_callback_conf *callbacks;
+
+	callbacks = (kbase_pm_callback_conf *) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS);
+
+	if (callbacks)
+		callbacks->power_off_callback(kbdev);
+}
+
+mali_error kbase_pm_init(kbase_device *kbdev)
+{
+	mali_error ret = MALI_ERROR_NONE;
+	kbase_pm_callback_conf *callbacks;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	mutex_init(&kbdev->pm.lock);
+
+	kbdev->pm.gpu_powered = MALI_FALSE;
+	kbdev->pm.suspending = MALI_FALSE;
+#ifdef CONFIG_MALI_DEBUG
+	kbdev->pm.driver_ready_for_irqs = MALI_FALSE;
+#endif /* CONFIG_MALI_DEBUG */
+	kbdev->pm.gpu_in_desired_state = MALI_TRUE;
+	init_waitqueue_head(&kbdev->pm.gpu_in_desired_state_wait);
+
+	callbacks = (kbase_pm_callback_conf *) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS);
+	if (callbacks) {
+		kbdev->pm.callback_power_on = callbacks->power_on_callback;
+		kbdev->pm.callback_power_off = callbacks->power_off_callback;
+		kbdev->pm.callback_power_suspend =
+					callbacks->power_suspend_callback;
+		kbdev->pm.callback_power_resume =
+					callbacks->power_resume_callback;
+		kbdev->pm.callback_power_runtime_init = callbacks->power_runtime_init_callback;
+		kbdev->pm.callback_power_runtime_term = callbacks->power_runtime_term_callback;
+		kbdev->pm.callback_power_runtime_on = callbacks->power_runtime_on_callback;
+		kbdev->pm.callback_power_runtime_off = callbacks->power_runtime_off_callback;
+	} else {
+		kbdev->pm.callback_power_on = NULL;
+		kbdev->pm.callback_power_off = NULL;
+		kbdev->pm.callback_power_suspend = NULL;
+		kbdev->pm.callback_power_resume = NULL;
+		kbdev->pm.callback_power_runtime_init = NULL;
+		kbdev->pm.callback_power_runtime_term = NULL;
+		kbdev->pm.callback_power_runtime_on = NULL;
+		kbdev->pm.callback_power_runtime_off = NULL;
+	}
+
+	kbdev->pm.platform_dvfs_frequency = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_POWER_MANAGEMENT_DVFS_FREQ);
+
+	/* Initialise the metrics subsystem */
+	ret = kbasep_pm_metrics_init(kbdev);
+	if (MALI_ERROR_NONE != ret)
+		return ret;
+
+	init_waitqueue_head(&kbdev->pm.l2_powered_wait);
+	kbdev->pm.l2_powered = 0;
+
+	init_waitqueue_head(&kbdev->pm.reset_done_wait);
+	kbdev->pm.reset_done = MALI_FALSE;
+
+	init_waitqueue_head(&kbdev->pm.zero_active_count_wait);
+	kbdev->pm.active_count = 0;
+
+	spin_lock_init(&kbdev->pm.power_change_lock);
+	spin_lock_init(&kbdev->pm.gpu_cycle_counter_requests_lock);
+	spin_lock_init(&kbdev->pm.gpu_powered_lock);
+
+	if (MALI_ERROR_NONE != kbase_pm_ca_init(kbdev))
+		goto workq_fail;
+
+	if (MALI_ERROR_NONE != kbase_pm_policy_init(kbdev))
+		goto pm_policy_fail;
+
+	return MALI_ERROR_NONE;
+
+pm_policy_fail:
+	kbase_pm_ca_term(kbdev);
+workq_fail:
+	kbasep_pm_metrics_term(kbdev);
+	return MALI_ERROR_FUNCTION_FAILED;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_init)
+
+void kbase_pm_do_poweron(kbase_device *kbdev, mali_bool is_resume)
+{
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	/* Turn clocks and interrupts on - no-op if we haven't done a previous
+	 * kbase_pm_clock_off() */
+	kbase_pm_clock_on(kbdev, is_resume);
+
+	/* Update core status as required by the policy */
+	KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_DO_POWERON_START);
+	kbase_pm_update_cores_state(kbdev);
+	KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_DO_POWERON_END);
+
+	/* NOTE: We don't wait to reach the desired state, since running atoms
+	 * will wait for that state to be reached anyway */
+}
+
+void kbase_pm_do_poweroff(kbase_device *kbdev, mali_bool is_suspend)
+{
+	unsigned long flags;
+	mali_bool cores_are_available;
+
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	/* Force all cores off */
+	kbdev->pm.desired_shader_state = 0;
+
+	/* Force all cores to be unavailable, in the situation where 
+	 * transitions are in progress for some cores but not others,
+	 * and kbase_pm_check_transitions_nolock can not immediately
+	 * power off the cores */
+	kbdev->shader_available_bitmap = 0;
+	kbdev->tiler_available_bitmap = 0;
+	kbdev->l2_available_bitmap = 0;
+
+	KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_DO_POWEROFF_START);
+	cores_are_available = kbase_pm_check_transitions_nolock(kbdev);
+	KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_DO_POWEROFF_END);
+	/* Don't need 'cores_are_available', because we don't return anything */
+	CSTD_UNUSED(cores_are_available);
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	/* NOTE: We won't wait to reach the core's desired state, even if we're
+	 * powering off the GPU itself too. It's safe to cut the power whilst
+	 * they're transitioning to off, because the cores should be idle and all
+	 * cache flushes should already have occurred */
+
+	/* Consume any change-state events */
+	kbase_timeline_pm_check_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+	/* Disable interrupts and turn the clock off */
+	kbase_pm_clock_off(kbdev, is_suspend);
+}
+
+mali_error kbase_pm_powerup(kbase_device *kbdev)
+{
+	unsigned long flags;
+	mali_error ret;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	mutex_lock(&kbdev->pm.lock);
+
+	/* A suspend won't happen during startup/insmod */
+	KBASE_DEBUG_ASSERT(!kbase_pm_is_suspending(kbdev));
+
+	/* Power up the GPU, don't enable IRQs as we are not ready to receive them. */
+	ret = kbase_pm_init_hw(kbdev, MALI_FALSE );
+	if (ret != MALI_ERROR_NONE) {
+		mutex_unlock(&kbdev->pm.lock);
+		return ret;
+	}
+
+	kbasep_pm_read_present_cores(kbdev);
+
+	kbdev->pm.debug_core_mask = kbdev->shader_present_bitmap;
+
+	/* Pretend the GPU is active to prevent a power policy turning the GPU cores off */
+	kbdev->pm.active_count = 1;
+
+	spin_lock_irqsave(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+	/* Ensure cycle counter is off */
+	kbdev->pm.gpu_cycle_counter_requests = 0;
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CYCLE_COUNT_STOP, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+
+	/* We are ready to receive IRQ's now as power policy is set up, so enable them now. */
+#ifdef CONFIG_MALI_DEBUG
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+	kbdev->pm.driver_ready_for_irqs = MALI_TRUE;
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+#endif
+	kbase_pm_enable_interrupts(kbdev);
+
+	/* Turn on the GPU and any cores needed by the policy */
+	kbase_pm_do_poweron(kbdev, MALI_FALSE);
+	mutex_unlock(&kbdev->pm.lock);
+
+	/* Idle the GPU and/or cores, if the policy wants it to */
+	kbase_pm_context_idle(kbdev);
+
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_powerup)
+
+void kbase_pm_context_active(kbase_device *kbdev)
+{
+	(void)kbase_pm_context_active_handle_suspend(kbdev, KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE);
+}
+
+int kbase_pm_context_active_handle_suspend(kbase_device *kbdev, kbase_pm_suspend_handler suspend_handler)
+{	
+	int c;
+	int old_count;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	/* Trace timeline information about how long it took to handle the decision
+	 * to powerup. Sometimes the event might be missed due to reading the count
+	 * outside of mutex, but this is necessary to get the trace timing
+	 * correct. */
+	old_count = kbdev->pm.active_count;
+	if (old_count == 0)
+		kbase_timeline_pm_send_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_ACTIVE);
+
+	mutex_lock(&kbdev->pm.lock);
+	if (kbase_pm_is_suspending(kbdev))
+	{
+		switch (suspend_handler) {
+		case KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE:
+			if (kbdev->pm.active_count != 0 )
+				break;
+			/* FALLTHROUGH */
+		case KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE:
+			mutex_unlock(&kbdev->pm.lock);
+			if (old_count == 0)
+				kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_ACTIVE);
+			return 1;
+
+		case KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE:
+			/* FALLTHROUGH */
+		default:
+			KBASE_DEBUG_ASSERT_MSG(MALI_FALSE,"unreachable");
+			break;
+		}
+	}
+	c = ++kbdev->pm.active_count;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, PM_CONTEXT_ACTIVE, NULL, NULL, 0u, c);
+
+	/* Trace the event being handled */
+	if (old_count == 0)
+		kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_ACTIVE);
+
+	if (c == 1) {
+		/* First context active: Power on the GPU and any cores requested by
+		 * the policy */
+		kbase_pm_update_active(kbdev);
+
+		kbasep_pm_record_gpu_active(kbdev);
+	}
+
+	mutex_unlock(&kbdev->pm.lock);
+
+	return 0;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_context_active)
+
+void kbase_pm_context_idle(kbase_device *kbdev)
+{
+	int c;
+	int old_count;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	/* Trace timeline information about how long it took to handle the decision
+	 * to powerdown. Sometimes the event might be missed due to reading the
+	 * count outside of mutex, but this is necessary to get the trace timing
+	 * correct. */
+	old_count = kbdev->pm.active_count;
+	if (old_count == 0)
+		kbase_timeline_pm_send_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_IDLE);
+
+	mutex_lock(&kbdev->pm.lock);
+
+	c = --kbdev->pm.active_count;
+
+	KBASE_TRACE_ADD_REFCOUNT(kbdev, PM_CONTEXT_IDLE, NULL, NULL, 0u, c);
+
+	KBASE_DEBUG_ASSERT(c >= 0);
+
+	/* Trace the event being handled */
+	if (old_count == 0)
+		kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_IDLE);
+
+	if (c == 0) {
+		/* Last context has gone idle */
+		kbase_pm_update_active(kbdev);
+
+		kbasep_pm_record_gpu_idle(kbdev);
+
+		/* Wake up anyone waiting for this to become 0 (e.g. suspend). The
+		 * waiters must synchronize with us by locking the pm.lock after
+		 * waiting */
+		wake_up(&kbdev->pm.zero_active_count_wait);
+	}
+
+	mutex_unlock(&kbdev->pm.lock);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_context_idle)
+
+void kbase_pm_halt(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	mutex_lock(&kbdev->pm.lock);
+	kbase_pm_cancel_deferred_poweroff(kbdev);
+	kbase_pm_do_poweroff(kbdev, MALI_FALSE);
+	mutex_unlock(&kbdev->pm.lock);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_halt)
+
+void kbase_pm_term(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(kbdev->pm.active_count == 0);
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_cycle_counter_requests == 0);
+
+	/* Free any resources the policy allocated */
+	kbase_pm_policy_term(kbdev);
+	kbase_pm_ca_term(kbdev);
+
+	/* Shut down the metrics subsystem */
+	kbasep_pm_metrics_term(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_term)
+
+void kbase_pm_suspend(struct kbase_device *kbdev)
+{
+	int nr_keep_gpu_powered_ctxs;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	mutex_lock(&kbdev->pm.lock);
+	KBASE_DEBUG_ASSERT(!kbase_pm_is_suspending(kbdev));
+	kbdev->pm.suspending = MALI_TRUE;
+	mutex_unlock(&kbdev->pm.lock);
+
+	/* From now on, the active count will drop towards zero. Sometimes, it'll
+	 * go up briefly before going down again. However, once it reaches zero it
+	 * will stay there - guaranteeing that we've idled all pm references */
+
+	/* Suspend job scheduler and associated components, so that it releases all
+	 * the PM active count references */
+	kbasep_js_suspend(kbdev);
+
+	/* Suspend any counter collection that might be happening */
+	kbase_instr_hwcnt_suspend(kbdev);
+
+	/* Cancel the keep_gpu_powered calls */
+	for (nr_keep_gpu_powered_ctxs = atomic_read(&kbdev->keep_gpu_powered_count);
+		 nr_keep_gpu_powered_ctxs > 0 ;
+		 --nr_keep_gpu_powered_ctxs ) {
+		kbase_pm_context_idle(kbdev);
+	}
+
+	/* Wait for the active count to reach zero. This is not the same as
+	 * waiting for a power down, since not all policies power down when this
+	 * reaches zero. */
+	wait_event(kbdev->pm.zero_active_count_wait, kbdev->pm.active_count == 0);
+
+	/* NOTE: We synchronize with anything that was just finishing a
+	 * kbase_pm_context_idle() call by locking the pm.lock below */
+
+	/* Force power off the GPU and all cores (regardless of policy), only after
+	 * the PM active count reaches zero (otherwise, we risk turning it off
+	 * prematurely) */
+	mutex_lock(&kbdev->pm.lock);
+	kbase_pm_cancel_deferred_poweroff(kbdev);
+	kbase_pm_do_poweroff(kbdev, MALI_TRUE);
+	mutex_unlock(&kbdev->pm.lock);
+}
+
+void kbase_pm_resume(struct kbase_device *kbdev)
+{
+	int nr_keep_gpu_powered_ctxs;
+
+	/* MUST happen before any pm_context_active calls occur */
+	mutex_lock(&kbdev->pm.lock);
+	kbdev->pm.suspending = MALI_FALSE;
+	kbase_pm_do_poweron(kbdev, MALI_TRUE);
+	mutex_unlock(&kbdev->pm.lock);
+
+	/* Initial active call, to power on the GPU/cores if needed */
+	kbase_pm_context_active(kbdev);
+
+	/* Restore the keep_gpu_powered calls */
+	for (nr_keep_gpu_powered_ctxs = atomic_read(&kbdev->keep_gpu_powered_count);
+		 nr_keep_gpu_powered_ctxs > 0 ;
+		 --nr_keep_gpu_powered_ctxs ) {
+		kbase_pm_context_active(kbdev);
+	}
+
+	/* Re-enable instrumentation, if it was previously disabled */
+	kbase_instr_hwcnt_resume(kbdev);
+
+	/* Resume any blocked atoms (which may cause contexts to be scheduled in
+	 * and dependent atoms to run) */
+	kbase_resume_suspended_soft_jobs(kbdev);
+
+	/* Resume the Job Scheduler and associated components, and start running
+	 * atoms */
+	kbasep_js_resume(kbdev);
+
+	/* Matching idle call, to power off the GPU/cores if we didn't actually
+	 * need it and the policy doesn't want it on */
+	kbase_pm_context_idle(kbdev);
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.h
new file mode 100644
index 0000000..4647dfe
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm.h
@@ -0,0 +1,868 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm.h
+ * Power management API definitions
+ */
+
+#ifndef _KBASE_PM_H_
+#define _KBASE_PM_H_
+
+#include <mali_midg_regmap.h>
+#include <linux/atomic.h>
+
+/* Forward definition - see mali_kbase.h */
+struct kbase_device;
+
+#include "mali_kbase_pm_ca.h"
+#include "mali_kbase_pm_policy.h"
+
+#include "mali_kbase_pm_ca_fixed.h"
+#if MALI_CUSTOMER_RELEASE == 0
+#include "mali_kbase_pm_ca_random.h"
+#endif
+
+#include "mali_kbase_pm_always_on.h"
+#include "mali_kbase_pm_coarse_demand.h"
+#include "mali_kbase_pm_demand.h"
+#if MALI_CUSTOMER_RELEASE == 0
+#include "mali_kbase_pm_demand_always_powered.h"
+#include "mali_kbase_pm_fast_start.h"
+#endif
+
+/** The types of core in a GPU.
+ *
+ * These enumerated values are used in calls to:
+ * - @ref kbase_pm_get_present_cores
+ * - @ref kbase_pm_get_active_cores
+ * - @ref kbase_pm_get_trans_cores
+ * - @ref kbase_pm_get_ready_cores.
+ *
+ * They specify which type of core should be acted on.  These values are set in
+ * a manner that allows @ref core_type_to_reg function to be simpler and more
+ * efficient.
+ */
+typedef enum kbase_pm_core_type {
+	KBASE_PM_CORE_L3 = L3_PRESENT_LO,	    /**< The L3 cache */
+	KBASE_PM_CORE_L2 = L2_PRESENT_LO,	    /**< The L2 cache */
+	KBASE_PM_CORE_SHADER = SHADER_PRESENT_LO,   /**< Shader cores */
+	KBASE_PM_CORE_TILER = TILER_PRESENT_LO	    /**< Tiler cores */
+} kbase_pm_core_type;
+
+/** Initialize the power management framework.
+ *
+ * Must be called before any other power management function
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return MALI_ERROR_NONE if the power management framework was successfully initialized.
+ */
+mali_error kbase_pm_init(struct kbase_device *kbdev);
+
+/** Power up GPU after all modules have been initialized and interrupt handlers installed.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return MALI_ERROR_NONE if powerup was successful.
+ */
+mali_error kbase_pm_powerup(struct kbase_device *kbdev);
+
+/**
+ * Halt the power management framework.
+ * Should ensure that no new interrupts are generated,
+ * but allow any currently running interrupt handlers to complete successfully.
+ * The GPU is forced off by the time this function returns, regardless of
+ * whether or not the active power policy asks for the GPU to be powered off.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_halt(struct kbase_device *kbdev);
+
+/** Terminate the power management framework.
+ *
+ * No power management functions may be called after this
+ * (except @ref kbase_pm_init)
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_term(struct kbase_device *kbdev);
+
+/** Metrics data collected for use by the power management framework.
+ *
+ */
+typedef struct kbasep_pm_metrics_data {
+	int vsync_hit;
+	int utilisation;
+	int util_gl_share;
+	int util_cl_share[2]; /* 2 is a max number of core groups we can have */
+	ktime_t time_period_start;
+	u32 time_busy;
+	u32 time_idle;
+	mali_bool gpu_active;
+	u32 busy_cl[2];
+	u32 busy_gl;
+	u32 active_cl_ctx[2];
+	u32 active_gl_ctx;
+
+	spinlock_t lock;
+
+	struct hrtimer timer;
+	mali_bool timer_active;
+
+	void *platform_data;
+	struct kbase_device *kbdev;
+} kbasep_pm_metrics_data;
+
+/** Actions for DVFS.
+ *
+ * kbase_pm_get_dvfs_action will return one of these enumerated values to
+ * describe the action that the DVFS system should take.
+ */
+typedef enum kbase_pm_dvfs_action {
+	KBASE_PM_DVFS_NOP,	    /**< No change in clock frequency is requested */
+	KBASE_PM_DVFS_CLOCK_UP,	    /**< The clock frequency should be increased if possible */
+	KBASE_PM_DVFS_CLOCK_DOWN    /**< The clock frequency should be decreased if possible */
+} kbase_pm_dvfs_action;
+
+typedef union kbase_pm_policy_data {
+	kbasep_pm_policy_always_on always_on;
+	kbasep_pm_policy_coarse_demand coarse_demand;
+	kbasep_pm_policy_demand demand;
+#if MALI_CUSTOMER_RELEASE == 0 	
+	kbasep_pm_policy_demand_always_powered demand_always_powered;
+	kbasep_pm_policy_fast_start fast_start;
+#endif
+} kbase_pm_policy_data;
+
+typedef union kbase_pm_ca_policy_data {
+	kbasep_pm_ca_policy_fixed fixed;
+#if MALI_CUSTOMER_RELEASE == 0
+	kbasep_pm_ca_policy_random random;
+#endif
+} kbase_pm_ca_policy_data;
+
+/** Data stored per device for power management.
+ *
+ * This structure contains data for the power management framework. There is one instance of this structure per device
+ * in the system.
+ */
+typedef struct kbase_pm_device_data {
+	/** The lock protecting Power Management structures accessed
+	 * outside of IRQ.
+	 *
+	 * This lock must also be held whenever the GPU is being powered on or off.
+	 */
+	struct mutex lock;
+
+	/** The policy that is currently actively controlling core availability.
+	 *
+	 * @note: During an IRQ, this can be NULL when the policy is being changed
+	 * with kbase_pm_ca_set_policy(). The change is protected under
+	 * kbase_device::pm::power_change_lock. Direct access to this from IRQ
+	 * context must therefore check for NULL. If NULL, then
+	 * kbase_pm_ca_set_policy() will re-issue the policy functions that would've
+	 * been done under IRQ.
+	 */
+	const kbase_pm_ca_policy *ca_current_policy;
+
+	/** The policy that is currently actively controlling the power state.
+	 *
+	 * @note: During an IRQ, this can be NULL when the policy is being changed
+	 * with kbase_pm_set_policy(). The change is protected under
+	 * kbase_device::pm::power_change_lock. Direct access to this from IRQ
+	 * context must therefore check for NULL. If NULL, then
+	 * kbase_pm_set_policy() will re-issue the policy functions that would've
+	 * been done under IRQ.
+	 */
+	const kbase_pm_policy *pm_current_policy;
+
+	/** Private data for current CA policy */
+	kbase_pm_ca_policy_data ca_policy_data;
+
+	/** Private data for current PM policy */
+	kbase_pm_policy_data pm_policy_data;
+
+	/** Flag indicating when core availability policy is transitioning cores.
+	 * The core availability policy must set this when a change in core availability
+	 * is occuring.
+	 *
+	 * power_change_lock must be held when accessing this. */
+	mali_bool ca_in_transition;
+
+	/** Waiting for reset and a queue to wait for changes */
+	mali_bool reset_done;
+	wait_queue_head_t reset_done_wait;
+
+	/** Wait queue for whether the l2 cache has been powered as requested */
+	wait_queue_head_t l2_powered_wait;
+	/** State indicating whether all the l2 caches are powered.
+	 * Non-zero indicates they're *all* powered
+	 * Zero indicates that some (or all) are not powered */
+	int l2_powered;
+
+	/** The reference count of active contexts on this device. */
+	int active_count;
+	/** Flag indicating suspending/suspended */
+	mali_bool suspending;
+	/* Wait queue set when active_count == 0 */
+	wait_queue_head_t zero_active_count_wait;
+
+	/** The reference count of active gpu cycle counter users */
+	int gpu_cycle_counter_requests;
+	/** Lock to protect gpu_cycle_counter_requests */
+	spinlock_t gpu_cycle_counter_requests_lock;
+
+	/** A bit mask identifying the shader cores that the power policy would like to be on.
+	 * The current state of the cores may be different, but there should be transitions in progress that will
+	 * eventually achieve this state (assuming that the policy doesn't change its mind in the mean time.
+	 */
+	u64 desired_shader_state;
+	/** bit mask indicating which shader cores are currently in a power-on transition */
+	u64 powering_on_shader_state;
+	/** A bit mask identifying the tiler cores that the power policy would like to be on.
+	 * @see kbase_pm_device_data:desired_shader_state */
+	u64 desired_tiler_state;
+	/** bit mask indicating which tiler core are currently in a power-on transition */
+	u64 powering_on_tiler_state;
+
+	/** bit mask indicating which l2-caches are currently in a power-on transition */
+	u64 powering_on_l2_state;
+	/** bit mask indicating which l3-caches are currently in a power-on transition */
+	u64 powering_on_l3_state;
+
+	/** Lock protecting the power state of the device.
+	 *
+	 * This lock must be held when accessing the shader_available_bitmap, tiler_available_bitmap, l2_available_bitmap,
+	 * shader_inuse_bitmap and tiler_inuse_bitmap fields of kbase_device, and the ca_in_transition and shader_poweroff_pending
+	 * fields of kbase_pm_device_data. It is also held when the hardware power registers are being written to, to ensure
+	 * that two threads do not conflict over the power transitions that the hardware should make.
+	 */
+	spinlock_t power_change_lock;
+
+	/** This flag is set iff the GPU is powered as requested by the
+	 * desired_xxx_state variables */
+	mali_bool gpu_in_desired_state;
+	/* Wait queue set when gpu_in_desired_state != 0 */
+	wait_queue_head_t gpu_in_desired_state_wait;
+
+	/** Set to true when the GPU is powered and register accesses are possible, false otherwise */
+	mali_bool gpu_powered;
+
+	/** A bit mask identifying the available shader cores that are specified via sysfs */
+	u64 debug_core_mask;
+
+	/** Set to true when instrumentation is enabled, false otherwise */
+	mali_bool instr_enabled;
+
+	mali_bool cg1_disabled;
+
+#ifdef CONFIG_MALI_DEBUG
+	/** Debug state indicating whether sufficient initialization of the driver
+	 * has occurred to handle IRQs */
+	mali_bool driver_ready_for_irqs;
+#endif /* CONFIG_MALI_DEBUG */
+
+	/** Spinlock that must be held when:
+	 * - writing gpu_powered
+	 * - accessing driver_ready_for_irqs (in CONFIG_MALI_DEBUG builds) */
+	spinlock_t gpu_powered_lock;
+
+	/** Time in milliseconds between each dvfs sample */
+
+	u32 platform_dvfs_frequency;
+
+	/** Structure to hold metrics for the GPU */
+
+	kbasep_pm_metrics_data metrics;
+
+	/** Set to the number of poweroff timer ticks until the GPU is powered off */
+	int gpu_poweroff_pending;
+
+	/** Set to the number of poweroff timer ticks until shaders are powered off */
+	int shader_poweroff_pending_time;
+
+	/** Timer for powering off GPU */
+	struct hrtimer gpu_poweroff_timer;
+
+	struct workqueue_struct *gpu_poweroff_wq;
+
+	struct work_struct gpu_poweroff_work;
+
+	/** Period of GPU poweroff timer */
+	ktime_t gpu_poweroff_time;
+
+	/** Bit mask of shaders to be powered off on next timer callback */
+	u64 shader_poweroff_pending;
+
+	/** Set to MALI_TRUE if the poweroff timer is currently running, MALI_FALSE otherwise */
+	mali_bool poweroff_timer_running;
+
+	int poweroff_shader_ticks;
+
+	int poweroff_gpu_ticks;
+
+	/** Callback when the GPU needs to be turned on. See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 *
+	 * @return 1 if GPU state was lost, 0 otherwise
+	 */
+	int (*callback_power_on) (struct kbase_device *kbdev);
+
+	/** Callback when the GPU may be turned off. See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 */
+	void (*callback_power_off) (struct kbase_device *kbdev);
+
+	/** Callback when a suspend occurs and the GPU needs to be turned off.
+	 *  See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 */
+	void (*callback_power_suspend) (struct kbase_device *kbdev);
+
+	/** Callback when a resume occurs and the GPU needs to be turned on.
+	 *  See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 */
+	void (*callback_power_resume) (struct kbase_device *kbdev);
+
+	/** Callback for initializing the runtime power management.
+	 *
+	 * @param kbdev         The kbase device
+	 *
+	 * @return MALI_ERROR_NONE on success, else error code
+	 */
+	 mali_error(*callback_power_runtime_init) (struct kbase_device *kbdev);
+
+	/** Callback for terminating the runtime power management.
+	 *
+	 * @param kbdev         The kbase device
+	 */
+	void (*callback_power_runtime_term) (struct kbase_device *kbdev);
+
+	/** Callback when the GPU needs to be turned on. See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 *
+	 * @return 1 if GPU state was lost, 0 otherwise
+	 */
+	int (*callback_power_runtime_on) (struct kbase_device *kbdev);
+
+	/** Callback when the GPU may be turned off. See @ref kbase_pm_callback_conf
+	 *
+	 * @param kbdev         The kbase device
+	 */
+	void (*callback_power_runtime_off) (struct kbase_device *kbdev);
+
+} kbase_pm_device_data;
+
+/** The GPU is idle.
+ *
+ * The OS may choose to turn off idle devices
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_dev_idle(struct kbase_device *kbdev);
+
+/** The GPU is active.
+ *
+ * The OS should avoid opportunistically turning off the GPU while it is active
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_dev_activate(struct kbase_device *kbdev);
+
+/** Get details of the cores that are present in the device.
+ *
+ * This function can be called by the active power policy to return a bitmask of the cores (of a specified type)
+ * present in the GPU device and also a count of the number of cores.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param type      The type of core (see the @ref kbase_pm_core_type enumeration)
+ *
+ * @return          The bit mask of cores present
+ */
+u64 kbase_pm_get_present_cores(struct kbase_device *kbdev, kbase_pm_core_type type);
+
+/** Get details of the cores that are currently active in the device.
+ *
+ * This function can be called by the active power policy to return a bitmask of the cores (of a specified type) that
+ * are actively processing work (i.e. turned on *and* busy).
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param type      The type of core (see the @ref kbase_pm_core_type enumeration)
+ *
+ * @return          The bit mask of active cores
+ */
+u64 kbase_pm_get_active_cores(struct kbase_device *kbdev, kbase_pm_core_type type);
+
+/** Get details of the cores that are currently transitioning between power states.
+ *
+ * This function can be called by the active power policy to return a bitmask of the cores (of a specified type) that
+ * are currently transitioning between power states.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param type      The type of core (see the @ref kbase_pm_core_type enumeration)
+ *
+ * @return          The bit mask of transitioning cores
+ */
+u64 kbase_pm_get_trans_cores(struct kbase_device *kbdev, kbase_pm_core_type type);
+
+/** Get details of the cores that are currently powered and ready for jobs.
+ *
+ * This function can be called by the active power policy to return a bitmask of the cores (of a specified type) that
+ * are powered and ready for jobs (they may or may not be currently executing jobs).
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param type      The type of core (see the @ref kbase_pm_core_type enumeration)
+ *
+ * @return          The bit mask of ready cores
+ */
+u64 kbase_pm_get_ready_cores(struct kbase_device *kbdev, kbase_pm_core_type type);
+
+/** Turn the clock for the device on, and enable device interrupts.
+ *
+ * This function can be used by a power policy to turn the clock for the GPU on. It should be modified during
+ * integration to perform the necessary actions to ensure that the GPU is fully powered and clocked.
+ *
+ * @param kbdev       The kbase device structure for the device (must be a valid pointer)
+ * @param is_resume   MALI_TRUE if clock on due to resume after suspend,
+ *                    MALI_FALSE otherwise
+ */
+void kbase_pm_clock_on(struct kbase_device *kbdev, mali_bool is_resume);
+
+/** Disable device interrupts, and turn the clock for the device off.
+ *
+ * This function can be used by a power policy to turn the clock for the GPU off. It should be modified during
+ * integration to perform the necessary actions to turn the clock off (if this is possible in the integration).
+ *
+ * @param kbdev       The kbase device structure for the device (must be a valid pointer)
+ * @param is_suspend  MALI_TRUE if clock off due to suspend, MALI_FALSE otherwise
+ */
+void kbase_pm_clock_off(struct kbase_device *kbdev, mali_bool is_suspend);
+
+/** Enable interrupts on the device.
+ *
+ * Interrupts are also enabled after a call to kbase_pm_clock_on().
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_enable_interrupts(struct kbase_device *kbdev);
+
+/** Disable interrupts on the device.
+ *
+ * This prevents delivery of Power Management interrupts to the CPU so that
+ * kbase_pm_check_transitions_nolock() will not be called from the IRQ handler
+ * until @ref kbase_pm_enable_interrupts or kbase_pm_clock_on() is called.
+ *
+ * Interrupts are also disabled after a call to kbase_pm_clock_off().
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_disable_interrupts(struct kbase_device *kbdev);
+
+/** Initialize the hardware
+ *
+ * This function checks the GPU ID register to ensure that the GPU is supported by the driver and performs a reset on
+ * the device so that it is in a known state before the device is used.
+ *
+ * @param kbdev        The kbase device structure for the device (must be a valid pointer)
+ * @param enable_irqs  When set to MALI_TRUE gpu irqs will be enabled after this call, else
+ *                     they will be left disabled.
+ *
+ * @return MALI_ERROR_NONE if the device is supported and successfully reset.
+ */
+mali_error kbase_pm_init_hw(struct kbase_device *kbdev, mali_bool enable_irqs );
+
+/** The GPU has been reset successfully.
+ *
+ * This function must be called by the GPU interrupt handler when the RESET_COMPLETED bit is set. It signals to the
+ * power management initialization code that the GPU has been successfully reset.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_reset_done(struct kbase_device *kbdev);
+
+/** Increment the count of active contexts.
+ *
+ * This function should be called when a context is about to submit a job. It informs the active power policy that the
+ * GPU is going to be in use shortly and the policy is expected to start turning on the GPU.
+ *
+ * This function will block until the GPU is available.
+ *
+ * This function ASSERTS if a suspend is occuring/has occurred whilst this is
+ * in use. Use kbase_pm_contect_active_unless_suspending() instead.
+ *
+ * @note a Suspend is only visible to Kernel threads; user-space threads in a
+ * syscall cannot witness a suspend, because they are frozen before the suspend
+ * begins.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_context_active(struct kbase_device *kbdev);
+
+
+/** Handler codes for doing kbase_pm_context_active_handle_suspend() */
+typedef enum {
+	/** A suspend is not expected/not possible - this is the same as
+	 * kbase_pm_context_active() */
+	KBASE_PM_SUSPEND_HANDLER_NOT_POSSIBLE,
+	/** If we're suspending, fail and don't increase the active count */
+	KBASE_PM_SUSPEND_HANDLER_DONT_INCREASE,
+	/** If we're suspending, succeed and allow the active count to increase iff
+	 * it didn't go from 0->1 (i.e., we didn't re-activate the GPU).
+	 *
+	 * This should only be used when there is a bounded time on the activation
+	 * (e.g. guarantee it's going to be idled very soon after) */
+	KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE
+} kbase_pm_suspend_handler;
+
+/** Suspend 'safe' variant of kbase_pm_context_active()
+ *
+ * If a suspend is in progress, this allows for various different ways of
+ * handling the suspend. Refer to @ref kbase_pm_suspend_handler for details.
+ *
+ * We returns a status code indicating whether we're allowed to keep the GPU
+ * active during the suspend, depending on the handler code. If the status code
+ * indicates a failure, the caller must abort whatever operation it was
+ * attempting, and potentially queue it up for after the OS has resumed.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param suspend_handler The handler code for how to handle a suspend that might occur
+ * @return zero     Indicates success
+ * @return non-zero Indicates failure due to the system being suspending/suspended.
+ */
+int kbase_pm_context_active_handle_suspend(struct kbase_device *kbdev, kbase_pm_suspend_handler suspend_handler);
+
+/** Decrement the reference count of active contexts.
+ *
+ * This function should be called when a context becomes idle. After this call the GPU may be turned off by the power
+ * policy so the calling code should ensure that it does not access the GPU's registers.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_context_idle(struct kbase_device *kbdev);
+
+/** Check if there are any power transitions to make, and if so start them.
+ *
+ * This function will check the desired_xx_state members of kbase_pm_device_data and the actual status of the
+ * hardware to see if any power transitions can be made at this time to make the hardware state closer to the state
+ * desired by the power policy.
+ *
+ * The return value can be used to check whether all the desired cores are
+ * available, and so whether it's worth submitting a job (e.g. from a Power
+ * Management IRQ).
+ *
+ * Note that this still returns MALI_TRUE when desired_xx_state has no
+ * cores. That is: of the no cores desired, none were <em>un</em>available. In
+ * this case, the caller may still need to try submitting jobs. This is because
+ * the Core Availability Policy might have taken us to an intermediate state
+ * where no cores are powered, before powering on more cores (e.g. for core
+ * rotation)
+ *
+ * The caller must hold kbase_device::pm::power_change_lock
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @return          non-zero when all desired cores are available. That is,
+ *                  it's worthwhile for the caller to submit a job.
+ * @return          MALI_FALSE otherwise
+ */
+mali_bool kbase_pm_check_transitions_nolock(struct kbase_device *kbdev);
+
+/** Synchronous and locking variant of kbase_pm_check_transitions_nolock()
+ *
+ * On returning, the desired state at the time of the call will have been met.
+ *
+ * @note There is nothing to stop the core being switched off by calls to
+ * kbase_pm_release_cores() or kbase_pm_unrequest_cores(). Therefore, the
+ * caller must have already made a call to
+ * kbase_pm_request_cores()/kbase_pm_request_cores_sync() previously.
+ *
+ * The usual use-case for this is to ensure cores are 'READY' after performing
+ * a GPU Reset.
+ *
+ * Unlike kbase_pm_check_transitions_nolock(), the caller must not hold
+ * kbase_device::pm::power_change_lock, because this function will take that
+ * lock itself.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_check_transitions_sync(struct kbase_device *kbdev);
+
+/** Variant of kbase_pm_update_cores_state() where the caller must hold
+ * kbase_device::pm::power_change_lock
+ *
+ * @param kbdev       The kbase device structure for the device (must be a valid
+ *                    pointer)
+ */
+void kbase_pm_update_cores_state_nolock(struct kbase_device *kbdev);
+
+/** Update the desired state of shader cores from the Power Policy, and begin
+ * any power transitions.
+ *
+ * This function will update the desired_xx_state members of
+ * kbase_pm_device_data by calling into the current Power Policy. It will then
+ * begin power transitions to make the hardware acheive the desired shader core
+ * state.
+ *
+ * @param kbdev       The kbase device structure for the device (must be a valid
+ *                    pointer)
+ */
+void kbase_pm_update_cores_state(struct kbase_device *kbdev);
+
+/** Cancel any pending requests to power off the GPU and/or shader cores.
+ *
+ * This should be called by any functions which directly power off the GPU.
+ *
+ * @param kbdev       The kbase device structure for the device (must be a valid
+ *                    pointer)
+ */
+void kbase_pm_cancel_deferred_poweroff(struct kbase_device *kbdev);
+
+/** Read the bitmasks of present cores.
+ *
+ * This information is cached to avoid having to perform register reads whenever the information is required.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbasep_pm_read_present_cores(struct kbase_device *kbdev);
+
+/** Initialize the metrics gathering framework.
+ *
+ * This must be called before other metric gathering APIs are called.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return MALI_ERROR_NONE on success, MALI_ERROR_FUNCTION_FAILED on error
+ */
+mali_error kbasep_pm_metrics_init(struct kbase_device *kbdev);
+
+/** Terminate the metrics gathering framework.
+ *
+ * This must be called when metric gathering is no longer required. It is an error to call any metrics gathering
+ * function (other than kbasep_pm_metrics_init) after calling this function.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbasep_pm_metrics_term(struct kbase_device *kbdev);
+
+/** Record state of jobs currently active on GPU.
+ *
+ * This function record time spent executing jobs split per GL and CL
+ * contexts, per core group (only CL jobs).
+ *
+ * @param kbdev     The kbase device structure for the device
+ *                  (must be a valid pointer)
+ */
+void kbasep_pm_record_job_status(struct kbase_device *kbdev);
+
+/** Record that the GPU is active.
+ *
+ * This records that the GPU is now active. The previous GPU state must have been idle, the function will assert if
+ * this is not true in a debug build.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbasep_pm_record_gpu_active(struct kbase_device *kbdev);
+
+/** Record that the GPU is idle.
+ *
+ * This records that the GPU is now idle. The previous GPU state must have been active, the function will assert if
+ * this is not true in a debug build.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbasep_pm_record_gpu_idle(struct kbase_device *kbdev);
+
+/** Function to be called by the frame buffer driver to update the vsync metric.
+ *
+ * This function should be called by the frame buffer driver to update whether the system is hitting the vsync target
+ * or not. buffer_updated should be true if the vsync corresponded with a new frame being displayed, otherwise it
+ * should be false. This function does not need to be called every vsync, but only when the value of buffer_updated
+ * differs from a previous call.
+ *
+ * @param kbdev             The kbase device structure for the device (must be a valid pointer)
+ * @param buffer_updated    True if the buffer has been updated on this VSync, false otherwise
+ */
+void kbase_pm_report_vsync(struct kbase_device *kbdev, int buffer_updated);
+
+/** Configure the frame buffer device to set the vsync callback.
+ *
+ * This function should do whatever is necessary for this integration to ensure that kbase_pm_report_vsync is
+ * called appropriately.
+ *
+ * This function will need porting as part of the integration for a device.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_register_vsync_callback(struct kbase_device *kbdev);
+
+/** Free any resources that kbase_pm_register_vsync_callback allocated.
+ *
+ * This function should perform any cleanup required from the call to kbase_pm_register_vsync_callback.
+ * No call backs should occur after this function has returned.
+ *
+ * This function will need porting as part of the integration for a device.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_unregister_vsync_callback(struct kbase_device *kbdev);
+
+/** Determine whether the DVFS system should change the clock speed of the GPU.
+ *
+ * This function should be called regularly by the DVFS system to check whether the clock speed of the GPU needs
+ * updating. It will return one of three enumerated values of kbase_pm_dvfs_action:
+ *
+ * @param kbdev                     The kbase device structure for the device (must be a valid pointer)
+ * @retval KBASE_PM_DVFS_NOP        The clock does not need changing
+ * @retval KBASE_PM_DVFS_CLOCK_UP,  The clock frequency should be increased if possible.
+ * @retval KBASE_PM_DVFS_CLOCK_DOWN The clock frequency should be decreased if possible.
+ */
+kbase_pm_dvfs_action kbase_pm_get_dvfs_action(struct kbase_device *kbdev);
+
+/** Mark that the GPU cycle counter is needed, if the caller is the first caller
+ *  then the GPU cycle counters will be enabled.
+ *
+ * The GPU must be powered when calling this function (i.e. @ref kbase_pm_context_active must have been called).
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+
+void kbase_pm_request_gpu_cycle_counter(struct kbase_device *kbdev);
+
+/** Mark that the GPU cycle counter is no longer in use, if the caller is the last
+ *  caller then the GPU cycle counters will be disabled. A request must have been made
+ *  before a call to this.
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+
+void kbase_pm_release_gpu_cycle_counter(struct kbase_device *kbdev);
+
+/** Enables access to the GPU registers before power management has powered up the GPU
+ *  with kbase_pm_powerup().
+ *
+ *  Access to registers should be done using kbase_os_reg_read/write() at this stage,
+ *  not kbase_reg_read/write().
+ *
+ *  This results in the power management callbacks provided in the driver configuration
+ *  to get called to turn on power and/or clocks to the GPU.
+ *  See @ref kbase_pm_callback_conf.
+ *
+ * This should only be used before power management is powered up with kbase_pm_powerup()
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_register_access_enable(struct kbase_device *kbdev);
+
+/** Disables access to the GPU registers enabled earlier by a call to
+ *  kbase_pm_register_access_enable().
+ *
+ *  This results in the power management callbacks provided in the driver configuration
+ *  to get called to turn off power and/or clocks to the GPU.
+ *  See @ref kbase_pm_callback_conf
+ *
+ * This should only be used before power management is powered up with kbase_pm_powerup()
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_register_access_disable(struct kbase_device *kbdev);
+
+/**
+ * Suspend the GPU and prevent any further register accesses to it from Kernel
+ * threads.
+ *
+ * This is called in response to an OS suspend event, and calls into the various
+ * kbase components to complete the suspend.
+ *
+ * @note the mechanisms used here rely on all user-space threads being frozen
+ * by the OS before we suspend. Otherwise, an IOCTL could occur that powers up
+ * the GPU e.g. via atom submission.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_suspend(struct kbase_device *kbdev);
+
+/**
+ * Resume the GPU, allow register accesses to it, and resume running atoms on
+ * the GPU.
+ *
+ * This is called in response to an OS resume event, and calls into the various
+ * kbase components to complete the resume.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_resume(struct kbase_device *kbdev);
+
+/* NOTE: kbase_pm_is_suspending is in mali_kbase.h, because it is an inline function */
+
+/**
+ * Check if the power management metrics collection is active.
+ *
+ * Note that this returns if the power management metrics collection was
+ * active at the time of calling, it is possible that after the call the metrics
+ * collection enable may have changed state.
+ *
+ * The caller must handle the consequence that the state may have changed.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @return          MALI_TRUE if metrics collection was active else MALI_FALSE.
+ */
+
+mali_bool kbase_pm_metrics_is_active(struct kbase_device *kbdev);
+
+/**
+ * Power on the GPU, and any cores that are requested.
+ *
+ * @param kbdev        The kbase device structure for the device (must be a valid pointer)
+ * @param is_resume    MALI_TRUE if power on due to resume after suspend,
+ *                     MALI_FALSE otherwise
+ */
+void kbase_pm_do_poweron(struct kbase_device *kbdev, mali_bool is_resume);
+
+/**
+ * Power off the GPU, and any cores that have been requested.
+ *
+ * @param kbdev        The kbase device structure for the device (must be a valid pointer)
+ * @param is_suspend   MALI_TRUE if power off due to suspend,
+ *                     MALI_FALSE otherwise
+ */
+void kbase_pm_do_poweroff(struct kbase_device *kbdev, mali_bool is_suspend);
+
+#ifdef CONFIG_MALI_MIDGARD_DVFS
+
+/**
+ * Function provided by platform specific code when DVFS is enabled to allow
+ * the power management metrics system to report utilisation.
+ *
+ * @param kbdev           The kbase device structure for the device (must be a valid pointer)
+ * @param utilisation     The current calculated utilisation by the metrics system.
+ * @param util_gl_share   The current calculated gl share of utilisation.
+ * @param util_cl_share   The current calculated cl share of utilisation per core group.
+ * @return                Returns 0 on failure and non zero on success.
+ */
+
+int kbase_platform_dvfs_event(struct kbase_device *kbdev, u32 utilisation,
+	u32 util_gl_share, u32 util_cl_share[2]);
+#endif
+#endif				/* _KBASE_PM_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.c
new file mode 100644
index 0000000..b457ca2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.c
@@ -0,0 +1,62 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_always_on.c
+ * "Always on" power management policy
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+static u64 always_on_get_core_mask(struct kbase_device *kbdev)
+{
+	return kbdev->shader_present_bitmap;
+}
+
+static mali_bool always_on_get_core_active (struct kbase_device *kbdev)
+{
+	return MALI_TRUE;
+}
+
+static void always_on_init(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+static void always_on_term(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+/** The @ref kbase_pm_policy structure for the demand power policy.
+ *
+ * This is the static structure that defines the demand power policy's callback and name.
+ */
+const kbase_pm_policy kbase_pm_always_on_policy_ops = {
+	"always_on",			/* name */
+	always_on_init,			/* init */
+	always_on_term,			/* term */
+	always_on_get_core_mask,	/* get_core_mask */
+	always_on_get_core_active,	/* get_core_active */
+	0u,				/* flags */
+	KBASE_PM_POLICY_ID_ALWAYS_ON,	/* id */
+};
+
+KBASE_EXPORT_TEST_API(kbase_pm_always_on_policy_ops)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.h
new file mode 100644
index 0000000..a385804
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_always_on.h
@@ -0,0 +1,68 @@
+
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_always_on.h
+ * "Always on" power management policy
+ */
+
+#ifndef MALI_KBASE_PM_ALWAYS_ON_H
+#define MALI_KBASE_PM_ALWAYS_ON_H
+
+/**
+ * The "Always on" power management policy has the following
+ * characteristics:
+ * - When KBase indicates that the GPU will be powered up, but we don't yet
+ *   know which Job Chains are to be run:
+ *  - All Shader Cores are powered up, regardless of whether or not they will
+ *    be needed later.
+ * - When KBase indicates that a set of Shader Cores are needed to submit the
+ *   currently queued Job Chains:
+ *  - All Shader Cores are kept powered, regardless of whether or not they will
+ *    be needed
+ * - When KBase indicates that the GPU need not be powered:
+ *  - The Shader Cores are kept powered, regardless of whether or not they will
+ *    be needed. The GPU itself is also kept powered, even though it is not
+ *    needed.
+ *
+ * This policy is automatically overridden during system suspend: the desired
+ * core state is ignored, and the cores are forced off regardless of what the
+ * policy requests. After resuming from suspend, new changes to the desired
+ * core state made by the policy are honored.
+ *
+ * @note:
+ * - KBase indicates the GPU will be powered up when it has a User Process that
+ *   has just started to submit Job Chains.
+ * - KBase indicates the GPU need not be powered when all the Job Chains from
+ *   User Processes have finished, and it is waiting for a User Process to
+ *   submit some more Job Chains.
+ */
+
+/**
+ * Private structure for policy instance data.
+ *
+ * This contains data that is private to the particular power policy that is active.
+ */
+typedef struct kbasep_pm_policy_always_on {
+	/** No state needed - just have a dummy variable here */
+	int dummy;
+} kbasep_pm_policy_always_on;
+
+#endif 				/* MALI_KBASE_PM_ALWAYS_ON_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.c
new file mode 100644
index 0000000..e7cfba5
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.c
@@ -0,0 +1,173 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_ca.c
+ * Base kernel core availability APIs
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+extern const kbase_pm_ca_policy kbase_pm_ca_fixed_policy_ops;
+#if MALI_CUSTOMER_RELEASE == 0
+extern const kbase_pm_ca_policy kbase_pm_ca_random_policy_ops;
+#endif
+
+static const kbase_pm_ca_policy *const policy_list[] = {
+	&kbase_pm_ca_fixed_policy_ops,
+#if MALI_CUSTOMER_RELEASE == 0
+	&kbase_pm_ca_random_policy_ops
+#endif
+};
+
+/** The number of policies available in the system.
+ *  This is derived from the number of functions listed in policy_get_functions.
+ */
+#define POLICY_COUNT (sizeof(policy_list)/sizeof(*policy_list))
+
+mali_error kbase_pm_ca_init(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	kbdev->pm.ca_current_policy = policy_list[0];
+
+	kbdev->pm.ca_current_policy->init(kbdev);
+
+	return MALI_ERROR_NONE;
+}
+
+void kbase_pm_ca_term(kbase_device *kbdev)
+{
+	kbdev->pm.ca_current_policy->term(kbdev);
+}
+
+int kbase_pm_ca_list_policies(const kbase_pm_ca_policy * const **list)
+{
+	if (!list)
+		return POLICY_COUNT;
+
+	*list = policy_list;
+
+	return POLICY_COUNT;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_ca_list_policies)
+
+const kbase_pm_ca_policy *kbase_pm_ca_get_policy(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	return kbdev->pm.ca_current_policy;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_ca_get_policy)
+
+void kbase_pm_ca_set_policy(kbase_device *kbdev, const kbase_pm_ca_policy *new_policy)
+{
+	const kbase_pm_ca_policy *old_policy;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(new_policy != NULL);
+
+	KBASE_TRACE_ADD(kbdev, PM_CA_SET_POLICY, NULL, NULL, 0u, new_policy->id);
+
+	/* During a policy change we pretend the GPU is active */
+	/* A suspend won't happen here, because we're in a syscall from a userspace thread */
+	kbase_pm_context_active(kbdev);
+
+	mutex_lock(&kbdev->pm.lock);
+
+	/* Remove the policy to prevent IRQ handlers from working on it */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	old_policy = kbdev->pm.ca_current_policy;
+	kbdev->pm.ca_current_policy = NULL;
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	if (old_policy->term)
+		old_policy->term(kbdev);
+
+	if (new_policy->init)
+		new_policy->init(kbdev);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbdev->pm.ca_current_policy = new_policy;
+
+	/* If any core power state changes were previously attempted, but couldn't
+	 * be made because the policy was changing (current_policy was NULL), then
+	 * re-try them here. */
+	kbase_pm_update_cores_state_nolock(kbdev);
+
+	kbdev->pm.ca_current_policy->update_core_status(kbdev, kbdev->shader_ready_bitmap, kbdev->shader_transitioning_bitmap);
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	mutex_unlock(&kbdev->pm.lock);
+
+	/* Now the policy change is finished, we release our fake context active reference */
+	kbase_pm_context_idle(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_ca_set_policy)
+
+u64 kbase_pm_ca_get_core_mask(kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	/* All cores must be enabled when instrumentation is in use */
+	if (kbdev->pm.instr_enabled == MALI_TRUE)
+		return kbdev->shader_present_bitmap & kbdev->pm.debug_core_mask;
+
+	if (kbdev->pm.ca_current_policy == NULL)
+		return kbdev->shader_present_bitmap & kbdev->pm.debug_core_mask;
+
+	return kbdev->pm.ca_current_policy->get_core_mask(kbdev) & kbdev->pm.debug_core_mask;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_ca_get_core_mask)
+
+void kbase_pm_ca_update_core_status(kbase_device *kbdev, u64 cores_ready, u64 cores_transitioning)
+{
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	if (kbdev->pm.ca_current_policy != NULL)
+		kbdev->pm.ca_current_policy->update_core_status(kbdev, cores_ready, cores_transitioning);
+}
+
+void kbase_pm_ca_instr_enable(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbdev->pm.instr_enabled = MALI_TRUE;
+
+	kbase_pm_update_cores_state_nolock(kbdev);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+void kbase_pm_ca_instr_disable(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbdev->pm.instr_enabled = MALI_FALSE;
+
+	kbase_pm_update_cores_state_nolock(kbdev);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.h
new file mode 100644
index 0000000..f6a97c7
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca.h
@@ -0,0 +1,170 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_ca.h
+ * Base kernel core availability APIs
+ */
+
+#ifndef _KBASE_PM_CA_H_
+#define _KBASE_PM_CA_H_
+
+typedef enum kbase_pm_ca_policy_id {
+	KBASE_PM_CA_POLICY_ID_FIXED = 1,
+	KBASE_PM_CA_POLICY_ID_RANDOM
+} kbase_pm_ca_policy_id;
+
+typedef u32 kbase_pm_ca_policy_flags;
+
+/** Core availability policy structure.
+ *
+ * Each core availability policy exposes a (static) instance of this structure which contains function pointers to the
+ * policy's methods.
+ */
+typedef struct kbase_pm_ca_policy {
+	/** The name of this policy */
+	char *name;
+
+	/** Function called when the policy is selected
+	 *
+	 * This should initialize the kbdev->pm.ca_policy_data structure. It should not attempt
+	 * to make any changes to hardware state.
+	 *
+	 * It is undefined what state the cores are in when the function is called.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 */
+	void (*init) (struct kbase_device *kbdev);
+
+	/** Function called when the policy is unselected.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 */
+	void (*term) (struct kbase_device *kbdev);
+
+	/** Function called to get the current shader core availability mask
+	 *
+	 * When a change in core availability is occuring, the policy must set kbdev->pm.ca_in_transition
+	 * to MALI_TRUE. This is to indicate that reporting changes in power state cannot be optimized out,
+	 * even if kbdev->pm.desired_shader_state remains unchanged. This must be done by any functions
+	 * internal to the Core Availability Policy that change the return value of
+	 * kbase_pm_ca_policy::get_core_mask.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 *
+	 * @return     The current core availability mask */
+	u64 (*get_core_mask) (struct kbase_device *kbdev);
+
+	/** Function called to update the current core status
+	 *
+	 * If none of the cores in core group 0 are ready or transitioning, then the policy must
+	 * ensure that the next call to get_core_mask does not return 0 for all cores in core group
+	 * 0. It is an error to disable core group 0 through the core availability policy.
+	 *
+	 * When a change in core availability has finished, the policy must set kbdev->pm.ca_in_transition
+	 * to MALI_FALSE. This is to indicate that changes in power state can once again be optimized out
+	 * when kbdev->pm.desired_shader_state is unchanged.
+	 *
+	 * @param kbdev                   The kbase device structure for the device (must be a valid pointer)
+	 * @param cores_ready             The mask of cores currently powered and ready to run jobs
+	 * @param cores_transitioning     The mask of cores currently transitioning power state */
+	void (*update_core_status) (struct kbase_device *kbdev, u64 cores_ready, u64 cores_transitioning);
+
+	/** Field indicating flags for this policy */
+	kbase_pm_ca_policy_flags flags;
+
+	/** Field indicating an ID for this policy. This is not necessarily the
+	 * same as its index in the list returned by kbase_pm_list_policies().
+	 * It is used purely for debugging. */
+	kbase_pm_ca_policy_id id;
+} kbase_pm_ca_policy;
+
+/** Initialize core availability framework
+ *
+ * Must be called before calling any other core availability function
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return MALI_ERROR_NONE if the core availability framework was successfully initialized.
+ */
+mali_error kbase_pm_ca_init(struct kbase_device *kbdev);
+
+/** Terminate core availability framework
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_ca_term(struct kbase_device *kbdev);
+
+/** Return mask of currently available shaders cores
+ * Calls into the core availability policy
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return          The bit mask of available cores
+ */
+u64 kbase_pm_ca_get_core_mask(struct kbase_device *kbdev);
+
+/** Update core availability policy with current core power status
+ * Calls into the core availability policy
+ *
+ * @param kbdev                The kbase device structure for the device (must be a valid pointer)
+ * @param cores_ready          The bit mask of cores ready for job submission
+ * @param cores_transitioning  The bit mask of cores that are transitioning power state
+ */
+void kbase_pm_ca_update_core_status(struct kbase_device *kbdev, u64 cores_ready, u64 cores_transitioning);
+
+/** Enable override for instrumentation
+ *
+ * This overrides the output of the core availability policy, ensuring that all cores are available
+ *
+ * @param kbdev                The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_ca_instr_enable(struct kbase_device *kbdev);
+
+/** Disable override for instrumentation
+ *
+ * This disables any previously enabled override, and resumes normal policy functionality
+ *
+ * @param kbdev                The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_ca_instr_disable(struct kbase_device *kbdev);
+
+/** Get the current policy.
+ * Returns the policy that is currently active.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return The current policy
+ */
+const kbase_pm_ca_policy *kbase_pm_ca_get_policy(struct kbase_device *kbdev);
+
+/** Change the policy to the one specified.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param policy    The policy to change to (valid pointer returned from @ref kbase_pm_ca_list_policies)
+ */
+void kbase_pm_ca_set_policy(struct kbase_device *kbdev, const kbase_pm_ca_policy *policy);
+
+/** Retrieve a static list of the available policies.
+ * @param[out]  policies    An array pointer to take the list of policies. This may be NULL.
+ *                          The contents of this array must not be modified.
+ *
+ * @return The number of policies
+ */
+int kbase_pm_ca_list_policies(const kbase_pm_ca_policy * const **policies);
+
+#endif				/* _KBASE_PM_CA_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.c
new file mode 100644
index 0000000..e391ecf
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.c
@@ -0,0 +1,62 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_ca_fixed.c
+ * A power policy implementing fixed core availability
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+static void fixed_init(struct kbase_device *kbdev)
+{
+	kbdev->pm.ca_in_transition = MALI_FALSE;
+}
+
+static void fixed_term(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+static u64 fixed_get_core_mask(struct kbase_device *kbdev)
+{
+	return kbdev->shader_present_bitmap;
+}
+
+static void fixed_update_core_status (struct kbase_device *kbdev, u64 cores_ready, u64 cores_transitioning)
+{
+	CSTD_UNUSED(kbdev);
+	CSTD_UNUSED(cores_ready);
+	CSTD_UNUSED(cores_transitioning);
+}
+
+/** The @ref kbase_pm_policy structure for the fixed power policy.
+ *
+ * This is the static structure that defines the fixed power policy's callback and name.
+ */
+const kbase_pm_ca_policy kbase_pm_ca_fixed_policy_ops = {
+	"fixed",			/* name */
+	fixed_init,			/* init */
+	fixed_term,			/* term */
+	fixed_get_core_mask,		/* get_core_mask */
+	fixed_update_core_status,	/* update_core_status */
+	0u,				/* flags */
+	KBASE_PM_CA_POLICY_ID_FIXED,	/* id */
+};
+
+KBASE_EXPORT_TEST_API(kbase_pm_ca_fixed_policy_ops)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.h
new file mode 100644
index 0000000..9d95e07
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_ca_fixed.h
@@ -0,0 +1,37 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_ca_fixed.h
+ * A power policy implementing fixed core availability
+ */
+
+#ifndef MALI_KBASE_PM_CA_FIXED_H
+#define MALI_KBASE_PM_CA_FIXED_H
+
+/**
+ * Private structure for policy instance data.
+ *
+ * This contains data that is private to the particular power policy that is active.
+ */
+typedef struct kbasep_pm_ca_policy_fixed {
+	/** No state needed - just have a dummy variable here */
+	int dummy;
+} kbasep_pm_ca_policy_fixed;
+
+#endif /* MALI_KBASE_PM_CA_FIXED_H */
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.c
new file mode 100644
index 0000000..095e6f0
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.c
@@ -0,0 +1,68 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_coarse_demand.c
+ * "Coarse Demand" power management policy
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+static u64 coarse_demand_get_core_mask(struct kbase_device *kbdev)
+{
+	if (kbdev->pm.active_count == 0)
+		return 0;
+
+	return kbdev->shader_present_bitmap;
+}
+
+static mali_bool coarse_demand_get_core_active(struct kbase_device *kbdev)
+{
+	if (kbdev->pm.active_count == 0)
+		return MALI_FALSE;
+
+	return MALI_TRUE;
+}
+
+static void coarse_demand_init(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+static void coarse_demand_term(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+/** The @ref kbase_pm_policy structure for the demand power policy.
+ *
+ * This is the static structure that defines the demand power policy's callback and name.
+ */
+const kbase_pm_policy kbase_pm_coarse_demand_policy_ops = {
+	"coarse_demand",			/* name */
+	coarse_demand_init,			/* init */
+	coarse_demand_term,			/* term */
+	coarse_demand_get_core_mask,		/* get_core_mask */
+	coarse_demand_get_core_active,		/* get_core_active */
+	0u,					/* flags */
+	KBASE_PM_POLICY_ID_COARSE_DEMAND,	/* id */
+};
+
+KBASE_EXPORT_TEST_API(kbase_pm_coarse_demand_policy_ops)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.h
new file mode 100644
index 0000000..afe3fc9
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_coarse_demand.h
@@ -0,0 +1,60 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_coarse_demand.h
+ * "Coarse Demand" power management policy
+ */
+
+#ifndef MALI_KBASE_PM_COARSE_DEMAND_H
+#define MALI_KBASE_PM_COARSE_DEMAND_H
+
+/**
+ * The "Coarse" demand power management policy has the following
+ * characteristics:
+ * - When KBase indicates that the GPU will be powered up, but we don't yet
+ *   know which Job Chains are to be run:
+ *  - All Shader Cores are powered up, regardless of whether or not they will
+ *    be needed later.
+ * - When KBase indicates that a set of Shader Cores are needed to submit the
+ *   currently queued Job Chains:
+ *  - All Shader Cores are kept powered, regardless of whether or not they will
+ *    be needed
+ * - When KBase indicates that the GPU need not be powered:
+ *  - The Shader Cores are powered off, and the GPU itself is powered off too.
+ *
+ * @note:
+ * - KBase indicates the GPU will be powered up when it has a User Process that
+ *   has just started to submit Job Chains.
+ * - KBase indicates the GPU need not be powered when all the Job Chains from
+ *   User Processes have finished, and it is waiting for a User Process to
+ *   submit some more Job Chains.
+ */
+
+/**
+ * Private structure for policy instance data.
+ *
+ * This contains data that is private to the particular power policy that is active.
+ */
+typedef struct kbasep_pm_policy_coarse_demand {
+	/** No state needed - just have a dummy variable here */
+	int dummy;
+} kbasep_pm_policy_coarse_demand;
+
+#endif				/* MALI_KBASE_PM_COARSE_DEMAND_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.c
new file mode 100644
index 0000000..fd94294
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.c
@@ -0,0 +1,70 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_demand.c
+ * A simple demand based power management policy
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+static u64 demand_get_core_mask(struct kbase_device *kbdev)
+{
+	u64 desired = kbdev->shader_needed_bitmap | kbdev->shader_inuse_bitmap;
+
+	if (0 == kbdev->pm.active_count)
+		return 0;
+
+	return desired;
+}
+
+static mali_bool demand_get_core_active (struct kbase_device *kbdev)
+{
+	if (0 == kbdev->pm.active_count)
+		return MALI_FALSE;
+
+	return MALI_TRUE;
+}
+
+static void demand_init(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+static void demand_term(struct kbase_device *kbdev)
+{
+	CSTD_UNUSED(kbdev);
+}
+
+/** The @ref kbase_pm_policy structure for the demand power policy.
+ *
+ * This is the static structure that defines the demand power policy's callback and name.
+ */
+const kbase_pm_policy kbase_pm_demand_policy_ops = {
+	"demand",			/* name */
+	demand_init,			/* init */
+	demand_term,			/* term */
+	demand_get_core_mask,		/* get_core_mask */
+	demand_get_core_active,		/* get_core_active */
+	0u,				/* flags */
+	KBASE_PM_POLICY_ID_DEMAND,	/* id */
+};
+
+KBASE_EXPORT_TEST_API(kbase_pm_demand_policy_ops)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.h
new file mode 100644
index 0000000..8579181
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_demand.h
@@ -0,0 +1,57 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_demand.h
+ * A simple demand based power management policy
+ */
+
+#ifndef MALI_KBASE_PM_DEMAND_H
+#define MALI_KBASE_PM_DEMAND_H
+
+/**
+ * The demand power management policy has the following characteristics:
+ * - When KBase indicates that the GPU will be powered up, but we don't yet
+ *   know which Job Chains are to be run:
+ *  - The Shader Cores are not powered up
+ * - When KBase indicates that a set of Shader Cores are needed to submit the
+ *   currently queued Job Chains:
+ *  - Only those Shader Cores are powered up
+ * - When KBase indicates that the GPU need not be powered:
+ *  - The Shader Cores are powered off, and the GPU itself is powered off too.
+ *
+ * @note:
+ * - KBase indicates the GPU will be powered up when it has a User Process that
+ *   has just started to submit Job Chains.
+ * - KBase indicates the GPU need not be powered when all the Job Chains from
+ *   User Processes have finished, and it is waiting for a User Process to
+ *   submit some more Job Chains.
+ */
+
+/**
+ * Private structure for policy instance data.
+ *
+ * This contains data that is private to the particular power policy that is active.
+ */
+typedef struct kbasep_pm_policy_demand {
+	/** No state needed - just have a dummy variable here */
+	int dummy;
+} kbasep_pm_policy_demand;
+
+#endif				/* MALI_KBASE_PM_DEMAND_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_driver.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_driver.c
new file mode 100644
index 0000000..d8c9bd8
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_driver.c
@@ -0,0 +1,952 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_driver.c
+ * Base kernel Power Management hardware control
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_pm.h>
+#include <mali_kbase_config_defaults.h>
+
+#if MALI_MOCK_TEST
+#define MOCKABLE(function) function##_original
+#else
+#define MOCKABLE(function) function
+#endif				/* MALI_MOCK_TEST */
+
+/** Actions that can be performed on a core.
+ *
+ * This enumeration is private to the file. Its values are set to allow @ref core_type_to_reg function,
+ * which decodes this enumeration, to be simpler and more efficient.
+ */
+typedef enum kbasep_pm_action {
+	ACTION_PRESENT = 0,
+	ACTION_READY = (SHADER_READY_LO - SHADER_PRESENT_LO),
+	ACTION_PWRON = (SHADER_PWRON_LO - SHADER_PRESENT_LO),
+	ACTION_PWROFF = (SHADER_PWROFF_LO - SHADER_PRESENT_LO),
+	ACTION_PWRTRANS = (SHADER_PWRTRANS_LO - SHADER_PRESENT_LO),
+	ACTION_PWRACTIVE = (SHADER_PWRACTIVE_LO - SHADER_PRESENT_LO)
+} kbasep_pm_action;
+
+/** Decode a core type and action to a register.
+ *
+ * Given a core type (defined by @ref kbase_pm_core_type) and an action (defined by @ref kbasep_pm_action) this
+ * function will return the register offset that will perform the action on the core type. The register returned is
+ * the \c _LO register and an offset must be applied to use the \c _HI register.
+ *
+ * @param core_type The type of core
+ * @param action    The type of action
+ *
+ * @return The register offset of the \c _LO register that performs an action of type \c action on a core of type \c
+ * core_type.
+ */
+static u32 core_type_to_reg(kbase_pm_core_type core_type, kbasep_pm_action action)
+{
+	return core_type + action;
+}
+
+/** Invokes an action on a core set
+ *
+ * This function performs the action given by \c action on a set of cores of a type given by \c core_type. It is a
+ * static function used by @ref kbase_pm_transition_core_type
+ *
+ * @param kbdev     The kbase device structure of the device
+ * @param core_type The type of core that the action should be performed on
+ * @param cores     A bit mask of cores to perform the action on (low 32 bits)
+ * @param action    The action to perform on the cores
+ */
+STATIC void kbase_pm_invoke(kbase_device *kbdev, kbase_pm_core_type core_type, u64 cores, kbasep_pm_action action)
+{
+	u32 reg;
+	u32 lo = cores & 0xFFFFFFFF;
+	u32 hi = (cores >> 32) & 0xFFFFFFFF;
+
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	reg = core_type_to_reg(core_type, action);
+
+	KBASE_DEBUG_ASSERT(reg);
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+	if (cores) {
+		if (action == ACTION_PWRON)
+			kbase_trace_mali_pm_power_on(core_type, cores);
+		else if (action == ACTION_PWROFF)
+			kbase_trace_mali_pm_power_off(core_type, cores);
+	}
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+	/* Tracing */
+	if (cores) {
+		if (action == ACTION_PWRON)
+			switch (core_type) {
+				case KBASE_PM_CORE_SHADER:
+					KBASE_TRACE_ADD(kbdev, PM_PWRON, NULL, NULL, 0u, lo);
+					break;
+				case KBASE_PM_CORE_TILER:
+					KBASE_TRACE_ADD(kbdev, PM_PWRON_TILER, NULL, NULL, 0u, lo);
+					break;
+				case KBASE_PM_CORE_L2:
+					KBASE_TRACE_ADD(kbdev, PM_PWRON_L2, NULL, NULL, 0u, lo);
+					break;
+				default:
+					/* L3 not handled */
+					break;
+			}
+		else if (action == ACTION_PWROFF)
+			switch (core_type) {
+				case KBASE_PM_CORE_SHADER:
+					KBASE_TRACE_ADD(kbdev, PM_PWROFF, NULL, NULL, 0u, lo);
+					break;
+				case KBASE_PM_CORE_TILER:
+					KBASE_TRACE_ADD(kbdev, PM_PWROFF_TILER, NULL, NULL, 0u, lo);
+					break;
+				case KBASE_PM_CORE_L2:
+					KBASE_TRACE_ADD(kbdev, PM_PWROFF_L2, NULL, NULL, 0u, lo);
+					break;
+				default:
+					/* L3 not handled */
+					break;
+			}
+	}
+
+	if (lo != 0)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(reg), lo, NULL);
+
+	if (hi != 0)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(reg + 4), hi, NULL);
+}
+
+/** Get information about a core set
+ *
+ * This function gets information (chosen by \c action) about a set of cores of a type given by \c core_type. It is a
+ * static function used by @ref kbase_pm_get_present_cores, @ref kbase_pm_get_active_cores, @ref
+ * kbase_pm_get_trans_cores and @ref kbase_pm_get_ready_cores.
+ *
+ * @param kbdev     The kbase device structure of the device
+ * @param core_type The type of core that the should be queried
+ * @param action    The property of the cores to query
+ *
+ * @return A bit mask specifying the state of the cores
+ */
+static u64 kbase_pm_get_state(kbase_device *kbdev, kbase_pm_core_type core_type, kbasep_pm_action action)
+{
+	u32 reg;
+	u32 lo, hi;
+
+	reg = core_type_to_reg(core_type, action);
+
+	KBASE_DEBUG_ASSERT(reg);
+
+	lo = kbase_reg_read(kbdev, GPU_CONTROL_REG(reg), NULL);
+	hi = kbase_reg_read(kbdev, GPU_CONTROL_REG(reg + 4), NULL);
+
+	return (((u64) hi) << 32) | ((u64) lo);
+}
+
+void kbasep_pm_read_present_cores(kbase_device *kbdev)
+{
+	kbdev->shader_present_bitmap = kbase_pm_get_state(kbdev, KBASE_PM_CORE_SHADER, ACTION_PRESENT);
+	kbdev->tiler_present_bitmap = kbase_pm_get_state(kbdev, KBASE_PM_CORE_TILER, ACTION_PRESENT);
+	kbdev->l2_present_bitmap = kbase_pm_get_state(kbdev, KBASE_PM_CORE_L2, ACTION_PRESENT);
+	kbdev->l3_present_bitmap = kbase_pm_get_state(kbdev, KBASE_PM_CORE_L3, ACTION_PRESENT);
+
+	kbdev->shader_inuse_bitmap = 0;
+	kbdev->shader_needed_bitmap = 0;
+	kbdev->shader_available_bitmap = 0;
+	kbdev->tiler_available_bitmap = 0;
+	kbdev->l2_users_count = 0;
+	kbdev->l2_available_bitmap = 0;
+	kbdev->tiler_needed_cnt = 0;
+	kbdev->tiler_inuse_cnt = 0;
+
+	memset(kbdev->shader_needed_cnt, 0, sizeof(kbdev->shader_needed_cnt));
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_read_present_cores)
+
+/** Get the cores that are present
+ */
+u64 kbase_pm_get_present_cores(kbase_device *kbdev, kbase_pm_core_type type)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	switch (type) {
+	case KBASE_PM_CORE_L3:
+		return kbdev->l3_present_bitmap;
+		break;
+	case KBASE_PM_CORE_L2:
+		return kbdev->l2_present_bitmap;
+		break;
+	case KBASE_PM_CORE_SHADER:
+		return kbdev->shader_present_bitmap;
+		break;
+	case KBASE_PM_CORE_TILER:
+		return kbdev->tiler_present_bitmap;
+		break;
+	}
+	KBASE_DEBUG_ASSERT(0);
+	return 0;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_get_present_cores)
+
+/** Get the cores that are "active" (busy processing work)
+ */
+u64 kbase_pm_get_active_cores(kbase_device *kbdev, kbase_pm_core_type type)
+{
+	return kbase_pm_get_state(kbdev, type, ACTION_PWRACTIVE);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_get_active_cores)
+
+/** Get the cores that are transitioning between power states
+ */
+u64 kbase_pm_get_trans_cores(kbase_device *kbdev, kbase_pm_core_type type)
+{
+	return kbase_pm_get_state(kbdev, type, ACTION_PWRTRANS);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_get_trans_cores)
+/** Get the cores that are powered on
+ */
+u64 kbase_pm_get_ready_cores(kbase_device *kbdev, kbase_pm_core_type type)
+{
+	u64 result;
+	result = kbase_pm_get_state(kbdev, type, ACTION_READY);
+
+	switch (type) {
+		case KBASE_PM_CORE_SHADER:
+			KBASE_TRACE_ADD(kbdev, PM_CORES_POWERED, NULL, NULL, 0u, (u32) result);
+			break;
+		case KBASE_PM_CORE_TILER:
+			KBASE_TRACE_ADD(kbdev, PM_CORES_POWERED_TILER, NULL, NULL, 0u, (u32) result);
+			break;
+		case KBASE_PM_CORE_L2:
+			KBASE_TRACE_ADD(kbdev, PM_CORES_POWERED_L2, NULL, NULL, 0u, (u32) result);
+			break;
+		default:
+			/* NB: L3 not currently traced */
+			break;
+	}
+
+	return result;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_get_ready_cores)
+
+/** Perform power transitions for a particular core type.
+ *
+ * This function will perform any available power transitions to make the actual hardware state closer to the desired
+ * state. If a core is currently transitioning then changes to the power state of that call cannot be made until the
+ * transition has finished. Cores which are not present in the hardware are ignored if they are specified in the
+ * desired_state bitmask, however the return value will always be 0 in this case.
+ *
+ * @param kbdev             The kbase device
+ * @param type              The core type to perform transitions for
+ * @param desired_state     A bit mask of the desired state of the cores
+ * @param in_use            A bit mask of the cores that are currently running jobs.
+ *                          These cores have to be kept powered up because there are jobs
+ *                          running (or about to run) on them.
+ * @param[out] available    Receives a bit mask of the cores that the job scheduler can use to submit jobs to.
+ *                          May be NULL if this is not needed.
+ * @param[in,out] powering_on Bit mask to update with cores that are transitioning to a power-on state.
+ *
+ * @return MALI_TRUE if the desired state has been reached, MALI_FALSE otherwise
+ */
+STATIC mali_bool kbase_pm_transition_core_type(kbase_device *kbdev, kbase_pm_core_type type, u64 desired_state,
+					       u64 in_use, u64 * const available, u64 *powering_on)
+{
+	u64 present;
+	u64 ready;
+	u64 trans;
+	u64 powerup;
+	u64 powerdown;
+	u64 powering_on_trans;
+	u64 desired_state_in_use;
+
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	/* Get current state */
+	present = kbase_pm_get_present_cores(kbdev, type);
+	trans = kbase_pm_get_trans_cores(kbdev, type);
+	ready = kbase_pm_get_ready_cores(kbdev, type);
+
+	powering_on_trans = trans & *powering_on;
+	*powering_on = powering_on_trans;
+
+	if (available != NULL)
+		*available = (ready | powering_on_trans) & desired_state;
+
+	/* Update desired state to include the in-use cores. These have to be kept powered up because there are jobs
+	 * running or about to run on these cores
+	 */
+	desired_state_in_use = desired_state | in_use;
+
+	/* Update state of whether l2 caches are powered */
+	if (type == KBASE_PM_CORE_L2) {
+		if ((ready == present) && (desired_state_in_use == ready) && (trans == 0)) {
+			/* All are ready, none will be turned off, and none are transitioning */
+			kbdev->pm.l2_powered = 1;
+			if (kbdev->l2_users_count > 0) {
+				/* Notify any registered l2 cache users (optimized out when no users waiting) */
+				wake_up(&kbdev->pm.l2_powered_wait);
+			}
+		} else {
+			kbdev->pm.l2_powered = 0;
+		}
+	}
+
+	if (desired_state_in_use == ready && (trans == 0))
+		return MALI_TRUE;
+
+	/* Restrict the cores to those that are actually present */
+	powerup = desired_state_in_use & present;
+	powerdown = (~desired_state_in_use) & present;
+
+	/* Restrict to cores that are not already in the desired state */
+	powerup &= ~ready;
+	powerdown &= ready;
+
+	/* Don't transition any cores that are already transitioning, except for
+	 * Mali cores that support the following case:
+	 *
+	 * If the SHADER_PWRON or TILER_PWRON registers are written to turn on
+	 * a core that is currently transitioning to power off, then this is 
+	 * remembered and the shader core is automatically powered up again once
+	 * the original transition completes. Once the automatic power on is
+	 * complete any job scheduled on the shader core should start.
+	 */
+	powerdown &= ~trans;
+
+	if (kbase_hw_has_feature(kbdev, BASE_HW_FEATURE_PWRON_DURING_PWROFF_TRANS))
+		if (KBASE_PM_CORE_SHADER == type || KBASE_PM_CORE_TILER == type)
+			trans = powering_on_trans; /* for exception cases, only mask off cores in power on transitions */
+
+	powerup &= ~trans;
+
+	/* Perform transitions if any */
+	kbase_pm_invoke(kbdev, type, powerup, ACTION_PWRON);
+	kbase_pm_invoke(kbdev, type, powerdown, ACTION_PWROFF);
+
+	/* Recalculate cores transitioning on, and re-evaluate our state */
+	powering_on_trans |= powerup;
+	*powering_on = powering_on_trans;
+	if (available != NULL)
+		*available = (ready | powering_on_trans) & desired_state;
+
+	return MALI_FALSE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_transition_core_type)
+
+/** Determine which caches should be on for a particular core state.
+ *
+ * This function takes a bit mask of the present caches and the cores (or caches) that are attached to the caches that
+ * will be powered. It then computes which caches should be turned on to allow the cores requested to be powered up.
+ *
+ * @param present       The bit mask of present caches
+ * @param cores_powered A bit mask of cores (or L2 caches) that are desired to be powered
+ *
+ * @return A bit mask of the caches that should be turned on
+ */
+STATIC u64 get_desired_cache_status(u64 present, u64 cores_powered)
+{
+	u64 desired = 0;
+
+	while (present) {
+		/* Find out which is the highest set bit */
+		u64 bit = fls64(present) - 1;
+		u64 bit_mask = 1ull << bit;
+		/* Create a mask which has all bits from 'bit' upwards set */
+
+		u64 mask = ~(bit_mask - 1);
+
+		/* If there are any cores powered at this bit or above (that haven't previously been processed) then we need
+		 * this core on */
+		if (cores_powered & mask)
+			desired |= bit_mask;
+
+		/* Remove bits from cores_powered and present */
+		cores_powered &= ~mask;
+		present &= ~bit_mask;
+	}
+
+	return desired;
+}
+
+KBASE_EXPORT_TEST_API(get_desired_cache_status)
+
+mali_bool MOCKABLE(kbase_pm_check_transitions_nolock) (struct kbase_device *kbdev)
+{
+	mali_bool cores_are_available = MALI_FALSE;
+	mali_bool in_desired_state = MALI_TRUE;
+	u64 desired_l2_state;
+	u64 desired_l3_state;
+	u64 cores_powered;
+	u64 tiler_available_bitmap;
+	u64 shader_available_bitmap;
+	u64 shader_ready_bitmap;
+	u64 shader_transitioning_bitmap;
+	u64 l2_available_bitmap;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	spin_lock(&kbdev->pm.gpu_powered_lock);
+	if (kbdev->pm.gpu_powered == MALI_FALSE) {
+		spin_unlock(&kbdev->pm.gpu_powered_lock);
+		if (kbdev->pm.desired_shader_state == 0 && kbdev->pm.desired_tiler_state == 0)
+			return MALI_TRUE;
+		return MALI_FALSE;
+	}
+
+	/* Trace that a change-state is being requested, and that it took
+	 * (effectively) no time to start it. This is useful for counting how many
+	 * state changes occurred, in a way that's backwards-compatible with
+	 * processing the trace data */
+	kbase_timeline_pm_send_event(kbdev, KBASE_TIMELINE_PM_EVENT_CHANGE_GPU_STATE);
+	kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_CHANGE_GPU_STATE);
+
+	/* If any cores are already powered then, we must keep the caches on */
+	cores_powered = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_SHADER);
+
+	cores_powered |= kbdev->pm.desired_shader_state;
+
+	/* If there are l2 cache users registered, keep all l2s powered even if all other cores are off. */
+	if (kbdev->l2_users_count > 0)
+		cores_powered |= kbdev->l2_present_bitmap;
+
+	desired_l2_state = get_desired_cache_status(kbdev->l2_present_bitmap, cores_powered);
+
+	/* If any l2 cache is on, then enable l2 #0, for use by job manager */
+	if (0 != desired_l2_state) {
+		desired_l2_state |= 1;
+		/* Also enable tiler if l2 cache is powered */
+		kbdev->pm.desired_tiler_state = kbdev->tiler_present_bitmap;
+	} else {
+		kbdev->pm.desired_tiler_state = 0;
+	}
+
+	desired_l3_state = get_desired_cache_status(kbdev->l3_present_bitmap, desired_l2_state);
+
+	in_desired_state &= kbase_pm_transition_core_type(kbdev, KBASE_PM_CORE_L3, desired_l3_state, 0, NULL, &kbdev->pm.powering_on_l3_state);
+	in_desired_state &= kbase_pm_transition_core_type(kbdev, KBASE_PM_CORE_L2, desired_l2_state, 0, &l2_available_bitmap, &kbdev->pm.powering_on_l2_state);
+
+	if( kbdev->l2_available_bitmap != l2_available_bitmap)
+	{
+		KBASE_TIMELINE_POWER_L2(kbdev,l2_available_bitmap);
+	}
+
+	kbdev->l2_available_bitmap = l2_available_bitmap;
+
+	if (in_desired_state) {
+
+		in_desired_state &= kbase_pm_transition_core_type(kbdev, KBASE_PM_CORE_TILER, kbdev->pm.desired_tiler_state, 0, &tiler_available_bitmap, &kbdev->pm.powering_on_tiler_state);
+		in_desired_state &= kbase_pm_transition_core_type(kbdev, KBASE_PM_CORE_SHADER, kbdev->pm.desired_shader_state, kbdev->shader_inuse_bitmap, &shader_available_bitmap, &kbdev->pm.powering_on_shader_state);
+
+		if (kbdev->shader_available_bitmap != shader_available_bitmap) {
+			KBASE_TRACE_ADD(kbdev, PM_CORES_CHANGE_AVAILABLE, NULL, NULL, 0u, (u32) shader_available_bitmap);
+			KBASE_TIMELINE_POWER_SHADER(kbdev, shader_available_bitmap);
+		}
+
+		kbdev->shader_available_bitmap = shader_available_bitmap;
+
+		if (kbdev->tiler_available_bitmap != tiler_available_bitmap) {
+			KBASE_TRACE_ADD(kbdev, PM_CORES_CHANGE_AVAILABLE_TILER, NULL, NULL, 0u, (u32) tiler_available_bitmap);
+			KBASE_TIMELINE_POWER_TILER(kbdev, tiler_available_bitmap);
+		}
+
+		kbdev->tiler_available_bitmap = tiler_available_bitmap;
+
+	} else if ((l2_available_bitmap & kbdev->tiler_present_bitmap) != kbdev->tiler_present_bitmap) {
+		tiler_available_bitmap = 0;
+
+		if (kbdev->tiler_available_bitmap != tiler_available_bitmap) {
+			KBASE_TIMELINE_POWER_TILER(kbdev, tiler_available_bitmap);
+		}
+
+		kbdev->tiler_available_bitmap = tiler_available_bitmap;
+	}
+
+	/* State updated for slow-path waiters */
+	kbdev->pm.gpu_in_desired_state = in_desired_state;
+
+	shader_ready_bitmap = kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_SHADER);
+	shader_transitioning_bitmap = kbase_pm_get_trans_cores(kbdev, KBASE_PM_CORE_SHADER);
+
+	/* Determine whether the cores are now available (even if the set of
+	 * available cores is empty). Note that they can be available even if we've
+	 * not finished transitioning to the desired state */
+	if ((kbdev->shader_available_bitmap & kbdev->pm.desired_shader_state) == kbdev->pm.desired_shader_state
+		&& (kbdev->tiler_available_bitmap & kbdev->pm.desired_tiler_state) == kbdev->pm.desired_tiler_state) {
+		cores_are_available = MALI_TRUE;
+
+		KBASE_TRACE_ADD(kbdev, PM_CORES_AVAILABLE, NULL, NULL, 0u, (u32)(kbdev->shader_available_bitmap & kbdev->pm.desired_shader_state));
+		KBASE_TRACE_ADD(kbdev, PM_CORES_AVAILABLE_TILER, NULL, NULL, 0u, (u32)(kbdev->tiler_available_bitmap & kbdev->pm.desired_tiler_state));
+
+		/* Log timelining information about handling events that power up
+		 * cores, to match up either with immediate submission either because
+		 * cores already available, or from PM IRQ */
+		if (!in_desired_state)
+			kbase_timeline_pm_send_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+	}
+
+	if (in_desired_state) {
+		KBASE_DEBUG_ASSERT(cores_are_available);
+
+#ifdef CONFIG_MALI_GATOR_SUPPORT
+		kbase_trace_mali_pm_status(KBASE_PM_CORE_L3, kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_L3));
+		kbase_trace_mali_pm_status(KBASE_PM_CORE_L2, kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_L2));
+		kbase_trace_mali_pm_status(KBASE_PM_CORE_SHADER, kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_SHADER));
+		kbase_trace_mali_pm_status(KBASE_PM_CORE_TILER, kbase_pm_get_ready_cores(kbdev, KBASE_PM_CORE_TILER));
+#endif				/* CONFIG_MALI_GATOR_SUPPORT */
+
+		KBASE_TRACE_ADD(kbdev, PM_DESIRED_REACHED, NULL, NULL, kbdev->pm.gpu_in_desired_state, (u32)kbdev->pm.desired_shader_state);
+		KBASE_TRACE_ADD(kbdev, PM_DESIRED_REACHED_TILER, NULL, NULL, 0u, (u32)kbdev->pm.desired_tiler_state);
+
+		/* Log timelining information for synchronous waiters */
+		kbase_timeline_pm_send_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+		/* Wake slow-path waiters. Job scheduler does not use this. */
+		KBASE_TRACE_ADD(kbdev, PM_WAKE_WAITERS, NULL, NULL, 0u, 0);
+		wake_up(&kbdev->pm.gpu_in_desired_state_wait);
+	}
+	
+	spin_unlock(&kbdev->pm.gpu_powered_lock);
+
+	/* kbase_pm_ca_update_core_status can cause one-level recursion into
+	 * this function, so it must only be called once all changes to kbdev
+	 * have been committed, and after the gpu_powered_lock has been
+	 * dropped. */
+	if (kbdev->shader_ready_bitmap != shader_ready_bitmap ||
+	    kbdev->shader_transitioning_bitmap != shader_transitioning_bitmap) {
+		kbdev->shader_ready_bitmap = shader_ready_bitmap;
+		kbdev->shader_transitioning_bitmap = shader_transitioning_bitmap;
+
+		kbase_pm_ca_update_core_status(kbdev, shader_ready_bitmap, shader_transitioning_bitmap);
+	}
+
+	/* The core availability policy is not allowed to keep core group 0 off */
+	if (!((shader_ready_bitmap | shader_transitioning_bitmap) & kbdev->gpu_props.props.coherency_info.group[0].core_mask) &&
+	    !(kbase_pm_ca_get_core_mask(kbdev) & kbdev->gpu_props.props.coherency_info.group[0].core_mask))
+		BUG();
+
+	/* The core availability policy is allowed to keep core group 1 off, 
+	 * but all jobs specifically targeting CG1 must fail */
+	if (!((shader_ready_bitmap | shader_transitioning_bitmap) & kbdev->gpu_props.props.coherency_info.group[1].core_mask) &&
+	    !(kbase_pm_ca_get_core_mask(kbdev) & kbdev->gpu_props.props.coherency_info.group[1].core_mask))
+		kbdev->pm.cg1_disabled = MALI_TRUE;
+	else
+		kbdev->pm.cg1_disabled = MALI_FALSE;
+
+	return cores_are_available;
+}
+KBASE_EXPORT_TEST_API(kbase_pm_check_transitions_nolock)
+
+void kbase_pm_check_transitions_sync(struct kbase_device *kbdev)
+{
+	unsigned long flags;
+	mali_bool cores_are_available;
+	/* Force the transition to be checked and reported - the cores may be
+	 * 'available' (for job submission) but not fully powered up. */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	cores_are_available = kbase_pm_check_transitions_nolock(kbdev);
+	/* Don't need 'cores_are_available', because we don't return anything */
+	CSTD_UNUSED(cores_are_available);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	/* Wait for cores */
+	wait_event(kbdev->pm.gpu_in_desired_state_wait, kbdev->pm.gpu_in_desired_state);
+
+	/* Log timelining information that a change in state has completed */
+	kbase_timeline_pm_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+}
+KBASE_EXPORT_TEST_API(kbase_pm_check_transitions_sync)
+
+void kbase_pm_enable_interrupts(kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	/*
+	 * Clear all interrupts,
+	 * and unmask them all.
+	 */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), GPU_IRQ_REG_ALL, NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), GPU_IRQ_REG_ALL, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), 0xFFFFFFFF, NULL);
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_MASK), 0xFFFFFFFF, NULL);
+
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), 0xFFFFFFFF, NULL);
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0xFFFFFFFF, NULL);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_enable_interrupts)
+
+void kbase_pm_disable_interrupts(kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	/*
+	 * Mask all interrupts,
+	 * and clear them all.
+	 */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), 0, NULL);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_CLEAR), GPU_IRQ_REG_ALL, NULL);
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_MASK), 0, NULL);
+	kbase_reg_write(kbdev, JOB_CONTROL_REG(JOB_IRQ_CLEAR), 0xFFFFFFFF, NULL);
+
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_MASK), 0, NULL);
+	kbase_reg_write(kbdev, MMU_REG(MMU_IRQ_CLEAR), 0xFFFFFFFF, NULL);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_disable_interrupts)
+
+/*
+ * pmu layout:
+ * 0x0000: PMU TAG (RO) (0xCAFECAFE)
+ * 0x0004: PMU VERSION ID (RO) (0x00000000)
+ * 0x0008: CLOCK ENABLE (RW) (31:1 SBZ, 0 CLOCK STATE)
+ */
+void kbase_pm_clock_on(kbase_device *kbdev, mali_bool is_resume)
+{
+	mali_bool reset_required = is_resume;
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	if (kbdev->pm.gpu_powered) {
+		/* Already turned on */
+		KBASE_DEBUG_ASSERT(!is_resume);
+		return;
+	}
+
+	KBASE_TRACE_ADD(kbdev, PM_GPU_ON, NULL, NULL, 0u, 0u);
+
+	if (is_resume && kbdev->pm.callback_power_resume) {
+		kbdev->pm.callback_power_resume(kbdev);
+	} else if (kbdev->pm.callback_power_on) {
+		kbdev->pm.callback_power_on(kbdev);
+		/* If your platform properly keeps the GPU state you may use the return
+		 * value of the callback_power_on function to conditionally reset the
+		 * GPU on power up. Currently we are conservative and always reset the
+		 * GPU. */
+		reset_required = MALI_TRUE;
+	}
+
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+	kbdev->pm.gpu_powered = MALI_TRUE;
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (reset_required) {
+		/* GPU state was lost, reset GPU to ensure it is in a
+		 * consistent state */
+		kbase_pm_init_hw(kbdev, MALI_TRUE);
+	}
+
+	/* Lastly, enable the interrupts */
+	kbase_pm_enable_interrupts(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_clock_on)
+
+void kbase_pm_clock_off(kbase_device *kbdev, mali_bool is_suspend)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	/* ASSERT that the cores should now be unavailable. No lock needed. */
+	KBASE_DEBUG_ASSERT(kbdev->shader_available_bitmap == 0u);
+
+	if (!kbdev->pm.gpu_powered) {
+		/* Already turned off */
+		if (is_suspend && kbdev->pm.callback_power_suspend)
+			kbdev->pm.callback_power_suspend(kbdev);
+		return;
+	}
+
+	KBASE_TRACE_ADD(kbdev, PM_GPU_OFF, NULL, NULL, 0u, 0u);
+
+	/* Disable interrupts. This also clears any outstanding interrupts */
+	kbase_pm_disable_interrupts(kbdev);
+	/* Ensure that any IRQ handlers have finished */
+	kbase_synchronize_irqs(kbdev);
+
+	/* The GPU power may be turned off from this point */
+	spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+	kbdev->pm.gpu_powered = MALI_FALSE;
+	spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+
+	if (is_suspend && kbdev->pm.callback_power_suspend)
+		kbdev->pm.callback_power_suspend(kbdev);
+	else if (kbdev->pm.callback_power_off)
+		kbdev->pm.callback_power_off(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_clock_off)
+
+struct kbasep_reset_timeout_data {
+	struct hrtimer timer;
+	mali_bool timed_out;
+	kbase_device *kbdev;
+};
+
+void kbase_pm_reset_done(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	kbdev->pm.reset_done = MALI_TRUE;
+	wake_up(&kbdev->pm.reset_done_wait);
+}
+
+/**
+ * Wait for the RESET_COMPLETED IRQ to occur, then reset the waiting state.
+ */
+STATIC void kbase_pm_wait_for_reset(kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	wait_event(kbdev->pm.reset_done_wait, (kbdev->pm.reset_done));
+	kbdev->pm.reset_done = MALI_FALSE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_reset_done)
+
+static enum hrtimer_restart kbasep_reset_timeout(struct hrtimer *timer)
+{
+	struct kbasep_reset_timeout_data *rtdata = container_of(timer, struct kbasep_reset_timeout_data, timer);
+
+	rtdata->timed_out = 1;
+
+	/* Set the wait queue to wake up kbase_pm_init_hw even though the reset hasn't completed */
+	kbase_pm_reset_done(rtdata->kbdev);
+
+	return HRTIMER_NORESTART;
+}
+
+static void kbase_pm_hw_issues(kbase_device *kbdev)
+{
+	u32 value = 0;
+	u32 config_value;
+
+	/* Needed due to MIDBASE-1494: LS_PAUSEBUFFER_DISABLE. See PRLAM-8443.
+	 * and
+	 * needed due to MIDGLES-3539. See PRLAM-11035 */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_8443) ||
+			kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_11035))
+		value |= SC_LS_PAUSEBUFFER_DISABLE;
+
+	/* Needed due to MIDBASE-2054: SDC_DISABLE_OQ_DISCARD. See PRLAM-10327. */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10327))
+		value |= SC_SDC_DISABLE_OQ_DISCARD;
+
+	/* Enable alternative hardware counter selection if configured. */
+	if (DEFAULT_ALTERNATIVE_HWC)
+		value |= SC_ALT_COUNTERS;
+
+	/* Needed due to MIDBASE-2795. ENABLE_TEXGRD_FLAGS. See PRLAM-10797. */
+	if (kbase_hw_has_issue(kbdev, BASE_HW_ISSUE_10797))
+		value |= SC_ENABLE_TEXGRD_FLAGS;
+
+	if (value != 0)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(SHADER_CONFIG), value, NULL);
+
+	/* Limit the GPU bus bandwidth if the platform needs this. */
+	value = kbase_reg_read(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG), NULL);
+
+	/* Limit read ID width for AXI */
+	config_value = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_ARID_LIMIT);
+	value &= ~(L2_MMU_CONFIG_LIMIT_EXTERNAL_READS);
+	value |= (config_value & 0x3) << L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT;
+
+	/* Limit write ID width for AXI */
+	config_value = (u32) kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_AWID_LIMIT);
+	value &= ~(L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES);
+	value |= (config_value & 0x3) << L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT;
+
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(L2_MMU_CONFIG), value, NULL);
+}
+
+mali_error kbase_pm_init_hw(kbase_device *kbdev, mali_bool enable_irqs )
+{
+	unsigned long flags;
+	struct kbasep_reset_timeout_data rtdata;
+
+	KBASE_DEBUG_ASSERT(NULL != kbdev);
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	/* Ensure the clock is on before attempting to access the hardware */
+	if (!kbdev->pm.gpu_powered) {
+		if (kbdev->pm.callback_power_on)
+			kbdev->pm.callback_power_on(kbdev);
+
+		spin_lock_irqsave(&kbdev->pm.gpu_powered_lock, flags);
+		kbdev->pm.gpu_powered = MALI_TRUE;
+		spin_unlock_irqrestore(&kbdev->pm.gpu_powered_lock, flags);
+	}
+
+	/* Ensure interrupts are off to begin with, this also clears any outstanding interrupts */
+	kbase_pm_disable_interrupts(kbdev);
+
+	/* Prepare for the soft-reset */
+	kbdev->pm.reset_done = MALI_FALSE;
+
+	/* The cores should be made unavailable due to the reset */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	if (kbdev->shader_available_bitmap != 0u)
+			KBASE_TRACE_ADD(kbdev, PM_CORES_CHANGE_AVAILABLE, NULL, NULL, 0u, (u32)0u);
+	if (kbdev->tiler_available_bitmap != 0u)
+			KBASE_TRACE_ADD(kbdev, PM_CORES_CHANGE_AVAILABLE_TILER, NULL, NULL, 0u, (u32)0u);
+	kbdev->shader_available_bitmap = 0u;
+	kbdev->tiler_available_bitmap = 0u;
+	kbdev->l2_available_bitmap = 0u;
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	/* Soft reset the GPU */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_SOFT_RESET, NULL, NULL, 0u, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_SOFT_RESET, NULL);
+
+	/* Unmask the reset complete interrupt only */
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_IRQ_MASK), RESET_COMPLETED, NULL);
+
+	/* Initialize a structure for tracking the status of the reset */
+	rtdata.kbdev = kbdev;
+	rtdata.timed_out = 0;
+
+	/* Create a timer to use as a timeout on the reset */
+	hrtimer_init_on_stack(&rtdata.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	rtdata.timer.function = kbasep_reset_timeout;
+
+	hrtimer_start(&rtdata.timer, HR_TIMER_DELAY_MSEC(RESET_TIMEOUT), HRTIMER_MODE_REL);
+
+	/* Wait for the RESET_COMPLETED interrupt to be raised */
+	kbase_pm_wait_for_reset(kbdev);
+
+	if (rtdata.timed_out == 0) {
+		/* GPU has been reset */
+		hrtimer_cancel(&rtdata.timer);
+		destroy_hrtimer_on_stack(&rtdata.timer);
+		goto out;
+	}
+
+	/* No interrupt has been received - check if the RAWSTAT register says the reset has completed */
+	if (kbase_reg_read(kbdev, GPU_CONTROL_REG(GPU_IRQ_RAWSTAT), NULL) & RESET_COMPLETED) {
+		/* The interrupt is set in the RAWSTAT; this suggests that the interrupts are not getting to the CPU */
+		dev_warn(kbdev->dev, "Reset interrupt didn't reach CPU. Check interrupt assignments.\n");
+		/* If interrupts aren't working we can't continue. */
+		destroy_hrtimer_on_stack(&rtdata.timer);
+		goto out;
+	}
+
+	/* The GPU doesn't seem to be responding to the reset so try a hard reset */
+	dev_err(kbdev->dev, "Failed to soft-reset GPU (timed out after %d ms), now attempting a hard reset\n", RESET_TIMEOUT);
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET, NULL);
+
+	/* Restart the timer to wait for the hard reset to complete */
+	rtdata.timed_out = 0;
+
+	hrtimer_start(&rtdata.timer, HR_TIMER_DELAY_MSEC(RESET_TIMEOUT), HRTIMER_MODE_REL);
+
+	/* Wait for the RESET_COMPLETED interrupt to be raised */
+	kbase_pm_wait_for_reset(kbdev);
+
+	if (rtdata.timed_out == 0) {
+		/* GPU has been reset */
+		hrtimer_cancel(&rtdata.timer);
+		destroy_hrtimer_on_stack(&rtdata.timer);
+		goto out;
+	}
+
+	destroy_hrtimer_on_stack(&rtdata.timer);
+
+	dev_err(kbdev->dev, "Failed to hard-reset the GPU (timed out after %d ms)\n", RESET_TIMEOUT);
+
+	/* The GPU still hasn't reset, give up */
+	return MALI_ERROR_FUNCTION_FAILED;
+
+ out:
+	/* Re-enable interrupts if requested*/
+	if ( enable_irqs )
+	{
+		kbase_pm_enable_interrupts(kbdev);
+	}
+	/* If cycle counter was in use-re enable it */
+	spin_lock_irqsave(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+
+	if (kbdev->pm.gpu_cycle_counter_requests)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CYCLE_COUNT_START, NULL);
+
+	spin_unlock_irqrestore(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+
+	kbase_pm_hw_issues(kbdev);
+
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_init_hw)
+
+void kbase_pm_request_gpu_cycle_counter(kbase_device *kbdev)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_powered);
+
+	spin_lock_irqsave(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_cycle_counter_requests < INT_MAX);
+
+	++kbdev->pm.gpu_cycle_counter_requests;
+
+	if (1 == kbdev->pm.gpu_cycle_counter_requests)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CYCLE_COUNT_START, NULL);
+
+	spin_unlock_irqrestore(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_request_gpu_cycle_counter)
+
+void kbase_pm_release_gpu_cycle_counter(kbase_device *kbdev)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_cycle_counter_requests > 0);
+
+	--kbdev->pm.gpu_cycle_counter_requests;
+
+	if (0 == kbdev->pm.gpu_cycle_counter_requests)
+		kbase_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_CYCLE_COUNT_STOP, NULL);
+
+	spin_unlock_irqrestore(&kbdev->pm.gpu_cycle_counter_requests_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_release_gpu_cycle_counter)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics.c
new file mode 100644
index 0000000..a26b5f2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics.c
@@ -0,0 +1,340 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_metrics.c
+ * Metrics for power management
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+/* When VSync is being hit aim for utilisation between 70-90% */
+#define KBASE_PM_VSYNC_MIN_UTILISATION          70
+#define KBASE_PM_VSYNC_MAX_UTILISATION          90
+/* Otherwise aim for 10-40% */
+#define KBASE_PM_NO_VSYNC_MIN_UTILISATION       10
+#define KBASE_PM_NO_VSYNC_MAX_UTILISATION       40
+
+/* Shift used for kbasep_pm_metrics_data.time_busy/idle - units of (1 << 8) ns
+   This gives a maximum period between samples of 2^(32+8)/100 ns = slightly under 11s.
+   Exceeding this will cause overflow */
+#define KBASE_PM_TIME_SHIFT			8
+
+static enum hrtimer_restart dvfs_callback(struct hrtimer *timer)
+{
+	unsigned long flags;
+	kbase_pm_dvfs_action action;
+	kbasep_pm_metrics_data *metrics;
+
+	KBASE_DEBUG_ASSERT(timer != NULL);
+
+	metrics = container_of(timer, kbasep_pm_metrics_data, timer);
+	action = kbase_pm_get_dvfs_action(metrics->kbdev);
+
+	spin_lock_irqsave(&metrics->lock, flags);
+
+	if (metrics->timer_active)
+		hrtimer_start(timer,
+					  HR_TIMER_DELAY_MSEC(metrics->kbdev->pm.platform_dvfs_frequency),
+					  HRTIMER_MODE_REL);
+
+	spin_unlock_irqrestore(&metrics->lock, flags);
+
+	return HRTIMER_NORESTART;
+}
+
+mali_error kbasep_pm_metrics_init(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	kbdev->pm.metrics.kbdev = kbdev;
+	kbdev->pm.metrics.vsync_hit = 0;
+	kbdev->pm.metrics.utilisation = 0;
+	kbdev->pm.metrics.util_cl_share[0] = 0;
+	kbdev->pm.metrics.util_cl_share[1] = 0;
+	kbdev->pm.metrics.util_gl_share = 0;
+
+	kbdev->pm.metrics.time_period_start = ktime_get();
+	kbdev->pm.metrics.time_busy = 0;
+	kbdev->pm.metrics.time_idle = 0;
+	kbdev->pm.metrics.gpu_active = MALI_TRUE;
+	kbdev->pm.metrics.timer_active = MALI_TRUE;
+	kbdev->pm.metrics.active_cl_ctx[0] = 0;
+	kbdev->pm.metrics.active_cl_ctx[1] = 0;
+	kbdev->pm.metrics.active_gl_ctx = 0;
+	kbdev->pm.metrics.busy_cl[0] = 0;
+	kbdev->pm.metrics.busy_cl[1] = 0;
+	kbdev->pm.metrics.busy_gl = 0;
+
+	spin_lock_init(&kbdev->pm.metrics.lock);
+
+	hrtimer_init(&kbdev->pm.metrics.timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	kbdev->pm.metrics.timer.function = dvfs_callback;
+
+	hrtimer_start(&kbdev->pm.metrics.timer, HR_TIMER_DELAY_MSEC(kbdev->pm.platform_dvfs_frequency), HRTIMER_MODE_REL);
+
+	kbase_pm_register_vsync_callback(kbdev);
+
+	return MALI_ERROR_NONE;
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_metrics_init)
+
+void kbasep_pm_metrics_term(kbase_device *kbdev)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+	kbdev->pm.metrics.timer_active = MALI_FALSE;
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+
+	hrtimer_cancel(&kbdev->pm.metrics.timer);
+
+	kbase_pm_unregister_vsync_callback(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_metrics_term)
+
+/*caller needs to hold kbdev->pm.metrics.lock before calling this function*/
+void kbasep_pm_record_job_status(kbase_device *kbdev)
+{
+	ktime_t now;
+	ktime_t diff;
+	u32 ns_time;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	now = ktime_get();
+	diff = ktime_sub(now, kbdev->pm.metrics.time_period_start);
+
+	ns_time = (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+	kbdev->pm.metrics.time_busy += ns_time;
+	kbdev->pm.metrics.busy_gl += ns_time * kbdev->pm.metrics.active_gl_ctx;
+	kbdev->pm.metrics.busy_cl[0] += ns_time * kbdev->pm.metrics.active_cl_ctx[0];
+	kbdev->pm.metrics.busy_cl[1] += ns_time * kbdev->pm.metrics.active_cl_ctx[1];
+	kbdev->pm.metrics.time_period_start = now;
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_record_job_status)
+
+void kbasep_pm_record_gpu_idle(kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.metrics.gpu_active == MALI_TRUE);
+
+	kbdev->pm.metrics.gpu_active = MALI_FALSE;
+
+	kbasep_pm_record_job_status(kbdev);
+
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_record_gpu_idle)
+
+void kbasep_pm_record_gpu_active(kbase_device *kbdev)
+{
+	unsigned long flags;
+	ktime_t now;
+	ktime_t diff;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.metrics.gpu_active == MALI_FALSE);
+
+	kbdev->pm.metrics.gpu_active = MALI_TRUE;
+
+	now = ktime_get();
+	diff = ktime_sub(now, kbdev->pm.metrics.time_period_start);
+
+	kbdev->pm.metrics.time_idle += (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+	kbdev->pm.metrics.time_period_start = now;
+
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbasep_pm_record_gpu_active)
+
+void kbase_pm_report_vsync(kbase_device *kbdev, int buffer_updated)
+{
+	unsigned long flags;
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+	kbdev->pm.metrics.vsync_hit = buffer_updated;
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_report_vsync)
+
+/*caller needs to hold kbdev->pm.metrics.lock before calling this function*/
+int kbase_pm_get_dvfs_utilisation(kbase_device *kbdev, int *util_gl_share, int util_cl_share[2])
+{
+	int utilisation = 0;
+	int busy;
+	ktime_t now = ktime_get();
+	ktime_t diff;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	diff = ktime_sub(now, kbdev->pm.metrics.time_period_start);
+
+	if (kbdev->pm.metrics.gpu_active) {
+		u32 ns_time = (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+		kbdev->pm.metrics.time_busy += ns_time;
+		kbdev->pm.metrics.busy_cl[0] += ns_time * kbdev->pm.metrics.active_cl_ctx[0];
+		kbdev->pm.metrics.busy_cl[1] += ns_time * kbdev->pm.metrics.active_cl_ctx[1];
+		kbdev->pm.metrics.busy_gl += ns_time * kbdev->pm.metrics.active_gl_ctx;
+		kbdev->pm.metrics.time_period_start = now;
+	} else {
+		kbdev->pm.metrics.time_idle += (u32) (ktime_to_ns(diff) >> KBASE_PM_TIME_SHIFT);
+		kbdev->pm.metrics.time_period_start = now;
+	}
+
+	if (kbdev->pm.metrics.time_idle + kbdev->pm.metrics.time_busy == 0) {
+		/* No data - so we return NOP */
+		utilisation = -1;
+		if (util_gl_share)
+			*util_gl_share = -1;
+		if (util_cl_share) {
+			util_cl_share[0] = -1;
+			util_cl_share[1] = -1;
+		}
+		goto out;
+	}
+
+	utilisation = (100 * kbdev->pm.metrics.time_busy) /
+			(kbdev->pm.metrics.time_idle +
+			 kbdev->pm.metrics.time_busy);
+
+	busy = kbdev->pm.metrics.busy_gl +
+		kbdev->pm.metrics.busy_cl[0] +
+		kbdev->pm.metrics.busy_cl[1];
+
+	if (busy != 0) {
+		if (util_gl_share)
+			*util_gl_share =
+				(100 * kbdev->pm.metrics.busy_gl) / busy;
+		if (util_cl_share) {
+			util_cl_share[0] =
+				(100 * kbdev->pm.metrics.busy_cl[0]) / busy;
+			util_cl_share[1] =
+				(100 * kbdev->pm.metrics.busy_cl[1]) / busy;
+		}
+	} else {
+		if (util_gl_share)
+			*util_gl_share = -1;
+		if (util_cl_share) {
+			util_cl_share[0] = -1;
+			util_cl_share[1] = -1;
+		}
+	}
+
+out:
+
+	kbdev->pm.metrics.time_idle = 0;
+	kbdev->pm.metrics.time_busy = 0;
+	kbdev->pm.metrics.busy_cl[0] = 0;
+	kbdev->pm.metrics.busy_cl[1] = 0;
+	kbdev->pm.metrics.busy_gl = 0;
+
+	return utilisation;
+}
+
+kbase_pm_dvfs_action kbase_pm_get_dvfs_action(kbase_device *kbdev)
+{
+	unsigned long flags;
+	int utilisation, util_gl_share;
+	int util_cl_share[2];
+	kbase_pm_dvfs_action action;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+
+	utilisation = kbase_pm_get_dvfs_utilisation(kbdev, &util_gl_share, util_cl_share);
+
+	if (utilisation < 0 || util_gl_share < 0 || util_cl_share < 0) {
+		action = KBASE_PM_DVFS_NOP;
+		utilisation = 0;
+		util_gl_share = 0;
+		util_cl_share[0] = 0;
+		util_cl_share[1] = 0;
+		goto out;
+	}
+
+	if (kbdev->pm.metrics.vsync_hit) {
+		/* VSync is being met */
+		if (utilisation < KBASE_PM_VSYNC_MIN_UTILISATION)
+			action = KBASE_PM_DVFS_CLOCK_DOWN;
+		else if (utilisation > KBASE_PM_VSYNC_MAX_UTILISATION)
+			action = KBASE_PM_DVFS_CLOCK_UP;
+		else
+			action = KBASE_PM_DVFS_NOP;
+	} else {
+		/* VSync is being missed */
+		if (utilisation < KBASE_PM_NO_VSYNC_MIN_UTILISATION)
+			action = KBASE_PM_DVFS_CLOCK_DOWN;
+		else if (utilisation > KBASE_PM_NO_VSYNC_MAX_UTILISATION)
+			action = KBASE_PM_DVFS_CLOCK_UP;
+		else
+			action = KBASE_PM_DVFS_NOP;
+	}
+
+	kbdev->pm.metrics.utilisation = utilisation;
+	kbdev->pm.metrics.util_cl_share[0] = util_cl_share[0];
+	kbdev->pm.metrics.util_cl_share[1] = util_cl_share[1];
+	kbdev->pm.metrics.util_gl_share = util_gl_share;
+out:
+#ifdef CONFIG_MALI_MIDGARD_DVFS
+	kbase_platform_dvfs_event(kbdev, utilisation, util_gl_share, util_cl_share);
+#endif				/*CONFIG_MALI_MIDGARD_DVFS */
+	kbdev->pm.metrics.time_idle = 0;
+	kbdev->pm.metrics.time_busy = 0;
+	kbdev->pm.metrics.busy_cl[0] = 0;
+	kbdev->pm.metrics.busy_cl[1] = 0;
+	kbdev->pm.metrics.busy_gl = 0;
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+
+	return action;
+}
+KBASE_EXPORT_TEST_API(kbase_pm_get_dvfs_action)
+
+mali_bool kbase_pm_metrics_is_active(kbase_device *kbdev)
+{
+	mali_bool isactive;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.metrics.lock, flags);
+	isactive = (kbdev->pm.metrics.timer_active == MALI_TRUE);
+	spin_unlock_irqrestore(&kbdev->pm.metrics.lock, flags);
+
+	return isactive;
+}
+KBASE_EXPORT_TEST_API(kbase_pm_metrics_is_active)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics_dummy.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics_dummy.c
new file mode 100644
index 0000000..81dd06b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_metrics_dummy.c
@@ -0,0 +1,39 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_pm_metrics_dummy.c
+ * Dummy Metrics for power management.
+ */
+
+#include <mali_kbase.h>
+#include <mali_kbase_pm.h>
+
+void kbase_pm_register_vsync_callback(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	/* no VSync metrics will be available */
+	kbdev->pm.metrics.platform_data = NULL;
+}
+
+void kbase_pm_unregister_vsync_callback(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.c
new file mode 100644
index 0000000..7e43512
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.c
@@ -0,0 +1,800 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_policy.c
+ * Power policy API implementations
+ */
+
+#include <mali_kbase.h>
+#include <mali_midg_regmap.h>
+#include <mali_kbase_gator.h>
+#include <mali_kbase_pm.h>
+
+extern const kbase_pm_policy kbase_pm_always_on_policy_ops;
+extern const kbase_pm_policy kbase_pm_coarse_demand_policy_ops;
+extern const kbase_pm_policy kbase_pm_demand_policy_ops;
+
+#if MALI_CUSTOMER_RELEASE == 0 
+extern const kbase_pm_policy kbase_pm_fast_start_policy_ops;
+extern const kbase_pm_policy kbase_pm_demand_always_powered_policy_ops;
+#endif
+
+static const kbase_pm_policy *const policy_list[] = {
+#ifdef CONFIG_MALI_NO_MALI
+	&kbase_pm_always_on_policy_ops,
+	&kbase_pm_demand_policy_ops,
+	&kbase_pm_coarse_demand_policy_ops,
+#if MALI_CUSTOMER_RELEASE == 0 
+	&kbase_pm_demand_always_powered_policy_ops,
+	&kbase_pm_fast_start_policy_ops,
+#endif
+#else				/* CONFIG_MALI_NO_MALI */
+	&kbase_pm_demand_policy_ops,
+	&kbase_pm_always_on_policy_ops,
+	&kbase_pm_coarse_demand_policy_ops,
+#if MALI_CUSTOMER_RELEASE == 0        
+	&kbase_pm_demand_always_powered_policy_ops,
+	&kbase_pm_fast_start_policy_ops,
+#endif
+#endif				/* CONFIG_MALI_NO_MALI */
+};
+
+/** The number of policies available in the system.
+ * This is derived from the number of functions listed in policy_get_functions.
+ */
+#define POLICY_COUNT (sizeof(policy_list)/sizeof(*policy_list))
+
+
+/* Function IDs for looking up Timeline Trace codes in kbase_pm_change_state_trace_code */
+typedef enum
+{
+	KBASE_PM_FUNC_ID_REQUEST_CORES_START,
+	KBASE_PM_FUNC_ID_REQUEST_CORES_END,
+	KBASE_PM_FUNC_ID_RELEASE_CORES_START,
+	KBASE_PM_FUNC_ID_RELEASE_CORES_END,
+	/* Note: kbase_pm_unrequest_cores() is on the slow path, and we neither
+	 * expect to hit it nor tend to hit it very much anyway. We can detect
+	 * whether we need more instrumentation by a difference between
+	 * PM_CHECKTRANS events and PM_SEND/HANDLE_EVENT. */
+
+	/* Must be the last */
+	KBASE_PM_FUNC_ID_COUNT
+} kbase_pm_func_id;
+
+
+/* State changes during request/unrequest/release-ing cores */
+enum
+{
+	KBASE_PM_CHANGE_STATE_SHADER = (1u << 0),
+	KBASE_PM_CHANGE_STATE_TILER  = (1u << 1),
+
+	/* These two must be last */
+	KBASE_PM_CHANGE_STATE_MASK = (KBASE_PM_CHANGE_STATE_TILER|KBASE_PM_CHANGE_STATE_SHADER),
+	KBASE_PM_CHANGE_STATE_COUNT = KBASE_PM_CHANGE_STATE_MASK + 1
+};
+typedef u32 kbase_pm_change_state;
+
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+/* Timeline Trace code lookups for each function */
+static u32 kbase_pm_change_state_trace_code[KBASE_PM_FUNC_ID_COUNT][KBASE_PM_CHANGE_STATE_COUNT] =
+{
+	/* kbase_pm_request_cores */
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_START][0] = 0,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_START][KBASE_PM_CHANGE_STATE_SHADER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_START,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_START][KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_TILER_START,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_START][KBASE_PM_CHANGE_STATE_SHADER|KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_TILER_START,
+
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_END][0] = 0,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_END][KBASE_PM_CHANGE_STATE_SHADER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_END,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_END][KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_TILER_END,
+	[KBASE_PM_FUNC_ID_REQUEST_CORES_END][KBASE_PM_CHANGE_STATE_SHADER|KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_TILER_END,
+
+	/* kbase_pm_release_cores */
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_START][0] = 0,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_START][KBASE_PM_CHANGE_STATE_SHADER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_START,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_START][KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_TILER_START,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_START][KBASE_PM_CHANGE_STATE_SHADER|KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_TILER_START,
+
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_END][0] = 0,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_END][KBASE_PM_CHANGE_STATE_SHADER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_END,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_END][KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_TILER_END,
+	[KBASE_PM_FUNC_ID_RELEASE_CORES_END][KBASE_PM_CHANGE_STATE_SHADER|KBASE_PM_CHANGE_STATE_TILER] =
+		SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_TILER_END
+};
+
+STATIC INLINE void kbase_timeline_pm_cores_func(kbase_device *kbdev,
+                                                kbase_pm_func_id func_id,
+                                                kbase_pm_change_state state)
+{
+	int trace_code;
+	KBASE_DEBUG_ASSERT(func_id >= 0 && func_id < KBASE_PM_FUNC_ID_COUNT);
+	KBASE_DEBUG_ASSERT(state != 0 && (state & KBASE_PM_CHANGE_STATE_MASK) == state);
+
+	trace_code = kbase_pm_change_state_trace_code[func_id][state];
+	KBASE_TIMELINE_PM_CHECKTRANS(kbdev, trace_code);
+}
+
+#else /* CONFIG_MALI_TRACE_TIMELINE */
+STATIC INLINE void kbase_timeline_pm_cores_func(kbase_device *kbdev,
+                                                kbase_pm_func_id func_id,
+                                                kbase_pm_change_state state)
+{
+}
+
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
+
+static enum hrtimer_restart kbasep_pm_do_gpu_poweroff_callback(struct hrtimer *timer)
+{
+	kbase_device *kbdev;
+
+	kbdev = container_of(timer, kbase_device, pm.gpu_poweroff_timer);
+
+	/* It is safe for this call to do nothing if the work item is already queued.
+	 * The worker function will read the must up-to-date state of kbdev->pm.gpu_poweroff_pending
+	 * under lock.
+	 *
+	 * If a state change occurs while the worker function is processing, this
+	 * call will succeed as a work item can be requeued once it has started
+	 * processing. 
+	 */
+	if (kbdev->pm.gpu_poweroff_pending)
+		queue_work(kbdev->pm.gpu_poweroff_wq, &kbdev->pm.gpu_poweroff_work);
+
+	if (kbdev->pm.shader_poweroff_pending) {
+		unsigned long flags;
+
+		spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+		if (kbdev->pm.shader_poweroff_pending) {
+			kbdev->pm.shader_poweroff_pending_time--;
+
+			KBASE_DEBUG_ASSERT(kbdev->pm.shader_poweroff_pending_time >= 0);
+
+			if (kbdev->pm.shader_poweroff_pending_time == 0) {
+				u64 prev_shader_state = kbdev->pm.desired_shader_state;
+
+				kbdev->pm.desired_shader_state &= ~kbdev->pm.shader_poweroff_pending;
+				kbdev->pm.shader_poweroff_pending = 0;
+
+				if (prev_shader_state != kbdev->pm.desired_shader_state ||
+			    	    kbdev->pm.ca_in_transition != MALI_FALSE) {
+					mali_bool cores_are_available;
+
+					KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_DEFERRED_START);
+					cores_are_available = kbase_pm_check_transitions_nolock(kbdev);
+					KBASE_TIMELINE_PM_CHECKTRANS(kbdev, SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_DEFERRED_END);		
+
+					/* Don't need 'cores_are_available', because we don't return anything */
+					CSTD_UNUSED(cores_are_available);
+				}
+			}
+		}
+
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+	}
+
+	hrtimer_add_expires(timer, kbdev->pm.gpu_poweroff_time);
+	return HRTIMER_RESTART;
+}
+
+static void kbasep_pm_do_gpu_poweroff_wq(struct work_struct *data)
+{
+	unsigned long flags;
+	kbase_device *kbdev;
+	mali_bool do_poweroff = MALI_FALSE;
+
+	kbdev = container_of(data, kbase_device, pm.gpu_poweroff_work);
+
+	mutex_lock(&kbdev->pm.lock);
+
+	if (kbdev->pm.gpu_poweroff_pending == 0) {
+		mutex_unlock(&kbdev->pm.lock);
+		return;
+	}
+
+	kbdev->pm.gpu_poweroff_pending--;
+
+	if (kbdev->pm.gpu_poweroff_pending > 0) {
+		mutex_unlock(&kbdev->pm.lock);
+		return;
+	}
+
+	KBASE_DEBUG_ASSERT(kbdev->pm.gpu_poweroff_pending == 0);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	/* Only power off the GPU if a request is still pending */
+	if (kbdev->pm.pm_current_policy->get_core_active(kbdev) == MALI_FALSE)
+		do_poweroff = MALI_TRUE;
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	if (do_poweroff != MALI_FALSE) {
+		kbdev->pm.poweroff_timer_running = MALI_FALSE;
+		/* Power off the GPU */
+		kbase_pm_do_poweroff(kbdev, MALI_FALSE);
+		hrtimer_cancel(&kbdev->pm.gpu_poweroff_timer);
+	}
+
+	mutex_unlock(&kbdev->pm.lock);
+}
+
+mali_error kbase_pm_policy_init(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	kbdev->pm.gpu_poweroff_wq = alloc_workqueue("kbase_pm_do_poweroff", WQ_HIGHPRI | WQ_UNBOUND, 1);
+	if (NULL == kbdev->pm.gpu_poweroff_wq)
+		return MALI_ERROR_OUT_OF_MEMORY;
+	INIT_WORK(&kbdev->pm.gpu_poweroff_work, kbasep_pm_do_gpu_poweroff_wq);
+
+	hrtimer_init(&kbdev->pm.gpu_poweroff_timer, CLOCK_MONOTONIC, HRTIMER_MODE_REL);
+	kbdev->pm.gpu_poweroff_timer.function = kbasep_pm_do_gpu_poweroff_callback;
+
+	kbdev->pm.pm_current_policy = policy_list[0];
+
+	kbdev->pm.pm_current_policy->init(kbdev);
+
+	kbdev->pm.gpu_poweroff_time = HR_TIMER_DELAY_NSEC(kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_PM_GPU_POWEROFF_TICK_NS));
+
+	kbdev->pm.poweroff_shader_ticks = kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_SHADER);
+	kbdev->pm.poweroff_gpu_ticks = kbasep_get_config_value(kbdev, kbdev->config_attributes, KBASE_CONFIG_ATTR_PM_POWEROFF_TICK_GPU);
+
+	return MALI_ERROR_NONE;
+}
+
+void kbase_pm_policy_term(kbase_device *kbdev)
+{
+	kbdev->pm.pm_current_policy->term(kbdev);
+}
+
+void kbase_pm_cancel_deferred_poweroff(kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	hrtimer_cancel(&kbdev->pm.gpu_poweroff_timer);
+
+	/* If wq is already running but is held off by pm.lock, make sure it has no effect */
+	kbdev->pm.gpu_poweroff_pending = 0;
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	kbdev->pm.shader_poweroff_pending = 0;
+	kbdev->pm.shader_poweroff_pending_time = 0;
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+void kbase_pm_update_active(kbase_device *kbdev)
+{
+	unsigned long flags;
+	mali_bool active;
+
+	lockdep_assert_held(&kbdev->pm.lock);
+
+	/* pm_current_policy will never be NULL while pm.lock is held */
+	KBASE_DEBUG_ASSERT(kbdev->pm.pm_current_policy);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	active = kbdev->pm.pm_current_policy->get_core_active(kbdev);
+
+	if (active != MALI_FALSE) {
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+		if (kbdev->pm.gpu_poweroff_pending) {
+			/* Cancel any pending power off request */
+			kbdev->pm.gpu_poweroff_pending = 0;
+
+			/* If a request was pending then the GPU was still powered, so no need to continue */
+			return;
+		}
+
+		if (!kbdev->pm.poweroff_timer_running && !kbdev->pm.gpu_powered) {
+			kbdev->pm.poweroff_timer_running = MALI_TRUE;
+			hrtimer_start(&kbdev->pm.gpu_poweroff_timer, kbdev->pm.gpu_poweroff_time, HRTIMER_MODE_REL);
+		}
+
+		/* Power on the GPU and any cores requested by the policy */
+		kbase_pm_do_poweron(kbdev, MALI_FALSE);
+	} else {
+		/* It is an error for the power policy to power off the GPU
+		 * when there are contexts active */
+		KBASE_DEBUG_ASSERT(kbdev->pm.active_count == 0);
+
+		if (kbdev->pm.shader_poweroff_pending) {
+			kbdev->pm.shader_poweroff_pending = 0;
+			kbdev->pm.shader_poweroff_pending_time = 0;
+		}
+
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+
+		/* Request power off */
+		if (kbdev->pm.gpu_powered) {
+			kbdev->pm.gpu_poweroff_pending = kbdev->pm.poweroff_gpu_ticks;
+			if (!kbdev->pm.poweroff_timer_running) {
+				/* Start timer if not running (eg if power policy has been changed from always_on
+				 * to something else). This will ensure the GPU is actually powered off */
+				kbdev->pm.poweroff_timer_running = MALI_TRUE;
+				hrtimer_start(&kbdev->pm.gpu_poweroff_timer, kbdev->pm.gpu_poweroff_time, HRTIMER_MODE_REL);
+			}
+		}
+	}
+}
+
+void kbase_pm_update_cores_state_nolock(kbase_device *kbdev)
+{
+	u64 desired_bitmap;
+	mali_bool cores_are_available;
+
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+
+	if (kbdev->pm.pm_current_policy == NULL)
+		return;
+
+	desired_bitmap = kbdev->pm.pm_current_policy->get_core_mask(kbdev); 
+	desired_bitmap &= kbase_pm_ca_get_core_mask(kbdev);
+
+	/* Enable core 0 if tiler required, regardless of core availability */
+	if (kbdev->tiler_needed_cnt > 0 || kbdev->tiler_inuse_cnt > 0)
+		desired_bitmap |= 1;
+
+	if (kbdev->pm.desired_shader_state != desired_bitmap)
+		KBASE_TRACE_ADD(kbdev, PM_CORES_CHANGE_DESIRED, NULL, NULL, 0u, (u32)desired_bitmap);
+
+	/* Are any cores being powered on? */
+	if (~kbdev->pm.desired_shader_state & desired_bitmap ||
+	    kbdev->pm.ca_in_transition != MALI_FALSE) {
+
+		/* Check if we are powering off any cores before updating shader state */
+		if (kbdev->pm.desired_shader_state & ~desired_bitmap) {
+			/* Start timer to power off cores */
+			kbdev->pm.shader_poweroff_pending |= (kbdev->pm.desired_shader_state & ~desired_bitmap);
+			kbdev->pm.shader_poweroff_pending_time = kbdev->pm.poweroff_shader_ticks;
+		}
+
+		kbdev->pm.desired_shader_state = desired_bitmap;
+
+		/* If any cores are being powered on, transition immediately */
+		cores_are_available = kbase_pm_check_transitions_nolock(kbdev);
+	} else if (kbdev->pm.desired_shader_state & ~desired_bitmap) {
+		/* Start timer to power off cores */
+		kbdev->pm.shader_poweroff_pending |= (kbdev->pm.desired_shader_state & ~desired_bitmap);
+		kbdev->pm.shader_poweroff_pending_time = kbdev->pm.poweroff_shader_ticks;
+	} else if (kbdev->pm.active_count == 0 && desired_bitmap != 0 && kbdev->pm.poweroff_timer_running) {
+		/* If power policy is keeping cores on despite there being no active contexts
+		 * then disable poweroff timer as it isn't required */
+		kbdev->pm.poweroff_timer_running = MALI_FALSE;
+		hrtimer_cancel(&kbdev->pm.gpu_poweroff_timer);
+	}
+
+	/* Ensure timer does not power off wanted cores and make sure to power off unwanted cores */
+	if (kbdev->pm.shader_poweroff_pending != 0) {
+		kbdev->pm.shader_poweroff_pending &= ~(kbdev->pm.desired_shader_state & desired_bitmap);
+		if (kbdev->pm.shader_poweroff_pending == 0)
+			kbdev->pm.shader_poweroff_pending_time = 0;
+	}
+
+	/* Don't need 'cores_are_available', because we don't return anything */
+	CSTD_UNUSED(cores_are_available);
+}
+
+void kbase_pm_update_cores_state(kbase_device *kbdev)
+{
+	unsigned long flags;
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	kbase_pm_update_cores_state_nolock(kbdev);
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+int kbase_pm_list_policies(const kbase_pm_policy * const **list)
+{
+	if (!list)
+		return POLICY_COUNT;
+
+	*list = policy_list;
+
+	return POLICY_COUNT;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_list_policies)
+
+const kbase_pm_policy *kbase_pm_get_policy(kbase_device *kbdev)
+{
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	return kbdev->pm.pm_current_policy;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_get_policy)
+
+void kbase_pm_set_policy(kbase_device *kbdev, const kbase_pm_policy *new_policy)
+{
+	const kbase_pm_policy *old_policy;
+	unsigned long flags;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+	KBASE_DEBUG_ASSERT(new_policy != NULL);
+
+	KBASE_TRACE_ADD(kbdev, PM_SET_POLICY, NULL, NULL, 0u, new_policy->id);
+
+	/* During a policy change we pretend the GPU is active */
+	/* A suspend won't happen here, because we're in a syscall from a userspace thread */
+	kbase_pm_context_active(kbdev);
+
+	mutex_lock(&kbdev->pm.lock);
+
+	/* Remove the policy to prevent IRQ handlers from working on it */
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	old_policy = kbdev->pm.pm_current_policy;
+	kbdev->pm.pm_current_policy = NULL;
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	KBASE_TRACE_ADD(kbdev, PM_CURRENT_POLICY_TERM, NULL, NULL, 0u, old_policy->id);
+	if (old_policy->term)
+		old_policy->term(kbdev);
+
+	KBASE_TRACE_ADD(kbdev, PM_CURRENT_POLICY_INIT, NULL, NULL, 0u, new_policy->id);
+	if (new_policy->init)
+		new_policy->init(kbdev);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+	kbdev->pm.pm_current_policy = new_policy;
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	/* If any core power state changes were previously attempted, but couldn't
+	 * be made because the policy was changing (current_policy was NULL), then
+	 * re-try them here. */
+	kbase_pm_update_active(kbdev);
+	kbase_pm_update_cores_state(kbdev);
+
+	mutex_unlock(&kbdev->pm.lock);
+
+	/* Now the policy change is finished, we release our fake context active reference */
+	kbase_pm_context_idle(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_set_policy)
+
+/** Check whether a state change has finished, and trace it as completed */
+STATIC void kbase_pm_trace_check_and_finish_state_change(kbase_device *kbdev)
+{
+	if ((kbdev->shader_available_bitmap & kbdev->pm.desired_shader_state) == kbdev->pm.desired_shader_state
+		&& (kbdev->tiler_available_bitmap & kbdev->pm.desired_tiler_state) == kbdev->pm.desired_tiler_state)
+		kbase_timeline_pm_check_handle_event(kbdev, KBASE_TIMELINE_PM_EVENT_GPU_STATE_CHANGED);
+}
+
+void kbase_pm_request_cores(kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores)
+{
+	unsigned long flags;
+	u64 cores;
+
+	kbase_pm_change_state change_gpu_state = 0u;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	cores = shader_cores;
+	while (cores) {
+		int bitnum = fls64(cores) - 1;
+		u64 bit = 1ULL << bitnum;
+
+		/* It should be almost impossible for this to overflow. It would require 2^32 atoms
+		 * to request a particular core, which would require 2^24 contexts to submit. This 
+		 * would require an amount of memory that is impossible on a 32-bit system and 
+		 * extremely unlikely on a 64-bit system. */
+		int cnt = ++kbdev->shader_needed_cnt[bitnum];
+
+		if (1 == cnt) {
+			kbdev->shader_needed_bitmap |= bit;
+			change_gpu_state |= KBASE_PM_CHANGE_STATE_SHADER;
+		}
+
+		cores &= ~bit;
+	}
+
+	if (tiler_required != MALI_FALSE) {
+		++kbdev->tiler_needed_cnt;
+
+		KBASE_DEBUG_ASSERT(kbdev->tiler_needed_cnt != 0);
+
+		/* For tiler jobs, we must make sure that core 0 is not turned off if it's already on.
+	         * However, it's safe for core 0 to be left off and turned on later whilst a tiler job
+		 * is running. Hence, we don't need to update the cores state immediately. Also,
+		 * attempts to turn off cores will always check the tiler_needed/inuse state first anyway.
+		 *
+		 * Finally, kbase_js_choose_affinity() ensures core 0 is always requested for tiler jobs
+		 * anyway. Hence when there's only a tiler job in the system, this will still cause
+		 * kbase_pm_update_cores_state_nolock() to be called.
+		 *
+		 * Note that we still need to keep track of tiler_needed/inuse_cnt, to ensure that
+		 * kbase_pm_update_cores_state_nolock() can override the core availability policy and
+		 * force core 0 to be powered when a tiler job is in the system. */
+	}
+
+	if (change_gpu_state) {
+		KBASE_TRACE_ADD(kbdev, PM_REQUEST_CHANGE_SHADER_NEEDED, NULL, NULL, 0u, (u32) kbdev->shader_needed_bitmap);
+
+		kbase_timeline_pm_cores_func(kbdev, KBASE_PM_FUNC_ID_REQUEST_CORES_START, change_gpu_state);
+		kbase_pm_update_cores_state_nolock(kbdev);
+		kbase_timeline_pm_cores_func(kbdev, KBASE_PM_FUNC_ID_REQUEST_CORES_END, change_gpu_state);
+	}
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_request_cores)
+
+void kbase_pm_unrequest_cores(kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores)
+{
+	unsigned long flags;
+
+	kbase_pm_change_state change_gpu_state = 0u;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	while (shader_cores) {
+		int bitnum = fls64(shader_cores) - 1;
+		u64 bit = 1ULL << bitnum;
+		int cnt;
+
+		KBASE_DEBUG_ASSERT(kbdev->shader_needed_cnt[bitnum] > 0);
+
+		cnt = --kbdev->shader_needed_cnt[bitnum];
+
+		if (0 == cnt) {
+			kbdev->shader_needed_bitmap &= ~bit;
+
+			change_gpu_state |= KBASE_PM_CHANGE_STATE_SHADER;
+		}
+
+		shader_cores &= ~bit;
+	}
+
+	if (tiler_required != MALI_FALSE) {
+		KBASE_DEBUG_ASSERT(kbdev->tiler_needed_cnt > 0);
+
+		--kbdev->tiler_needed_cnt;
+
+		/* Whilst tiler jobs must not allow core 0 to be turned off, we don't need to make an
+		 * extra call to kbase_pm_update_cores_state_nolock() to ensure core 0 is turned off
+		 * when the last tiler job unrequests cores: kbase_js_choose_affinity() ensures core 0
+		 * was originally requested for tiler jobs. Hence when there's only a tiler job in the
+		 * system, this will still cause kbase_pm_update_cores_state_nolock() to be called. */
+	}
+
+	if (change_gpu_state) {
+		KBASE_TRACE_ADD(kbdev, PM_UNREQUEST_CHANGE_SHADER_NEEDED, NULL, NULL, 0u, (u32) kbdev->shader_needed_bitmap);
+
+		kbase_pm_update_cores_state_nolock(kbdev);
+
+		/* Trace that any state change effectively completes immediately -
+		 * no-one will wait on the state change */
+		kbase_pm_trace_check_and_finish_state_change(kbdev);
+	}
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_unrequest_cores)
+
+kbase_pm_cores_ready kbase_pm_register_inuse_cores(kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores)
+{
+	unsigned long flags;
+	u64 prev_shader_needed;	/* Just for tracing */
+	u64 prev_shader_inuse;	/* Just for tracing */
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	prev_shader_needed = kbdev->shader_needed_bitmap;
+	prev_shader_inuse = kbdev->shader_inuse_bitmap;
+
+	/* If desired_shader_state does not contain the requested cores, then power
+	 * management is not attempting to powering those cores (most likely
+	 * due to core availability policy) and a new job affinity must be
+	 * chosen */
+	if ((kbdev->pm.desired_shader_state & shader_cores) != shader_cores) {
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+		return KBASE_NEW_AFFINITY;
+	}
+
+	if ((kbdev->shader_available_bitmap & shader_cores) != shader_cores ||
+	    (tiler_required != MALI_FALSE && !kbdev->tiler_available_bitmap)) {
+		/* Trace ongoing core transition */
+		kbase_timeline_pm_l2_transition_start(kbdev);
+		spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+		return KBASE_CORES_NOT_READY;
+	}
+
+	/* If we started to trace a state change, then trace it has being finished
+	 * by now, at the very latest */
+	kbase_pm_trace_check_and_finish_state_change(kbdev);
+	/* Trace core transition done */
+	kbase_timeline_pm_l2_transition_done(kbdev);
+
+	while (shader_cores) {
+		int bitnum = fls64(shader_cores) - 1;
+		u64 bit = 1ULL << bitnum;
+		int cnt;
+
+		KBASE_DEBUG_ASSERT(kbdev->shader_needed_cnt[bitnum] > 0);
+
+		cnt = --kbdev->shader_needed_cnt[bitnum];
+
+		if (0 == cnt)
+			kbdev->shader_needed_bitmap &= ~bit;
+
+		/* shader_inuse_cnt should not overflow because there can only be a
+		 * very limited number of jobs on the h/w at one time */
+
+		kbdev->shader_inuse_cnt[bitnum]++;
+		kbdev->shader_inuse_bitmap |= bit;
+
+		shader_cores &= ~bit;
+	}
+
+	if (tiler_required != MALI_FALSE) {
+		KBASE_DEBUG_ASSERT(kbdev->tiler_needed_cnt > 0);
+
+		--kbdev->tiler_needed_cnt;
+
+		kbdev->tiler_inuse_cnt++;
+
+		KBASE_DEBUG_ASSERT(kbdev->tiler_inuse_cnt != 0);
+	}
+
+	if (prev_shader_needed != kbdev->shader_needed_bitmap)
+		KBASE_TRACE_ADD(kbdev, PM_REGISTER_CHANGE_SHADER_NEEDED, NULL, NULL, 0u, (u32) kbdev->shader_needed_bitmap);
+
+	if (prev_shader_inuse != kbdev->shader_inuse_bitmap)
+		KBASE_TRACE_ADD(kbdev, PM_REGISTER_CHANGE_SHADER_INUSE, NULL, NULL, 0u, (u32) kbdev->shader_inuse_bitmap);
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+
+	return KBASE_CORES_READY;
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_register_inuse_cores)
+
+void kbase_pm_release_cores(kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores)
+{
+	unsigned long flags;
+	kbase_pm_change_state change_gpu_state = 0u;
+
+	KBASE_DEBUG_ASSERT(kbdev != NULL);
+
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	while (shader_cores) {
+		int bitnum = fls64(shader_cores) - 1;
+		u64 bit = 1ULL << bitnum;
+		int cnt;
+
+		KBASE_DEBUG_ASSERT(kbdev->shader_inuse_cnt[bitnum] > 0);
+
+		cnt = --kbdev->shader_inuse_cnt[bitnum];
+
+		if (0 == cnt) {
+			kbdev->shader_inuse_bitmap &= ~bit;
+			change_gpu_state |= KBASE_PM_CHANGE_STATE_SHADER;
+		}
+
+		shader_cores &= ~bit;
+	}
+
+	if (tiler_required != MALI_FALSE) {
+		KBASE_DEBUG_ASSERT(kbdev->tiler_inuse_cnt > 0);
+
+		--kbdev->tiler_inuse_cnt;
+
+		/* Whilst tiler jobs must not allow core 0 to be turned off, we don't need to make an
+		 * extra call to kbase_pm_update_cores_state_nolock() to ensure core 0 is turned off
+		 * when the last tiler job finishes: kbase_js_choose_affinity() ensures core 0 was
+		 * originally requested for tiler jobs. Hence when there's only a tiler job in the
+		 * system, this will still cause kbase_pm_update_cores_state_nolock() to be called */
+	}
+
+	if (change_gpu_state) {
+		KBASE_TRACE_ADD(kbdev, PM_RELEASE_CHANGE_SHADER_INUSE, NULL, NULL, 0u, (u32) kbdev->shader_inuse_bitmap);
+
+		kbase_timeline_pm_cores_func(kbdev, KBASE_PM_FUNC_ID_RELEASE_CORES_START, change_gpu_state);
+		kbase_pm_update_cores_state_nolock(kbdev);
+		kbase_timeline_pm_cores_func(kbdev, KBASE_PM_FUNC_ID_RELEASE_CORES_END, change_gpu_state);
+
+		/* Trace that any state change completed immediately */
+		kbase_pm_trace_check_and_finish_state_change(kbdev);
+	}
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_release_cores)
+
+void kbase_pm_request_cores_sync(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores)
+{
+	kbase_pm_request_cores(kbdev, tiler_required, shader_cores);
+
+	kbase_pm_check_transitions_sync(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_request_cores_sync)
+
+void kbase_pm_request_l2_caches(kbase_device *kbdev)
+{
+	unsigned long flags;
+	u32 prior_l2_users_count;
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	prior_l2_users_count = kbdev->l2_users_count++;
+
+	KBASE_DEBUG_ASSERT(kbdev->l2_users_count != 0);
+
+	if (!prior_l2_users_count)
+		kbase_pm_update_cores_state_nolock(kbdev);
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+	wait_event(kbdev->pm.l2_powered_wait, kbdev->pm.l2_powered == 1);
+
+	/* Trace that any state change completed immediately */
+	kbase_pm_trace_check_and_finish_state_change(kbdev);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_request_l2_caches)
+
+void kbase_pm_release_l2_caches(kbase_device *kbdev)
+{
+	unsigned long flags;
+	spin_lock_irqsave(&kbdev->pm.power_change_lock, flags);
+
+	KBASE_DEBUG_ASSERT(kbdev->l2_users_count > 0);
+
+	--kbdev->l2_users_count;
+
+	if (!kbdev->l2_users_count) {
+		kbase_pm_update_cores_state_nolock(kbdev);
+		/* Trace that any state change completed immediately */
+		kbase_pm_trace_check_and_finish_state_change(kbdev);
+	}
+
+	spin_unlock_irqrestore(&kbdev->pm.power_change_lock, flags);
+}
+
+KBASE_EXPORT_TEST_API(kbase_pm_release_l2_caches)
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.h
new file mode 100644
index 0000000..007cdde
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_pm_policy.h
@@ -0,0 +1,269 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_pm_policy.h
+ * Power policy API definitions
+ */
+
+#ifndef _KBASE_PM_POLICY_H_
+#define _KBASE_PM_POLICY_H_
+
+/** List of policy IDs */
+typedef enum kbase_pm_policy_id {
+	KBASE_PM_POLICY_ID_DEMAND = 1,
+	KBASE_PM_POLICY_ID_ALWAYS_ON,
+	KBASE_PM_POLICY_ID_COARSE_DEMAND,
+#if MALI_CUSTOMER_RELEASE == 0
+	KBASE_PM_POLICY_ID_DEMAND_ALWAYS_POWERED,
+	KBASE_PM_POLICY_ID_FAST_START
+#endif
+} kbase_pm_policy_id;
+
+typedef u32 kbase_pm_policy_flags;
+
+/** Power policy structure.
+ *
+ * Each power policy exposes a (static) instance of this structure which contains function pointers to the
+ * policy's methods.
+ */
+typedef struct kbase_pm_policy {
+	/** The name of this policy */
+	char *name;
+
+	/** Function called when the policy is selected
+	 *
+	 * This should initialize the kbdev->pm.pm_policy_data structure. It should not attempt
+	 * to make any changes to hardware state.
+	 *
+	 * It is undefined what state the cores are in when the function is called.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 */
+	void (*init) (struct kbase_device *kbdev);
+
+	/** Function called when the policy is unselected.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 */
+	void (*term) (struct kbase_device *kbdev);
+
+	/** Function called to get the current shader core mask
+	 *
+	 * The returned mask should meet or exceed (kbdev->shader_needed_bitmap | kbdev->shader_inuse_bitmap).
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 *
+	 * @return     The mask of shader cores to be powered */
+	u64 (*get_core_mask) (struct kbase_device *kbdev);
+
+	/** Function called to get the current overall GPU power state
+	 *
+	 * This function should consider the state of kbdev->pm.active_count. If this count is greater than 0 then
+	 * there is at least one active context on the device and the GPU should be powered. If it is equal to 0
+	 * then there are no active contexts and the GPU could be powered off if desired.
+	 *
+	 * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+	 *
+	 * @return     MALI_TRUE if the GPU should be powered, MALI_FALSE otherwise */
+	mali_bool (*get_core_active) (struct kbase_device *kbdev);
+
+	/** Field indicating flags for this policy */
+	kbase_pm_policy_flags flags;
+
+	/** Field indicating an ID for this policy. This is not necessarily the
+	 * same as its index in the list returned by kbase_pm_list_policies().
+	 * It is used purely for debugging. */
+	kbase_pm_policy_id id;
+} kbase_pm_policy;
+
+/** Initialize power policy framework
+ *
+ * Must be called before calling any other policy function
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return MALI_ERROR_NONE if the power policy framework was successfully initialized.
+ */
+mali_error kbase_pm_policy_init(struct kbase_device *kbdev);
+
+/** Terminate power policy framework
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_policy_term(struct kbase_device *kbdev);
+
+/** Update the active power state of the GPU
+ * Calls into the current power policy
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_update_active(struct kbase_device *kbdev);
+
+/** Update the desired core state of the GPU
+ * Calls into the current power policy
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_update_cores(struct kbase_device *kbdev);
+
+/** Get the current policy.
+ * Returns the policy that is currently active.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ *
+ * @return The current policy
+ */
+const kbase_pm_policy *kbase_pm_get_policy(struct kbase_device *kbdev);
+
+/** Change the policy to the one specified.
+ *
+ * @param kbdev     The kbase device structure for the device (must be a valid pointer)
+ * @param policy    The policy to change to (valid pointer returned from @ref kbase_pm_list_policies)
+ */
+void kbase_pm_set_policy(struct kbase_device *kbdev, const kbase_pm_policy *policy);
+
+/** Retrieve a static list of the available policies.
+ * @param[out]  policies    An array pointer to take the list of policies. This may be NULL.
+ *                          The contents of this array must not be modified.
+ *
+ * @return The number of policies
+ */
+int kbase_pm_list_policies(const kbase_pm_policy * const **policies);
+
+
+typedef enum kbase_pm_cores_ready {
+	KBASE_CORES_NOT_READY = 0,
+	KBASE_NEW_AFFINITY = 1,
+	KBASE_CORES_READY = 2
+} kbase_pm_cores_ready;
+
+
+/** Synchronous variant of kbase_pm_request_cores()
+ *
+ * When this function returns, the @a shader_cores will be in the READY state.
+ *
+ * This is safe variant of kbase_pm_check_transitions_sync(): it handles the
+ * work of ensuring the requested cores will remain powered until a matching
+ * call to kbase_pm_unrequest_cores()/kbase_pm_release_cores() (as appropriate)
+ * is made.
+ *
+ * @param kbdev           The kbase device structure for the device
+ * @param tiler_required  MALI_TRUE if the tiler is required, MALI_FALSE otherwise
+ * @param shader_cores    A bitmask of shader cores which are necessary for the job
+ */
+
+void kbase_pm_request_cores_sync(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores);
+
+/** Mark one or more cores as being required for jobs to be submitted.
+ *
+ * This function is called by the job scheduler to mark one or more cores
+ * as being required to submit jobs that are ready to run.
+ *
+ * The cores requested are reference counted and a subsequent call to @ref kbase_pm_register_inuse_cores or
+ * @ref kbase_pm_unrequest_cores should be made to dereference the cores as being 'needed'.
+ *
+ * The active power policy will meet or exceed the requirements of the
+ * requested cores in the system. Any core transitions needed will be begun
+ * immediately, but they might not complete/the cores might not be available
+ * until a Power Management IRQ.
+ *
+ * @param kbdev           The kbase device structure for the device
+ * @param tiler_required  MALI_TRUE if the tiler is required, MALI_FALSE otherwise
+ * @param shader_cores    A bitmask of shader cores which are necessary for the job
+ *
+ * @return MALI_ERROR_NONE if the cores were successfully requested.
+ */
+void kbase_pm_request_cores(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores);
+
+/** Unmark one or more cores as being required for jobs to be submitted.
+ *
+ * This function undoes the effect of @ref kbase_pm_request_cores. It should be used when a job is not
+ * going to be submitted to the hardware (e.g. the job is cancelled before it is enqueued).
+ *
+ * The active power policy will meet or exceed the requirements of the
+ * requested cores in the system. Any core transitions needed will be begun
+ * immediately, but they might not complete until a Power Management IRQ.
+ *
+ * The policy may use this as an indication that it can power down cores.
+ *
+ * @param kbdev           The kbase device structure for the device
+ * @param tiler_required  MALI_TRUE if the tiler is required, MALI_FALSE otherwise
+ * @param shader_cores    A bitmask of shader cores (as given to @ref kbase_pm_request_cores)
+ */
+void kbase_pm_unrequest_cores(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores);
+
+/** Register a set of cores as in use by a job.
+ *
+ * This function should be called after @ref kbase_pm_request_cores when the job is about to be submitted to
+ * the hardware. It will check that the necessary cores are available and if so update the 'needed' and 'inuse'
+ * bitmasks to reflect that the job is now committed to being run.
+ *
+ * If the necessary cores are not currently available then the function will return MALI_FALSE and have no effect.
+ *
+ * @param kbdev           The kbase device structure for the device
+ * @param tiler_required  MALI_TRUE if the tiler is required, MALI_FALSE otherwise
+ * @param shader_cores    A bitmask of shader cores (as given to @ref kbase_pm_request_cores)
+ *
+ * @return MALI_TRUE if the job can be submitted to the hardware or MALI_FALSE if the job is not ready to run.
+ */
+mali_bool kbase_pm_register_inuse_cores(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores);
+
+/** Release cores after a job has run.
+ *
+ * This function should be called when a job has finished running on the hardware. A call to @ref
+ * kbase_pm_register_inuse_cores must have previously occurred. The reference counts of the specified cores will be
+ * decremented which may cause the bitmask of 'inuse' cores to be reduced. The power policy may then turn off any
+ * cores which are no longer 'inuse'.
+ *
+ * @param kbdev         The kbase device structure for the device
+ * @param tiler_required  MALI_TRUE if the tiler is required, MALI_FALSE otherwise
+ * @param shader_cores  A bitmask of shader cores (as given to @ref kbase_pm_register_inuse_cores)
+ */
+void kbase_pm_release_cores(struct kbase_device *kbdev, mali_bool tiler_required, u64 shader_cores);
+
+/** Request the use of l2 caches for all core groups, power up, wait and prevent the power manager from
+ *  powering down the l2 caches.
+ *
+ *  This tells the power management that the caches should be powered up, and they
+ *  should remain powered, irrespective of the usage of shader cores. This does not
+ *  return until the l2 caches are powered up.
+ *
+ *  The caller must call @ref kbase_pm_release_l2_caches when they are finished to
+ *  allow normal power management of the l2 caches to resume.
+ *
+ *  This should only be used when power management is active.
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_request_l2_caches(struct kbase_device *kbdev);
+
+/** Release the use of l2 caches for all core groups and allow the power manager to
+ *  power them down when necessary.
+ *
+ *  This tells the power management that the caches can be powered down if necessary, with respect
+ *  to the usage of shader cores.
+ *
+ *  The caller must have called @ref kbase_pm_request_l2_caches prior to a call to this.
+ *
+ *  This should only be used when power management is active.
+ *
+ * @param kbdev    The kbase device structure for the device (must be a valid pointer)
+ */
+void kbase_pm_release_l2_caches(struct kbase_device *kbdev);
+
+#endif				/* _KBASE_PM_POLICY_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_profiling_gator_api.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_profiling_gator_api.h
new file mode 100644
index 0000000..6f9db71
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_profiling_gator_api.h
@@ -0,0 +1,40 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_profiling_gator_api.h
+ * Model interface
+ */
+
+#ifndef _KBASE_PROFILING_GATOR_API_H_
+#define _KBASE_PROFILING_GATOR_API_H_
+
+/*
+ * List of possible actions to be controlled by Streamline.
+ * The following numbers are used by gator to control
+ * the frame buffer dumping and s/w counter reporting.
+ */
+#define FBDUMP_CONTROL_ENABLE (1)
+#define FBDUMP_CONTROL_RATE (2)
+#define SW_COUNTER_ENABLE (3)
+#define FBDUMP_CONTROL_RESIZE_FACTOR (4)
+#define FBDUMP_CONTROL_MAX (5)
+#define FBDUMP_CONTROL_MIN FBDUMP_CONTROL_ENABLE
+
+void _mali_profiling_control(u32 action, u32 value);
+
+#endif				/* _KBASE_PROFILING_GATOR_API */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_replay.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_replay.c
new file mode 100644
index 0000000..1f4ac3c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_replay.c
@@ -0,0 +1,1069 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @file mali_kbase_replay.c
+ * Replay soft job handlers
+ */
+
+#include <mali_kbase_config.h>
+#include <mali_kbase.h>
+#include <mali_kbase_mem.h>
+#include <mali_kbase_debug.h>
+
+#define JOB_NOT_STARTED 0
+
+#define JOB_TYPE_MASK      0xfe
+#define JOB_TYPE_NULL      (1 << 1)
+#define JOB_TYPE_VERTEX    (5 << 1)
+#define JOB_TYPE_TILER     (7 << 1)
+#define JOB_TYPE_FUSED     (8 << 1)
+#define JOB_TYPE_FRAGMENT  (9 << 1)
+
+#define JOB_FLAG_DESC_SIZE           (1 << 0)
+#define JOB_FLAG_PERFORM_JOB_BARRIER (1 << 8)
+
+#define JOB_HEADER_32_FBD_OFFSET (31*4)
+
+#define FBD_POINTER_MASK (~0x3f)
+
+#define SFBD_TILER_OFFSET (48*4)
+
+#define MFBD_TILER_FLAGS_OFFSET (15*4)
+#define MFBD_TILER_OFFSET       (16*4)
+
+#define FBD_HIERARCHY_WEIGHTS 8
+#define FBD_HIERARCHY_MASK_MASK 0x1fff
+
+#define FBD_TYPE 1
+
+#define HIERARCHY_WEIGHTS 13
+
+#define JOB_HEADER_ID_MAX                 0xffff
+
+typedef struct job_head
+{
+	u32 status;
+	u32 not_complete_index;
+	u64 fault_addr;
+	u16 flags;
+	u16 index;
+	u16 dependencies[2];
+	union
+	{
+		u64 _64;
+		u32 _32;
+	} next;
+	u32 x[2];
+	union
+	{
+		u64 _64;
+		u32 _32;
+	} fragment_fbd;
+} job_head;
+
+static void dump_job_head(kbase_context *kctx, char *head_str, job_head *job)
+{
+#ifdef CONFIG_MALI_DEBUG
+	struct device *dev = kctx->kbdev->dev;
+
+	dev_dbg(dev, "%s\n", head_str);
+	dev_dbg(dev, "addr               = %p\n"
+					"status             = %x\n"
+					"not_complete_index = %x\n"
+					"fault_addr         = %llx\n"
+					"flags              = %x\n"
+					"index              = %x\n"
+					"dependencies       = %x,%x\n",
+									   job,
+								   job->status,
+						       job->not_complete_index,
+							       job->fault_addr,
+								    job->flags,
+								    job->index,
+							  job->dependencies[0],
+							 job->dependencies[1]);
+
+	if (job->flags & JOB_FLAG_DESC_SIZE)
+		dev_dbg(dev, "next               = %llx\n", job->next._64);
+	else
+		dev_dbg(dev, "next               = %x\n", job->next._32);
+#endif
+}
+
+
+static void *kbasep_map_page(kbase_context *kctx, mali_addr64 gpu_addr,
+								u64 *phys_addr)
+{
+	void *cpu_addr = NULL;
+	u64 page_index;
+	kbase_va_region *region;
+	phys_addr_t *page_array;
+
+	region = kbase_region_tracker_find_region_enclosing_address(kctx,
+								     gpu_addr);
+	if (!region || (region->flags & KBASE_REG_FREE))
+		return NULL;
+
+	page_index = (gpu_addr >> PAGE_SHIFT) - region->start_pfn;
+	if (page_index >= kbase_reg_current_backed_size(region))
+		return NULL;
+
+	page_array = kbase_get_phy_pages(region);
+	if (!page_array)
+		return NULL;
+
+	cpu_addr = kmap_atomic(pfn_to_page(PFN_DOWN(page_array[page_index])));
+	if (!cpu_addr)
+		return NULL;
+
+	if (phys_addr)
+		*phys_addr = page_array[page_index];
+
+	return cpu_addr + (gpu_addr & ~PAGE_MASK);
+}
+
+static void *kbasep_map_page_sync(kbase_context *kctx, mali_addr64 gpu_addr,
+								u64 *phys_addr)
+{
+	void *cpu_addr = kbasep_map_page(kctx, gpu_addr, phys_addr);
+
+	if (!cpu_addr)
+		return NULL;
+
+	kbase_sync_to_cpu(*phys_addr,
+				 (void *)((uintptr_t)cpu_addr & PAGE_MASK),
+								    PAGE_SIZE);
+
+	return cpu_addr;
+}
+
+static void kbasep_unmap_page(void *cpu_addr)
+{
+	kunmap_atomic((void *)((uintptr_t)cpu_addr & PAGE_MASK));
+}
+
+static void kbasep_unmap_page_sync(void *cpu_addr, u64 phys_addr)
+{
+	kbase_sync_to_memory(phys_addr,
+				 (void *)((uintptr_t)cpu_addr & PAGE_MASK),
+								    PAGE_SIZE);
+
+	kunmap_atomic((void *)((uintptr_t)cpu_addr & PAGE_MASK));
+}
+
+static mali_error kbasep_replay_reset_sfbd(kbase_context *kctx,
+					   mali_addr64 fbd_address,
+					   mali_addr64 tiler_heap_free,
+					   u16 hierarchy_mask,
+					   u32 default_weight)
+{
+	u64 phys_addr;
+	struct
+	{
+		u32 padding_1[1];
+		u32 flags;
+		u64 padding_2[2];
+		u64 heap_free_address;
+		u32 padding[8];
+		u32 weights[FBD_HIERARCHY_WEIGHTS];
+	} *fbd_tiler;
+	struct device *dev = kctx->kbdev->dev;
+
+	dev_dbg(dev, "fbd_address: %llx\n", fbd_address);
+
+	fbd_tiler = kbasep_map_page_sync(kctx, fbd_address + SFBD_TILER_OFFSET,
+								   &phys_addr);
+	if (!fbd_tiler) {
+		dev_err(dev, "kbasep_replay_reset_fbd: failed to map fbd\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+#ifdef CONFIG_MALI_DEBUG
+	dev_dbg(dev, "FBD tiler:\n"
+				"flags = %x\n"
+				"heap_free_address = %llx\n",
+							      fbd_tiler->flags,
+						 fbd_tiler->heap_free_address);
+#endif
+	if (hierarchy_mask) {
+		u32 weights[HIERARCHY_WEIGHTS];
+		u16 old_hierarchy_mask = fbd_tiler->flags &
+						       FBD_HIERARCHY_MASK_MASK;
+		int i, j = 0;
+
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++) {
+			if (old_hierarchy_mask & (1 << i)) {
+				KBASE_DEBUG_ASSERT(j < FBD_HIERARCHY_WEIGHTS);
+				weights[i] = fbd_tiler->weights[j++];
+			} else {
+				weights[i] = default_weight;
+			}
+		}
+
+
+		dev_dbg(dev,
+			      "Old hierarchy mask=%x  New hierarchy mask=%x\n",
+					   old_hierarchy_mask, hierarchy_mask);
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++)
+			dev_dbg(dev, " Hierarchy weight %02d: %08x\n",
+								i, weights[i]);
+
+		j = 0;
+
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++) {
+			if (hierarchy_mask & (1 << i)) {
+				KBASE_DEBUG_ASSERT(j < FBD_HIERARCHY_WEIGHTS);
+
+				dev_dbg(dev,
+				" Writing hierarchy level %02d (%08x) to %d\n",
+							     i, weights[i], j);
+
+				fbd_tiler->weights[j++] = weights[i];
+			}
+		}
+
+		for (; j < FBD_HIERARCHY_WEIGHTS; j++)
+			fbd_tiler->weights[j] = 0;
+
+		fbd_tiler->flags = hierarchy_mask | (1 << 16);
+	}
+
+	fbd_tiler->heap_free_address = tiler_heap_free;
+
+	dev_dbg(dev, "heap_free_address=%llx flags=%x\n",
+			       fbd_tiler->heap_free_address, fbd_tiler->flags);
+
+	kbasep_unmap_page_sync(fbd_tiler, phys_addr);
+
+	return MALI_ERROR_NONE;
+}
+
+static mali_error kbasep_replay_reset_mfbd(kbase_context *kctx,
+					   mali_addr64 fbd_address,
+					   mali_addr64 tiler_heap_free,
+					   u16 hierarchy_mask,
+					   u32 default_weight)
+{
+	u64 phys_addr, phys_addr_flags;
+	struct
+	{
+		u64 padding_1[2];
+		u64 heap_free_address;
+		u64 padding_2;
+		u32 weights[FBD_HIERARCHY_WEIGHTS];
+	} *fbd_tiler;
+	u32 *fbd_tiler_flags;
+	mali_bool flags_different_page;
+	struct device *dev = kctx->kbdev->dev;
+
+	dev_dbg(dev, "fbd_address: %llx\n", fbd_address);
+
+	fbd_tiler = kbasep_map_page_sync(kctx, fbd_address + MFBD_TILER_OFFSET,
+								   &phys_addr);
+	if (((fbd_address + MFBD_TILER_OFFSET) & PAGE_MASK) !=
+	    ((fbd_address + MFBD_TILER_FLAGS_OFFSET) & PAGE_MASK)) {
+		flags_different_page = MALI_TRUE;
+		fbd_tiler_flags = kbasep_map_page_sync(kctx,
+					 fbd_address + MFBD_TILER_FLAGS_OFFSET,
+							     &phys_addr_flags);
+	} else {
+		flags_different_page = MALI_FALSE;
+		fbd_tiler_flags = (u32 *)((uintptr_t)fbd_tiler -
+				  MFBD_TILER_OFFSET + MFBD_TILER_FLAGS_OFFSET);
+	}
+
+	if (!fbd_tiler || !fbd_tiler_flags) {
+		dev_err(dev, "kbasep_replay_reset_fbd: failed to map fbd\n");
+
+		if (fbd_tiler_flags && flags_different_page)
+			kbasep_unmap_page_sync(fbd_tiler_flags,
+							      phys_addr_flags);
+		if (fbd_tiler)
+			kbasep_unmap_page_sync(fbd_tiler, phys_addr);
+
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+#ifdef CONFIG_MALI_DEBUG
+	dev_dbg(dev, "FBD tiler:\n"
+				"heap_free_address = %llx\n",
+				 fbd_tiler->heap_free_address);
+#endif
+	if (hierarchy_mask) {
+		u32 weights[HIERARCHY_WEIGHTS];
+		u16 old_hierarchy_mask = (*fbd_tiler_flags) &
+						       FBD_HIERARCHY_MASK_MASK;
+		int i, j = 0;
+
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++) {
+			if (old_hierarchy_mask & (1 << i)) {
+				KBASE_DEBUG_ASSERT(j < FBD_HIERARCHY_WEIGHTS);
+				weights[i] = fbd_tiler->weights[j++];
+			}
+			else
+				weights[i] = default_weight;
+		}
+
+
+		dev_dbg(dev,
+			      "Old hierarchy mask=%x  New hierarchy mask=%x\n",
+					   old_hierarchy_mask, hierarchy_mask);
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++)
+			dev_dbg(dev, " Hierarchy weight %02d: %08x\n",
+								i, weights[i]);
+
+		j = 0;
+
+		for (i = 0; i < HIERARCHY_WEIGHTS; i++) {
+			if (hierarchy_mask & (1 << i)) {
+				KBASE_DEBUG_ASSERT(j < FBD_HIERARCHY_WEIGHTS);
+
+				dev_dbg(dev,
+				" Writing hierarchy level %02d (%08x) to %d\n",
+							     i, weights[i], j);
+
+				fbd_tiler->weights[j++] = weights[i];
+			}
+		}
+
+		for (; j < FBD_HIERARCHY_WEIGHTS; j++)
+			fbd_tiler->weights[j] = 0;
+
+		*fbd_tiler_flags = hierarchy_mask | (1 << 16);
+	}
+
+	fbd_tiler->heap_free_address = tiler_heap_free;
+
+	if (flags_different_page)
+		kbasep_unmap_page_sync(fbd_tiler_flags, phys_addr_flags);
+
+	kbasep_unmap_page_sync(fbd_tiler, phys_addr);
+
+	return MALI_ERROR_NONE;
+}
+
+/**
+ * @brief Reset the status of an FBD pointed to by a tiler job
+ *
+ * This performs two functions :
+ * - Set the hierarchy mask
+ * - Reset the tiler free heap address
+ *
+ * @param[in] kctx              Context pointer
+ * @param[in] job_header        Address of job header to reset.
+ * @param[in] tiler_heap_free   The value to reset Tiler Heap Free to
+ * @param[in] hierarchy_mask    The hierarchy mask to use
+ * @param[in] default_weight    Default hierarchy weight to write when no other
+ *                              weight is given in the FBD
+ * @param[in] job_64            MALI_TRUE if this job is using 64-bit
+ *                              descriptors
+ *
+ * @return MALI_ERROR_NONE on success, error code on failure
+ */
+static mali_error kbasep_replay_reset_tiler_job(kbase_context *kctx,
+						mali_addr64 job_header,
+						mali_addr64 tiler_heap_free,
+						u16 hierarchy_mask,
+						u32 default_weight,
+						mali_bool job_64)
+{
+	mali_addr64 fbd_address;
+
+	if (job_64) {
+		dev_err(kctx->kbdev->dev,
+				      "64-bit job descriptor not supported\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	} else {
+		u32 *job_ext;	
+
+		job_ext = kbasep_map_page(kctx,
+					 job_header + JOB_HEADER_32_FBD_OFFSET,
+									 NULL);
+		if (!job_ext) {
+			dev_err(kctx->kbdev->dev,
+			  "kbasep_replay_reset_tiler_job: failed to map jc\n");
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		fbd_address = *job_ext;
+
+		kbasep_unmap_page(job_ext);
+	}
+
+	if (fbd_address & FBD_TYPE) {
+		return kbasep_replay_reset_mfbd(kctx,
+						fbd_address & FBD_POINTER_MASK,
+						tiler_heap_free,
+						hierarchy_mask,
+						default_weight);
+	} else {
+		return kbasep_replay_reset_sfbd(kctx,
+						fbd_address & FBD_POINTER_MASK,
+						tiler_heap_free,
+						hierarchy_mask,
+						default_weight);
+	}
+}
+
+/**
+ * @brief Reset the status of a job
+ *
+ * This performs the following functions :
+ *
+ * - Reset the Job Status field of each job to NOT_STARTED.
+ * - Set the Job Type field of any Vertex Jobs to Null Job.
+ * - For any jobs using an FBD, set the Tiler Heap Free field to the value of
+ *   the tiler_heap_free parameter, and set the hierarchy level mask to the
+ *   hier_mask parameter.
+ * - Offset HW dependencies by the hw_job_id_offset parameter
+ * - Set the Perform Job Barrier flag if this job is the first in the chain
+ * - Read the address of the next job header
+ *
+ * @param[in] kctx              Context pointer
+ * @param[in,out] job_header    Address of job header to reset. Set to address
+ *                              of next job header on exit.
+ * @param[in] prev_jc           Previous job chain to link to, if this job is
+ *                              the last in the chain.
+ * @param[in] hw_job_id_offset  Offset for HW job IDs
+ * @param[in] tiler_heap_free   The value to reset Tiler Heap Free to
+ * @param[in] hierarchy_mask    The hierarchy mask to use
+ * @param[in] default_weight    Default hierarchy weight to write when no other
+ *                              weight is given in the FBD
+ * @param[in] first_in_chain    MALI_TRUE if this job is the first in the chain
+ * @param[in] fragment_chain    MALI_TRUE if this job is in the fragment chain
+ *
+ * @return MALI_ERROR_NONE on success, error code on failure
+ */
+static mali_error kbasep_replay_reset_job(kbase_context *kctx,
+						mali_addr64 *job_header,
+						mali_addr64 prev_jc,
+						mali_addr64 tiler_heap_free,
+						u16 hierarchy_mask,
+						u32 default_weight,
+						u16 hw_job_id_offset,
+						mali_bool first_in_chain,
+						mali_bool fragment_chain)
+{
+	job_head *job;
+	u64 phys_addr;
+	mali_addr64 new_job_header;
+	struct device *dev = kctx->kbdev->dev;
+
+	job = kbasep_map_page_sync(kctx, *job_header, &phys_addr);
+	if (!job) {
+		dev_err(dev, "kbasep_replay_parse_jc: failed to map jc\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	dump_job_head(kctx, "Job header:", job);
+
+	if (job->status == JOB_NOT_STARTED && !fragment_chain) {
+		dev_err(dev, "Job already not started\n");
+		kbasep_unmap_page_sync(job, phys_addr);
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+	job->status = JOB_NOT_STARTED;
+
+	if ((job->flags & JOB_TYPE_MASK) == JOB_TYPE_VERTEX)
+		job->flags = (job->flags & ~JOB_TYPE_MASK) | JOB_TYPE_NULL;
+
+	if ((job->flags & JOB_TYPE_MASK) == JOB_TYPE_FUSED) {
+		dev_err(dev, "Fused jobs can not be replayed\n");
+		kbasep_unmap_page_sync(job, phys_addr);
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	if (first_in_chain)
+		job->flags |= JOB_FLAG_PERFORM_JOB_BARRIER;
+
+	if ((job->dependencies[0] + hw_job_id_offset) > JOB_HEADER_ID_MAX ||
+	    (job->dependencies[1] + hw_job_id_offset) > JOB_HEADER_ID_MAX ||
+	    (job->index + hw_job_id_offset) > JOB_HEADER_ID_MAX) {
+		dev_err(dev, "Job indicies/dependencies out of valid range\n");
+		kbasep_unmap_page_sync(job, phys_addr);
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	if (job->dependencies[0])
+		job->dependencies[0] += hw_job_id_offset;
+	if (job->dependencies[1])
+		job->dependencies[1] += hw_job_id_offset;
+
+	job->index += hw_job_id_offset;
+
+	if (job->flags & JOB_FLAG_DESC_SIZE) {
+		new_job_header = job->next._64;
+		if (!job->next._64)
+			job->next._64 = prev_jc;
+	} else {
+		new_job_header = job->next._32;
+		if (!job->next._32)
+			job->next._32 = prev_jc;
+	}
+	dump_job_head(kctx, "Updated to:", job);
+
+	if ((job->flags & JOB_TYPE_MASK) == JOB_TYPE_TILER) {
+		kbasep_unmap_page_sync(job, phys_addr);
+		if (kbasep_replay_reset_tiler_job(kctx, *job_header,
+					tiler_heap_free, hierarchy_mask, 
+					default_weight,
+					job->flags & JOB_FLAG_DESC_SIZE) !=
+							MALI_ERROR_NONE)
+			return MALI_ERROR_FUNCTION_FAILED;
+
+	} else if ((job->flags & JOB_TYPE_MASK) == JOB_TYPE_FRAGMENT) {
+		u64 fbd_address;
+
+		if (job->flags & JOB_FLAG_DESC_SIZE) {
+			kbasep_unmap_page_sync(job, phys_addr);
+			dev_err(dev, "64-bit job descriptor not supported\n");
+			return MALI_ERROR_FUNCTION_FAILED;
+		} else {
+			fbd_address = (u64)job->fragment_fbd._32;
+		}
+
+		kbasep_unmap_page_sync(job, phys_addr);
+
+		if (fbd_address & FBD_TYPE) {
+			if (kbasep_replay_reset_mfbd(kctx,
+						fbd_address & FBD_POINTER_MASK,
+						tiler_heap_free,
+						hierarchy_mask,
+						default_weight) !=
+							       MALI_ERROR_NONE)
+				return MALI_ERROR_FUNCTION_FAILED;
+		} else {
+			if (kbasep_replay_reset_sfbd(kctx,
+						fbd_address & FBD_POINTER_MASK,
+						tiler_heap_free,
+						hierarchy_mask,
+						default_weight) !=
+							       MALI_ERROR_NONE)
+				return MALI_ERROR_FUNCTION_FAILED;
+		}
+	} else {
+		kbasep_unmap_page_sync(job, phys_addr);
+	}
+
+	*job_header = new_job_header;
+
+	return MALI_ERROR_NONE;
+}
+
+/**
+ * @brief Find the highest job ID in a job chain
+ *
+ * @param[in] kctx        Context pointer
+ * @param[in] jc          Job chain start address
+ * @param[out] hw_job_id  Highest job ID in chain
+ *
+ * @return MALI_ERROR_NONE on success, error code on failure
+ */
+static mali_error kbasep_replay_find_hw_job_id(kbase_context *kctx,
+						mali_addr64 jc,
+						u16 *hw_job_id)
+{
+	while (jc) {
+		job_head *job;
+		u64 phys_addr;
+
+		dev_dbg(kctx->kbdev->dev,
+			"kbasep_replay_find_hw_job_id: parsing jc=%llx\n", jc);
+
+		job = kbasep_map_page_sync(kctx, jc, &phys_addr);
+		if (!job) {
+			dev_err(kctx->kbdev->dev, "failed to map jc\n");
+
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+
+		if (job->index > *hw_job_id)
+			*hw_job_id = job->index;
+
+		if (job->flags & JOB_FLAG_DESC_SIZE)
+			jc = job->next._64;
+		else
+			jc = job->next._32;
+
+		kbasep_unmap_page_sync(job, phys_addr);
+	}
+
+	return MALI_ERROR_NONE;
+}
+
+/**
+ * @brief Reset the status of a number of jobs
+ *
+ * This function walks the provided job chain, and calls
+ * kbasep_replay_reset_job for each job. It also links the job chain to the
+ * provided previous job chain.
+ *
+ * The function will fail if any of the jobs passed already have status of
+ * NOT_STARTED.
+ *
+ * @param[in] kctx              Context pointer
+ * @param[in] jc                Job chain to be processed
+ * @param[in] prev_jc           Job chain to be added to. May be NULL
+ * @param[in] tiler_heap_free   The value to reset Tiler Heap Free to
+ * @param[in] hierarchy_mask    The hierarchy mask to use
+ * @param[in] default_weight    Default hierarchy weight to write when no other
+ *                              weight is given in the FBD
+ * @param[in] hw_job_id_offset  Offset for HW job IDs
+ * @param[in] fragment_chain    MAIL_TRUE if this chain is the fragment chain
+ *
+ * @return MALI_ERROR_NONE on success, error code otherwise
+ */
+static mali_error kbasep_replay_parse_jc(kbase_context *kctx,
+						mali_addr64 jc,
+						mali_addr64 prev_jc,
+						mali_addr64 tiler_heap_free,
+						u16 hierarchy_mask,
+						u32 default_weight,
+						u16 hw_job_id_offset,
+						mali_bool fragment_chain)
+{
+	mali_bool first_in_chain = MALI_TRUE;
+	int nr_jobs = 0;
+
+	dev_dbg(kctx->kbdev->dev,
+			      "kbasep_replay_parse_jc: jc=%llx hw_job_id=%x\n",
+							 jc, hw_job_id_offset);
+
+	while (jc) {
+		dev_dbg(kctx->kbdev->dev,
+				   "kbasep_replay_parse_jc: parsing jc=%llx\n",
+									   jc);
+
+		if (kbasep_replay_reset_job(kctx, &jc, prev_jc,
+				tiler_heap_free, hierarchy_mask,
+				default_weight, hw_job_id_offset,
+				first_in_chain, fragment_chain) != 
+							     MALI_ERROR_NONE)
+			return MALI_ERROR_FUNCTION_FAILED;
+
+		first_in_chain = MALI_FALSE;
+
+		nr_jobs++;
+		if (fragment_chain &&
+                		nr_jobs >= BASE_JD_REPLAY_F_CHAIN_JOB_LIMIT) {
+			dev_err(kctx->kbdev->dev,
+				"Exceeded maximum number of jobs in fragment chain\n");
+			return MALI_ERROR_FUNCTION_FAILED;
+		}
+	}
+
+	return MALI_ERROR_NONE;
+}
+
+/**
+ * @brief Reset the status of a replay job, and set up dependencies
+ *
+ * This performs the actions to allow the replay job to be re-run following
+ * completion of the passed dependency.
+ *
+ * @param[in] katom     The atom to be reset
+ * @param[in] dep_atom  The dependency to be attached to the atom
+ */
+static void kbasep_replay_reset_softjob(kbase_jd_atom *katom,
+						       kbase_jd_atom *dep_atom)
+{
+	katom->status = KBASE_JD_ATOM_STATE_QUEUED;
+	kbase_jd_katom_dep_set(&katom->dep[0],dep_atom, BASE_JD_DEP_TYPE_DATA);
+	list_add_tail(&katom->dep_item[0], &dep_atom->dep_head[0]);
+}
+
+/**
+ * @brief Allocate an unused katom
+ *
+ * This will search the provided context for an unused katom, and will mark it
+ * as KBASE_JD_ATOM_STATE_QUEUED.
+ *
+ * If no atoms are available then the function will fail.
+ *
+ * @param[in] kctx      Context pointer
+ * @return An atom ID, or -1 on failure
+ */
+static int kbasep_allocate_katom(kbase_context *kctx)
+{
+	kbase_jd_context *jctx = &kctx->jctx;
+	int i;
+
+	for (i = BASE_JD_ATOM_COUNT-1; i > 0; i--) {
+		if (jctx->atoms[i].status == KBASE_JD_ATOM_STATE_UNUSED) {
+			jctx->atoms[i].status = KBASE_JD_ATOM_STATE_QUEUED;
+			dev_dbg(kctx->kbdev->dev,
+				  "kbasep_allocate_katom: Allocated atom %d\n",
+									    i);
+			return i;
+		}
+	}
+
+	return -1;
+}
+
+/**
+ * @brief Release a katom
+ *
+ * This will mark the provided atom as available, and remove any dependencies.
+ *
+ * For use on error path.
+ *
+ * @param[in] kctx      Context pointer
+ * @param[in] atom_id   ID of atom to release
+ */
+static void kbasep_release_katom(kbase_context *kctx, int atom_id)
+{
+	kbase_jd_context *jctx = &kctx->jctx;
+
+	dev_dbg(kctx->kbdev->dev,
+				    "kbasep_release_katom: Released atom %d\n",
+								      atom_id);
+
+	while (!list_empty(&jctx->atoms[atom_id].dep_head[0]))
+		list_del(jctx->atoms[atom_id].dep_head[0].next);
+	while (!list_empty(&jctx->atoms[atom_id].dep_head[1]))
+		list_del(jctx->atoms[atom_id].dep_head[1].next);
+
+	jctx->atoms[atom_id].status = KBASE_JD_ATOM_STATE_UNUSED;
+}
+
+static void kbasep_replay_create_atom(kbase_context *kctx,
+				      base_jd_atom_v2 *atom,
+				      int atom_nr,
+				      int prio)
+{
+	atom->nr_extres = 0;
+	atom->extres_list.value = NULL;
+	atom->device_nr = 0;
+	/* Convert priority back from NICE range */
+	atom->prio = ((prio << 16) / ((20 << 16) / 128)) - 128;
+	atom->atom_number = atom_nr;
+
+	base_jd_atom_dep_set(&atom->pre_dep[0], 0 , BASE_JD_DEP_TYPE_INVALID);
+	base_jd_atom_dep_set(&atom->pre_dep[1], 0 , BASE_JD_DEP_TYPE_INVALID);
+
+	atom->udata.blob[0] = 0;
+	atom->udata.blob[1] = 0;
+}
+
+/**
+ * @brief Create two atoms for the purpose of replaying jobs
+ *
+ * Two atoms are allocated and created. The jc pointer is not set at this
+ * stage. The second atom has a dependency on the first. The remaining fields
+ * are set up as follows :
+ *
+ * - No external resources. Any required external resources will be held by the
+ *   replay atom.
+ * - device_nr is set to 0. This is not relevant as
+ *   BASE_JD_REQ_SPECIFIC_COHERENT_GROUP should not be set.
+ * - Priority is inherited from the replay job.
+ *
+ * @param[out] t_atom      Atom to use for tiler jobs
+ * @param[out] f_atom      Atom to use for fragment jobs
+ * @param[in]  prio        Priority of new atom (inherited from replay soft
+ *                         job)
+ * @return MALI_ERROR_NONE on success, error code on failure
+ */
+static mali_error kbasep_replay_create_atoms(kbase_context *kctx,
+					     base_jd_atom_v2 *t_atom,
+					     base_jd_atom_v2 *f_atom,
+					     int prio)
+{
+	int t_atom_nr, f_atom_nr;
+
+	t_atom_nr = kbasep_allocate_katom(kctx);
+	if (t_atom_nr < 0) {
+		dev_err(kctx->kbdev->dev, "Failed to allocate katom\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	f_atom_nr = kbasep_allocate_katom(kctx);
+	if (f_atom_nr < 0) {
+		dev_err(kctx->kbdev->dev, "Failed to allocate katom\n");
+		kbasep_release_katom(kctx, t_atom_nr);
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+	kbasep_replay_create_atom(kctx, t_atom, t_atom_nr, prio);
+	kbasep_replay_create_atom(kctx, f_atom, f_atom_nr, prio);
+
+	base_jd_atom_dep_set(&f_atom->pre_dep[0], t_atom_nr , BASE_JD_DEP_TYPE_DATA);
+
+	return MALI_ERROR_NONE;
+}
+
+#ifdef CONFIG_MALI_DEBUG
+static void payload_dump(kbase_context *kctx, base_jd_replay_payload *payload)
+{
+	mali_addr64 next;
+
+	dev_dbg(kctx->kbdev->dev, "Tiler jc list :\n");
+	next = payload->tiler_jc_list;
+
+	while (next) {
+		base_jd_replay_jc *jc_struct = kbasep_map_page(kctx, next, NULL);
+
+		if (!jc_struct)
+			return;
+
+		dev_dbg(kctx->kbdev->dev,
+					  "* jc_struct=%p jc=%llx next=%llx\n",
+								     jc_struct,
+								 jc_struct->jc,
+							      jc_struct->next);
+		next = jc_struct->next;
+
+		kbasep_unmap_page(jc_struct);
+	}
+}
+#endif
+
+/**
+ * @brief Parse a base_jd_replay_payload provided by userspace
+ *
+ * This will read the payload from userspace, and parse the job chains.
+ *
+ * @param[in] kctx         Context pointer
+ * @param[in] replay_atom  Replay soft job atom
+ * @param[in] t_atom       Atom to use for tiler jobs
+ * @param[in] f_atom       Atom to use for fragment jobs
+ * @return  MALI_ERROR_NONE on success, error code on failure
+ */
+static mali_error kbasep_replay_parse_payload(kbase_context *kctx, 
+					      kbase_jd_atom *replay_atom,
+					      base_jd_atom_v2 *t_atom,
+					      base_jd_atom_v2 *f_atom)
+{
+	base_jd_replay_payload *payload;
+	mali_addr64 next;
+	mali_addr64 prev_jc = 0;
+	u16 hw_job_id_offset = 0;
+	mali_error ret = MALI_ERROR_FUNCTION_FAILED;
+	u64 phys_addr;
+	struct device *dev = kctx->kbdev->dev;
+
+	dev_dbg(dev,
+			"kbasep_replay_parse_payload: replay_atom->jc = %llx  "
+			"sizeof(payload) = %d\n",
+					     replay_atom->jc, sizeof(payload));
+
+	kbase_gpu_vm_lock(kctx);
+
+	payload = kbasep_map_page_sync(kctx, replay_atom->jc, &phys_addr);
+
+	if (!payload) {
+		kbase_gpu_vm_unlock(kctx);
+		dev_err(dev, "kbasep_replay_parse_payload: failed to map payload into kernel space\n");
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+
+#ifdef CONFIG_MALI_DEBUG
+	dev_dbg(dev, "kbasep_replay_parse_payload: payload=%p\n", payload);
+	dev_dbg(dev, "Payload structure:\n"
+					"tiler_jc_list            = %llx\n"
+					"fragment_jc              = %llx\n"
+					"tiler_heap_free          = %llx\n"
+					"fragment_hierarchy_mask  = %x\n"
+					"tiler_hierarchy_mask     = %x\n"
+					"hierarchy_default_weight = %x\n"
+					"tiler_core_req           = %x\n"
+					"fragment_core_req        = %x\n",
+							payload->tiler_jc_list,
+							  payload->fragment_jc,
+						      payload->tiler_heap_free,
+					      payload->fragment_hierarchy_mask,
+						 payload->tiler_hierarchy_mask,
+					     payload->hierarchy_default_weight,
+						       payload->tiler_core_req,
+						   payload->fragment_core_req);
+	payload_dump(kctx, payload);
+#endif
+
+	t_atom->core_req = payload->tiler_core_req | BASEP_JD_REQ_EVENT_NEVER;
+	f_atom->core_req = payload->fragment_core_req | BASEP_JD_REQ_EVENT_NEVER;
+
+	/* Sanity check core requirements*/
+	if ((t_atom->core_req & BASEP_JD_REQ_ATOM_TYPE &
+			       ~BASE_JD_REQ_COHERENT_GROUP) != BASE_JD_REQ_T ||
+	    (f_atom->core_req & BASEP_JD_REQ_ATOM_TYPE &
+			      ~BASE_JD_REQ_COHERENT_GROUP) != BASE_JD_REQ_FS ||
+	     t_atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES ||
+	     f_atom->core_req & BASE_JD_REQ_EXTERNAL_RESOURCES) {
+		dev_err(dev, "Invalid core requirements\n");
+		goto out;
+	}
+	
+	/* Process tiler job chains */
+	next = payload->tiler_jc_list;
+	if (!next) {
+		dev_err(dev, "Invalid tiler JC list\n");
+		goto out;
+	}
+
+	while (next) {
+		base_jd_replay_jc *jc_struct = kbasep_map_page(kctx, next, NULL);
+		mali_addr64 jc;
+
+		if (!jc_struct) {
+			dev_err(dev, "Failed to map jc struct\n");
+			goto out;
+		}
+
+		jc = jc_struct->jc;
+		next = jc_struct->next;
+		if (next)
+			jc_struct->jc = 0;
+
+		kbasep_unmap_page(jc_struct);
+
+		if (jc) {
+			u16 max_hw_job_id = 0;
+
+			if (kbasep_replay_find_hw_job_id(kctx, jc,
+					    &max_hw_job_id) != MALI_ERROR_NONE)
+				goto out;
+
+			if (kbasep_replay_parse_jc(kctx, jc, prev_jc,
+					     payload->tiler_heap_free,
+					     payload->tiler_hierarchy_mask,
+					     payload->hierarchy_default_weight,
+				             hw_job_id_offset, MALI_FALSE) !=
+							     MALI_ERROR_NONE) {
+				goto out;
+			}
+
+			hw_job_id_offset += max_hw_job_id;
+
+			prev_jc = jc;
+		}
+	}
+	t_atom->jc = prev_jc;
+
+	/* Process fragment job chain */
+	f_atom->jc = payload->fragment_jc;
+	if (kbasep_replay_parse_jc(kctx, payload->fragment_jc, 0,
+					 payload->tiler_heap_free,
+					 payload->fragment_hierarchy_mask,
+					 payload->hierarchy_default_weight, 0,
+					       MALI_TRUE) != MALI_ERROR_NONE) {
+		goto out;
+	}
+
+	if (!t_atom->jc || !f_atom->jc) {
+		dev_err(dev, "Invalid payload\n");
+		goto out;
+	}
+
+	dev_dbg(dev, "t_atom->jc=%llx f_atom->jc=%llx\n",
+						       t_atom->jc, f_atom->jc);
+	ret = MALI_ERROR_NONE;
+
+out:	
+	kbasep_unmap_page_sync(payload, phys_addr);
+
+	kbase_gpu_vm_unlock(kctx);
+
+	return ret;
+}
+
+/**
+ * @brief Process a replay job
+ *
+ * Called from kbase_process_soft_job.
+ *
+ * On exit, if the job has completed, katom->event_code will have been updated.
+ * If the job has not completed, and is replaying jobs, then the atom status
+ * will have been reset to KBASE_JD_ATOM_STATE_QUEUED.
+ *
+ * @param[in] katom  The atom to be processed
+ * @return           MALI_REPLAY_STATUS_COMPLETE  if the atom has completed
+ *                   MALI_REPLAY_STATUS_REPLAYING if the atom is replaying jobs
+ *                   Set MALI_REPLAY_FLAG_JS_RESCHED if 
+ *                   kbasep_js_try_schedule_head_ctx required
+ */
+int kbase_replay_process(kbase_jd_atom *katom)
+{
+	kbase_context *kctx = katom->kctx;
+	kbase_jd_context *jctx = &kctx->jctx;
+	mali_bool need_to_try_schedule_context = MALI_FALSE;
+	base_jd_atom_v2 t_atom, f_atom;
+	kbase_jd_atom *t_katom, *f_katom;
+	struct device *dev = kctx->kbdev->dev;
+
+	if (katom->event_code == BASE_JD_EVENT_DONE) {
+		dev_dbg(dev, "Previous job succeeded - not replaying\n");
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	if (jctx->sched_info.ctx.is_dying) {
+		dev_dbg(dev, "Not replaying; context is dying\n");
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	dev_warn(dev, "Replaying jobs retry=%d\n", katom->retry_count);
+
+	katom->retry_count++;
+	if (katom->retry_count > BASEP_JD_REPLAY_LIMIT) {
+		dev_err(dev, "Replay exceeded limit - failing jobs\n");
+		/* katom->event_code is already set to the failure code of the
+		   previous job */
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	if (kbasep_replay_create_atoms(kctx, &t_atom, &f_atom,
+				       katom->nice_prio) != MALI_ERROR_NONE) {
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	t_katom = &jctx->atoms[t_atom.atom_number];
+	f_katom = &jctx->atoms[f_atom.atom_number];
+
+	if (kbasep_replay_parse_payload(kctx, katom, &t_atom, &f_atom) !=
+							     MALI_ERROR_NONE) {
+		kbasep_release_katom(kctx, t_atom.atom_number);
+		kbasep_release_katom(kctx, f_atom.atom_number);
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	kbasep_replay_reset_softjob(katom, f_katom);
+
+	need_to_try_schedule_context |= jd_submit_atom(kctx, &t_atom, t_katom);
+	if (t_katom->event_code == BASE_JD_EVENT_JOB_INVALID) {
+		dev_err(dev, "Replay failed to submit atom\n");
+		kbasep_release_katom(kctx, f_atom.atom_number);
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+	need_to_try_schedule_context |= jd_submit_atom(kctx, &f_atom, f_katom);
+	if (f_katom->event_code == BASE_JD_EVENT_JOB_INVALID) {
+		dev_err(dev, "Replay failed to submit atom\n");
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+		katom->status = KBASE_JD_ATOM_STATE_COMPLETED;
+		return MALI_REPLAY_STATUS_COMPLETE;
+	}
+
+	katom->event_code = BASE_JD_EVENT_DONE;
+
+	if (need_to_try_schedule_context)
+		return MALI_REPLAY_STATUS_REPLAYING | 
+						MALI_REPLAY_FLAG_JS_RESCHED;
+	return MALI_REPLAY_STATUS_REPLAYING;
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.c
new file mode 100644
index 0000000..bb483ee
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.c
@@ -0,0 +1,73 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_security.c
+ * Base kernel security capability API
+ */
+
+#include <mali_kbase.h>
+
+static inline mali_bool kbasep_am_i_root(void)
+{
+#if KBASE_HWCNT_DUMP_BYPASS_ROOT
+	return MALI_TRUE;
+#else
+	/* Check if root */
+	if (uid_eq(current_euid(), GLOBAL_ROOT_UID))
+		return MALI_TRUE;
+	return MALI_FALSE;
+#endif /*KBASE_HWCNT_DUMP_BYPASS_ROOT*/
+}
+
+/**
+ * kbase_security_has_capability - see mali_kbase_caps.h for description.
+ */
+
+mali_bool kbase_security_has_capability(kbase_context *kctx, kbase_security_capability cap, u32 flags)
+{
+	/* Assume failure */
+	mali_bool access_allowed = MALI_FALSE;
+	mali_bool audit = (KBASE_SEC_FLAG_AUDIT & flags) ? MALI_TRUE : MALI_FALSE;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+	CSTD_UNUSED(kctx);
+
+	/* Detect unsupported flags */
+	KBASE_DEBUG_ASSERT(((~KBASE_SEC_FLAG_MASK) & flags) == 0);
+
+	/* Determine if access is allowed for the given cap */
+	switch (cap) {
+	case KBASE_SEC_MODIFY_PRIORITY:
+	case KBASE_SEC_INSTR_HW_COUNTERS_COLLECT:
+		/* Access is granted only if the caller is privileged */
+		access_allowed = kbasep_am_i_root();
+		break;
+	}
+
+	/* Report problem if requested */
+	if (MALI_FALSE == access_allowed) {
+		if (MALI_FALSE != audit)
+			dev_warn(kctx->kbdev->dev, "Security capability failure: %d, %p", cap, (void *)kctx);
+	}
+
+	return access_allowed;
+}
+
+KBASE_EXPORT_TEST_API(kbase_security_has_capability)
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.h
new file mode 100644
index 0000000..783e281
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_security.h
@@ -0,0 +1,52 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_security.h
+ * Base kernel security capability APIs
+ */
+
+#ifndef _KBASE_SECURITY_H_
+#define _KBASE_SECURITY_H_
+
+/* Security flags */
+#define KBASE_SEC_FLAG_NOAUDIT (0u << 0)	/* Silently handle privilege failure */
+#define KBASE_SEC_FLAG_AUDIT   (1u << 0)	/* Write audit message on privilege failure */
+#define KBASE_SEC_FLAG_MASK    (KBASE_SEC_FLAG_AUDIT)	/* Mask of all valid flag bits */
+
+/* List of unique capabilities that have security access privileges */
+typedef enum {
+	/* Instrumentation Counters access privilege */
+	KBASE_SEC_INSTR_HW_COUNTERS_COLLECT = 1,
+	KBASE_SEC_MODIFY_PRIORITY
+	    /* Add additional access privileges here */
+} kbase_security_capability;
+
+/**
+ * kbase_security_has_capability - determine whether a task has a particular effective capability
+ * @param[in]   kctx    The task context.
+ * @param[in]   cap     The capability to check for.
+ * @param[in]   flags   Additional configuration information
+ *                      Such as whether to write an audit message or not.
+ * @return MALI_TRUE if success (capability is allowed), MALI_FALSE otherwise.
+ */
+
+mali_bool kbase_security_has_capability(kbase_context *kctx, kbase_security_capability cap, u32 flags);
+
+#endif				/* _KBASE_SECURITY_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_softjobs.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_softjobs.c
new file mode 100644
index 0000000..8175b84
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_softjobs.c
@@ -0,0 +1,437 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+
+#ifdef CONFIG_SYNC
+#include "sync.h"
+#include <linux/syscalls.h>
+#include "mali_kbase_sync.h"
+#endif
+
+
+/* Mask to check cache alignment of data structures */
+#define KBASE_CACHE_ALIGNMENT_MASK		((1<<L1_CACHE_SHIFT)-1)
+
+/**
+ * @file mali_kbase_softjobs.c
+ *
+ * This file implements the logic behind software only jobs that are
+ * executed within the driver rather than being handed over to the GPU.
+ */
+
+static int kbase_dump_cpu_gpu_time(kbase_jd_atom *katom)
+{
+	kbase_va_region *reg;
+	phys_addr_t addr = 0;
+	u64 pfn;
+	u32 offset;
+	char *page;
+	struct timespec ts;
+	base_dump_cpu_gpu_counters data;
+	u64 system_time;
+	u64 cycle_counter;
+	mali_addr64 jc = katom->jc;
+	kbase_context *kctx = katom->kctx;
+	int pm_active_err;
+
+	u32 hi1, hi2;
+
+	memset(&data, 0, sizeof(data));
+
+	/* Take the PM active reference as late as possible - otherwise, it could
+	 * delay suspend until we process the atom (which may be at the end of a
+	 * long chain of dependencies */
+	pm_active_err = kbase_pm_context_active_handle_suspend(kctx->kbdev, KBASE_PM_SUSPEND_HANDLER_DONT_REACTIVATE);
+	if (pm_active_err) {
+		kbasep_js_device_data *js_devdata = &kctx->kbdev->js_data;
+
+		/* We're suspended - queue this on the list of suspended jobs
+		 * Use dep_item[1], because dep_item[0] is in use for 'waiting_soft_jobs' */
+		mutex_lock(&js_devdata->runpool_mutex);
+		list_add_tail(&katom->dep_item[1], &js_devdata->suspended_soft_jobs_list);
+		mutex_unlock(&js_devdata->runpool_mutex);
+
+		return pm_active_err;
+	}
+
+	kbase_pm_request_gpu_cycle_counter(kctx->kbdev);
+
+	/* Read hi, lo, hi to ensure that overflow from lo to hi is handled correctly */
+	do {
+		hi1 = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(CYCLE_COUNT_HI), NULL);
+		cycle_counter = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(CYCLE_COUNT_LO), NULL);
+		hi2 = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(CYCLE_COUNT_HI), NULL);
+		cycle_counter |= (((u64) hi1) << 32);
+	} while (hi1 != hi2);
+
+	/* Read hi, lo, hi to ensure that overflow from lo to hi is handled correctly */
+	do {
+		hi1 = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(TIMESTAMP_HI), NULL);
+		system_time = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(TIMESTAMP_LO), NULL);
+		hi2 = kbase_reg_read(kctx->kbdev, GPU_CONTROL_REG(TIMESTAMP_HI), NULL);
+		system_time |= (((u64) hi1) << 32);
+	} while (hi1 != hi2);
+
+	/* Record the CPU's idea of current time */
+	getnstimeofday(&ts);
+
+	kbase_pm_release_gpu_cycle_counter(kctx->kbdev);
+
+	kbase_pm_context_idle(kctx->kbdev);
+
+	data.sec = ts.tv_sec;
+	data.usec = ts.tv_nsec / 1000;
+	data.system_time = system_time;
+	data.cycle_counter = cycle_counter;
+
+	pfn = jc >> PAGE_SHIFT;
+	offset = jc & ~PAGE_MASK;
+
+	/* Assume this atom will be cancelled until we know otherwise */
+	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+	if (offset > 0x1000 - sizeof(data)) {
+		/* Wouldn't fit in the page */
+		return 0;
+	}
+
+	kbase_gpu_vm_lock(kctx);
+	reg = kbase_region_tracker_find_region_enclosing_address(kctx, jc);
+	if (reg &&
+	    (reg->flags & KBASE_REG_GPU_WR) &&
+	    reg->alloc && reg->alloc->pages)
+		addr = reg->alloc->pages[pfn - reg->start_pfn];
+
+	kbase_gpu_vm_unlock(kctx);
+	if (!addr)
+		return 0;
+
+	page = kmap(pfn_to_page(PFN_DOWN(addr)));
+	if (!page)
+		return 0;
+
+	memcpy(page + offset, &data, sizeof(data));
+	kbase_sync_to_cpu(addr + offset, page + offset, sizeof(data));
+	kunmap(pfn_to_page(PFN_DOWN(addr)));
+
+	/* Atom was fine - mark it as done */
+	katom->event_code = BASE_JD_EVENT_DONE;
+
+	return 0;
+}
+
+#ifdef CONFIG_SYNC
+
+/* Complete an atom that has returned '1' from kbase_process_soft_job (i.e. has waited)
+ *
+ * @param katom     The atom to complete
+ */
+static void complete_soft_job(kbase_jd_atom *katom)
+{
+	kbase_context *kctx = katom->kctx;
+
+	mutex_lock(&kctx->jctx.lock);
+	list_del(&katom->dep_item[0]);
+	kbase_finish_soft_job(katom);
+	if (jd_done_nolock(katom))
+		kbasep_js_try_schedule_head_ctx(kctx->kbdev);
+	mutex_unlock(&kctx->jctx.lock);
+}
+
+static base_jd_event_code kbase_fence_trigger(kbase_jd_atom *katom, int result)
+{
+	struct sync_pt *pt;
+	struct sync_timeline *timeline;
+
+	if (!list_is_singular(&katom->fence->pt_list_head)) {
+		/* Not exactly one item in the list - so it didn't (directly) come from us */
+		return BASE_JD_EVENT_JOB_CANCELLED;
+	}
+
+	pt = list_first_entry(&katom->fence->pt_list_head, struct sync_pt, pt_list);
+	timeline = pt->parent;
+
+	if (!kbase_sync_timeline_is_ours(timeline)) {
+		/* Fence has a sync_pt which isn't ours! */
+		return BASE_JD_EVENT_JOB_CANCELLED;
+	}
+
+	kbase_sync_signal_pt(pt, result);
+
+	sync_timeline_signal(timeline);
+
+	return (result < 0) ? BASE_JD_EVENT_JOB_CANCELLED : BASE_JD_EVENT_DONE;
+}
+
+static void kbase_fence_wait_worker(struct work_struct *data)
+{
+	kbase_jd_atom *katom;
+	kbase_context *kctx;
+
+	katom = container_of(data, kbase_jd_atom, work);
+	kctx = katom->kctx;
+
+	complete_soft_job(katom);
+}
+
+static void kbase_fence_wait_callback(struct sync_fence *fence, struct sync_fence_waiter *waiter)
+{
+	kbase_jd_atom *katom = container_of(waiter, kbase_jd_atom, sync_waiter);
+	kbase_context *kctx;
+
+	KBASE_DEBUG_ASSERT(NULL != katom);
+
+	kctx = katom->kctx;
+
+	KBASE_DEBUG_ASSERT(NULL != kctx);
+
+	/* Propagate the fence status to the atom.
+	 * If negative then cancel this atom and its dependencies.
+	 */
+	if (fence->status < 0)
+	{
+		katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+	}
+
+	/* To prevent a potential deadlock we schedule the work onto the job_done_wq workqueue
+	 *
+	 * The issue is that we may signal the timeline while holding kctx->jctx.lock and
+	 * the callbacks are run synchronously from sync_timeline_signal. So we simply defer the work.
+	 */
+
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+	INIT_WORK(&katom->work, kbase_fence_wait_worker);
+	queue_work(kctx->jctx.job_done_wq, &katom->work);
+}
+
+static int kbase_fence_wait(kbase_jd_atom *katom)
+{
+	int ret;
+
+	KBASE_DEBUG_ASSERT(NULL != katom);
+	KBASE_DEBUG_ASSERT(NULL != katom->kctx);
+
+	sync_fence_waiter_init(&katom->sync_waiter, kbase_fence_wait_callback);
+
+	ret = sync_fence_wait_async(katom->fence, &katom->sync_waiter);
+
+	if (ret == 1) {
+		/* Already signalled */
+		return 0;
+	} else if (ret < 0) {
+		goto cancel_atom;
+	}
+	return 1;
+
+ cancel_atom:
+	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+	/* We should cause the dependant jobs in the bag to be failed,
+	 * to do this we schedule the work queue to complete this job */
+	KBASE_DEBUG_ASSERT(0 == object_is_on_stack(&katom->work));
+	INIT_WORK(&katom->work, kbase_fence_wait_worker);
+	queue_work(katom->kctx->jctx.job_done_wq, &katom->work);
+	return 1;
+}
+
+static void kbase_fence_cancel_wait(kbase_jd_atom *katom)
+{
+	if (sync_fence_cancel_async(katom->fence, &katom->sync_waiter) != 0)
+	{
+		/* The wait wasn't cancelled - leave the cleanup for kbase_fence_wait_callback */
+		return;
+	}
+
+	/* Wait was cancelled - zap the atoms */
+	katom->event_code = BASE_JD_EVENT_JOB_CANCELLED;
+
+	kbase_finish_soft_job(katom);
+
+	if (jd_done_nolock(katom))
+		kbasep_js_try_schedule_head_ctx(katom->kctx->kbdev);
+}
+#endif /* CONFIG_SYNC */
+
+int kbase_process_soft_job(kbase_jd_atom *katom)
+{
+	int status;
+	switch (katom->core_req & BASEP_JD_REQ_ATOM_TYPE) {
+	case BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME:
+		return kbase_dump_cpu_gpu_time(katom);
+#ifdef CONFIG_SYNC
+	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
+		KBASE_DEBUG_ASSERT(katom->fence != NULL);
+		katom->event_code = kbase_fence_trigger(katom, katom->event_code == BASE_JD_EVENT_DONE ? 0 : -EFAULT);
+		/* Release the reference as we don't need it any more */
+		sync_fence_put(katom->fence);
+		katom->fence = NULL;
+		break;
+	case BASE_JD_REQ_SOFT_FENCE_WAIT:
+		return kbase_fence_wait(katom);
+#endif				/* CONFIG_SYNC */
+	case BASE_JD_REQ_SOFT_REPLAY:
+		status = kbase_replay_process(katom);
+		if (status & MALI_REPLAY_FLAG_JS_RESCHED)
+			pr_err("replay called from kbase_process_soft_job - missing resched!\n");
+		return status & MALI_REPLAY_STATUS_MASK;
+	}
+
+	/* Atom is complete */
+	return 0;
+}
+
+void kbase_cancel_soft_job(kbase_jd_atom *katom)
+{
+	switch (katom->core_req & BASEP_JD_REQ_ATOM_TYPE) {
+#ifdef CONFIG_SYNC
+	case BASE_JD_REQ_SOFT_FENCE_WAIT:
+		kbase_fence_cancel_wait(katom);
+		break;
+#endif
+	default:
+		/* This soft-job doesn't support cancellation! */
+		KBASE_DEBUG_ASSERT(0);
+	}
+}
+
+mali_error kbase_prepare_soft_job(kbase_jd_atom *katom)
+{
+	switch (katom->core_req & BASEP_JD_REQ_ATOM_TYPE) {
+	case BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME:
+		{
+			if(0 != (katom->jc & KBASE_CACHE_ALIGNMENT_MASK))
+				return MALI_ERROR_FUNCTION_FAILED;
+		}
+		break;
+#ifdef CONFIG_SYNC
+	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
+		{
+			base_fence fence;
+			int fd;
+			if (0 != copy_from_user(&fence, (__user void *)(uintptr_t) katom->jc, sizeof(fence)))
+				return MALI_ERROR_FUNCTION_FAILED;
+
+			fd = kbase_stream_create_fence(fence.basep.stream_fd);
+			if (fd < 0)
+				return MALI_ERROR_FUNCTION_FAILED;
+
+			katom->fence = sync_fence_fdget(fd);
+
+			if (katom->fence == NULL) {
+				/* The only way the fence can be NULL is if userspace closed it for us.
+				 * So we don't need to clear it up */
+				return MALI_ERROR_FUNCTION_FAILED;
+			}
+			fence.basep.fd = fd;
+			if (0 != copy_to_user((__user void *)(uintptr_t) katom->jc, &fence, sizeof(fence))) {
+				katom->fence = NULL;
+				sys_close(fd);
+				return MALI_ERROR_FUNCTION_FAILED;
+			}
+		}
+		break;
+	case BASE_JD_REQ_SOFT_FENCE_WAIT:
+		{
+			base_fence fence;
+			if (0 != copy_from_user(&fence, (__user void *)(uintptr_t) katom->jc, sizeof(fence)))
+				return MALI_ERROR_FUNCTION_FAILED;
+
+			/* Get a reference to the fence object */
+			katom->fence = sync_fence_fdget(fence.basep.fd);
+			if (katom->fence == NULL)
+				return MALI_ERROR_FUNCTION_FAILED;
+		}
+		break;
+#endif				/* CONFIG_SYNC */
+	case BASE_JD_REQ_SOFT_REPLAY:
+		break;
+	default:
+		/* Unsupported soft-job */
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+	return MALI_ERROR_NONE;
+}
+
+void kbase_finish_soft_job(kbase_jd_atom *katom)
+{
+	switch (katom->core_req & BASEP_JD_REQ_ATOM_TYPE) {
+	case BASE_JD_REQ_SOFT_DUMP_CPU_GPU_TIME:
+		/* Nothing to do */
+		break;
+#ifdef CONFIG_SYNC
+	case BASE_JD_REQ_SOFT_FENCE_TRIGGER:
+		if (katom->fence) {
+			/* The fence has not yet been signalled, so we do it now */
+			kbase_fence_trigger(katom, katom->event_code == BASE_JD_EVENT_DONE ? 0 : -EFAULT);
+			sync_fence_put(katom->fence);
+			katom->fence = NULL;
+		}
+		break;
+	case BASE_JD_REQ_SOFT_FENCE_WAIT:
+		/* Release the reference to the fence object */
+		sync_fence_put(katom->fence);
+		katom->fence = NULL;
+		break;
+#endif				/* CONFIG_SYNC */
+	}
+}
+
+void kbase_resume_suspended_soft_jobs(kbase_device *kbdev)
+{
+	LIST_HEAD(local_suspended_soft_jobs);
+	kbase_jd_atom *tmp_iter;
+	kbase_jd_atom *katom_iter;
+	kbasep_js_device_data *js_devdata;
+	mali_bool resched = MALI_FALSE;
+	KBASE_DEBUG_ASSERT(kbdev);
+
+	js_devdata = &kbdev->js_data;
+
+	/* Move out the entire list */
+	mutex_lock(&js_devdata->runpool_mutex);
+	list_splice_init(&js_devdata->suspended_soft_jobs_list, &local_suspended_soft_jobs);
+	mutex_unlock(&js_devdata->runpool_mutex);
+
+	/* Each atom must be detached from the list and ran separately - it could
+	 * be re-added to the old list, but this is unlikely */
+	list_for_each_entry_safe(katom_iter, tmp_iter, &local_suspended_soft_jobs, dep_item[1])
+	{
+		kbase_context *kctx = katom_iter->kctx;
+		mutex_lock(&kctx->jctx.lock);
+
+		/* Remove from the global list */
+		list_del(&katom_iter->dep_item[1]);
+		/* Remove from the context's list of waiting soft jobs */
+		list_del(&katom_iter->dep_item[0]);
+
+		if (kbase_process_soft_job(katom_iter) == 0) {
+			kbase_finish_soft_job(katom_iter);
+			resched |= jd_done_nolock(katom_iter);
+		} else {
+			/* The job has not completed */
+			KBASE_DEBUG_ASSERT((katom_iter->core_req & BASEP_JD_REQ_ATOM_TYPE)
+						  		!= BASE_JD_REQ_SOFT_REPLAY);
+			list_add_tail(&katom_iter->dep_item[0], &kctx->waiting_soft_jobs);
+		}
+
+		mutex_unlock(&kctx->jctx.lock);
+	}
+
+	if (resched)
+		kbasep_js_try_schedule_head_ctx(kbdev);
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.c
new file mode 100644
index 0000000..f8db35e
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.c
@@ -0,0 +1,195 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_sync.c
+ *
+ */
+
+#ifdef CONFIG_SYNC
+
+#include "sync.h"
+#include <mali_kbase.h>
+
+struct mali_sync_timeline {
+	struct sync_timeline timeline;
+	atomic_t counter;
+	atomic_t signalled;
+};
+
+struct mali_sync_pt {
+	struct sync_pt pt;
+	u32 order;
+	int result;
+};
+
+static struct mali_sync_timeline *to_mali_sync_timeline(struct sync_timeline *timeline)
+{
+	return container_of(timeline, struct mali_sync_timeline, timeline);
+}
+
+static struct mali_sync_pt *to_mali_sync_pt(struct sync_pt *pt)
+{
+	return container_of(pt, struct mali_sync_pt, pt);
+}
+
+static struct sync_pt *timeline_dup(struct sync_pt *pt)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_pt *new_mpt;
+	struct sync_pt *new_pt = sync_pt_create(pt->parent, sizeof(struct mali_sync_pt));
+
+	if (!new_pt)
+		return NULL;
+
+	new_mpt = to_mali_sync_pt(new_pt);
+	new_mpt->order = mpt->order;
+	new_mpt->result = mpt->result;
+
+	return new_pt;
+
+}
+
+static int timeline_has_signaled(struct sync_pt *pt)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(pt->parent);
+	int result = mpt->result;
+
+	long diff = atomic_read(&mtl->signalled) - mpt->order;
+
+	if (diff >= 0)
+	{
+		return result < 0 ?  result : 1;
+	}
+	else
+		return 0;
+}
+
+static int timeline_compare(struct sync_pt *a, struct sync_pt *b)
+{
+	struct mali_sync_pt *ma = container_of(a, struct mali_sync_pt, pt);
+	struct mali_sync_pt *mb = container_of(b, struct mali_sync_pt, pt);
+
+	long diff = ma->order - mb->order;
+
+	if (diff < 0)
+		return -1;
+	else if (diff == 0)
+		return 0;
+	else
+		return 1;
+}
+
+static void timeline_value_str(struct sync_timeline *timeline, char * str,
+			       int size)
+{
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(timeline);
+	snprintf(str, size, "%d", atomic_read(&mtl->signalled));
+}
+
+static void pt_value_str(struct sync_pt *pt, char *str, int size)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	snprintf(str, size, "%d(%d)", mpt->order, mpt->result);
+}
+
+static struct sync_timeline_ops mali_timeline_ops = {
+	.driver_name = "Mali",
+	.dup = timeline_dup,
+	.has_signaled = timeline_has_signaled,
+	.compare = timeline_compare,
+	.timeline_value_str = timeline_value_str,
+	.pt_value_str       = pt_value_str,
+#if 0
+	.free_pt = timeline_free_pt,
+	.release_obj = timeline_release_obj
+#endif
+};
+
+int kbase_sync_timeline_is_ours(struct sync_timeline *timeline)
+{
+	return timeline->ops == &mali_timeline_ops;
+}
+
+struct sync_timeline *kbase_sync_timeline_alloc(const char *name)
+{
+	struct sync_timeline *tl;
+	struct mali_sync_timeline *mtl;
+
+	tl = sync_timeline_create(&mali_timeline_ops, sizeof(struct mali_sync_timeline), name);
+	if (!tl)
+		return NULL;
+
+	/* Set the counter in our private struct */
+	mtl = to_mali_sync_timeline(tl);
+	atomic_set(&mtl->counter, 0);
+	atomic_set(&mtl->signalled, 0);
+
+	return tl;
+}
+
+struct sync_pt *kbase_sync_pt_alloc(struct sync_timeline *parent)
+{
+	struct sync_pt *pt = sync_pt_create(parent, sizeof(struct mali_sync_pt));
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(parent);
+	struct mali_sync_pt *mpt;
+
+	if (!pt)
+		return NULL;
+
+	mpt = to_mali_sync_pt(pt);
+	mpt->order = atomic_inc_return(&mtl->counter);
+	mpt->result = 0;
+
+	return pt;
+}
+
+void kbase_sync_signal_pt(struct sync_pt *pt, int result)
+{
+	struct mali_sync_pt *mpt = to_mali_sync_pt(pt);
+	struct mali_sync_timeline *mtl = to_mali_sync_timeline(pt->parent);
+	int signalled;
+	int diff;
+
+	mpt->result = result;
+
+	do {
+
+		signalled = atomic_read(&mtl->signalled);
+
+		diff = signalled - mpt->order;
+
+		if (diff > 0) {
+			/* The timeline is already at or ahead of this point.
+			 * This should not happen unless userspace has been
+			 * signalling fences out of order, so warn but don't
+			 * violate the sync_pt API.
+			 * The warning is only in debug builds to prevent
+			 * a malicious user being able to spam dmesg.
+			 */
+#ifdef CONFIG_MALI_DEBUG
+			pr_err("Fences were triggered in a different order to allocation!");
+#endif				/* CONFIG_MALI_DEBUG */
+			return;
+		}
+	} while (atomic_cmpxchg(&mtl->signalled, signalled, mpt->order) != signalled);
+}
+
+#endif				/* CONFIG_SYNC */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.h
new file mode 100644
index 0000000..b9f2951
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync.h
@@ -0,0 +1,83 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_sync.h
+ *
+ */
+
+#ifndef MALI_KBASE_SYNC_H
+#define MALI_KBASE_SYNC_H
+
+#include "sync.h"
+#include <malisw/mali_malisw.h>
+
+/*
+ * Create a stream object.
+ * Built on top of timeline object.
+ * Exposed as a file descriptor.
+ * Life-time controlled via the file descriptor:
+ * - dup to add a ref
+ * - close to remove a ref
+ */
+mali_error kbase_stream_create(const char *name, int *const out_fd);
+
+/*
+ * Create a fence in a stream object
+ */
+int kbase_stream_create_fence(int tl_fd);
+
+/*
+ * Validate a fd to be a valid fence
+ * No reference is taken.
+ *
+ * This function is only usable to catch unintentional user errors early,
+ * it does not stop malicious code changing the fd after this function returns.
+ */
+mali_error kbase_fence_validate(int fd);
+
+/* Returns true if the specified timeline is allocated by Mali */
+int kbase_sync_timeline_is_ours(struct sync_timeline *timeline);
+
+/* Allocates a timeline for Mali
+ *
+ * One timeline should be allocated per API context.
+ */
+struct sync_timeline *kbase_sync_timeline_alloc(const char *name);
+
+/* Allocates a sync point within the timeline.
+ *
+ * The timeline must be the one allocated by kbase_sync_timeline_alloc
+ *
+ * Sync points must be triggered in *exactly* the same order as they are allocated.
+ */
+struct sync_pt *kbase_sync_pt_alloc(struct sync_timeline *parent);
+
+/* Signals a particular sync point
+ *
+ * Sync points must be triggered in *exactly* the same order as they are allocated.
+ *
+ * If they are signalled in the wrong order then a message will be printed in debug
+ * builds and otherwise attempts to signal order sync_pts will be ignored.
+ *
+ * result can be negative to indicate error, any other value is interpreted as success.
+ */
+void kbase_sync_signal_pt(struct sync_pt *pt, int result);
+
+#endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync_user.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync_user.c
new file mode 100644
index 0000000..ce1c3ff
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_sync_user.c
@@ -0,0 +1,133 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_kbase_sync_user.c
+ *
+ */
+
+#ifdef CONFIG_SYNC
+
+#include <linux/sched.h>
+#include <linux/fdtable.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/module.h>
+#include <linux/anon_inodes.h>
+#include <linux/version.h>
+#include <linux/uaccess.h>
+#include <mali_kbase_sync.h>
+#include <mali_base_kernel_sync.h>
+
+static int kbase_stream_close(struct inode *inode, struct file *file)
+{
+	struct sync_timeline *tl;
+	tl = (struct sync_timeline *)file->private_data;
+	BUG_ON(!tl);
+	sync_timeline_destroy(tl);
+	return 0;
+}
+
+static const struct file_operations stream_fops = {
+	.owner = THIS_MODULE,
+	.release = kbase_stream_close,
+};
+
+mali_error kbase_stream_create(const char *name, int *const out_fd)
+{
+	struct sync_timeline *tl;
+	BUG_ON(!out_fd);
+
+	tl = kbase_sync_timeline_alloc(name);
+	if (!tl)
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	*out_fd = anon_inode_getfd(name, &stream_fops, tl, O_RDONLY | O_CLOEXEC);
+
+	if (*out_fd < 0) {
+		sync_timeline_destroy(tl);
+		return MALI_ERROR_FUNCTION_FAILED;
+	} else {
+		return MALI_ERROR_NONE;
+	}
+}
+
+int kbase_stream_create_fence(int tl_fd)
+{
+	struct sync_timeline *tl;
+	struct sync_pt *pt;
+	struct sync_fence *fence;
+	int fd;
+	struct file *tl_file;
+
+	tl_file = fget(tl_fd);
+	if (tl_file == NULL)
+		return -EBADF;
+
+	if (tl_file->f_op != &stream_fops) {
+		fd = -EBADF;
+		goto out;
+	}
+
+	tl = tl_file->private_data;
+
+	pt = kbase_sync_pt_alloc(tl);
+	if (!pt) {
+		fd = -EFAULT;
+		goto out;
+	}
+
+	fence = sync_fence_create("mali_fence", pt);
+	if (!fence) {
+		sync_pt_free(pt);
+		fd = -EFAULT;
+		goto out;
+	}
+
+	/* from here the fence owns the sync_pt */
+
+	/* create a fd representing the fence */
+	fd = get_unused_fd_flags(O_RDWR | O_CLOEXEC);
+	if (fd < 0) {
+		sync_fence_put(fence);
+		goto out;
+	}
+
+	/* bind fence to the new fd */
+	sync_fence_install(fence, fd);
+
+ out:
+	fput(tl_file);
+
+	return fd;
+}
+
+mali_error kbase_fence_validate(int fd)
+{
+	struct sync_fence *fence;
+	fence = sync_fence_fdget(fd);
+	if (NULL != fence) {
+		sync_fence_put(fence);
+		return MALI_ERROR_NONE;
+	} else {
+		return MALI_ERROR_FUNCTION_FAILED;
+	}
+}
+
+#endif				/* CONFIG_SYNC */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_defs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_defs.h
new file mode 100644
index 0000000..2e5b744
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_defs.h
@@ -0,0 +1,232 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/* ***** IMPORTANT: THIS IS NOT A NORMAL HEADER FILE         *****
+ * *****            DO NOT INCLUDE DIRECTLY                  *****
+ * *****            THE LACK OF HEADER GUARDS IS INTENTIONAL ***** */
+
+/*
+ * The purpose of this header file is just to contain a list of trace code idenitifers
+ *
+ * Each identifier is wrapped in a macro, so that its string form and enum form can be created
+ *
+ * Each macro is separated with a comma, to allow insertion into an array initializer or enum definition block.
+ *
+ * This allows automatic creation of an enum and a corresponding array of strings
+ *
+ * Before #including, the includer MUST #define KBASE_TRACE_CODE_MAKE_CODE.
+ * After #including, the includer MUST #under KBASE_TRACE_CODE_MAKE_CODE.
+ *
+ * e.g.:
+ * #define KBASE_TRACE_CODE( X ) KBASE_TRACE_CODE_ ## X
+ * typedef enum
+ * {
+ * #define KBASE_TRACE_CODE_MAKE_CODE( X ) KBASE_TRACE_CODE( X )
+ * #include "mali_kbase_trace_defs.h"
+ * #undef  KBASE_TRACE_CODE_MAKE_CODE
+ * } kbase_trace_code;
+ *
+ * IMPORTANT: THIS FILE MUST NOT BE USED FOR ANY OTHER PURPOSE OTHER THAN THE ABOVE
+ *
+ *
+ * The use of the macro here is:
+ * - KBASE_TRACE_CODE_MAKE_CODE( X )
+ *
+ * Which produces:
+ * - For an enum, KBASE_TRACE_CODE_X
+ * - For a string, "X"
+ *
+ *
+ * For example:
+ * - KBASE_TRACE_CODE_MAKE_CODE( JM_JOB_COMPLETE ) expands to:
+ *  - KBASE_TRACE_CODE_JM_JOB_COMPLETE for the enum
+ *  - "JM_JOB_COMPLETE" for the string
+ * - To use it to trace an event, do:
+ *  - KBASE_TRACE_ADD( kbdev, JM_JOB_COMPLETE, subcode, kctx, uatom, val );
+ */
+
+#if 0 /* Dummy section to avoid breaking formatting */
+int dummy_array[] = {
+#endif
+
+/*
+ * Core events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_CTX_DESTROY),	/* no info_val, no gpu_addr, no atom */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_CTX_HWINSTR_TERM),	/* no info_val, no gpu_addr, no atom */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_IRQ),	/* info_val == GPU_IRQ_STATUS register */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_IRQ_CLEAR),	/* info_val == bits cleared */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_IRQ_DONE),	/* info_val == GPU_IRQ_STATUS register */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_SOFT_RESET),
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_HARD_RESET),
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_PRFCNT_CLEAR),
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_PRFCNT_SAMPLE),	/* GPU addr==dump address */
+	KBASE_TRACE_CODE_MAKE_CODE(CORE_GPU_CLEAN_INV_CACHES),
+
+/*
+ * Job Slot management events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_IRQ),	/* info_val==irq rawstat at start */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_IRQ_END),
+					/* info_val==jobs processed */
+/* In the following:
+ *
+ * - ctx is set if a corresponding job found (NULL otherwise, e.g. some soft-stop cases)
+ * - uatom==kernel-side mapped uatom address (for correlation with user-side)
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_JOB_DONE),	/* info_val==exit code; gpu_addr==chain gpuaddr */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SUBMIT),
+					/* gpu_addr==JSn_HEAD_NEXT written, info_val==lower 32 bits of affinity */
+/* gpu_addr is as follows:
+ * - If JSn_STATUS active after soft-stop, val==gpu addr written to JSn_HEAD on submit
+ * - otherwise gpu_addr==0 */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SOFTSTOP),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SOFTSTOP_0),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SOFTSTOP_1),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_HARDSTOP),	/* gpu_addr==JSn_HEAD read */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_HARDSTOP_0),	/* gpu_addr==JSn_HEAD read */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_HARDSTOP_1),	/* gpu_addr==JSn_HEAD read */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_UPDATE_HEAD),	/* gpu_addr==JSn_TAIL read */
+/* gpu_addr is as follows:
+ * - If JSn_STATUS active before soft-stop, val==JSn_HEAD
+ * - otherwise gpu_addr==0 */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_CHECK_HEAD),	/* gpu_addr==JSn_HEAD read */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_FLUSH_WORKQS),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_FLUSH_WORKQS_DONE),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_ZAP_NON_SCHEDULED),	/* info_val == is_scheduled */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_ZAP_SCHEDULED),
+						/* info_val == is_scheduled */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_ZAP_DONE),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SLOT_SOFT_OR_HARD_STOP),	/* info_val == nr jobs submitted */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SLOT_EVICT),	/* gpu_addr==JSn_HEAD_NEXT last written */
+	KBASE_TRACE_CODE_MAKE_CODE(JM_SUBMIT_AFTER_RESET),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_BEGIN_RESET_WORKER),
+	KBASE_TRACE_CODE_MAKE_CODE(JM_END_RESET_WORKER),
+/*
+ * Job dispatch events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_DONE),/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_DONE_WORKER),	/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_DONE_WORKER_END),
+						/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_DONE_TRY_RUN_NEXT_JOB),
+							/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_ZAP_CONTEXT),	/* gpu_addr==0, info_val==0, uatom==0 */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_CANCEL),
+					/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JD_CANCEL_WORKER),
+						/* gpu_addr==value to write into JSn_HEAD */
+/*
+ * Scheduler Core events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_RETAIN_CTX_NOLOCK),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_ADD_JOB),	/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_REMOVE_JOB),	/* gpu_addr==last value written/would be written to JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_RETAIN_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_RELEASE_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_TRY_SCHEDULE_HEAD_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_JOB_DONE_TRY_RUN_NEXT_JOB),	/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_JOB_DONE_RETRY_NEEDED),
+							/* gpu_addr==value to write into JSn_HEAD */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_FAST_START_EVICTS_CTX),
+							/* kctx is the one being evicted, info_val == kctx to put in  */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_AFFINITY_SUBMIT_TO_BLOCKED),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_AFFINITY_CURRENT),	/* info_val == lower 32 bits of affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CORE_REF_REQUEST_CORES_FAILED),
+								/* info_val == lower 32 bits of affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CORE_REF_REGISTER_INUSE_FAILED),
+								/* info_val == lower 32 bits of affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CORE_REF_REQUEST_ON_RECHECK_FAILED),	/* info_val == lower 32 bits of rechecked affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CORE_REF_REGISTER_ON_RECHECK_FAILED),	/* info_val == lower 32 bits of rechecked affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CORE_REF_AFFINITY_WOULD_VIOLATE),
+								/* info_val == lower 32 bits of affinity */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CTX_ATTR_NOW_ON_CTX),	/* info_val == the ctx attribute now on ctx */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CTX_ATTR_NOW_ON_RUNPOOL),
+							/* info_val == the ctx attribute now on runpool */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CTX_ATTR_NOW_OFF_CTX),/* info_val == the ctx attribute now off ctx */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_CTX_ATTR_NOW_OFF_RUNPOOL),	/* info_val == the ctx attribute now off runpool */
+/*
+ * Scheduler Policy events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_INIT_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_TERM_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_TRY_EVICT_CTX),	/* info_val == whether it was evicted */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_FOREACH_CTX_JOBS),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_ENQUEUE_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_DEQUEUE_HEAD_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_RUNPOOL_ADD_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_RUNPOOL_REMOVE_CTX),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_DEQUEUE_JOB),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_DEQUEUE_JOB_IRQ),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_ENQUEUE_JOB),	/* gpu_addr==JSn_HEAD to write if the job were run */
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_TIMER_START),
+	KBASE_TRACE_CODE_MAKE_CODE(JS_POLICY_TIMER_END),
+/*
+ * Power Management Events
+ */
+	KBASE_TRACE_CODE_MAKE_CODE(PM_JOB_SUBMIT_AFTER_POWERING_UP),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_JOB_SUBMIT_AFTER_POWERED_UP),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWRON),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWRON_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWRON_L2),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWROFF),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWROFF_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_PWROFF_L2),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_POWERED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_POWERED_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_POWERED_L2),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_CHANGE_DESIRED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_CHANGE_DESIRED_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_CHANGE_AVAILABLE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_CHANGE_AVAILABLE_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_AVAILABLE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CORES_AVAILABLE_TILER),
+	/* PM_DESIRED_REACHED: gpu_addr == pm.gpu_in_desired_state */
+	KBASE_TRACE_CODE_MAKE_CODE(PM_DESIRED_REACHED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_DESIRED_REACHED_TILER),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REGISTER_CHANGE_SHADER_INUSE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REGISTER_CHANGE_TILER_INUSE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REGISTER_CHANGE_SHADER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REGISTER_CHANGE_TILER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_RELEASE_CHANGE_SHADER_INUSE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_RELEASE_CHANGE_TILER_INUSE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_UNREQUEST_CHANGE_SHADER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_UNREQUEST_CHANGE_TILER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REQUEST_CHANGE_SHADER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_REQUEST_CHANGE_TILER_NEEDED),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_WAKE_WAITERS),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CONTEXT_ACTIVE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CONTEXT_IDLE),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_GPU_ON),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_GPU_OFF),
+	KBASE_TRACE_CODE_MAKE_CODE(PM_SET_POLICY),	/* info_val == policy number, or -1 for "Already changing" */
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CA_SET_POLICY),
+
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CURRENT_POLICY_INIT),	/* info_val == policy number */
+	KBASE_TRACE_CODE_MAKE_CODE(PM_CURRENT_POLICY_TERM),	/* info_val == policy number */
+/* Unused code just to make it easier to not have a comma at the end.
+ * All other codes MUST come before this */
+	KBASE_TRACE_CODE_MAKE_CODE(DUMMY)
+
+
+#if 0 /* Dummy section to avoid breaking formatting */
+};
+#endif
+
+/* ***** THE LACK OF HEADER GUARDS IS INTENTIONAL ***** */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.c
new file mode 100644
index 0000000..0968025
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.c
@@ -0,0 +1,231 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+#include <mali_kbase_jm.h>
+
+#define CREATE_TRACE_POINTS
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+#include "mali_timeline.h"
+
+#include <linux/debugfs.h>
+#include <linux/seq_file.h>
+
+struct kbase_trace_timeline_desc
+{
+	char *enum_str;
+	char *desc;
+	char *format;
+	char *format_desc;
+};
+
+struct kbase_trace_timeline_desc kbase_trace_timeline_desc_table[] =
+{
+	#define KBASE_TIMELINE_TRACE_CODE(enum_val, desc, format, format_desc) { #enum_val, desc, format, format_desc }
+	#include "mali_kbase_trace_timeline_defs.h"
+	#undef KBASE_TIMELINE_TRACE_CODE
+};
+
+#define KBASE_NR_TRACE_CODES ARRAY_SIZE(kbase_trace_timeline_desc_table)
+
+STATIC void *kbasep_trace_timeline_seq_start(struct seq_file *s, loff_t *pos)
+{
+	if (*pos >= KBASE_NR_TRACE_CODES)
+		return NULL;
+
+	return &kbase_trace_timeline_desc_table[*pos];
+}
+
+STATIC void kbasep_trace_timeline_seq_stop(struct seq_file *s, void *data)
+{
+}
+
+STATIC void *kbasep_trace_timeline_seq_next(struct seq_file *s, void *data, loff_t *pos)
+{
+	(*pos)++;
+
+	if (*pos == KBASE_NR_TRACE_CODES)
+		return NULL;
+
+	return &kbase_trace_timeline_desc_table[*pos];
+}
+
+STATIC int kbasep_trace_timeline_seq_show(struct seq_file *s, void *data)
+{
+	struct kbase_trace_timeline_desc *trace_desc = data;
+
+	seq_printf(s, "%s#%s#%s#%s\n", trace_desc->enum_str, trace_desc->desc, trace_desc->format, trace_desc->format_desc);
+	return 0;
+}
+
+
+static const struct seq_operations kbasep_trace_timeline_seq_ops = {
+	.start = kbasep_trace_timeline_seq_start,
+	.next = kbasep_trace_timeline_seq_next,
+	.stop = kbasep_trace_timeline_seq_stop,
+	.show = kbasep_trace_timeline_seq_show,
+};
+
+STATIC int kbasep_trace_timeline_debugfs_open(struct inode *inode, struct file *file)
+{
+	return seq_open(file, &kbasep_trace_timeline_seq_ops);
+}
+
+static const struct file_operations kbasep_trace_timeline_debugfs_fops = {
+	.open = kbasep_trace_timeline_debugfs_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = seq_release_private,
+};
+
+mali_error kbasep_trace_timeline_debugfs_init(kbase_device *kbdev)
+{
+	kbdev->timeline.dentry = debugfs_create_file("mali_timeline_defs",
+			S_IRUGO, kbdev->mali_debugfs_directory, NULL,
+			&kbasep_trace_timeline_debugfs_fops);
+	if (IS_ERR(kbdev->timeline.dentry))
+		return MALI_ERROR_FUNCTION_FAILED;
+
+	return MALI_ERROR_NONE;
+}
+
+void kbasep_trace_timeline_debugfs_term(kbase_device *kbdev)
+{
+	debugfs_remove(kbdev->timeline.dentry);
+}
+
+void kbase_timeline_job_slot_submit(kbase_device *kbdev, kbase_context *kctx,
+                                    kbase_jd_atom *katom, int js)
+{
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+
+	if(kbdev->timeline.slot_atoms_submitted[js] > 0) {
+		KBASE_TIMELINE_JOB_START_NEXT(kctx, js, 1);
+	} else {
+		base_atom_id atom_number = kbase_jd_atom_id(kctx, katom);
+		KBASE_TIMELINE_JOB_START_HEAD(kctx, js, 1);
+		KBASE_TIMELINE_JOB_START(kctx, js, atom_number);
+	}
+	++kbdev->timeline.slot_atoms_submitted[js];
+
+	KBASE_TIMELINE_ATOMS_SUBMITTED(kctx, js, kbdev->timeline.slot_atoms_submitted[js]);
+}
+
+void kbase_timeline_job_slot_done(kbase_device *kbdev, kbase_context *kctx,
+                                  kbase_jd_atom *katom, int js,
+                                  kbasep_js_atom_done_code done_code)
+{
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+
+	if (done_code & KBASE_JS_ATOM_DONE_EVICTED_FROM_NEXT) {
+		KBASE_TIMELINE_JOB_START_NEXT(kctx, js, 0);
+	} else {
+		/* Job finished in JSn_HEAD */
+		base_atom_id atom_number = kbase_jd_atom_id(kctx, katom);
+		KBASE_TIMELINE_JOB_START_HEAD(kctx, js, 0);
+		KBASE_TIMELINE_JOB_STOP(kctx, js, atom_number);
+		/* see if we need to trace the job in JSn_NEXT moving to JSn_HEAD */
+		if (kbdev->timeline.slot_atoms_submitted[js] > 1) {
+			/* Tag events with next_katom's kctx */
+			kbase_jm_slot *slot = &kbdev->jm_slots[js];
+			kbase_jd_atom *next_katom;
+			kbase_context *next_kctx;
+			KBASE_DEBUG_ASSERT(kbasep_jm_nr_jobs_submitted(slot) > 0);
+
+			/* Peek the next atom - note that the atom in JSn_HEAD will already
+			 * have been dequeued */
+			next_katom = kbasep_jm_peek_idx_submit_slot(slot, 0);
+			next_kctx = next_katom->kctx;
+			KBASE_TIMELINE_JOB_START_NEXT(next_kctx, js, 0);
+			KBASE_TIMELINE_JOB_START_HEAD(next_kctx, js, 1);
+			KBASE_TIMELINE_JOB_START(next_kctx, js, kbase_jd_atom_id(next_kctx, next_katom));
+		}
+	}
+
+	--kbdev->timeline.slot_atoms_submitted[js];
+
+	KBASE_TIMELINE_ATOMS_SUBMITTED(kctx, js, kbdev->timeline.slot_atoms_submitted[js]);
+}
+
+void kbase_timeline_pm_send_event(kbase_device *kbdev, kbase_timeline_pm_event event_sent)
+{
+	int uid = 0;
+	int old_uid;
+
+	/* If a producer already exists for the event, try to use their UID (multiple-producers) */
+	uid = atomic_read(&kbdev->timeline.pm_event_uid[event_sent]);
+	old_uid = uid;
+
+	/* Get a new non-zero UID if we don't have one yet */
+	while (!uid)
+		uid = atomic_inc_return(&kbdev->timeline.pm_event_uid_counter);
+
+	/* Try to use this UID */
+	if ( old_uid != atomic_cmpxchg(&kbdev->timeline.pm_event_uid[event_sent], old_uid, uid))
+		/* If it changed, raced with another producer: we've lost this UID */
+		uid = 0;
+
+	KBASE_TIMELINE_PM_SEND_EVENT(kbdev, event_sent, uid);
+}
+
+void kbase_timeline_pm_check_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event)
+{
+	int uid = atomic_read(&kbdev->timeline.pm_event_uid[event]);
+
+	if (uid != 0) {
+		if (uid != atomic_cmpxchg(&kbdev->timeline.pm_event_uid[event], uid, 0))
+			/* If it changed, raced with another consumer: we've lost this UID */
+			uid = 0;
+
+		KBASE_TIMELINE_PM_HANDLE_EVENT(kbdev, event, uid);
+	}
+}
+
+void kbase_timeline_pm_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event)
+{
+	int uid = atomic_read(&kbdev->timeline.pm_event_uid[event]);
+
+	if (uid != atomic_cmpxchg(&kbdev->timeline.pm_event_uid[event], uid, 0))
+		/* If it changed, raced with another consumer: we've lost this UID */
+		uid = 0;
+
+	KBASE_TIMELINE_PM_HANDLE_EVENT(kbdev, event, uid);
+}
+
+void kbase_timeline_pm_l2_transition_start(kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+	/* Simply log the start of the transition */
+	kbdev->timeline.l2_transitioning = MALI_TRUE;
+	KBASE_TIMELINE_POWERING_L2(kbdev);
+}
+
+void kbase_timeline_pm_l2_transition_done(kbase_device *kbdev)
+{
+	lockdep_assert_held(&kbdev->pm.power_change_lock);
+	/* Simply log the end of the transition */
+	if( MALI_FALSE != kbdev->timeline.l2_transitioning )
+	{
+		kbdev->timeline.l2_transitioning = MALI_FALSE;
+		KBASE_TIMELINE_POWERED_L2(kbdev);
+	}
+}
+
+#endif /* CONFIG_MALI_TRACE_TIMELINE */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.h
new file mode 100644
index 0000000..fc2a383
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline.h
@@ -0,0 +1,368 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#if !defined(_KBASE_TRACE_TIMELINE_H)
+#define _KBASE_TRACE_TIMELINE_H
+
+#ifdef CONFIG_MALI_TRACE_TIMELINE
+
+typedef enum
+{
+	#define KBASE_TIMELINE_TRACE_CODE(enum_val, desc, format, format_desc) enum_val
+	#include "mali_kbase_trace_timeline_defs.h"
+	#undef KBASE_TIMELINE_TRACE_CODE
+} kbase_trace_timeline_code;
+
+/** Initialize Timeline DebugFS entries */
+mali_error kbasep_trace_timeline_debugfs_init(kbase_device *kbdev);
+/** Terminate Timeline DebugFS entries */
+void kbasep_trace_timeline_debugfs_term(kbase_device *kbdev);
+
+/* mali_timeline.h defines kernel tracepoints used by the KBASE_TIMELINE
+ * functions.
+ * Output is timestamped by either sched_clock() (default), local_clock(), or
+ * cpu_clock(), depending on /sys/kernel/debug/tracing/trace_clock */
+#include "mali_timeline.h"
+
+/* Trace number of atoms in flight for kctx (atoms either not completed, or in
+   process of being returned to user */
+#define KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, count)                                 \
+	do                                                                          \
+	{                                                                           \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_atoms_in_flight(ts.tv_sec, ts.tv_nsec,          \
+		                                            (int)kctx->timeline.owner_tgid, \
+		                                            count);                         \
+	} while (0)
+
+/* Trace atom_id being Ready to Run */
+#define KBASE_TIMELINE_ATOM_READY(kctx, atom_id)                   \
+	do                                                             \
+	{                                                              \
+		struct timespec ts;                                        \
+		getnstimeofday(&ts);                                       \
+		trace_mali_timeline_atom(ts.tv_sec, ts.tv_nsec,            \
+		                         CTX_FLOW_ATOM_READY,              \
+		                         (int)kctx->timeline.owner_tgid,   \
+		                         atom_id);                         \
+	} while (0)
+
+
+/* Trace number of atoms submitted to job slot js
+ *
+ * NOTE: This uses a different tracepoint to the head/next/soft-stop actions,
+ * so that those actions can be filtered out separately from this
+ *
+ * This is because this is more useful, as we can use it to calculate general
+ * utilization easily and accurately */
+#define KBASE_TIMELINE_ATOMS_SUBMITTED(kctx, js, count)                             \
+	do                                                                          \
+	{                                                                           \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_gpu_slot_active(ts.tv_sec, ts.tv_nsec,          \
+		                                    SW_SET_GPU_SLOT_ACTIVE,         \
+		                                    (int)kctx->timeline.owner_tgid, \
+		                                    js, count);                     \
+	} while (0)
+
+
+/* Trace atoms present in JSn_NEXT */
+#define KBASE_TIMELINE_JOB_START_NEXT(kctx, js, count)                             \
+	do                                                                          \
+	{                                                                           \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_gpu_slot_action(ts.tv_sec, ts.tv_nsec,          \
+		                                    SW_SET_GPU_SLOT_NEXT, \
+		                                    (int)kctx->timeline.owner_tgid, \
+		                                    js, count);                     \
+	} while (0)
+
+/* Trace atoms present in JSn_HEAD */
+#define KBASE_TIMELINE_JOB_START_HEAD(kctx, js, count)                             \
+	do                                                                          \
+	{                                                                           \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_gpu_slot_action(ts.tv_sec, ts.tv_nsec,          \
+		                                    SW_SET_GPU_SLOT_HEAD,           \
+		                                    (int)kctx->timeline.owner_tgid, \
+		                                    js, count);                     \
+	} while (0)
+
+/* Trace that a soft stop/evict from next is being attempted on a slot */
+#define KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, count) \
+	do                                                                          \
+	{                                                                           \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_gpu_slot_action(ts.tv_sec, ts.tv_nsec,          \
+		                                    SW_SET_GPU_SLOT_STOPPING,       \
+		                                    (kctx)?(int)kctx->timeline.owner_tgid:0, \
+		                                    js, count);                     \
+	} while (0)
+
+
+
+/* Trace state of overall GPU power */
+#define KBASE_TIMELINE_GPU_POWER(kbdev, active)                                    \
+	do                                                                         \
+	{                                                                          \
+		struct timespec ts;                                                \
+		getnstimeofday(&ts);                                               \
+		trace_mali_timeline_gpu_power_active(ts.tv_sec, ts.tv_nsec,        \
+		                                     SW_SET_GPU_POWER_ACTIVE, active); \
+	} while (0)
+
+/* Trace state of tiler power */
+#define KBASE_TIMELINE_POWER_TILER(kbdev, bitmap)                               \
+	do                                                                      \
+	{                                                                       \
+		struct timespec ts;                                             \
+		getnstimeofday(&ts);                                            \
+		trace_mali_timeline_gpu_power_active(ts.tv_sec, ts.tv_nsec,     \
+		                                     SW_SET_GPU_POWER_TILER_ACTIVE, \
+		                                     hweight64(bitmap));        \
+	} while (0)
+
+/* Trace number of shaders currently powered */
+#define KBASE_TIMELINE_POWER_SHADER(kbdev, bitmap)                               \
+	do                                                                       \
+	{                                                                        \
+		struct timespec ts;                                              \
+		getnstimeofday(&ts);                                             \
+		trace_mali_timeline_gpu_power_active(ts.tv_sec, ts.tv_nsec,      \
+		                                     SW_SET_GPU_POWER_SHADER_ACTIVE, \
+		                                     hweight64(bitmap));         \
+	} while (0)
+
+/* Trace state of L2 power */
+#define KBASE_TIMELINE_POWER_L2(kbdev, bitmap)                              \
+	do                                                                      \
+	{                                                                       \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_gpu_power_active(ts.tv_sec, ts.tv_nsec,         \
+				                             SW_SET_GPU_POWER_L2_ACTIVE,    \
+				                             hweight64(bitmap));            \
+	}while(0)
+
+/* Trace state of L2 cache*/
+#define KBASE_TIMELINE_POWERING_L2(kbdev)                                   \
+	do                                                                      \
+	{                                                                       \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_l2_power_active(ts.tv_sec, ts.tv_nsec,	        \
+		                                    SW_FLOW_GPU_POWER_L2_POWERING,  \
+		                                    1);                             \
+	}while(0)
+
+#define KBASE_TIMELINE_POWERED_L2(kbdev)                                    \
+	do                                                                      \
+	{                                                                       \
+		struct timespec ts;                                                 \
+		getnstimeofday(&ts);                                                \
+		trace_mali_timeline_l2_power_active(ts.tv_sec, ts.tv_nsec,          \
+		                                    SW_FLOW_GPU_POWER_L2_ACTIVE,    \
+		                                     1);                            \
+	}while(0)
+
+/* Trace kbase_pm_send_event message send */
+#define KBASE_TIMELINE_PM_SEND_EVENT(kbdev, event_type, pm_event_id) \
+	do                                                               \
+	{                                                                \
+		struct timespec ts;                                          \
+		getnstimeofday(&ts);                                         \
+		trace_mali_timeline_pm_event(ts.tv_sec, ts.tv_nsec,          \
+		                             SW_FLOW_PM_SEND_EVENT,          \
+		                             event_type, pm_event_id);       \
+	} while (0)
+
+/* Trace kbase_pm_worker message receive */
+#define KBASE_TIMELINE_PM_HANDLE_EVENT(kbdev, event_type, pm_event_id) \
+	do                                                                 \
+	{                                                                  \
+		struct timespec ts;                                            \
+		getnstimeofday(&ts);                                           \
+		trace_mali_timeline_pm_event(ts.tv_sec, ts.tv_nsec,            \
+		                             SW_FLOW_PM_HANDLE_EVENT,          \
+		                             event_type, pm_event_id);        \
+	} while (0)
+
+
+/* Trace atom_id starting in JSn_HEAD */
+#define KBASE_TIMELINE_JOB_START(kctx, js, _consumerof_atom_number)     \
+	do                                                                  \
+	{                                                                   \
+		struct timespec ts;                                             \
+		getnstimeofday(&ts);                                            \
+		trace_mali_timeline_slot_atom(ts.tv_sec, ts.tv_nsec,            \
+		                              HW_START_GPU_JOB_CHAIN_SW_APPROX, \
+		                              (int)kctx->timeline.owner_tgid,   \
+		                              js, _consumerof_atom_number);     \
+	} while (0)
+
+/* Trace atom_id stopping on JSn_HEAD */
+#define KBASE_TIMELINE_JOB_STOP(kctx, js, _producerof_atom_number_completed) \
+	do                                                                  \
+	{                                                                   \
+		struct timespec ts;                                             \
+		getnstimeofday(&ts);                                            \
+		trace_mali_timeline_slot_atom(ts.tv_sec, ts.tv_nsec,            \
+		                              HW_STOP_GPU_JOB_CHAIN_SW_APPROX,  \
+		                              (int)kctx->timeline.owner_tgid,   \
+		                              js, _producerof_atom_number_completed);     \
+	} while (0)
+
+/** Trace beginning/end of a call to kbase_pm_check_transitions_nolock from a
+ * certin caller */
+#define KBASE_TIMELINE_PM_CHECKTRANS(kbdev, trace_code) \
+	do                                                                  \
+	{                                                                   \
+		struct timespec ts;                                             \
+		getnstimeofday(&ts);                                            \
+		trace_mali_timeline_pm_checktrans(ts.tv_sec, ts.tv_nsec,        \
+		                                  trace_code, \
+		                                  1);     \
+	} while (0)
+
+/* NOTE: kbase_timeline_pm_cores_func() is in mali_kbase_pm_policy.c */
+
+/**
+ * Trace that an atom is starting on a job slot
+ *
+ * The caller must be holding kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_timeline_job_slot_submit(kbase_device *kbdev, kbase_context *kctx,
+                                    kbase_jd_atom *katom, int js);
+
+/**
+ * Trace that an atom has done on a job slot
+ *
+ * 'Done' in this sense can occur either because:
+ * - the atom in JSn_HEAD finished
+ * - the atom in JSn_NEXT was evicted
+ *
+ * Whether the atom finished or was evicted is passed in @a done_code
+ *
+ * It is assumed that the atom has already been removed from the submit slot,
+ * with either:
+ * - kbasep_jm_dequeue_submit_slot()
+ * - kbasep_jm_dequeue_tail_submit_slot()
+ *
+ * The caller must be holding kbasep_js_device_data::runpool_irq::lock
+ */
+void kbase_timeline_job_slot_done(kbase_device *kbdev, kbase_context *kctx,
+                                  kbase_jd_atom *katom, int js,
+                                  kbasep_js_atom_done_code done_code);
+
+
+/** Trace a pm event starting */
+void kbase_timeline_pm_send_event(kbase_device *kbdev,
+                                  kbase_timeline_pm_event event_sent);
+
+/** Trace a pm event finishing */
+void kbase_timeline_pm_check_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event);
+
+/** Check whether a pm event was present, and if so trace finishing it */
+void kbase_timeline_pm_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event);
+
+/** Trace L2 power-up start */
+void kbase_timeline_pm_l2_transition_start(kbase_device *kbdev);
+
+/** Trace L2 power-up done */
+void kbase_timeline_pm_l2_transition_done(kbase_device *kbdev);
+
+#else
+
+#define KBASE_TIMELINE_ATOMS_IN_FLIGHT(kctx, count) CSTD_NOP()
+
+#define KBASE_TIMELINE_ATOM_READY(kctx, atom_id) CSTD_NOP()
+
+#define KBASE_TIMELINE_ATOMS_SUBMITTED(kctx, js, count) CSTD_NOP()
+
+#define KBASE_TIMELINE_JOB_START_NEXT(kctx, js, count) CSTD_NOP()
+
+#define KBASE_TIMELINE_JOB_START_HEAD(kctx, js, count) CSTD_NOP()
+
+#define KBASE_TIMELINE_TRY_SOFT_STOP(kctx, js, count) CSTD_NOP()
+
+#define KBASE_TIMELINE_GPU_POWER(kbdev, active) CSTD_NOP()
+
+#define KBASE_TIMELINE_POWER_TILER(kbdev, bitmap) CSTD_NOP()
+
+#define KBASE_TIMELINE_POWER_SHADER(kbdev, bitmap) CSTD_NOP()
+
+#define KBASE_TIMELINE_POWER_L2(kbdev, active) CSTD_NOP()
+
+#define KBASE_TIMELINE_POWERING_L2(kbdev) CSTD_NOP()
+
+#define KBASE_TIMELINE_POWERED_L2(kbdev)  CSTD_NOP()
+
+#define KBASE_TIMELINE_PM_SEND_EVENT(kbdev, event_type, pm_event_id) CSTD_NOP()
+
+#define KBASE_TIMELINE_PM_HANDLE_EVENT(kbdev, event_type, pm_event_id) CSTD_NOP()
+
+#define KBASE_TIMELINE_JOB_START(kctx, js, _consumerof_atom_number) CSTD_NOP()
+
+#define KBASE_TIMELINE_JOB_STOP(kctx, js, _producerof_atom_number_completed) CSTD_NOP()
+
+#define KBASE_TIMELINE_PM_CHECKTRANS(kbdev, trace_code) CSTD_NOP()
+
+static INLINE void kbase_timeline_job_slot_submit(kbase_device *kbdev, kbase_context *kctx,
+                                    kbase_jd_atom *katom, int js)
+{
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+}
+
+static INLINE void kbase_timeline_job_slot_done(kbase_device *kbdev, kbase_context *kctx,
+                                    kbase_jd_atom *katom, int js,
+                                    kbasep_js_atom_done_code done_code)
+{
+	lockdep_assert_held(&kbdev->js_data.runpool_irq.lock);
+}
+
+static INLINE void kbase_timeline_pm_send_event(kbase_device *kbdev, kbase_timeline_pm_event event_sent)
+{
+}
+
+static INLINE void kbase_timeline_pm_check_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event)
+{
+}
+
+static INLINE void kbase_timeline_pm_handle_event(kbase_device *kbdev, kbase_timeline_pm_event event)
+{
+}
+
+static INLINE void kbase_timeline_pm_l2_transition_start(kbase_device *kbdev)
+{
+
+}
+
+static INLINE void kbase_timeline_pm_l2_transition_done(kbase_device *kbdev)
+{
+
+}
+#endif				/* CONFIG_MALI_TRACE_TIMELINE */
+
+#endif				/* _KBASE_TRACE_TIMELINE_H */
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline_defs.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline_defs.h
new file mode 100644
index 0000000..2795c0b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_trace_timeline_defs.h
@@ -0,0 +1,132 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/* ***** IMPORTANT: THIS IS NOT A NORMAL HEADER FILE         *****
+ * *****            DO NOT INCLUDE DIRECTLY                  *****
+ * *****            THE LACK OF HEADER GUARDS IS INTENTIONAL ***** */
+
+/*
+ * Conventions on Event Names:
+ *
+ * - The prefix determines something about how the timeline should be
+ *   displayed, and is split up into various parts, separated by underscores:
+ *  - 'SW' and 'HW' as the first part will be used to determine whether a
+ *     timeline is to do with Software or Hardware - effectively, separate
+ *     'channels' for Software and Hardware
+ *  - 'START', 'STOP', 'ENTER', 'LEAVE' can be used in the second part, and
+ *    signify related pairs of events - these are optional.
+ *  - 'FLOW' indicates a generic event, which can use dependencies
+ * - This gives events such as:
+ *  - 'SW_ENTER_FOO'
+ *  - 'SW_LEAVE_FOO'
+ *  - 'SW_FLOW_BAR_1'
+ *  - 'SW_FLOW_BAR_2'
+ *  - 'HW_START_BAZ'
+ *  - 'HW_STOP_BAZ'
+ * - And an unadorned HW event:
+ *  - 'HW_BAZ_FROZBOZ'
+ */
+
+/*
+ * Conventions on parameter names:
+ * - anything with 'instance' in the name will have a separate timeline based
+ *   on that instances.
+ * - underscored-prefixed parameters will by hidden by default on timelines
+ *
+ * Hence:
+ * - Different job slots have their own 'instance', based on the instance value
+ * - Per-context info (e.g. atoms on a context) have their own 'instance'
+ *   (i.e. each context should be on a different timeline)
+ *
+ * Note that globally-shared resources can be tagged with a tgid, but we don't
+ * want an instance per context:
+ * - There's no point having separate Job Slot timelines for each context, that
+ *   would be confusing - there's only really 3 job slots!
+ * - There's no point having separate Shader-powered timelines for each
+ *   context, that would be confusing - all shader cores (whether it be 4, 8,
+ *   etc) are shared in the system.
+ */
+
+	/*
+	 * CTX events
+	 */
+	/* Separate timelines for each context 'instance'*/
+	KBASE_TIMELINE_TRACE_CODE(CTX_SET_NR_ATOMS_IN_FLIGHT,     "CTX: Atoms in flight",            "%d,%d",    "_instance_tgid,_value_number_of_atoms"),
+	KBASE_TIMELINE_TRACE_CODE(CTX_FLOW_ATOM_READY,            "CTX: Atoms Ready to Run",         "%d,%d,%d", "_instance_tgid,_consumerof_atom_number,_producerof_atom_number_ready"),
+
+	/*
+	 * SW Events
+	 */
+	/* Separate timelines for each slot 'instance' */
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_SLOT_ACTIVE,         "SW: GPU slot active",             "%d,%d,%d", "_tgid,_instance_slot,_value_number_of_atoms"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_SLOT_NEXT,           "SW: GPU atom in NEXT",            "%d,%d,%d", "_tgid,_instance_slot,_value_is_an_atom_in_next"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_SLOT_HEAD,           "SW: GPU atom in HEAD",            "%d,%d,%d", "_tgid,_instance_slot,_value_is_an_atom_in_head"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_SLOT_STOPPING,       "SW: Try Soft-Stop on GPU slot",   "%d,%d,%d", "_tgid,_instance_slot,_value_is_slot_stopping"),
+	/* Shader and overall power is shared - can't have separate instances of
+	 * it, just tagging with the context */
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_POWER_ACTIVE,        "SW: GPU power active",            "%d,%d",    "_tgid,_value_is_power_active"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_POWER_TILER_ACTIVE,  "SW: GPU tiler powered",           "%d,%d",    "_tgid,_value_number_of_tilers"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_POWER_SHADER_ACTIVE, "SW: GPU shaders powered",         "%d,%d",    "_tgid,_value_number_of_shaders"),
+	KBASE_TIMELINE_TRACE_CODE(SW_SET_GPU_POWER_L2_ACTIVE,     "SW: GPU L2 powered",              "%d,%d",    "_tgid,_value_number_of_l2"),
+
+	/* SW Power event messaging. _event_type is one from the kbase_pm_event enum  */
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_SEND_EVENT,          "SW: PM Send Event",               "%d,%d,%d", "_tgid,_event_type,_writerof_pm_event_id"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_HANDLE_EVENT,        "SW: PM Handle Event",             "%d,%d,%d", "_tgid,_event_type,_finalconsumerof_pm_event_id"),
+	/* SW L2 power events */
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_GPU_POWER_L2_POWERING,  "SW: GPU L2 powering",             "%d,%d", "_tgid,_writerof_l2_transitioning"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_GPU_POWER_L2_ACTIVE,	  "SW: GPU L2 powering done",        "%d,%d", "_tgid,_finalconsumerof_l2_transitioning"),
+
+	/*
+	 * BEGIN: Significant SW Functions that call kbase_pm_check_transitions_nolock()
+	 */
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_DO_POWEROFF_START, "SW: PM CheckTrans from kbase_pm_do_poweroff", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_do_poweroff"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_DO_POWEROFF_END,   "SW: PM CheckTrans from kbase_pm_do_poweroff", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_do_poweroff"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_DO_POWERON_START, "SW: PM CheckTrans from kbase_pm_do_poweron", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_do_poweron"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_DO_POWERON_END,   "SW: PM CheckTrans from kbase_pm_do_poweron", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_do_poweron"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_GPU_INTERRUPT_START, "SW: PM CheckTrans from kbase_gpu_interrupt", "%d,%d", "_tgid,_writerof_pm_checktrans_gpu_interrupt"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_GPU_INTERRUPT_END,   "SW: PM CheckTrans from kbase_gpu_interrupt", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_gpu_interrupt"),
+
+	/*
+	 * Significant Indirect callers of kbase_pm_check_transitions_nolock()
+	 */
+	/* kbase_pm_request_cores */
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_START, "SW: PM CheckTrans from kbase_pm_request_cores(shader)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_request_cores_shader"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_END,   "SW: PM CheckTrans from kbase_pm_request_cores(shader)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_request_cores_shader"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_TILER_START, "SW: PM CheckTrans from kbase_pm_request_cores(tiler)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_request_cores_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_TILER_END,   "SW: PM CheckTrans from kbase_pm_request_cores(tiler)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_request_cores_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_TILER_START, "SW: PM CheckTrans from kbase_pm_request_cores(shader+tiler)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_request_cores_shader_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_REQUEST_CORES_SHADER_TILER_END,   "SW: PM CheckTrans from kbase_pm_request_cores(shader+tiler)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_request_cores_shader_tiler"),
+	/* kbase_pm_release_cores */
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_START, "SW: PM CheckTrans from kbase_pm_release_cores(shader)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_release_cores_shader"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_END,   "SW: PM CheckTrans from kbase_pm_release_cores(shader)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_release_cores_shader"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_TILER_START, "SW: PM CheckTrans from kbase_pm_release_cores(tiler)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_release_cores_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_TILER_END,   "SW: PM CheckTrans from kbase_pm_release_cores(tiler)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_release_cores_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_TILER_START, "SW: PM CheckTrans from kbase_pm_release_cores(shader+tiler)", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_release_cores_shader_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_SHADER_TILER_END,   "SW: PM CheckTrans from kbase_pm_release_cores(shader+tiler)", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_release_cores_shader_tiler"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_DEFERRED_START, "SW: PM CheckTrans from kbasep_pm_do_shader_poweroff_callback", "%d,%d", "_tgid,_writerof_pm_checktrans_pm_do_shader_poweroff_callback"),
+	KBASE_TIMELINE_TRACE_CODE(SW_FLOW_PM_CHECKTRANS_PM_RELEASE_CORES_DEFERRED_END,   "SW: PM CheckTrans from kbasep_pm_do_shader_poweroff_callback", "%d,%d", "_tgid,_finalconsumerof_pm_checktrans_pm_do_shader_poweroff_callback"),
+	/*
+	 * END: SW Functions that call kbase_pm_check_transitions_nolock()
+	 */
+
+	/*
+	 * HW Events
+	 */
+	KBASE_TIMELINE_TRACE_CODE(HW_START_GPU_JOB_CHAIN_SW_APPROX,     "HW: Job Chain start (SW approximated)", "%d,%d,%d", "_tgid,job_slot,_consumerof_atom_number_ready"),
+	KBASE_TIMELINE_TRACE_CODE(HW_STOP_GPU_JOB_CHAIN_SW_APPROX,      "HW: Job Chain stop (SW approximated)",  "%d,%d,%d", "_tgid,job_slot,_producerof_atom_number_completed")
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_uku.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_uku.h
new file mode 100644
index 0000000..e15a598
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_uku.h
@@ -0,0 +1,336 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_UKU_H_
+#define _KBASE_UKU_H_
+
+#include "mali_uk.h"
+#include <malisw/mali_malisw.h>
+#include "mali_base_kernel.h"
+
+/* This file needs to support being included from kernel and userside (which use different defines) */
+#if defined(CONFIG_MALI_ERROR_INJECT)
+#define SUPPORT_MALI_ERROR_INJECT
+#elif defined(MALI_ERROR_INJECT)
+#if MALI_ERROR_INJECT
+#define SUPPORT_MALI_ERROR_INJECT
+#endif
+#endif
+#if defined(CONFIG_MALI_NO_MALI)
+#define SUPPORT_MALI_NO_MALI
+#elif defined(MALI_NO_MALI)
+#if MALI_NO_MALI
+#define SUPPORT_MALI_NO_MALI
+#endif
+#endif
+
+#if defined(SUPPORT_MALI_NO_MALI) || defined(SUPPORT_MALI_ERROR_INJECT)
+#include "mali_kbase_model_dummy.h"
+#endif
+
+#include "mali_kbase_gpuprops_types.h"
+
+#define BASE_UK_VERSION_MAJOR 7
+#define BASE_UK_VERSION_MINOR 0
+
+typedef struct kbase_uk_mem_alloc {
+	uk_header header;
+	/* IN */
+	u64 va_pages;
+	u64 commit_pages;
+	u64 extent;
+	/* IN/OUT */
+	u64 flags;
+	/* OUT */
+	u64 gpu_va;
+	u16 va_alignment;
+	u8  padding[6];
+} kbase_uk_mem_alloc;
+
+typedef struct kbase_uk_mem_free {
+	uk_header header;
+	/* IN */
+	mali_addr64 gpu_addr;
+	/* OUT */
+} kbase_uk_mem_free;
+
+/* used by both aliasing and importing */
+#define KBASE_MEM_NEED_MMAP         (1UL << BASE_MEM_FLAGS_NR_BITS)
+
+typedef struct kbase_uk_mem_alias {
+	uk_header header;
+	/* IN/OUT */
+	u64 flags;
+	/* IN */
+	u64 stride;
+	u64 nents;
+	kbase_pointer ai;
+	/* OUT */
+	u64         gpu_va;
+	u64         va_pages;
+} kbase_uk_mem_alias;
+
+typedef struct kbase_uk_mem_import {
+	uk_header header;
+	/* IN */
+	kbase_pointer phandle;
+	u32 type;
+	u32 padding;
+	/* IN/OUT */
+#define KBASE_MEM_IMPORT_HAVE_PAGES   (1UL << (BASE_MEM_FLAGS_NR_BITS + 1))
+	u64         flags;
+	/* OUT */
+	mali_addr64 gpu_va;
+	u64         va_pages;
+} kbase_uk_mem_import;
+
+typedef struct kbase_uk_mem_flags_change {
+	uk_header header;
+	/* IN */
+	mali_addr64 gpu_va;
+	u64 flags;
+	u64 mask;
+} kbase_uk_mem_flags_change;
+
+typedef struct kbase_uk_job_submit {
+	uk_header header;
+	/* IN */
+	kbase_pointer addr;
+	u32 nr_atoms;
+	u32 stride;		/* bytes between atoms, i.e. sizeof(base_jd_atom_v2) */
+	/* OUT */
+} kbase_uk_job_submit;
+
+typedef struct kbase_uk_post_term {
+	uk_header header;
+} kbase_uk_post_term;
+
+typedef struct kbase_uk_sync_now {
+	uk_header header;
+
+	/* IN */
+	base_syncset sset;
+
+	/* OUT */
+} kbase_uk_sync_now;
+
+typedef struct kbase_uk_hwcnt_setup {
+	uk_header header;
+
+	/* IN */
+	mali_addr64 dump_buffer;
+	u32 jm_bm;
+	u32 shader_bm;
+	u32 tiler_bm;
+	u32 l3_cache_bm;
+	u32 mmu_l2_bm;
+	u32 padding;
+	/* OUT */
+} kbase_uk_hwcnt_setup;
+
+typedef struct kbase_uk_hwcnt_dump {
+	uk_header header;
+} kbase_uk_hwcnt_dump;
+
+typedef struct kbase_uk_hwcnt_clear {
+	uk_header header;
+} kbase_uk_hwcnt_clear;
+
+typedef struct kbase_uk_fence_validate {
+	uk_header header;
+	/* IN */
+	s32 fd;
+	u32 padding;
+	/* OUT */
+} kbase_uk_fence_validate;
+
+typedef struct kbase_uk_stream_create {
+	uk_header header;
+	/* IN */
+	char name[32];
+	/* OUT */
+	s32 fd;
+	u32 padding;
+} kbase_uk_stream_create;
+
+typedef struct kbase_uk_cpuprops {
+	uk_header header;
+
+	/* IN */
+	struct base_cpu_props props;
+	/* OUT */
+} kbase_uk_cpuprops;
+
+typedef struct kbase_uk_gpuprops {
+	uk_header header;
+
+	/* IN */
+	struct mali_base_gpu_props props;
+	/* OUT */
+} kbase_uk_gpuprops;
+
+typedef struct kbase_uk_mem_query {
+	uk_header header;
+	/* IN */
+	mali_addr64 gpu_addr;
+#define KBASE_MEM_QUERY_COMMIT_SIZE  1
+#define KBASE_MEM_QUERY_VA_SIZE      2
+#define KBASE_MEM_QUERY_FLAGS        3
+	u64         query;
+	/* OUT */
+	u64         value;
+} kbase_uk_mem_query;
+	
+typedef struct kbase_uk_mem_commit {
+	uk_header header;
+	/* IN */
+	mali_addr64 gpu_addr;
+	u64         pages;
+	/* OUT */
+	u32 result_subcode;
+	u32 padding;
+} kbase_uk_mem_commit;
+
+typedef struct kbase_uk_find_cpu_offset {
+	uk_header header;
+	/* IN */
+	mali_addr64 gpu_addr;
+	u64 cpu_addr;
+	u64 size;
+	/* OUT */
+	mali_size64 offset;
+} kbase_uk_find_cpu_offset;
+
+#define KBASE_GET_VERSION_BUFFER_SIZE 64
+typedef struct kbase_uk_get_ddk_version {
+	uk_header header;
+	/* OUT */
+	char version_buffer[KBASE_GET_VERSION_BUFFER_SIZE];
+	u32 version_string_size;
+	u32 padding;
+} kbase_uk_get_ddk_version;
+
+typedef struct kbase_uk_set_flags {
+	uk_header header;
+	/* IN */
+	u32 create_flags;
+	u32 padding;
+} kbase_uk_set_flags;
+
+#if MALI_UNIT_TEST
+#define TEST_ADDR_COUNT 4
+#define KBASE_TEST_BUFFER_SIZE 128
+typedef struct kbase_exported_test_data {
+	mali_addr64 test_addr[TEST_ADDR_COUNT];		/**< memory address */
+	u32 test_addr_pages[TEST_ADDR_COUNT];		/**<  memory size in pages */
+	kbase_pointer kctx;				/**<  base context created by process */
+	kbase_pointer mm;				/**< pointer to process address space */
+	u8 buffer1[KBASE_TEST_BUFFER_SIZE];   /**<  unit test defined parameter */
+	u8 buffer2[KBASE_TEST_BUFFER_SIZE];   /**<  unit test defined parameter */
+} kbase_exported_test_data;
+
+typedef struct kbase_uk_set_test_data {
+	uk_header header;
+	/* IN */
+	kbase_exported_test_data test_data;
+} kbase_uk_set_test_data;
+
+#endif				/* MALI_UNIT_TEST */
+
+#ifdef SUPPORT_MALI_ERROR_INJECT
+typedef struct kbase_uk_error_params {
+	uk_header header;
+	/* IN */
+	kbase_error_params params;
+} kbase_uk_error_params;
+#endif				/* SUPPORT_MALI_ERROR_INJECT */
+
+#ifdef SUPPORT_MALI_NO_MALI
+typedef struct kbase_uk_model_control_params {
+	uk_header header;
+	/* IN */
+	kbase_model_control_params params;
+} kbase_uk_model_control_params;
+#endif				/* SUPPORT_MALI_NO_MALI */
+
+#define KBASE_MAXIMUM_EXT_RESOURCES       255
+
+typedef struct kbase_uk_ext_buff_kds_data {
+	uk_header header;
+	kbase_pointer external_resource;
+	kbase_pointer file_descriptor;
+	u32 num_res;		/* limited to KBASE_MAXIMUM_EXT_RESOURCES */
+	u32 padding;
+} kbase_uk_ext_buff_kds_data;
+
+typedef struct kbase_uk_keep_gpu_powered {
+	uk_header header;
+	u32       enabled;
+	u32       padding;
+} kbase_uk_keep_gpu_powered;
+
+typedef struct kbase_uk_profiling_controls {
+	uk_header header;
+	u32 profiling_controls[FBDUMP_CONTROL_MAX];
+} kbase_uk_profiling_controls;
+
+typedef enum kbase_uk_function_id {
+	KBASE_FUNC_MEM_ALLOC = (UK_FUNC_ID + 0),
+	KBASE_FUNC_MEM_IMPORT,
+	KBASE_FUNC_MEM_COMMIT,
+	KBASE_FUNC_MEM_QUERY,
+	KBASE_FUNC_MEM_FREE,
+	KBASE_FUNC_MEM_FLAGS_CHANGE,
+	KBASE_FUNC_MEM_ALIAS,
+
+	KBASE_FUNC_SYNC  = (UK_FUNC_ID + 8),
+
+	KBASE_FUNC_POST_TERM,
+
+	KBASE_FUNC_HWCNT_SETUP,
+	KBASE_FUNC_HWCNT_DUMP,
+	KBASE_FUNC_HWCNT_CLEAR,
+
+	KBASE_FUNC_CPU_PROPS_REG_DUMP,
+	KBASE_FUNC_GPU_PROPS_REG_DUMP,
+
+	KBASE_FUNC_FIND_CPU_OFFSET,
+
+	KBASE_FUNC_GET_VERSION,
+	KBASE_FUNC_EXT_BUFFER_LOCK,
+	KBASE_FUNC_SET_FLAGS,
+
+	KBASE_FUNC_SET_TEST_DATA,
+	KBASE_FUNC_INJECT_ERROR,
+	KBASE_FUNC_MODEL_CONTROL,
+
+	KBASE_FUNC_KEEP_GPU_POWERED,
+
+	KBASE_FUNC_FENCE_VALIDATE,
+	KBASE_FUNC_STREAM_CREATE,
+	KBASE_FUNC_GET_PROFILING_CONTROLS,
+	KBASE_FUNC_SET_PROFILING_CONTROLS, /* to be used only for testing
+					   * purposes, otherwise these controls
+					   * are set through gator API */
+	KBASE_FUNC_JOB_SUBMIT = (UK_FUNC_ID + 27)
+
+} kbase_uk_function_id;
+
+
+#endif				/* _KBASE_UKU_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.c b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.c
new file mode 100644
index 0000000..c11c678
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.c
@@ -0,0 +1,32 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <mali_kbase.h>
+
+mali_bool kbasep_list_member_of(const struct list_head *base, struct list_head *entry)
+{
+	struct list_head *pos = base->next;
+	while (pos != base) {
+		if (pos == entry)
+			return MALI_TRUE;
+
+		pos = pos->next;
+	}
+	return MALI_FALSE;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.h b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.h
new file mode 100644
index 0000000..5998816
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_kbase_utility.h
@@ -0,0 +1,37 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_UTILITY_H
+#define _KBASE_UTILITY_H
+
+#ifndef _KBASE_H_
+#error "Don't include this file directly, use mali_kbase.h instead"
+#endif
+
+/** Test whether the given list entry is a member of the given list.
+ *
+ * @param base      The head of the list to be tested
+ * @param entry     The list entry to be tested
+ *
+ * @return          MALI_TRUE if entry is a member of base
+ *                  MALI_FALSE otherwise
+ */
+mali_bool kbasep_list_member_of(const struct list_head *base, struct list_head *entry);
+
+#endif				/* _KBASE_UTILITY_H */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_linux_trace.h b/drivers/gpu/mali-t6xx/r4p1/mali_linux_trace.h
new file mode 100644
index 0000000..537610b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_linux_trace.h
@@ -0,0 +1,129 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#if !defined(_TRACE_MALI_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _TRACE_MALI_H
+
+#include <linux/stringify.h>
+#include <linux/tracepoint.h>
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM mali
+#define TRACE_SYSTEM_STRING __stringify(TRACE_SYSTEM)
+#define TRACE_INCLUDE_FILE mali_linux_trace
+
+#define MALI_JOB_SLOTS_EVENT_CHANGED
+
+/**
+ * mali_job_slots_event - called from mali_kbase_core_linux.c
+ * @event_id: ORed together bitfields representing a type of event, made with the GATOR_MAKE_EVENT() macro.
+ */
+TRACE_EVENT(mali_job_slots_event, TP_PROTO(unsigned int event_id, unsigned int tgid, unsigned int pid, unsigned char job_id), TP_ARGS(event_id, tgid, pid, job_id), TP_STRUCT__entry(__field(unsigned int, event_id)
+																		       __field(unsigned int, tgid)
+																		       __field(unsigned int, pid)
+																		       __field(unsigned char, job_id)
+	    ), TP_fast_assign(__entry->event_id = event_id; __entry->tgid = tgid; __entry->pid = pid; __entry->job_id = job_id;), TP_printk("event=%u tgid=%u pid=%u job_id=%u", __entry->event_id, __entry->tgid, __entry->pid, __entry->job_id)
+	);
+
+/**
+ * mali_pm_status - Called by mali_kbase_pm_driver.c
+ * @event_id: core type (shader, tiler, l2 cache, l3 cache)
+ * @value: 64bits bitmask reporting either power status of the cores (1-ON, 0-OFF)
+ */
+TRACE_EVENT(mali_pm_status, TP_PROTO(unsigned int event_id, unsigned long long value), TP_ARGS(event_id, value), TP_STRUCT__entry(__field(unsigned int, event_id)
+																  __field(unsigned long long, value)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event %u = %llu", __entry->event_id, __entry->value)
+	);
+
+/**
+ * mali_pm_power_on - Called by mali_kbase_pm_driver.c
+ * @event_id: core type (shader, tiler, l2 cache, l3 cache)
+ * @value: 64bits bitmask reporting the cores to power up
+ */
+TRACE_EVENT(mali_pm_power_on, TP_PROTO(unsigned int event_id, unsigned long long value), TP_ARGS(event_id, value), TP_STRUCT__entry(__field(unsigned int, event_id)
+																    __field(unsigned long long, value)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event %u = %llu", __entry->event_id, __entry->value)
+	);
+
+/**
+ * mali_pm_power_off - Called by mali_kbase_pm_driver.c
+ * @event_id: core type (shader, tiler, l2 cache, l3 cache)
+ * @value: 64bits bitmask reporting the cores to power down
+ */
+TRACE_EVENT(mali_pm_power_off, TP_PROTO(unsigned int event_id, unsigned long long value), TP_ARGS(event_id, value), TP_STRUCT__entry(__field(unsigned int, event_id)
+																     __field(unsigned long long, value)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event %u = %llu", __entry->event_id, __entry->value)
+	);
+
+/**
+ * mali_page_fault_insert_pages - Called by page_fault_worker()
+ * it reports an MMU page fault resulting in new pages being mapped.
+ * @event_id: MMU address space number.
+ * @value: number of newly allocated pages
+ */
+TRACE_EVENT(mali_page_fault_insert_pages, TP_PROTO(int event_id, unsigned long value), TP_ARGS(event_id, value), TP_STRUCT__entry(__field(int, event_id)
+																  __field(unsigned long, value)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event %d = %lu", __entry->event_id, __entry->value)
+	);
+
+/**
+ * mali_mmu_as_in_use - Called by assign_and_activate_kctx_addr_space()
+ * it reports that a certain MMU address space is in use now.
+ * @event_id: MMU address space number.
+ */
+TRACE_EVENT(mali_mmu_as_in_use, TP_PROTO(int event_id), TP_ARGS(event_id), TP_STRUCT__entry(__field(int, event_id)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event=%d", __entry->event_id)
+	);
+
+/**
+ * mali_mmu_as_released - Called by kbasep_js_runpool_release_ctx_internal()
+ * it reports that a certain MMU address space has been released now.
+ * @event_id: MMU address space number.
+ */
+TRACE_EVENT(mali_mmu_as_released, TP_PROTO(int event_id), TP_ARGS(event_id), TP_STRUCT__entry(__field(int, event_id)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event=%d", __entry->event_id)
+	);
+
+/**
+ * mali_total_alloc_pages_change - Called by kbase_atomic_add_pages()
+ *                                 and by kbase_atomic_sub_pages()
+ * it reports that the total number of allocated pages is changed.
+ * @event_id: number of pages to be added or subtracted (according to the sign).
+ */
+TRACE_EVENT(mali_total_alloc_pages_change, TP_PROTO(long long int event_id), TP_ARGS(event_id), TP_STRUCT__entry(__field(long long int, event_id)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event=%lld", __entry->event_id)
+	);
+
+/**
+ * mali_sw_counter - not currently used
+ * @event_id: counter id
+ */
+TRACE_EVENT(mali_sw_counter, TP_PROTO(unsigned int event_id, signed long long value), TP_ARGS(event_id, value), TP_STRUCT__entry(__field(int, event_id)
+																 __field(long long, value)
+	    ), TP_fast_assign(__entry->event_id = event_id;), TP_printk("event %d = %lld", __entry->event_id, __entry->value)
+	);
+
+#endif				/*  _TRACE_MALI_H */
+
+#undef TRACE_INCLUDE_PATH
+#undef linux
+#define TRACE_INCLUDE_PATH .
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_midg_regmap.h b/drivers/gpu/mali-t6xx/r4p1/mali_midg_regmap.h
new file mode 100644
index 0000000..d3848ad
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_midg_regmap.h
@@ -0,0 +1,512 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _MIDGARD_REGMAP_H_
+#define _MIDGARD_REGMAP_H_
+
+/*
+ * Begin Register Offsets
+ */
+
+#define GPU_CONTROL_BASE        0x0000
+#define GPU_CONTROL_REG(r)      (GPU_CONTROL_BASE + (r))
+#define GPU_ID                  0x000	/* (RO) GPU and revision identifier */
+#define L2_FEATURES             0x004	/* (RO) Level 2 cache features */
+#define L3_FEATURES             0x008	/* (RO) Level 3 cache features */
+#define TILER_FEATURES          0x00C	/* (RO) Tiler Features */
+#define MEM_FEATURES            0x010	/* (RO) Memory system features */
+#define MMU_FEATURES            0x014	/* (RO) MMU features */
+#define AS_PRESENT              0x018	/* (RO) Address space slots present */
+#define JS_PRESENT              0x01C	/* (RO) Job slots present */
+#define GPU_IRQ_RAWSTAT         0x020	/* (RW) */
+#define GPU_IRQ_CLEAR           0x024	/* (WO) */
+#define GPU_IRQ_MASK            0x028	/* (RW) */
+#define GPU_IRQ_STATUS          0x02C	/* (RO) */
+
+/* IRQ flags */
+#define GPU_FAULT               (1 << 0)	/* A GPU Fault has occurred */
+#define MULTIPLE_GPU_FAULTS     (1 << 7)	/* More than one GPU Fault occurred. */
+#define RESET_COMPLETED         (1 << 8)	/* Set when a reset has completed. Intended to use with SOFT_RESET
+						   commands which may take time. */
+#define POWER_CHANGED_SINGLE    (1 << 9)	/* Set when a single core has finished powering up or down. */
+#define POWER_CHANGED_ALL       (1 << 10)	/* Set when all cores have finished powering up or down
+						   and the power manager is idle. */
+
+#define PRFCNT_SAMPLE_COMPLETED (1 << 16)	/* Set when a performance count sample has completed. */
+#define CLEAN_CACHES_COMPLETED  (1 << 17)	/* Set when a cache clean operation has completed. */
+
+#define GPU_IRQ_REG_ALL (GPU_FAULT | MULTIPLE_GPU_FAULTS | RESET_COMPLETED \
+			| POWER_CHANGED_ALL | PRFCNT_SAMPLE_COMPLETED)
+
+#define GPU_COMMAND             0x030	/* (WO) */
+#define GPU_STATUS              0x034	/* (RO) */
+
+#define GROUPS_L2_COHERENT      (1 << 0)	/* Cores groups are l2 coherent */
+#define GROUPS_L3_COHERENT      (1 << 1)	/* Cores groups are l3 coherent */
+
+#define GPU_FAULTSTATUS         0x03C	/* (RO) GPU exception type and fault status */
+#define GPU_FAULTADDRESS_LO     0x040	/* (RO) GPU exception fault address, low word */
+#define GPU_FAULTADDRESS_HI     0x044	/* (RO) GPU exception fault address, high word */
+
+#define PWR_KEY                 0x050	/* (WO) Power manager key register */
+#define PWR_OVERRIDE0           0x054	/* (RW) Power manager override settings */
+#define PWR_OVERRIDE1           0x058	/* (RW) Power manager override settings */
+
+#define PRFCNT_BASE_LO          0x060	/* (RW) Performance counter memory region base address, low word */
+#define PRFCNT_BASE_HI          0x064	/* (RW) Performance counter memory region base address, high word */
+#define PRFCNT_CONFIG           0x068	/* (RW) Performance counter configuration */
+#define PRFCNT_JM_EN            0x06C	/* (RW) Performance counter enable flags for Job Manager */
+#define PRFCNT_SHADER_EN        0x070	/* (RW) Performance counter enable flags for shader cores */
+#define PRFCNT_TILER_EN         0x074	/* (RW) Performance counter enable flags for tiler */
+#define PRFCNT_L3_CACHE_EN      0x078	/* (RW) Performance counter enable flags for L3 cache */
+#define PRFCNT_MMU_L2_EN        0x07C	/* (RW) Performance counter enable flags for MMU/L2 cache */
+
+#define CYCLE_COUNT_LO          0x090	/* (RO) Cycle counter, low word */
+#define CYCLE_COUNT_HI          0x094	/* (RO) Cycle counter, high word */
+#define TIMESTAMP_LO            0x098	/* (RO) Global time stamp counter, low word */
+#define TIMESTAMP_HI            0x09C	/* (RO) Global time stamp counter, high word */
+
+#define THREAD_MAX_THREADS		0x0A0	/* (RO) Maximum number of threads per core */
+#define THREAD_MAX_WORKGROUP_SIZE 0x0A4	/* (RO) Maximum workgroup size */
+#define THREAD_MAX_BARRIER_SIZE 0x0A8	/* (RO) Maximum threads waiting at a barrier */
+#define THREAD_FEATURES         0x0AC	/* (RO) Thread features */
+
+#define TEXTURE_FEATURES_0      0x0B0	/* (RO) Support flags for indexed texture formats 0..31 */
+#define TEXTURE_FEATURES_1      0x0B4	/* (RO) Support flags for indexed texture formats 32..63 */
+#define TEXTURE_FEATURES_2      0x0B8	/* (RO) Support flags for indexed texture formats 64..95 */
+
+#define TEXTURE_FEATURES_REG(n) GPU_CONTROL_REG(TEXTURE_FEATURES_0 + ((n) << 2))
+
+#define JS0_FEATURES            0x0C0	/* (RO) Features of job slot 0 */
+#define JS1_FEATURES            0x0C4	/* (RO) Features of job slot 1 */
+#define JS2_FEATURES            0x0C8	/* (RO) Features of job slot 2 */
+#define JS3_FEATURES            0x0CC	/* (RO) Features of job slot 3 */
+#define JS4_FEATURES            0x0D0	/* (RO) Features of job slot 4 */
+#define JS5_FEATURES            0x0D4	/* (RO) Features of job slot 5 */
+#define JS6_FEATURES            0x0D8	/* (RO) Features of job slot 6 */
+#define JS7_FEATURES            0x0DC	/* (RO) Features of job slot 7 */
+#define JS8_FEATURES            0x0E0	/* (RO) Features of job slot 8 */
+#define JS9_FEATURES            0x0E4	/* (RO) Features of job slot 9 */
+#define JS10_FEATURES           0x0E8	/* (RO) Features of job slot 10 */
+#define JS11_FEATURES           0x0EC	/* (RO) Features of job slot 11 */
+#define JS12_FEATURES           0x0F0	/* (RO) Features of job slot 12 */
+#define JS13_FEATURES           0x0F4	/* (RO) Features of job slot 13 */
+#define JS14_FEATURES           0x0F8	/* (RO) Features of job slot 14 */
+#define JS15_FEATURES           0x0FC	/* (RO) Features of job slot 15 */
+
+#define JS_FEATURES_REG(n)      GPU_CONTROL_REG(JS0_FEATURES + ((n) << 2))
+
+#define SHADER_PRESENT_LO       0x100	/* (RO) Shader core present bitmap, low word */
+#define SHADER_PRESENT_HI       0x104	/* (RO) Shader core present bitmap, high word */
+
+#define TILER_PRESENT_LO        0x110	/* (RO) Tiler core present bitmap, low word */
+#define TILER_PRESENT_HI        0x114	/* (RO) Tiler core present bitmap, high word */
+
+#define L2_PRESENT_LO           0x120	/* (RO) Level 2 cache present bitmap, low word */
+#define L2_PRESENT_HI           0x124	/* (RO) Level 2 cache present bitmap, high word */
+
+#define L3_PRESENT_LO           0x130	/* (RO) Level 3 cache present bitmap, low word */
+#define L3_PRESENT_HI           0x134	/* (RO) Level 3 cache present bitmap, high word */
+
+#define SHADER_READY_LO         0x140	/* (RO) Shader core ready bitmap, low word */
+#define SHADER_READY_HI         0x144	/* (RO) Shader core ready bitmap, high word */
+
+#define TILER_READY_LO          0x150	/* (RO) Tiler core ready bitmap, low word */
+#define TILER_READY_HI          0x154	/* (RO) Tiler core ready bitmap, high word */
+
+#define L2_READY_LO             0x160	/* (RO) Level 2 cache ready bitmap, low word */
+#define L2_READY_HI             0x164	/* (RO) Level 2 cache ready bitmap, high word */
+
+#define L3_READY_LO             0x170	/* (RO) Level 3 cache ready bitmap, low word */
+#define L3_READY_HI             0x174	/* (RO) Level 3 cache ready bitmap, high word */
+
+#define SHADER_PWRON_LO         0x180	/* (WO) Shader core power on bitmap, low word */
+#define SHADER_PWRON_HI         0x184	/* (WO) Shader core power on bitmap, high word */
+
+#define TILER_PWRON_LO          0x190	/* (WO) Tiler core power on bitmap, low word */
+#define TILER_PWRON_HI          0x194	/* (WO) Tiler core power on bitmap, high word */
+
+#define L2_PWRON_LO             0x1A0	/* (WO) Level 2 cache power on bitmap, low word */
+#define L2_PWRON_HI             0x1A4	/* (WO) Level 2 cache power on bitmap, high word */
+
+#define L3_PWRON_LO             0x1B0	/* (WO) Level 3 cache power on bitmap, low word */
+#define L3_PWRON_HI             0x1B4	/* (WO) Level 3 cache power on bitmap, high word */
+
+#define SHADER_PWROFF_LO        0x1C0	/* (WO) Shader core power off bitmap, low word */
+#define SHADER_PWROFF_HI        0x1C4	/* (WO) Shader core power off bitmap, high word */
+
+#define TILER_PWROFF_LO         0x1D0	/* (WO) Tiler core power off bitmap, low word */
+#define TILER_PWROFF_HI         0x1D4	/* (WO) Tiler core power off bitmap, high word */
+
+#define L2_PWROFF_LO            0x1E0	/* (WO) Level 2 cache power off bitmap, low word */
+#define L2_PWROFF_HI            0x1E4	/* (WO) Level 2 cache power off bitmap, high word */
+
+#define L3_PWROFF_LO            0x1F0	/* (WO) Level 3 cache power off bitmap, low word */
+#define L3_PWROFF_HI            0x1F4	/* (WO) Level 3 cache power off bitmap, high word */
+
+#define SHADER_PWRTRANS_LO      0x200	/* (RO) Shader core power transition bitmap, low word */
+#define SHADER_PWRTRANS_HI      0x204	/* (RO) Shader core power transition bitmap, high word */
+
+#define TILER_PWRTRANS_LO       0x210	/* (RO) Tiler core power transition bitmap, low word */
+#define TILER_PWRTRANS_HI       0x214	/* (RO) Tiler core power transition bitmap, high word */
+
+#define L2_PWRTRANS_LO          0x220	/* (RO) Level 2 cache power transition bitmap, low word */
+#define L2_PWRTRANS_HI          0x224	/* (RO) Level 2 cache power transition bitmap, high word */
+
+#define L3_PWRTRANS_LO          0x230	/* (RO) Level 3 cache power transition bitmap, low word */
+#define L3_PWRTRANS_HI          0x234	/* (RO) Level 3 cache power transition bitmap, high word */
+
+#define SHADER_PWRACTIVE_LO     0x240	/* (RO) Shader core active bitmap, low word */
+#define SHADER_PWRACTIVE_HI     0x244	/* (RO) Shader core active bitmap, high word */
+
+#define TILER_PWRACTIVE_LO      0x250	/* (RO) Tiler core active bitmap, low word */
+#define TILER_PWRACTIVE_HI      0x254	/* (RO) Tiler core active bitmap, high word */
+
+#define L2_PWRACTIVE_LO         0x260	/* (RO) Level 2 cache active bitmap, low word */
+#define L2_PWRACTIVE_HI         0x264	/* (RO) Level 2 cache active bitmap, high word */
+
+#define L3_PWRACTIVE_LO         0x270	/* (RO) Level 3 cache active bitmap, low word */
+#define L3_PWRACTIVE_HI         0x274	/* (RO) Level 3 cache active bitmap, high word */
+
+#define SHADER_CONFIG           0xF04	/* (RW) Shader core configuration settings (Mali-T60x additional register) */
+#define L2_MMU_CONFIG           0xF0C	/* (RW) Configuration of the L2 cache and MMU (Mali-T60x additional register) */
+
+#define JOB_CONTROL_BASE        0x1000
+
+#define JOB_CONTROL_REG(r)      (JOB_CONTROL_BASE + (r))
+
+#define JOB_IRQ_RAWSTAT         0x000	/* Raw interrupt status register */
+#define JOB_IRQ_CLEAR           0x004	/* Interrupt clear register */
+#define JOB_IRQ_MASK            0x008	/* Interrupt mask register */
+#define JOB_IRQ_STATUS          0x00C	/* Interrupt status register */
+#define JOB_IRQ_JS_STATE        0x010	/* status==active and _next == busy snapshot from last JOB_IRQ_CLEAR */
+#define JOB_IRQ_THROTTLE        0x014	/* cycles to delay delivering an interrupt externally. The JOB_IRQ_STATUS is NOT affected by this, just the delivery of the interrupt.  */
+
+#define JOB_SLOT0               0x800	/* Configuration registers for job slot 0 */
+#define JOB_SLOT1               0x880	/* Configuration registers for job slot 1 */
+#define JOB_SLOT2               0x900	/* Configuration registers for job slot 2 */
+#define JOB_SLOT3               0x980	/* Configuration registers for job slot 3 */
+#define JOB_SLOT4               0xA00	/* Configuration registers for job slot 4 */
+#define JOB_SLOT5               0xA80	/* Configuration registers for job slot 5 */
+#define JOB_SLOT6               0xB00	/* Configuration registers for job slot 6 */
+#define JOB_SLOT7               0xB80	/* Configuration registers for job slot 7 */
+#define JOB_SLOT8               0xC00	/* Configuration registers for job slot 8 */
+#define JOB_SLOT9               0xC80	/* Configuration registers for job slot 9 */
+#define JOB_SLOT10              0xD00	/* Configuration registers for job slot 10 */
+#define JOB_SLOT11              0xD80	/* Configuration registers for job slot 11 */
+#define JOB_SLOT12              0xE00	/* Configuration registers for job slot 12 */
+#define JOB_SLOT13              0xE80	/* Configuration registers for job slot 13 */
+#define JOB_SLOT14              0xF00	/* Configuration registers for job slot 14 */
+#define JOB_SLOT15              0xF80	/* Configuration registers for job slot 15 */
+
+#define JOB_SLOT_REG(n, r)      (JOB_CONTROL_REG(JOB_SLOT0 + ((n) << 7)) + (r))
+
+#define JSn_HEAD_LO             0x00	/* (RO) Job queue head pointer for job slot n, low word */
+#define JSn_HEAD_HI             0x04	/* (RO) Job queue head pointer for job slot n, high word */
+#define JSn_TAIL_LO             0x08	/* (RO) Job queue tail pointer for job slot n, low word */
+#define JSn_TAIL_HI             0x0C	/* (RO) Job queue tail pointer for job slot n, high word */
+#define JSn_AFFINITY_LO         0x10	/* (RO) Core affinity mask for job slot n, low word */
+#define JSn_AFFINITY_HI         0x14	/* (RO) Core affinity mask for job slot n, high word */
+#define JSn_CONFIG              0x18	/* (RO) Configuration settings for job slot n */
+
+#define JSn_COMMAND             0x20	/* (WO) Command register for job slot n */
+#define JSn_STATUS              0x24	/* (RO) Status register for job slot n */
+
+#define JSn_HEAD_NEXT_LO        0x40	/* (RW) Next job queue head pointer for job slot n, low word */
+#define JSn_HEAD_NEXT_HI        0x44	/* (RW) Next job queue head pointer for job slot n, high word */
+
+#define JSn_AFFINITY_NEXT_LO    0x50	/* (RW) Next core affinity mask for job slot n, low word */
+#define JSn_AFFINITY_NEXT_HI    0x54	/* (RW) Next core affinity mask for job slot n, high word */
+#define JSn_CONFIG_NEXT         0x58	/* (RW) Next configuration settings for job slot n */
+
+#define JSn_COMMAND_NEXT        0x60	/* (RW) Next command register for job slot n */
+
+#define MEMORY_MANAGEMENT_BASE  0x2000
+#define MMU_REG(r)              (MEMORY_MANAGEMENT_BASE + (r))
+
+#define MMU_IRQ_RAWSTAT         0x000	/* (RW) Raw interrupt status register */
+#define MMU_IRQ_CLEAR           0x004	/* (WO) Interrupt clear register */
+#define MMU_IRQ_MASK            0x008	/* (RW) Interrupt mask register */
+#define MMU_IRQ_STATUS          0x00C	/* (RO) Interrupt status register */
+
+#define MMU_AS0                 0x400	/* Configuration registers for address space 0 */
+#define MMU_AS1                 0x440	/* Configuration registers for address space 1 */
+#define MMU_AS2                 0x480	/* Configuration registers for address space 2 */
+#define MMU_AS3                 0x4C0	/* Configuration registers for address space 3 */
+#define MMU_AS4                 0x500	/* Configuration registers for address space 4 */
+#define MMU_AS5                 0x540	/* Configuration registers for address space 5 */
+#define MMU_AS6                 0x580	/* Configuration registers for address space 6 */
+#define MMU_AS7                 0x5C0	/* Configuration registers for address space 7 */
+#define MMU_AS8                 0x600	/* Configuration registers for address space 8 */
+#define MMU_AS9                 0x640	/* Configuration registers for address space 9 */
+#define MMU_AS10                0x680	/* Configuration registers for address space 10 */
+#define MMU_AS11                0x6C0	/* Configuration registers for address space 11 */
+#define MMU_AS12                0x700	/* Configuration registers for address space 12 */
+#define MMU_AS13                0x740	/* Configuration registers for address space 13 */
+#define MMU_AS14                0x780	/* Configuration registers for address space 14 */
+#define MMU_AS15                0x7C0	/* Configuration registers for address space 15 */
+
+#define MMU_AS_REG(n, r)        (MMU_REG(MMU_AS0 + ((n) << 6)) + (r))
+
+#define ASn_TRANSTAB_LO         0x00	/* (RW) Translation Table Base Address for address space n, low word */
+#define ASn_TRANSTAB_HI         0x04	/* (RW) Translation Table Base Address for address space n, high word */
+#define ASn_MEMATTR_LO          0x08	/* (RW) Memory attributes for address space n, low word. */
+#define ASn_MEMATTR_HI          0x0C	/* (RW) Memory attributes for address space n, high word. */
+#define ASn_LOCKADDR_LO         0x10	/* (RW) Lock region address for address space n, low word */
+#define ASn_LOCKADDR_HI         0x14	/* (RW) Lock region address for address space n, high word */
+#define ASn_COMMAND             0x18	/* (WO) MMU command register for address space n */
+#define ASn_FAULTSTATUS         0x1C	/* (RO) MMU fault status register for address space n */
+#define ASn_FAULTADDRESS_LO     0x20	/* (RO) Fault Address for address space n, low word */
+#define ASn_FAULTADDRESS_HI     0x24	/* (RO) Fault Address for address space n, high word */
+#define ASn_STATUS              0x28	/* (RO) Status flags for address space n */
+
+/* End Register Offsets */
+
+/*
+ * MMU_IRQ_RAWSTAT register values. Values are valid also for
+   MMU_IRQ_CLEAR, MMU_IRQ_MASK, MMU_IRQ_STATUS registers.
+ */
+
+#define MMU_REGS_PAGE_FAULT_FLAGS   16
+
+/* Macros return bit number to retrvie page fault or bus eror flag from MMU registers */
+#define MMU_REGS_PAGE_FAULT_FLAG(n) (n)
+#define MMU_REGS_BUS_ERROR_FLAG(n)  (n + MMU_REGS_PAGE_FAULT_FLAGS)
+
+/*
+ * Begin MMU TRANSTAB register values
+ */
+#define ASn_TRANSTAB_ADDR_SPACE_MASK   0xfffff000
+#define ASn_TRANSTAB_ADRMODE_UNMAPPED  (0u << 0)
+#define ASn_TRANSTAB_ADRMODE_IDENTITY  (1u << 1)
+#define ASn_TRANSTAB_ADRMODE_TABLE     (3u << 0)
+#define ASn_TRANSTAB_READ_INNER        (1u << 2)
+#define ASn_TRANSTAB_SHARE_OUTER       (1u << 4)
+
+#define MMU_TRANSTAB_ADRMODE_MASK      0x00000003
+
+/*
+ * Begin MMU STATUS register values
+ */
+#define ASn_STATUS_FLUSH_ACTIVE 0x01
+
+#define ASn_FAULTSTATUS_ACCESS_TYPE_MASK    (0x3<<8)
+#define ASn_FAULTSTATUS_ACCESS_TYPE_EX      (0x1<<8)
+#define ASn_FAULTSTATUS_ACCESS_TYPE_READ    (0x2<<8)
+#define ASn_FAULTSTATUS_ACCESS_TYPE_WRITE   (0x3<<8)
+
+/*
+ * Begin Command Values
+ */
+
+/* JSn_COMMAND register commands */
+#define JSn_COMMAND_NOP         0x00	/* NOP Operation. Writing this value is ignored */
+#define JSn_COMMAND_START       0x01	/* Start processing a job chain. Writing this value is ignored */
+#define JSn_COMMAND_SOFT_STOP   0x02	/* Gently stop processing a job chain */
+#define JSn_COMMAND_HARD_STOP   0x03	/* Rudely stop processing a job chain */
+#define JSn_COMMAND_SOFT_STOP_0 0x04	/* Execute SOFT_STOP if JOB_CHAIN_FLAG is 0 */
+#define JSn_COMMAND_HARD_STOP_0 0x05	/* Execute HARD_STOP if JOB_CHAIN_FLAG is 0 */
+#define JSn_COMMAND_SOFT_STOP_1 0x06	/* Execute SOFT_STOP if JOB_CHAIN_FLAG is 1 */
+#define JSn_COMMAND_HARD_STOP_1 0x07	/* Execute HARD_STOP if JOB_CHAIN_FLAG is 1 */
+
+/* ASn_COMMAND register commands */
+#define ASn_COMMAND_NOP         0x00	/* NOP Operation */
+#define ASn_COMMAND_UPDATE      0x01	/* Broadcasts the values in ASn_TRANSTAB and ASn_MEMATTR to all MMUs */
+#define ASn_COMMAND_LOCK        0x02	/* Issue a lock region command to all MMUs */
+#define ASn_COMMAND_UNLOCK      0x03	/* Issue a flush region command to all MMUs */
+#define ASn_COMMAND_FLUSH       0x04	/* Flush all L2 caches then issue a flush region command to all MMUs
+					   (deprecated - only for use with T60x) */
+#define ASn_COMMAND_FLUSH_PT    0x04	/* Flush all L2 caches then issue a flush region command to all MMUs */
+#define ASn_COMMAND_FLUSH_MEM   0x05	/* Wait for memory accesses to complete, flush all the L1s cache then
+					   flush all L2 caches then issue a flush region command to all MMUs */
+
+/* Possible values of JSn_CONFIG and JSn_CONFIG_NEXT registers */
+#define JSn_CONFIG_START_FLUSH_NO_ACTION        (0u << 0)
+#define JSn_CONFIG_START_FLUSH_CLEAN            (1u << 8)
+#define JSn_CONFIG_START_FLUSH_CLEAN_INVALIDATE (3u << 8)
+#define JSn_CONFIG_START_MMU                    (1u << 10)
+#define JSn_CONFIG_JOB_CHAIN_FLAG               (1u << 11)
+#define JSn_CONFIG_END_FLUSH_NO_ACTION          JSn_CONFIG_START_FLUSH_NO_ACTION
+#define JSn_CONFIG_END_FLUSH_CLEAN              (1u << 12)
+#define JSn_CONFIG_END_FLUSH_CLEAN_INVALIDATE   (3u << 12)
+#define JSn_CONFIG_THREAD_PRI(n)                ((n) << 16)
+
+/* JSn_STATUS register values */
+
+/* NOTE: Please keep this values in sync with enum base_jd_event_code in mali_base_kernel.h.
+ * The values are separated to avoid dependency of userspace and kernel code.
+ */
+
+/* Group of values representing the job status insead a particular fault */
+#define JSn_STATUS_NO_EXCEPTION_BASE   0x00
+#define JSn_STATUS_INTERRUPTED         (JSn_STATUS_NO_EXCEPTION_BASE + 0x02)	/* 0x02 means INTERRUPTED */
+#define JSn_STATUS_STOPPED             (JSn_STATUS_NO_EXCEPTION_BASE + 0x03)	/* 0x03 means STOPPED */
+#define JSn_STATUS_TERMINATED          (JSn_STATUS_NO_EXCEPTION_BASE + 0x04)	/* 0x04 means TERMINATED */
+
+/* General fault values */
+#define JSn_STATUS_FAULT_BASE          0x40
+#define JSn_STATUS_CONFIG_FAULT        (JSn_STATUS_FAULT_BASE)	/* 0x40 means CONFIG FAULT */
+#define JSn_STATUS_POWER_FAULT         (JSn_STATUS_FAULT_BASE + 0x01)	/* 0x41 means POWER FAULT */
+#define JSn_STATUS_READ_FAULT          (JSn_STATUS_FAULT_BASE + 0x02)	/* 0x42 means READ FAULT */
+#define JSn_STATUS_WRITE_FAULT         (JSn_STATUS_FAULT_BASE + 0x03)	/* 0x43 means WRITE FAULT */
+#define JSn_STATUS_AFFINITY_FAULT      (JSn_STATUS_FAULT_BASE + 0x04)	/* 0x44 means AFFINITY FAULT */
+#define JSn_STATUS_BUS_FAULT           (JSn_STATUS_FAULT_BASE + 0x08)	/* 0x48 means BUS FAULT */
+
+/* Instruction or data faults */
+#define JSn_STATUS_INSTRUCTION_FAULT_BASE  0x50
+#define JSn_STATUS_INSTR_INVALID_PC        (JSn_STATUS_INSTRUCTION_FAULT_BASE)	/* 0x50 means INSTR INVALID PC */
+#define JSn_STATUS_INSTR_INVALID_ENC       (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x01)	/* 0x51 means INSTR INVALID ENC */
+#define JSn_STATUS_INSTR_TYPE_MISMATCH     (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x02)	/* 0x52 means INSTR TYPE MISMATCH */
+#define JSn_STATUS_INSTR_OPERAND_FAULT     (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x03)	/* 0x53 means INSTR OPERAND FAULT */
+#define JSn_STATUS_INSTR_TLS_FAULT         (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x04)	/* 0x54 means INSTR TLS FAULT */
+#define JSn_STATUS_INSTR_BARRIER_FAULT     (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x05)	/* 0x55 means INSTR BARRIER FAULT */
+#define JSn_STATUS_INSTR_ALIGN_FAULT       (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x06)	/* 0x56 means INSTR ALIGN FAULT */
+/* NOTE: No fault with 0x57 code defined in spec. */
+#define JSn_STATUS_DATA_INVALID_FAULT      (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x08)	/* 0x58 means DATA INVALID FAULT */
+#define JSn_STATUS_TILE_RANGE_FAULT        (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x09)	/* 0x59 means TILE RANGE FAULT */
+#define JSn_STATUS_ADDRESS_RANGE_FAULT     (JSn_STATUS_INSTRUCTION_FAULT_BASE + 0x0A)	/* 0x5A means ADDRESS RANGE FAULT */
+
+/* Other faults */
+#define JSn_STATUS_MEMORY_FAULT_BASE   0x60
+#define JSn_STATUS_OUT_OF_MEMORY       (JSn_STATUS_MEMORY_FAULT_BASE)	/* 0x60 means OUT OF MEMORY */
+#define JSn_STATUS_UNKNOWN             0x7F	/* 0x7F means UNKNOWN */
+
+/* GPU_COMMAND values */
+#define GPU_COMMAND_NOP                0x00	/* No operation, nothing happens */
+#define GPU_COMMAND_SOFT_RESET         0x01	/* Stop all external bus interfaces, and then reset the entire GPU. */
+#define GPU_COMMAND_HARD_RESET         0x02	/* Immediately reset the entire GPU. */
+#define GPU_COMMAND_PRFCNT_CLEAR       0x03	/* Clear all performance counters, setting them all to zero. */
+#define GPU_COMMAND_PRFCNT_SAMPLE      0x04	/* Sample all performance counters, writing them out to memory */
+#define GPU_COMMAND_CYCLE_COUNT_START  0x05	/* Starts the cycle counter, and system timestamp propagation */
+#define GPU_COMMAND_CYCLE_COUNT_STOP   0x06	/* Stops the cycle counter, and system timestamp propagation */
+#define GPU_COMMAND_CLEAN_CACHES       0x07	/* Clean all caches */
+#define GPU_COMMAND_CLEAN_INV_CACHES   0x08	/* Clean and invalidate all caches */
+
+/* End Command Values */
+
+/* GPU_STATUS values */
+#define GPU_STATUS_PRFCNT_ACTIVE           (1 << 2)	/* Set if the performance counters are active. */
+
+/* PRFCNT_CONFIG register values */
+#define PRFCNT_CONFIG_AS_SHIFT    4	/* address space bitmap starts from bit 4 of the register */
+#define PRFCNT_CONFIG_MODE_OFF    0	/* The performance counters are disabled. */
+#define PRFCNT_CONFIG_MODE_MANUAL 1	/* The performance counters are enabled, but are only written out when a PRFCNT_SAMPLE command is issued using the GPU_COMMAND register. */
+#define PRFCNT_CONFIG_MODE_TILE   2	/* The performance counters are enabled, and are written out each time a tile finishes rendering. */
+
+/* AS<n>_MEMATTR values: */
+/* Use GPU implementation-defined  caching policy. */
+#define ASn_MEMATTR_IMPL_DEF_CACHE_POLICY 0x48
+/* The attribute set to force all resources to be cached. */
+#define ASn_MEMATTR_FORCE_TO_CACHE_ALL    0x4F
+/* Inner write-alloc cache setup, no outer caching */
+#define ASn_MEMATTR_WRITE_ALLOC           0x4D
+/* symbol for default MEMATTR to use */
+#define ASn_MEMATTR_INDEX_DEFAULT               0
+/* HW implementation defined caching */
+#define ASn_MEMATTR_INDEX_IMPL_DEF_CACHE_POLICY 0
+/* Force cache on */
+#define ASn_MEMATTR_INDEX_FORCE_TO_CACHE_ALL    1
+/* Write-alloc inner */
+#define ASn_MEMATTR_INDEX_WRITE_ALLOC           2
+
+/* GPU_ID register */
+#define GPU_ID_VERSION_STATUS_SHIFT       0
+#define GPU_ID_VERSION_MINOR_SHIFT        4
+#define GPU_ID_VERSION_MAJOR_SHIFT        12
+#define GPU_ID_VERSION_PRODUCT_ID_SHIFT   16
+#define GPU_ID_VERSION_STATUS             (0xF  << GPU_ID_VERSION_STATUS_SHIFT)
+#define GPU_ID_VERSION_MINOR              (0xFF << GPU_ID_VERSION_MINOR_SHIFT)
+#define GPU_ID_VERSION_MAJOR              (0xF  << GPU_ID_VERSION_MAJOR_SHIFT)
+#define GPU_ID_VERSION_PRODUCT_ID         (0xFFFF << GPU_ID_VERSION_PRODUCT_ID_SHIFT)
+
+/* Values for GPU_ID_VERSION_PRODUCT_ID bitfield */
+#define GPU_ID_PI_T60X                    0x6956
+#define GPU_ID_PI_T62X                    0x0620
+#define GPU_ID_PI_T76X                    0x0750
+#define GPU_ID_PI_T72X                    0x0720
+
+/* Values for GPU_ID_VERSION_STATUS field for PRODUCT_ID GPU_ID_PI_T60X */
+#define GPU_ID_S_15DEV0                   0x1
+#define GPU_ID_S_EAC                      0x2
+
+/* Helper macro to create a GPU_ID assuming valid values for id, major, minor, status */
+#define GPU_ID_MAKE(id, major, minor, status) \
+		(((id) << GPU_ID_VERSION_PRODUCT_ID_SHIFT) | \
+		((major) << GPU_ID_VERSION_MAJOR_SHIFT) |   \
+		((minor) << GPU_ID_VERSION_MINOR_SHIFT) |   \
+		((status) << GPU_ID_VERSION_STATUS_SHIFT))
+
+/* End GPU_ID register */
+
+/* JS<n>_FEATURES register */
+
+#define JSn_FEATURE_NULL_JOB              (1u << 1)
+#define JSn_FEATURE_SET_VALUE_JOB         (1u << 2)
+#define JSn_FEATURE_CACHE_FLUSH_JOB       (1u << 3)
+#define JSn_FEATURE_COMPUTE_JOB           (1u << 4)
+#define JSn_FEATURE_VERTEX_JOB            (1u << 5)
+#define JSn_FEATURE_GEOMETRY_JOB          (1u << 6)
+#define JSn_FEATURE_TILER_JOB             (1u << 7)
+#define JSn_FEATURE_FUSED_JOB             (1u << 8)
+#define JSn_FEATURE_FRAGMENT_JOB          (1u << 9)
+
+/* End JS<n>_FEATURES register */
+
+/* L2_MMU_CONFIG register */
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT        (24)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_READS              (0x3 << L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_OCTANT       (0x1 << L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_QUARTER      (0x2 << L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_HALF         (0x3 << L2_MMU_CONFIG_LIMIT_EXTERNAL_READS_SHIFT)
+
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT       (26)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES             (0x3 << L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_OCTANT      (0x1 << L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_QUARTER     (0x2 << L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT)
+#define L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_HALF        (0x3 << L2_MMU_CONFIG_LIMIT_EXTERNAL_WRITES_SHIFT)
+/* End L2_MMU_CONFIG register */
+
+/* THREAD_* registers */
+
+/* THREAD_FEATURES IMPLEMENTATION_TECHNOLOGY values */
+#define IMPLEMENTATION_UNSPECIFIED  0
+#define IMPLEMENTATION_SILICON      1
+#define IMPLEMENTATION_FPGA         2
+#define IMPLEMENTATION_MODEL        3
+
+/* Default values when registers are not supported by the implemented hardware */
+#define THREAD_MT_DEFAULT     256
+#define THREAD_MWS_DEFAULT    256
+#define THREAD_MBS_DEFAULT    256
+#define THREAD_MR_DEFAULT     1024
+#define THREAD_MTQ_DEFAULT    4
+#define THREAD_MTGS_DEFAULT   10
+
+/* End THREAD_* registers */
+
+/* SHADER_CONFIG register */
+
+#define SC_ALT_COUNTERS             (1ul << 3)
+#define SC_OVERRIDE_FWD_PIXEL_KILL  (1ul << 4)
+#define SC_SDC_DISABLE_OQ_DISCARD   (1ul << 6)
+#define SC_LS_PAUSEBUFFER_DISABLE   (1ul << 16)
+#define SC_ENABLE_TEXGRD_FLAGS      (1ul << 25)
+/* End SHADER_CONFIG register */
+
+#endif				/* _MIDGARD_REGMAP_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_timeline.h b/drivers/gpu/mali-t6xx/r4p1/mali_timeline.h
new file mode 100644
index 0000000..f8b8710
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_timeline.h
@@ -0,0 +1,369 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#undef TRACE_SYSTEM
+#define TRACE_SYSTEM mali_timeline
+
+#if !defined(_MALI_TIMELINE_H) || defined(TRACE_HEADER_MULTI_READ)
+#define _MALI_TIMELINE_H
+
+#include <linux/tracepoint.h>
+
+TRACE_EVENT(mali_timeline_atoms_in_flight,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int tgid,
+		int count),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		tgid,
+		count),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, tgid)
+			__field(int, count)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->tgid = tgid;
+		__entry->count = count;
+	),
+
+	TP_printk("%i,%i.%.9i,%i,%i", CTX_SET_NR_ATOMS_IN_FLIGHT,
+				(int)__entry->ts_sec,
+				(int)__entry->ts_nsec,
+				__entry->tgid,
+				__entry->count)
+);
+
+
+TRACE_EVENT(mali_timeline_atom,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int tgid,
+		int atom_id),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		tgid,
+		atom_id),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, tgid)
+			__field(int, atom_id)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->tgid = tgid;
+		__entry->atom_id = atom_id;
+	),
+
+	TP_printk("%i,%i.%.9i,%i,%i,%i", __entry->event_type,
+				(int)__entry->ts_sec,
+				(int)__entry->ts_nsec,
+				__entry->tgid,
+				__entry->atom_id,
+				__entry->atom_id)
+);
+
+TRACE_EVENT(mali_timeline_gpu_slot_active,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int tgid,
+		int js,
+		int count),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		tgid,
+		js,
+		count),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, tgid)
+			__field(int, js)
+			__field(int, count)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->tgid = tgid;
+		__entry->js = js;
+		__entry->count = count;
+	),
+
+	TP_printk("%i,%i.%.9i,%i,%i,%i", __entry->event_type,
+				(int)__entry->ts_sec,
+				(int)__entry->ts_nsec,
+				__entry->tgid,
+				__entry->js,
+				__entry->count)
+);
+
+TRACE_EVENT(mali_timeline_gpu_slot_action,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int tgid,
+		int js,
+		int count),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		tgid,
+		js,
+		count),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, tgid)
+			__field(int, js)
+			__field(int, count)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->tgid = tgid;
+		__entry->js = js;
+		__entry->count = count;
+	),
+
+	TP_printk("%i,%i.%.9i,%i,%i,%i", __entry->event_type,
+				(int)__entry->ts_sec,
+				(int)__entry->ts_nsec,
+				__entry->tgid,
+				__entry->js,
+				__entry->count)
+);
+
+TRACE_EVENT(mali_timeline_gpu_power_active,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int active),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		active),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, active)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->active = active;
+	),
+
+	TP_printk("%i,%i.%.9i,0,%i", __entry->event_type,
+	                   (int)__entry->ts_sec,
+	                   (int)__entry->ts_nsec,
+	                   __entry->active)
+
+);
+
+TRACE_EVENT(mali_timeline_l2_power_active,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int state),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		state),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, state)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->state = state;
+	),
+
+	TP_printk("%i,%i.%.9i,0,%i", __entry->event_type,
+	                   (int)__entry->ts_sec,
+	                   (int)__entry->ts_nsec,
+	                   __entry->state)
+
+);
+TRACE_EVENT(mali_timeline_pm_event,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int pm_event_type,
+		unsigned int pm_event_id),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		pm_event_type,
+		pm_event_id),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, pm_event_type)
+			__field(unsigned int, pm_event_id)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->pm_event_type = pm_event_type;
+		__entry->pm_event_id = pm_event_id;
+	),
+
+	TP_printk("%i,%i.%.9i,0,%i,%u", __entry->event_type,
+	                   (int)__entry->ts_sec,
+	                   (int)__entry->ts_nsec,
+	                   __entry->pm_event_type, __entry->pm_event_id)
+
+);
+
+TRACE_EVENT(mali_timeline_slot_atom,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int event_type,
+		int tgid,
+		int js,
+		int atom_id),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		event_type,
+		tgid,
+		js,
+		atom_id),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, event_type)
+			__field(int, tgid)
+			__field(int, js)
+			__field(int, atom_id)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->event_type = event_type;
+		__entry->tgid = tgid;
+		__entry->js = js;
+		__entry->atom_id = atom_id;
+	),
+
+	TP_printk("%i,%i.%.9i,%i,%i,%i", __entry->event_type,
+				(int)__entry->ts_sec,
+				(int)__entry->ts_nsec,
+				__entry->tgid,
+				__entry->js,
+				__entry->atom_id)
+);
+
+TRACE_EVENT(mali_timeline_pm_checktrans,
+
+	TP_PROTO(u64 ts_sec,
+		u32 ts_nsec,
+		int trans_code,
+		int trans_id),
+
+	TP_ARGS(ts_sec,
+		ts_nsec,
+		trans_code,
+		trans_id),
+
+	TP_STRUCT__entry(
+			__field(u64, ts_sec)
+			__field(u32, ts_nsec)
+			__field(int, trans_code)
+			__field(int, trans_id)
+	),
+
+	TP_fast_assign(
+		__entry->ts_sec = ts_sec;
+		__entry->ts_nsec = ts_nsec;
+		__entry->trans_code = trans_code;
+		__entry->trans_id = trans_id;
+	),
+
+	TP_printk("%i,%i.%.9i,0,%i", __entry->trans_code,
+	                   (int)__entry->ts_sec,
+	                   (int)__entry->ts_nsec,
+	                   __entry->trans_id)
+
+);
+
+
+#endif				/* _MALI_TIMELINE_H */
+
+#undef TRACE_INCLUDE_PATH
+#define TRACE_INCLUDE_PATH .
+
+/* This part must be outside protection */
+#include <trace/define_trace.h>
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/mali_uk.h b/drivers/gpu/mali-t6xx/r4p1/mali_uk.h
new file mode 100644
index 0000000..c577e83
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/mali_uk.h
@@ -0,0 +1,143 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_uk.h
+ * Types and definitions that are common across OSs for both the user
+ * and kernel side of the User-Kernel interface.
+ */
+
+#ifndef _UK_H_
+#define _UK_H_
+
+#ifdef __cplusplus
+extern "C" {
+#endif				/* __cplusplus */
+
+#include <malisw/mali_stdtypes.h>
+
+/**
+ * @addtogroup base_api
+ * @{
+ */
+
+/**
+ * @defgroup uk_api User-Kernel Interface API
+ *
+ * The User-Kernel Interface abstracts the communication mechanism between the user and kernel-side code of device
+ * drivers developed as part of the Midgard DDK. Currently that includes the Base driver and the UMP driver.
+ *
+ * It exposes an OS independent API to user-side code (UKU) which routes functions calls to an OS-independent
+ * kernel-side API (UKK) via an OS-specific communication mechanism.
+ *
+ * This API is internal to the Midgard DDK and is not exposed to any applications.
+ *
+ * @{
+ */
+
+/**
+ * These are identifiers for kernel-side drivers implementing a UK interface, aka UKK clients. The
+ * UK module maps this to an OS specific device name, e.g. "gpu_base" -> "GPU0:". Specify this
+ * identifier to select a UKK client to the uku_open() function.
+ *
+ * When a new UKK client driver is created a new identifier needs to be added to the uk_client_id
+ * enumeration and the uku_open() implemenation for the various OS ports need to be updated to
+ * provide a mapping of the identifier to the OS specific device name.
+ *
+ */
+	typedef enum uk_client_id {
+	/**
+	 * Value used to identify the Base driver UK client.
+	 */
+		UK_CLIENT_MALI_T600_BASE,
+
+	/** The number of uk clients supported. This must be the last member of the enum */
+		UK_CLIENT_COUNT
+	} uk_client_id;
+
+/**
+ * Each function callable through the UK interface has a unique number.
+ * Functions provided by UK clients start from number UK_FUNC_ID.
+ * Numbers below UK_FUNC_ID are used for internal UK functions.
+ */
+	typedef enum uk_func {
+		UKP_FUNC_ID_CHECK_VERSION,   /**< UKK Core internal function */
+	/**
+	 * Each UK client numbers the functions they provide starting from
+	 * number UK_FUNC_ID. This number is then eventually assigned to the
+	 * id field of the uk_header structure when preparing to make a
+	 * UK call. See your UK client for a list of their function numbers.
+	 */
+		UK_FUNC_ID = 512
+	} uk_func;
+
+/**
+ * Arguments for a UK call are stored in a structure. This structure consists
+ * of a fixed size header and a payload. The header carries a 32-bit number
+ * identifying the UK function to be called (see uk_func). When the UKK client
+ * receives this header and executed the requested UK function, it will use
+ * the same header to store the result of the function in the form of a
+ * mali_error return code. The size of this structure is such that the
+ * first member of the payload following the header can be accessed efficiently
+ * on a 32 and 64-bit kernel and the structure has the same size regardless
+ * of a 32 or 64-bit kernel. The uk_kernel_size_type type should be defined
+ * accordingly in the OS specific mali_uk_os.h header file.
+ */
+	typedef union uk_header {
+		/**
+		 * 32-bit number identifying the UK function to be called.
+		 * Also see uk_func.
+		 */
+		u32 id;
+		/**
+		 * The mali_error return code returned by the called UK function.
+		 * See the specification of the particular UK function you are
+		 * calling for the meaning of the error codes returned. All
+		 * UK functions return MALI_ERROR_NONE on success.
+		 */
+		u32 ret;
+		/*
+		 * Used to ensure 64-bit alignment of this union. Do not remove.
+		 * This field is used for padding and does not need to be initialized.
+		 */
+		u64 sizer;
+	} uk_header;
+
+/**
+ * This structure carries a 16-bit major and minor number and is sent along with an internal UK call
+ * used during uku_open to identify the versions of the UK module in use by the user-side and kernel-side.
+ */
+	typedef struct uku_version_check_args {
+		uk_header header;
+			  /**< UK call header */
+		u16 major;
+		   /**< This field carries the user-side major version on input and the kernel-side major version on output */
+		u16 minor;
+		   /**< This field carries the user-side minor version on input and the kernel-side minor version on output. */
+		u8 padding[4];
+	} uku_version_check_args;
+
+/** @} end group uk_api */
+
+	/** @} *//* end group base_api */
+
+#ifdef __cplusplus
+}
+#endif				/* __cplusplus */
+#endif				/* _UK_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd.h
new file mode 100644
index 0000000..9226d97
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd.h
@@ -0,0 +1,481 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @addtogroup malisw
+ * @{
+ */
+
+/* ============================================================================
+    Description
+============================================================================ */
+/**
+ * @defgroup arm_cstd_coding_standard ARM C standard types and constants
+ * The common files are a set of standard headers which are used by all parts
+ * of this development, describing types, and generic constants.
+ *
+ * Files in group:
+ *     - arm_cstd.h
+ *     - arm_cstd_compilers.h
+ *     - arm_cstd_types.h
+ *     - arm_cstd_types_rvct.h
+ *     - arm_cstd_types_gcc.h
+ *     - arm_cstd_types_msvc.h
+ *     - arm_cstd_pack_push.h
+ *     - arm_cstd_pack_pop.h
+ */
+
+/**
+ * @addtogroup arm_cstd_coding_standard
+ * @{
+ */
+
+#ifndef _ARM_CSTD_
+#define _ARM_CSTD_
+
+/* ============================================================================
+	Import standard C99 types
+============================================================================ */
+#include "arm_cstd_compilers.h"
+#include "arm_cstd_types.h"
+
+/* ============================================================================
+	Min and Max Values
+============================================================================ */
+#if !defined(INT8_MAX)
+	#define INT8_MAX                ((int8_t) 0x7F)
+#endif
+#if !defined(INT8_MIN)
+	#define INT8_MIN                (-INT8_MAX - 1)
+#endif
+
+#if !defined(INT16_MAX)
+	#define INT16_MAX               ((int16_t)0x7FFF)
+#endif
+#if !defined(INT16_MIN)
+	#define INT16_MIN               (-INT16_MAX - 1)
+#endif
+
+#if !defined(INT32_MAX)
+	#define INT32_MAX               ((int32_t)0x7FFFFFFF)
+#endif
+#if !defined(INT32_MIN)
+	#define INT32_MIN               (-INT32_MAX - 1)
+#endif
+
+#if !defined(INT64_MAX)
+	#define INT64_MAX               ((int64_t)0x7FFFFFFFFFFFFFFFLL)
+#endif
+#if !defined(INT64_MIN)
+	#define INT64_MIN               (-INT64_MAX - 1)
+#endif
+
+#if !defined(UINT8_MAX)
+	#define UINT8_MAX               ((uint8_t) 0xFF)
+#endif
+
+#if !defined(UINT16_MAX)
+	#define UINT16_MAX              ((uint16_t)0xFFFF)
+#endif
+
+#if !defined(UINT32_MAX)
+	#define UINT32_MAX              ((uint32_t)0xFFFFFFFF)
+#endif
+
+#if !defined(UINT64_MAX)
+	#define UINT64_MAX              ((uint64_t)0xFFFFFFFFFFFFFFFFULL)
+#endif
+
+/* fallbacks if limits.h wasn't available */
+#if !defined(UCHAR_MAX)
+	#define UCHAR_MAX               ((unsigned char)~0U)
+#endif
+
+#if !defined(SCHAR_MAX)
+	#define SCHAR_MAX               ((signed char)(UCHAR_MAX >> 1))
+#endif
+#if !defined(SCHAR_MIN)
+	#define SCHAR_MIN               ((signed char)(-SCHAR_MAX - 1))
+#endif
+
+#if !defined(USHRT_MAX)
+	#define USHRT_MAX               ((unsigned short)~0U)
+#endif
+
+#if !defined(SHRT_MAX)
+	#define SHRT_MAX                ((signed short)(USHRT_MAX >> 1))
+#endif
+#if !defined(SHRT_MIN)
+	#define SHRT_MIN                ((signed short)(-SHRT_MAX - 1))
+#endif
+
+#if !defined(UINT_MAX)
+	#define UINT_MAX                ((unsigned int)~0U)
+#endif
+
+#if !defined(INT_MAX)
+	#define INT_MAX                 ((signed int)(UINT_MAX >> 1))
+#endif
+#if !defined(INT_MIN)
+	#define INT_MIN                 ((signed int)(-INT_MAX - 1))
+#endif
+
+#if !defined(ULONG_MAX)
+	#define ULONG_MAX               ((unsigned long)~0UL)
+#endif
+
+#if !defined(LONG_MAX)
+	#define LONG_MAX                ((signed long)(ULONG_MAX >> 1))
+#endif
+#if !defined(LONG_MIN)
+	#define LONG_MIN                ((signed long)(-LONG_MAX - 1))
+#endif
+
+#if !defined(ULLONG_MAX)
+	#define ULLONG_MAX              ((unsigned long long)~0ULL)
+#endif
+
+#if !defined(LLONG_MAX)
+	#define LLONG_MAX               ((signed long long)(ULLONG_MAX >> 1))
+#endif
+#if !defined(LLONG_MIN)
+	#define LLONG_MIN               ((signed long long)(-LLONG_MAX - 1))
+#endif
+
+#if !defined(SIZE_MAX)
+	#if 1 == CSTD_CPU_32BIT
+		#define SIZE_MAX            UINT32_MAX
+	#elif 1 == CSTD_CPU_64BIT
+		#define SIZE_MAX            UINT64_MAX
+	#endif
+#endif
+
+/* ============================================================================
+	Keywords
+============================================================================ */
+/* Portable keywords. */
+
+#if !defined(CONST)
+/**
+ * @hideinitializer
+ * Variable is a C @c const, which can be made non-const for testing purposes.
+ */
+	#define CONST                   const
+#endif
+
+#if !defined(STATIC)
+/**
+ * @hideinitializer
+ * Variable is a C @c static, which can be made non-static for testing
+ * purposes.
+ */
+	#define STATIC                  static
+#endif
+
+/**
+ * Specifies a function as being exported outside of a logical module.
+ */
+#define PUBLIC
+
+/**
+ * @def PROTECTED
+ * Specifies a a function which is internal to an logical module, but which
+ * should not be used outside of that module. This cannot be enforced by the
+ * compiler, as a module is typically more than one translation unit.
+ */
+#define PROTECTED
+
+/**
+ * Specifies a function as being internal to a translation unit. Private
+ * functions would typically be declared as STATIC, unless they are being
+ * exported for unit test purposes.
+ */
+#define PRIVATE STATIC
+
+/**
+ * Specify an assertion value which is evaluated at compile time. Recommended
+ * usage is specification of a @c static @c INLINE function containing all of
+ * the assertions thus:
+ *
+ * @code
+ * static INLINE [module]_compile_time_assertions( void )
+ * {
+ *     COMPILE_TIME_ASSERT( sizeof(uintptr_t) == sizeof(intptr_t) );
+ * }
+ * @endcode
+ *
+ * @note Use @c static not @c STATIC. We never want to turn off this @c static
+ * specification for testing purposes.
+ */
+#define CSTD_COMPILE_TIME_ASSERT( expr ) \
+	do { switch(0){case 0: case (expr):;} } while( FALSE )
+
+/**
+ * @hideinitializer
+ * @deprecated Prefered form is @c CSTD_UNUSED
+ * Function-like macro for suppressing unused variable warnings. Where possible
+ * such variables should be removed; this macro is present for cases where we
+ * much support API backwards compatibility.
+ */
+#define UNUSED( x )                 ((void)(x))
+
+/**
+ * @hideinitializer
+ * Function-like macro for suppressing unused variable warnings. Where possible
+ * such variables should be removed; this macro is present for cases where we
+ * much support API backwards compatibility.
+ */
+#define CSTD_UNUSED( x )            ((void)(x))
+
+/**
+ * @hideinitializer
+ * Function-like macro for use where "no behavior" is desired. This is useful
+ * when compile time macros turn a function-like macro in to a no-op, but
+ * where having no statement is otherwise invalid.
+ */
+#define CSTD_NOP( ... )             ((void)#__VA_ARGS__)
+
+/**
+ * @hideinitializer
+ * Function-like macro for converting a pointer in to a u64 for storing into
+ * an external data structure. This is commonly used when pairing a 32-bit
+ * CPU with a 64-bit peripheral, such as a Midgard GPU. C's type promotion
+ * is complex and a straight cast does not work reliably as pointers are
+ * often considered as signed.
+ */
+#define CSTD_PTR_TO_U64( x )        ((uint64_t)((uintptr_t)(x)))
+
+/**
+ * @hideinitializer
+ * Function-like macro for stringizing a single level macro.
+ * @code
+ * #define MY_MACRO 32
+ * CSTD_STR1( MY_MACRO )
+ * > "MY_MACRO"
+ * @endcode
+ */
+#define CSTD_STR1( x )             #x
+
+/**
+ * @hideinitializer
+ * Function-like macro for stringizing a macro's value. This should not be used
+ * if the macro is defined in a way which may have no value; use the
+ * alternative @c CSTD_STR2N macro should be used instead.
+ * @code
+ * #define MY_MACRO 32
+ * CSTD_STR2( MY_MACRO )
+ * > "32"
+ * @endcode
+ */
+#define CSTD_STR2( x )              CSTD_STR1( x )
+
+/**
+ * @hideinitializer
+ * Utility function for stripping the first character off a string.
+ */
+static INLINE char* arm_cstd_strstrip( char * string )
+{
+	return ++string;
+}
+
+/**
+ * @hideinitializer
+ * Function-like macro for stringizing a single level macro where the macro
+ * itself may not have a value. Parameter @c a should be set to any single
+ * character which is then stripped by the macro via an inline function. This
+ * should only be used via the @c CSTD_STR2N macro; for printing a single
+ * macro only the @c CSTD_STR1 macro is a better alternative.
+ *
+ * This macro requires run-time code to handle the case where the macro has
+ * no value (you can't concat empty strings in the preprocessor).
+ */
+#define CSTD_STR1N( a, x )          arm_cstd_strstrip( CSTD_STR1( a##x ) )
+
+/**
+ * @hideinitializer
+ * Function-like macro for stringizing a two level macro where the macro itself
+ * may not have a value.
+ * @code
+ * #define MY_MACRO 32
+ * CSTD_STR2N( MY_MACRO )
+ * > "32"
+ *
+ * #define MY_MACRO 32
+ * CSTD_STR2N( MY_MACRO )
+ * > "32"
+ * @endcode
+ */
+#define CSTD_STR2N( x )              CSTD_STR1N( _, x )
+
+/* ============================================================================
+	Validate portability constructs
+============================================================================ */
+static INLINE void arm_cstd_compile_time_assertions( void )
+{
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uint8_t)  == 1 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(int8_t)   == 1 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uint16_t) == 2 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(int16_t)  == 2 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uint32_t) == 4 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(int32_t)  == 4 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uint64_t) == 8 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(int64_t)  == 8 );
+	CSTD_COMPILE_TIME_ASSERT( sizeof(intptr_t) == sizeof(uintptr_t) );
+
+	CSTD_COMPILE_TIME_ASSERT( 1 == TRUE );
+	CSTD_COMPILE_TIME_ASSERT( 0 == FALSE );
+
+#if 1 == CSTD_CPU_32BIT
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uintptr_t) == 4 );
+#elif 1 == CSTD_CPU_64BIT
+	CSTD_COMPILE_TIME_ASSERT( sizeof(uintptr_t) == 8 );
+#endif
+
+}
+
+/* ============================================================================
+	Useful function-like macro
+============================================================================ */
+/**
+ * @brief Return the lesser of two values.
+ * As a macro it may evaluate its arguments more than once.
+ * @see CSTD_MAX
+ */
+#define CSTD_MIN( x, y )            ((x) < (y) ? (x) : (y))
+
+/**
+ * @brief Return the greater of two values.
+ * As a macro it may evaluate its arguments more than once.
+ * If called on the same two arguments as CSTD_MIN it is guaranteed to return
+ * the one that CSTD_MIN didn't return. This is significant for types where not
+ * all values are comparable e.g. NaNs in floating-point types. But if you want
+ * to retrieve the min and max of two values, consider using a conditional swap
+ * instead.
+ */
+#define CSTD_MAX( x, y )            ((x) < (y) ? (y) : (x))
+
+/**
+ * @brief Clamp value @c x to within @c min and @c max inclusive.
+ */
+#define CSTD_CLAMP( x, min, max )   ((x)<(min) ? (min):((x)>(max) ? (max):(x)))
+
+/**
+ * Flag a cast as a reinterpretation, usually of a pointer type.
+ */
+#define CSTD_REINTERPRET_CAST(type) (type)
+
+/**
+ * Flag a cast as casting away const, usually of a pointer type.
+ */
+#define CSTD_CONST_CAST(type)       (type)
+
+/**
+ * Flag a cast as a (potentially complex) value conversion, usually of a
+ * numerical type.
+ */
+#define CSTD_STATIC_CAST(type)      (type)
+
+/* ============================================================================
+	Useful bit constants
+============================================================================ */
+/**
+ * @cond arm_cstd_utilities
+ */
+
+/* Common bit constant values, useful in embedded programming. */
+#define F_BIT_0       ((uint32_t)0x00000001)
+#define F_BIT_1       ((uint32_t)0x00000002)
+#define F_BIT_2       ((uint32_t)0x00000004)
+#define F_BIT_3       ((uint32_t)0x00000008)
+#define F_BIT_4       ((uint32_t)0x00000010)
+#define F_BIT_5       ((uint32_t)0x00000020)
+#define F_BIT_6       ((uint32_t)0x00000040)
+#define F_BIT_7       ((uint32_t)0x00000080)
+#define F_BIT_8       ((uint32_t)0x00000100)
+#define F_BIT_9       ((uint32_t)0x00000200)
+#define F_BIT_10      ((uint32_t)0x00000400)
+#define F_BIT_11      ((uint32_t)0x00000800)
+#define F_BIT_12      ((uint32_t)0x00001000)
+#define F_BIT_13      ((uint32_t)0x00002000)
+#define F_BIT_14      ((uint32_t)0x00004000)
+#define F_BIT_15      ((uint32_t)0x00008000)
+#define F_BIT_16      ((uint32_t)0x00010000)
+#define F_BIT_17      ((uint32_t)0x00020000)
+#define F_BIT_18      ((uint32_t)0x00040000)
+#define F_BIT_19      ((uint32_t)0x00080000)
+#define F_BIT_20      ((uint32_t)0x00100000)
+#define F_BIT_21      ((uint32_t)0x00200000)
+#define F_BIT_22      ((uint32_t)0x00400000)
+#define F_BIT_23      ((uint32_t)0x00800000)
+#define F_BIT_24      ((uint32_t)0x01000000)
+#define F_BIT_25      ((uint32_t)0x02000000)
+#define F_BIT_26      ((uint32_t)0x04000000)
+#define F_BIT_27      ((uint32_t)0x08000000)
+#define F_BIT_28      ((uint32_t)0x10000000)
+#define F_BIT_29      ((uint32_t)0x20000000)
+#define F_BIT_30      ((uint32_t)0x40000000)
+#define F_BIT_31      ((uint32_t)0x80000000)
+
+/* Common 2^n size values, useful in embedded programming. */
+#define C_SIZE_1B     ((uint32_t)0x00000001)
+#define C_SIZE_2B     ((uint32_t)0x00000002)
+#define C_SIZE_4B     ((uint32_t)0x00000004)
+#define C_SIZE_8B     ((uint32_t)0x00000008)
+#define C_SIZE_16B    ((uint32_t)0x00000010)
+#define C_SIZE_32B    ((uint32_t)0x00000020)
+#define C_SIZE_64B    ((uint32_t)0x00000040)
+#define C_SIZE_128B   ((uint32_t)0x00000080)
+#define C_SIZE_256B   ((uint32_t)0x00000100)
+#define C_SIZE_512B   ((uint32_t)0x00000200)
+#define C_SIZE_1KB    ((uint32_t)0x00000400)
+#define C_SIZE_2KB    ((uint32_t)0x00000800)
+#define C_SIZE_4KB    ((uint32_t)0x00001000)
+#define C_SIZE_8KB    ((uint32_t)0x00002000)
+#define C_SIZE_16KB   ((uint32_t)0x00004000)
+#define C_SIZE_32KB   ((uint32_t)0x00008000)
+#define C_SIZE_64KB   ((uint32_t)0x00010000)
+#define C_SIZE_128KB  ((uint32_t)0x00020000)
+#define C_SIZE_256KB  ((uint32_t)0x00040000)
+#define C_SIZE_512KB  ((uint32_t)0x00080000)
+#define C_SIZE_1MB    ((uint32_t)0x00100000)
+#define C_SIZE_2MB    ((uint32_t)0x00200000)
+#define C_SIZE_4MB    ((uint32_t)0x00400000)
+#define C_SIZE_8MB    ((uint32_t)0x00800000)
+#define C_SIZE_16MB   ((uint32_t)0x01000000)
+#define C_SIZE_32MB   ((uint32_t)0x02000000)
+#define C_SIZE_64MB   ((uint32_t)0x04000000)
+#define C_SIZE_128MB  ((uint32_t)0x08000000)
+#define C_SIZE_256MB  ((uint32_t)0x10000000)
+#define C_SIZE_512MB  ((uint32_t)0x20000000)
+#define C_SIZE_1GB    ((uint32_t)0x40000000)
+#define C_SIZE_2GB    ((uint32_t)0x80000000)
+
+/**
+ * @endcond
+ */
+
+/**
+ * @}
+ */
+
+/**
+ * @}
+ */
+
+#endif  /* End (_ARM_CSTD_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_compilers.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_compilers.h
new file mode 100644
index 0000000..55637cf
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_compilers.h
@@ -0,0 +1,617 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_COMPILERS_H_
+#define _ARM_CSTD_COMPILERS_H_
+
+/* ============================================================================
+	Document default definitions - assuming nothing set at this point.
+============================================================================ */
+/**
+ * @addtogroup arm_cstd_coding_standard
+ * @{
+ */
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if toolchain is Microsoft Visual Studio, 0
+ * otherwise.
+ */
+#define CSTD_TOOLCHAIN_MSVC         0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if toolchain is the GNU Compiler Collection, 0
+ * otherwise.
+ */
+#define CSTD_TOOLCHAIN_GCC          0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if toolchain is ARM RealView Compiler Tools, 0
+ * otherwise. Note - if running RVCT in GCC mode this define will be set to 0;
+ * @c CSTD_TOOLCHAIN_GCC and @c CSTD_TOOLCHAIN_RVCT_GCC_MODE will both be
+ * defined as 1.
+ */
+#define CSTD_TOOLCHAIN_RVCT         0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if toolchain is ARM RealView Compiler Tools running
+ * in GCC mode, 0 otherwise.
+ */
+#define CSTD_TOOLCHAIN_RVCT_GCC_MODE 0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor is an x86 32-bit machine, 0 otherwise.
+ */
+#define CSTD_CPU_X86_32             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor is an x86-64 (AMD64) machine, 0
+ * otherwise.
+ */
+#define CSTD_CPU_X86_64             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor is an ARM machine, 0 otherwise.
+ */
+#define CSTD_CPU_ARM                0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor is an AARCH64 machine, 0 otherwise.
+ */
+#define CSTD_CPU_AARCH64            0
+
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor is a MIPS machine, 0 otherwise.
+ */
+#define CSTD_CPU_MIPS               0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if CPU is 32-bit, 0 otherwise.
+ */
+#define CSTD_CPU_32BIT              0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if CPU is 64-bit, 0 otherwise.
+ */
+#define CSTD_CPU_64BIT              0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if processor configured as big-endian, 0 if it
+ * is little-endian.
+ */
+#define CSTD_CPU_BIG_ENDIAN         0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a version of Windows, 0 if
+ * it is not.
+ */
+#define CSTD_OS_WINDOWS             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 32-bit version of Windows,
+ * 0 if it is not.
+ */
+#define CSTD_OS_WIN32               0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 64-bit version of Windows,
+ * 0 if it is not.
+ */
+#define CSTD_OS_WIN64               0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is Linux, 0 if it is not.
+ */
+#define CSTD_OS_LINUX               0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if we are compiling Linux kernel code, 0 otherwise.
+ */
+#define CSTD_OS_LINUX_KERNEL        0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 32-bit version of Linux,
+ * 0 if it is not.
+ */
+#define CSTD_OS_LINUX32             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 64-bit version of Linux,
+ * 0 if it is not.
+ */
+#define CSTD_OS_LINUX64             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is Android, 0 if it is not.
+ */
+#define CSTD_OS_ANDROID             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if we are compiling Android kernel code, 0 otherwise.
+ */
+#define CSTD_OS_ANDROID_KERNEL      0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 32-bit version of Android,
+ * 0 if it is not.
+ */
+#define CSTD_OS_ANDROID32           0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 64-bit version of Android,
+ * 0 if it is not.
+ */
+#define CSTD_OS_ANDROID64           0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a version of Apple OS,
+ * 0 if it is not.
+ */
+#define CSTD_OS_APPLEOS             0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 32-bit version of Apple OS,
+ * 0 if it is not.
+ */
+#define CSTD_OS_APPLEOS32           0
+
+/**
+ * @hideinitializer
+ * Defined with value of 1 if operating system is a 64-bit version of Apple OS,
+ * 0 if it is not.
+ */
+#define CSTD_OS_APPLEOS64           0
+
+/**
+ * @def CSTD_OS_SYMBIAN
+ * @hideinitializer
+ * Defined with value of 1 if operating system is Symbian, 0 if it is not.
+ */
+#define CSTD_OS_SYMBIAN             0
+
+/**
+ * @def CSTD_OS_NONE
+ * @hideinitializer
+ * Defined with value of 1 if there is no operating system (bare metal), 0
+ * otherwise
+ */
+#define CSTD_OS_NONE                0
+
+/* ============================================================================
+	Determine the compiler in use
+============================================================================ */
+
+/* Default empty definitions of compiler-specific option enable/disable.  This will be overridden 
+ * if applicable by preprocessor defines below. */
+#define CSTD_PUSH_WARNING_GCC_WADDRESS
+#define CSTD_POP_WARNING_GCC_WADDRESS
+
+#if defined(_MSC_VER)
+	#undef CSTD_TOOLCHAIN_MSVC
+	#define CSTD_TOOLCHAIN_MSVC         1
+
+#elif defined(__GNUC__)
+	#undef CSTD_TOOLCHAIN_GCC
+	#define CSTD_TOOLCHAIN_GCC          1
+
+	/* Detect RVCT pretending to be GCC. */
+	#if defined(__ARMCC_VERSION)
+		#undef CSTD_TOOLCHAIN_RVCT_GCC_MODE
+		#define CSTD_TOOLCHAIN_RVCT_GCC_MODE    1
+	#endif
+
+	#if (__GNUC__ > 4) || (__GNUC__ == 4 && __GNUC_MINOR__ >= 6 && MALI_GCC_WORKAROUND_MIDCOM_4598 == 0)
+		/* As a workaround to MIDCOM-4598 (GCC internal defect), these pragmas are not compiled if the GCC version 
+		 * is within a certain range, or if a #define is enabled by the build system.  For more, see a comment 
+		 * in the build system also referring to the MIDCOM issue mentioned, where the environment is updated 
+		 * for the GNU toolchain.  */
+		#undef CSTD_PUSH_WARNING_GCC_WADDRESS
+		#define CSTD_PUSH_WARNING_GCC_WADDRESS \
+			do\
+			{\
+				_Pragma("GCC diagnostic push")\
+				_Pragma("GCC diagnostic ignored \"-Waddress\"")\
+			}while(MALI_FALSE)
+
+		#undef CSTD_POP_WARNING_GCC_WADDRESS
+		#define CSTD_POP_WARNING_GCC_WADDRESS \
+			do\
+			{\
+				_Pragma("GCC diagnostic pop")\
+			}while(MALI_FALSE)
+	#endif
+
+#elif defined(__ARMCC_VERSION)
+	#undef CSTD_TOOLCHAIN_RVCT
+	#define CSTD_TOOLCHAIN_RVCT         1
+
+#else
+	#warning "Unsupported or unknown toolchain"
+
+#endif
+
+/* ============================================================================
+	Determine the processor
+============================================================================ */
+#if 1 == CSTD_TOOLCHAIN_MSVC
+	#if defined(_M_IX86)
+		#undef CSTD_CPU_X86_32
+		#define CSTD_CPU_X86_32         1
+
+	#elif defined(_M_X64) || defined(_M_AMD64)
+		#undef CSTD_CPU_X86_64
+		#define CSTD_CPU_X86_64         1
+
+	#elif defined(_M_ARM)
+		#undef CSTD_CPU_ARM
+		#define CSTD_CPU_ARM            1
+
+	#elif defined(_M_MIPS)
+		#undef CSTD_CPU_MIPS
+		#define CSTD_CPU_MIPS           1
+
+	#else
+		#warning "Unsupported or unknown host CPU for MSVC tools"
+
+	#endif
+
+#elif 1 == CSTD_TOOLCHAIN_GCC
+	#if defined(__amd64__)
+		#undef CSTD_CPU_X86_64
+		#define CSTD_CPU_X86_64         1
+
+	#elif defined(__i386__)
+		#undef CSTD_CPU_X86_32
+		#define CSTD_CPU_X86_32         1
+
+	#elif defined(__arm__)
+		#undef CSTD_CPU_ARM
+		#define CSTD_CPU_ARM            1
+
+	#elif defined(__aarch64__)
+		#undef CSTD_CPU_AARCH64
+		#define CSTD_CPU_AARCH64        1
+
+	#elif defined(__mips__)
+		#undef CSTD_CPU_MIPS
+		#define CSTD_CPU_MIPS           1
+
+	#else
+		#warning "Unsupported or unknown host CPU for GCC tools"
+
+	#endif
+
+#elif 1 == CSTD_TOOLCHAIN_RVCT
+	#undef CSTD_CPU_ARM
+	#define CSTD_CPU_ARM                1
+
+#else
+	#warning "Unsupported or unknown toolchain"
+
+#endif
+
+/* ============================================================================
+	Determine the Processor Endianness
+============================================================================ */
+
+#if ((1 == CSTD_CPU_X86_32) || (1 == CSTD_CPU_X86_64))
+	/* Note: x86 and x86-64 are always little endian, so leave at default. */
+
+#elif 1 == CSTD_CPU_AARCH64
+	/* No big endian support? */
+
+#elif 1 == CSTD_TOOLCHAIN_RVCT
+	#if defined(__BIG_ENDIAN)
+		#undef CSTD_ENDIAN_BIG
+		#define CSTD_ENDIAN_BIG         1
+	#endif
+
+#elif ((1 == CSTD_TOOLCHAIN_GCC) && (1 == CSTD_CPU_ARM))
+	#if defined(__ARMEB__)
+		#undef CSTD_ENDIAN_BIG
+		#define CSTD_ENDIAN_BIG         1
+	#endif
+
+#elif ((1 == CSTD_TOOLCHAIN_GCC) && (1 == CSTD_CPU_MIPS))
+	#if defined(__MIPSEB__)
+		#undef CSTD_ENDIAN_BIG
+		#define CSTD_ENDIAN_BIG         1
+	#endif
+
+#elif 1 == CSTD_TOOLCHAIN_MSVC
+	/* Note: Microsoft only support little endian, so leave at default. */
+
+#else
+	#warning "Unsupported or unknown CPU"
+
+#endif
+
+/* ============================================================================
+	Determine the operating system and addressing width
+============================================================================ */
+#if 1 == CSTD_TOOLCHAIN_MSVC
+	#if defined(_WIN32) && !defined(_WIN64)
+		#undef CSTD_OS_WINDOWS
+		#define CSTD_OS_WINDOWS         1
+		#undef CSTD_OS_WIN32
+		#define CSTD_OS_WIN32           1
+		#undef CSTD_CPU_32BIT
+		#define CSTD_CPU_32BIT          1
+
+	#elif defined(_WIN32) && defined(_WIN64)
+		#undef CSTD_OS_WINDOWS
+		#define CSTD_OS_WINDOWS         1
+		#undef CSTD_OS_WIN64
+		#define CSTD_OS_WIN64           1
+		#undef CSTD_CPU_64BIT
+		#define CSTD_CPU_64BIT          1
+
+	#else
+		#warning "Unsupported or unknown host OS for MSVC tools"
+
+	#endif
+
+#elif 1 == CSTD_TOOLCHAIN_GCC
+	#if defined(_WIN32) && defined(_WIN64)
+		#undef CSTD_OS_WINDOWS
+		#define CSTD_OS_WINDOWS         1
+		#undef CSTD_OS_WIN64
+		#define CSTD_OS_WIN64           1
+		#undef CSTD_CPU_64BIT
+		#define CSTD_CPU_64BIT          1
+
+	#elif defined(_WIN32) && !defined(_WIN64)
+		#undef CSTD_OS_WINDOWS
+		#define CSTD_OS_WINDOWS         1
+		#undef CSTD_OS_WIN32
+		#define CSTD_OS_WIN32           1
+		#undef CSTD_CPU_32BIT
+		#define CSTD_CPU_32BIT          1
+
+	#elif defined(ANDROID)
+		#undef CSTD_OS_ANDROID
+		#define CSTD_OS_ANDROID         1
+
+		#if defined(__KERNEL__)
+			#undef CSTD_OS_ANDROID_KERNEL
+			#define CSTD_OS_ANDROID_KERNEL  1
+		#endif
+
+		#if defined(__LP64__) || defined(_LP64)
+			#undef CSTD_OS_ANDROID64
+			#define CSTD_OS_ANDROID64       1
+			#undef CSTD_CPU_64BIT
+			#define CSTD_CPU_64BIT          1
+		#else
+			#undef CSTD_OS_ANDROID32
+			#define CSTD_OS_ANDROID32       1
+			#undef CSTD_CPU_32BIT
+			#define CSTD_CPU_32BIT          1
+		#endif
+
+	#elif defined(__KERNEL__) || defined(__linux)
+		#undef CSTD_OS_LINUX
+		#define CSTD_OS_LINUX           1
+		
+		#if defined(__KERNEL__)
+			#undef CSTD_OS_LINUX_KERNEL
+			#define CSTD_OS_LINUX_KERNEL    1
+		#endif
+
+		#if defined(__LP64__) || defined(_LP64)
+			#undef CSTD_OS_LINUX64
+			#define CSTD_OS_LINUX64         1
+			#undef CSTD_CPU_64BIT
+			#define CSTD_CPU_64BIT          1
+		#else
+			#undef CSTD_OS_LINUX32
+			#define CSTD_OS_LINUX32         1
+			#undef CSTD_CPU_32BIT
+			#define CSTD_CPU_32BIT          1
+		#endif
+
+	#elif defined(__APPLE__)
+		#undef CSTD_OS_APPLEOS
+		#define CSTD_OS_APPLEOS         1
+
+		#if defined(__LP64__) || defined(_LP64)
+			#undef CSTD_OS_APPLEOS64
+			#define CSTD_OS_APPLEOS64       1
+			#undef CSTD_CPU_64BIT
+			#define CSTD_CPU_64BIT          1
+		#else
+			#undef CSTD_OS_APPLEOS32
+			#define CSTD_OS_APPLEOS32       1
+			#undef CSTD_CPU_32BIT
+			#define CSTD_CPU_32BIT          1
+		#endif
+
+	#elif defined(__SYMBIAN32__)
+		#undef CSTD_OS_SYMBIAN
+		#define CSTD_OS_SYMBIAN         1
+		#undef CSTD_CPU_32BIT
+		#define CSTD_CPU_32BIT          1
+
+	#else
+		#undef CSTD_OS_NONE
+		#define CSTD_OS_NONE            1
+		#undef CSTD_CPU_32BIT
+		#define CSTD_CPU_32BIT          1
+
+#endif
+
+#elif 1 == CSTD_TOOLCHAIN_RVCT
+
+	#if defined(ANDROID)
+		#undef CSTD_OS_ANDROID
+		#undef CSTD_OS_ANDROID32
+		#define CSTD_OS_ANDROID         1
+		#define CSTD_OS_ANDROID32       1
+
+	#elif defined(__linux)
+		#undef CSTD_OS_LINUX
+		#undef CSTD_OS_LINUX32
+		#define CSTD_OS_LINUX           1
+		#define CSTD_OS_LINUX32         1
+
+	#elif defined(__SYMBIAN32__)
+		#undef CSTD_OS_SYMBIAN
+		#define CSTD_OS_SYMBIAN         1
+
+	#else
+		#undef CSTD_OS_NONE
+		#define CSTD_OS_NONE            1
+
+#endif
+
+#else
+	#warning "Unsupported or unknown host OS"
+
+#endif
+
+/* ============================================================================
+	Determine the correct linker symbol Import and Export Macros
+============================================================================ */
+/**
+ * @defgroup arm_cstd_linkage_specifiers Linkage Specifiers
+ * @{
+ *
+ * This set of macros contain system-dependent linkage specifiers which
+ * determine the visibility of symbols across DLL boundaries. A header for a
+ * particular DLL should define a set of local macros derived from these,
+ * and should not use these macros to decorate functions directly as there may
+ * be multiple DLLs being used.
+ *
+ * These DLL library local macros should be (with appropriate library prefix)
+ * <tt>[MY_LIBRARY]_API</tt>, <tt>[MY_LIBRARY]_IMPL</tt>, and
+ * <tt>[MY_LIBRARY]_LOCAL</tt>.
+ *
+ *    - <tt>[MY_LIBRARY]_API</tt> should be use to decorate the function
+ *      declarations in the header. It should be defined as either
+ *      @c CSTD_LINK_IMPORT or @c CSTD_LINK_EXPORT, depending whether the
+ *      current situation is a compile of the DLL itself (use export) or a
+ *      compile of an external user of the DLL (use import).
+ *    - <tt>[MY_LIBRARY]_IMPL</tt> should be defined as @c CSTD_LINK_IMPL
+ *      and should be used to decorate the definition of functions in the C
+ *      file.
+ *    - <tt>[MY_LIBRARY]_LOCAL</tt> should be used to decorate function
+ *      declarations which are exported across translation units within the
+ *      DLL, but which are not exported outside of the DLL boundary.
+ *
+ * Functions which are @c static in either a C file or in a header file do not
+ * need any form of linkage decoration, and should therefore have no linkage
+ * macro applied to them.
+ */
+
+/**
+ * @def CSTD_LINK_IMPORT
+ * Specifies a function as being imported to a translation unit across a DLL
+ * boundary.
+ */
+
+/**
+ * @def CSTD_LINK_EXPORT
+ * Specifies a function as being exported across a DLL boundary by a
+ * translation unit.
+ */
+
+/**
+ * @def CSTD_LINK_IMPL
+ * Specifies a function which will be exported across a DLL boundary as
+ * being implemented by a translation unit.
+ */
+
+/**
+ * @def CSTD_LINK_LOCAL
+ * Specifies a function which is internal to a DLL, and which should not be
+ * exported outside of it.
+ */
+
+/**
+ * @}
+ */
+
+#if 1 ==  CSTD_OS_LINUX
+	#define CSTD_LINK_IMPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_EXPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_IMPL   __attribute__((visibility("default")))
+	#define CSTD_LINK_LOCAL  __attribute__((visibility("hidden")))
+
+#elif 1 ==  CSTD_OS_WINDOWS
+	#define CSTD_LINK_IMPORT __declspec(dllimport)
+	#define CSTD_LINK_EXPORT __declspec(dllexport)
+	#define CSTD_LINK_IMPL   __declspec(dllexport)
+	#define CSTD_LINK_LOCAL
+
+#elif 1 ==  CSTD_OS_SYMBIAN
+	#define CSTD_LINK_IMPORT IMPORT_C
+	#define CSTD_LINK_EXPORT IMPORT_C
+	#define CSTD_LINK_IMPL   EXPORT_C
+	#define CSTD_LINK_LOCAL
+
+#elif 1 ==  CSTD_OS_APPLEOS
+	#define CSTD_LINK_IMPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_EXPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_IMPL   __attribute__((visibility("default")))
+	#define CSTD_LINK_LOCAL  __attribute__((visibility("hidden")))
+
+#elif 1 ==  CSTD_OS_ANDROID
+	#define CSTD_LINK_IMPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_EXPORT __attribute__((visibility("default")))
+	#define CSTD_LINK_IMPL   __attribute__((visibility("default")))
+	#define CSTD_LINK_LOCAL  __attribute__((visibility("hidden")))
+
+#else /* CSTD_OS_NONE */
+	#define CSTD_LINK_IMPORT
+	#define CSTD_LINK_EXPORT
+	#define CSTD_LINK_IMPL
+	#define CSTD_LINK_LOCAL
+
+#endif
+
+/**
+ * @}
+ */
+
+#endif /* End (_ARM_CSTD_COMPILERS_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_pop.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_pop.h
new file mode 100644
index 0000000..20862ec
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_pop.h
@@ -0,0 +1,27 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_PACK_POP_H_
+#define _ARM_CSTD_PACK_POP_H_
+
+#if 1 == CSTD_TOOLCHAIN_MSVC
+	#include <poppack.h>
+#endif
+
+#endif /* End (_ARM_CSTD_PACK_POP_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_push.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_push.h
new file mode 100644
index 0000000..bc24e69
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_pack_push.h
@@ -0,0 +1,27 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_PACK_PUSH_H_
+#define _ARM_CSTD_PACK_PUSH_H_
+
+#if 1 == CSTD_TOOLCHAIN_MSVC
+	#include <pshpack1.h>
+#endif
+
+#endif /* End (_ARM_CSTD_PACK_PUSH_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types.h
new file mode 100644
index 0000000..efeefa5
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types.h
@@ -0,0 +1,33 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_TYPES_H_
+#define _ARM_CSTD_TYPES_H_
+
+#if 1 == CSTD_TOOLCHAIN_MSVC
+	#include "arm_cstd_types_msvc.h"
+#elif 1 == CSTD_TOOLCHAIN_GCC
+	#include "arm_cstd_types_gcc.h"
+#elif 1 == CSTD_TOOLCHAIN_RVCT
+	#include "arm_cstd_types_rvct.h"
+#else
+	#error "Toolchain not recognized"
+#endif
+
+#endif /* End (_ARM_CSTD_TYPES_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_gcc.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_gcc.h
new file mode 100644
index 0000000..510e799
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_gcc.h
@@ -0,0 +1,89 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_TYPES_GCC_H_
+#define _ARM_CSTD_TYPES_GCC_H_
+
+/* ============================================================================
+	Type definitions
+============================================================================ */
+/* All modern versions of GCC support stdint outside of C99 Mode. */
+/* However, Linux kernel limits what headers are available! */
+#if 1 == CSTD_OS_LINUX_KERNEL
+	#include <linux/kernel.h>
+	#include <linux/types.h>
+	#include <linux/stddef.h>
+	#include <linux/version.h>
+
+	/* Fix up any types which CSTD provdes but which Linux is missing. */
+	/* Note Linux assumes pointers are "long", so this is safe. */
+	typedef long                intptr_t;
+
+#else
+	#include <stdint.h>
+	#include <stddef.h>
+	#include <limits.h>
+#endif
+
+typedef uint32_t                bool_t;
+
+#if !defined(TRUE)
+	#define TRUE                ((bool_t)1)
+#endif
+
+#if !defined(FALSE)
+	#define FALSE               ((bool_t)0)
+#endif
+
+/* ============================================================================
+	Keywords
+============================================================================ */
+/* Doxygen documentation for these is in the RVCT header. */
+#define ASM                     __asm__
+
+#define INLINE                  __inline__
+
+#define FORCE_INLINE            __attribute__((__always_inline__)) __inline__
+
+#define NEVER_INLINE            __attribute__((__noinline__))
+
+#define PURE                    __attribute__((__pure__))
+
+#define PACKED                  __attribute__((__packed__))
+
+/* GCC does not support pointers to UNALIGNED data, so we do not define it to
+ * force a compile error if this macro is used. */
+
+#define RESTRICT                __restrict__
+
+/* RVCT in GCC mode does not support the CHECK_RESULT attribute. */
+#if 0 == CSTD_TOOLCHAIN_RVCT_GCC_MODE
+	#define CHECK_RESULT        __attribute__((__warn_unused_result__))
+#else
+	#define CHECK_RESULT
+#endif
+
+/* RVCT in GCC mode does not support the __func__ name outside of C99. */
+#if (0 == CSTD_TOOLCHAIN_RVCT_GCC_MODE)
+	#define CSTD_FUNC           __func__
+#else
+	#define CSTD_FUNC           __FUNCTION__
+#endif
+
+#endif /* End (_ARM_CSTD_TYPES_GCC_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_rvct.h b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_rvct.h
new file mode 100644
index 0000000..a8efda0
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/arm_cstd/arm_cstd_types_rvct.h
@@ -0,0 +1,192 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _ARM_CSTD_TYPES_RVCT_H_
+#define _ARM_CSTD_TYPES_RVCT_H_
+
+/* ============================================================================
+	Type definitions
+============================================================================ */
+#include <stddef.h>
+#include <limits.h>
+
+#if 199901L <= __STDC_VERSION__
+	#include <inttypes.h>
+#else
+	typedef unsigned char           uint8_t;
+	typedef signed char             int8_t;
+	typedef unsigned short          uint16_t;
+	typedef signed short            int16_t;
+	typedef unsigned int            uint32_t;
+	typedef signed int              int32_t;
+	typedef unsigned __int64        uint64_t;
+	typedef signed __int64          int64_t;
+	typedef ptrdiff_t               intptr_t;
+	typedef size_t                  uintptr_t;
+#endif
+
+typedef uint32_t                    bool_t;
+
+#if !defined(TRUE)
+	#define TRUE                ((bool_t)1)
+#endif
+
+#if !defined(FALSE)
+	#define FALSE               ((bool_t)0)
+#endif
+
+/* ============================================================================
+	Keywords
+============================================================================ */
+/**
+ * @addtogroup arm_cstd_coding_standard
+ * @{
+ */
+
+/**
+ * @def ASM
+ * @hideinitializer
+ * Mark an assembler block. Such blocks are often compiler specific, so often
+ * need to be surrounded in appropriate @c ifdef and @c endif blocks
+ * using the relevant @c CSTD_TOOLCHAIN macro.
+ */
+#define ASM                     __asm
+
+/**
+ * @def INLINE
+ * @hideinitializer
+ * Mark a definition as something which should be inlined. This is not always
+ * possible on a given compiler, and may be disabled at lower optimization
+ * levels.
+ */
+#define INLINE                  __inline
+
+/**
+ * @def FORCE_INLINE
+ * @hideinitializer
+ * Mark a definition as something which should be inlined. This provides a much
+ * stronger hint to the compiler than @c INLINE, and if supported should always
+ * result in an inlined function being emitted. If not supported this falls
+ * back to using the @c INLINE definition.
+ */
+#define FORCE_INLINE            __forceinline
+
+/**
+ * @def NEVER_INLINE
+ * @hideinitializer
+ * Mark a definition as something which should not be inlined. This provides a
+ * stronger hint to the compiler than the function should not be inlined,
+ * bypassing any heuristic rules the compiler normally applies. If not
+ * supported by a toolchain this falls back to being an empty macro.
+ */
+#define NEVER_INLINE            __declspec(noinline)
+
+/**
+ * @def PURE
+ * @hideinitializer
+ * Denotes that a function's return is only dependent on its inputs, enabling
+ * more efficient optimizations. Falls back to an empty macro if not supported.
+ */
+#define PURE                    __pure
+
+/**
+ * @def PACKED
+ * @hideinitializer
+ * Denotes that a structure should be stored in a packed form. This macro must
+ * be used in conjunction with the @c arm_cstd_pack_* headers for portability:
+ *
+ * @code
+ * #include <cstd/arm_cstd_pack_push.h>
+ *
+ * struct PACKED myStruct {
+ *     ...
+ * };
+ *
+ * #include <cstd/arm_cstd_pack_pop.h>
+ * PACKED
+ * @endcode
+ */
+#define PACKED                  __packed
+
+/**
+ * @def UNALIGNED
+ * @hideinitializer
+ * Denotes that a pointer points to a buffer with lower alignment than the
+ * natural alignment required by the C standard. This should only be used
+ * in extreme cases, as the emitted code is normally more efficient if memory
+ * is aligned.
+ *
+ * @warning This is \b NON-PORTABLE. The GNU tools are anti-unaligned pointers
+ * and have no support for such a construction.
+ */
+#define UNALIGNED               __packed
+
+/**
+ * @def RESTRICT
+ * @hideinitializer
+ * Denotes that a pointer does not overlap with any other points currently in
+ * scope, increasing the range of optimizations which can be performed by the
+ * compiler.
+ *
+ * @warning Specification of @c RESTRICT is a contract between the programmer
+ * and the compiler. If you place @c RESTICT on buffers which do actually
+ * overlap the behavior is undefined, and likely to vary at different
+ * optimization levels.!
+ */
+#define RESTRICT                __restrict
+
+/**
+ * @def CHECK_RESULT
+ * @hideinitializer
+ * Function attribute which causes a warning to be emitted if the compiler's
+ * return value is not used by the caller. Compiles to an empty macro if
+ * there is no supported mechanism for this check in the underlying compiler.
+ *
+ * @note At the time of writing this is only supported by GCC. RVCT does not
+ * support this attribute, even in GCC mode, so engineers are encouraged to
+ * compile their code using GCC even if primarily working with another
+ * compiler.
+ *
+ * @code
+ * CHECK_RESULT int my_func( void );
+ * @endcode
+  */
+#define CHECK_RESULT
+
+/**
+ * @def CSTD_FUNC
+ * Specify the @c CSTD_FUNC macro, a portable construct containing the name of
+ * the current function. On most compilers it is illegal to use this macro
+ * outside of a function scope. If not supported by the compiler we define
+ * @c CSTD_FUNC as an empty string.
+ *
+ * @warning Due to the implementation of this on most modern compilers this
+ * expands to a magically defined "static const" variable, not a constant
+ * string. This makes injecting @c CSTD_FUNC directly in to compile-time
+ * strings impossible, so if you want to make the function name part of a
+ * larger string you must use a printf-like function with a @c @%s template
+ * which is populated with @c CSTD_FUNC
+ */
+#define CSTD_FUNC            __FUNCTION__
+
+/**
+ * @}
+ */
+
+#endif /* End (_ARM_CSTD_TYPES_RVCT_H_) */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/mali_malisw.h b/drivers/gpu/mali-t6xx/r4p1/malisw/mali_malisw.h
new file mode 100644
index 0000000..ed1d07e
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/mali_malisw.h
@@ -0,0 +1,238 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _MALISW_H_
+#define _MALISW_H_
+
+/**
+ * @file mali_malisw.h
+ * Driver-wide include for common macros and types.
+ */
+
+/**
+ * @defgroup malisw Mali software definitions and types
+ * @{
+ */
+
+#include <stddef.h>
+
+#include "mali_stdtypes.h"
+
+/** @brief Gets the container object when given a pointer to a member of an object. */
+#define CONTAINER_OF(ptr, type, member) ((type *)((char *)(ptr) - offsetof(type,member)))
+
+/** @brief Gets the number of elements of type s in a fixed length array of s */
+#define NELEMS(s)       (sizeof(s)/sizeof((s)[0]))
+
+/**
+ * @brief The lesser of two values.
+ * May evaluate its arguments more than once.
+ * @see CSTD_MIN
+ */
+#define MIN(x,y) CSTD_MIN(x,y)
+
+/**
+ * @brief The greater of two values.
+ * May evaluate its arguments more than once.
+ * @see CSTD_MAX
+ */
+#define MAX(x,y) CSTD_MAX(x,y)
+
+/**
+ * @brief Clamp value x to within min and max inclusive
+ * May evaluate its arguments more than once.
+ * @see CSTD_CLAMP
+ */
+#define CLAMP( x, min, max ) CSTD_CLAMP( x, min, max )
+
+/**
+ * @brief Convert a pointer into a u64 for storing in a data structure.
+ * This is commonly used when pairing a 32-bit CPU with a 64-bit peripheral,
+ * such as a Midgard GPU. C's type promotion is complex and a straight cast
+ * does not work reliably as pointers are often considered as signed.
+ */
+#define PTR_TO_U64( x ) CSTD_PTR_TO_U64( x )
+
+/**
+ * @name Mali library linkage specifiers
+ * These directly map to the cstd versions described in detail here: @ref arm_cstd_linkage_specifiers
+ * @{
+ */
+#define MALI_IMPORT CSTD_LINK_IMPORT
+#define MALI_EXPORT CSTD_LINK_EXPORT
+#define MALI_IMPL   CSTD_LINK_IMPL
+#define MALI_LOCAL  CSTD_LINK_LOCAL
+
+/** @brief Decorate exported function prototypes.
+ *
+ * The file containing the implementation of the function should define this to be MALI_EXPORT before including
+ * malisw/mali_malisw.h.
+ */
+#ifndef MALI_API
+#define MALI_API MALI_IMPORT
+#endif
+/** @} */
+
+/** @name Testable static functions
+ * @{
+ *
+ * These macros can be used to allow functions to be static in release builds but exported from a shared library in unit
+ * test builds, allowing them to be tested or used to assist testing.
+ *
+ * Example mali_foo_bar.c containing the function to test:
+ *
+ * @code
+ * #define MALI_API MALI_EXPORT
+ *
+ * #include <malisw/mali_malisw.h>
+ * #include "mali_foo_testable_statics.h"
+ *
+ * MALI_TESTABLE_STATIC_IMPL void my_func()
+ * {
+ *	//Implementation
+ * }
+ * @endcode
+ *
+ * Example mali_foo_testable_statics.h:
+ *
+ * @code
+ * #if 1 == MALI_UNIT_TEST
+ * #include <malisw/mali_malisw.h>
+ *
+ * MALI_TESTABLE_STATIC_API void my_func();
+ *
+ * #endif
+ * @endcode
+ *
+ * Example mali_foo_tests.c:
+ *
+ * @code
+ * #include <foo/src/mali_foo_testable_statics.h>
+ *
+ * void my_test_func()
+ * {
+ * 	my_func();
+ * }
+ * @endcode
+ */
+
+/** @brief Decorate testable static function implementations.
+ *
+ * A header file containing a MALI_TESTABLE_STATIC_API-decorated prototype for each static function will be required
+ * when MALI_UNIT_TEST == 1 in order to link the function from the test.
+ */
+#if 1 == MALI_UNIT_TEST
+#define MALI_TESTABLE_STATIC_IMPL MALI_IMPL
+#else
+#define MALI_TESTABLE_STATIC_IMPL static
+#endif
+
+/** @brief Decorate testable static function prototypes.
+ *
+ * @note Prototypes should @em only be declared when MALI_UNIT_TEST == 1
+ */
+#define MALI_TESTABLE_STATIC_API MALI_API
+/** @} */
+
+/** @name Testable local functions
+ * @{
+ *
+ * These macros can be used to allow functions to be local to a shared library in release builds but be exported in unit
+ * test builds, allowing them to be tested or used to assist testing.
+ *
+ * Example mali_foo_bar.c containing the function to test:
+ *
+ * @code
+ * #define MALI_API MALI_EXPORT
+ *
+ * #include <malisw/mali_malisw.h>
+ * #include "mali_foo_bar.h"
+ *
+ * MALI_TESTABLE_LOCAL_IMPL void my_func()
+ * {
+ *	//Implementation
+ * }
+ * @endcode
+ *
+ * Example mali_foo_bar.h:
+ *
+ * @code
+ * #include <malisw/mali_malisw.h>
+ *
+ * MALI_TESTABLE_LOCAL_API void my_func();
+ *
+ * @endcode
+ *
+ * Example mali_foo_tests.c:
+ *
+ * @code
+ * #include <foo/src/mali_foo_bar.h>
+ *
+ * void my_test_func()
+ * {
+ * 	my_func();
+ * }
+ * @endcode
+ */
+
+/** @brief Decorate testable local function implementations.
+ *
+ * This can be used to have a function normally local to the shared library except in debug builds where it will be
+ * exported.
+ */
+#ifdef CONFIG_MALI_DEBUG
+#define MALI_TESTABLE_LOCAL_IMPL MALI_IMPL
+#else
+#define MALI_TESTABLE_LOCAL_IMPL MALI_LOCAL
+#endif /* CONFIG_MALI_DEBUG */
+
+/** @brief Decorate testable local function prototypes.
+ *
+ * This can be used to have a function normally local to the shared library except in debug builds where it will be
+ * exported.
+ */
+#ifdef CONFIG_MALI_DEBUG
+#define MALI_TESTABLE_LOCAL_API MALI_API
+#else
+#define MALI_TESTABLE_LOCAL_API MALI_LOCAL
+#endif /* CONFIG_MALI_DEBUG */
+/** @} */
+
+/**
+ * Flag a cast as a reinterpretation, usually of a pointer type.
+ * @see CSTD_REINTERPRET_CAST
+ */
+#define REINTERPRET_CAST(type) CSTD_REINTERPRET_CAST(type)
+
+/**
+ * Flag a cast as casting away const, usually of a pointer type.
+ * @see CSTD_CONST_CAST
+ */
+#define CONST_CAST(type) (type) CSTD_CONST_CAST(type)
+
+/**
+ * Flag a cast as a (potentially complex) value conversion, usually of a numerical type.
+ * @see CSTD_STATIC_CAST
+ */
+#define STATIC_CAST(type) (type) CSTD_STATIC_CAST(type)
+
+
+/** @} */
+
+#endif /* _MALISW_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/malisw/mali_stdtypes.h b/drivers/gpu/mali-t6xx/r4p1/malisw/mali_stdtypes.h
new file mode 100644
index 0000000..6253112
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/malisw/mali_stdtypes.h
@@ -0,0 +1,254 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _MALISW_STDTYPES_H_
+#define _MALISW_STDTYPES_H_
+
+/**
+ * @file mali_stdtypes.h
+ * This file defines the standard types used by the Mali codebase.
+ */
+
+/**
+ * @addtogroup malisw
+ * @{
+ */
+
+/**
+ * @defgroup malisw_stdtypes Mali software standard types
+ *
+ * Basic driver-wide types.
+ */
+
+/**
+ * @addtogroup malisw_stdtypes
+ * @{
+ */
+
+#include "arm_cstd/arm_cstd.h"
+
+/**
+ * @name Scalar types.
+ * These are the scalar types used within the mali driver.
+ * @{
+ */
+/* Note: if compiling the Linux kernel then avoid redefining these. */
+#if 0 == CSTD_OS_LINUX_KERNEL
+	typedef uint64_t u64;
+	typedef uint32_t u32;
+	typedef uint16_t u16;
+	typedef uint8_t  u8;
+	
+	typedef int64_t  s64;
+	typedef int32_t  s32;
+	typedef int16_t  s16;
+	typedef int8_t   s8;
+#endif
+
+typedef double   f64;
+typedef float    f32;
+typedef u16      f16;
+
+typedef u32      mali_fixed16_16;
+/* @} */
+
+/**
+ * @name Boolean types.
+ * The intended use is for bool8 to be used when storing boolean values in
+ * structures, casting to mali_bool to be used in code sections.
+ * @{
+ */
+typedef bool_t     mali_bool;
+typedef u8         mali_bool8;
+
+#define MALI_FALSE FALSE
+#define MALI_TRUE  TRUE
+/* @} */
+
+/**
+ * @name Integer bounding values
+ * Maximum and minimum values for integer types
+ * @{
+ */
+#ifndef U64_MAX
+#define U64_MAX	 UINT64_MAX
+#endif
+#ifndef U32_MAX
+#define U32_MAX	 UINT32_MAX
+#endif
+#ifndef U16_MAX
+#define U16_MAX	 UINT16_MAX
+#endif
+#ifndef U8_MAX
+#define U8_MAX	 UINT8_MAX
+#endif
+
+#ifndef S64_MAX
+#define S64_MAX  INT64_MAX
+#endif
+#ifndef S64_MIN
+#define S64_MIN  INT64_MIN
+#endif
+#ifndef S32_MAX
+#define S32_MAX  INT32_MAX
+#endif
+#ifndef S32_MIN
+#define S32_MIN  INT32_MIN
+#endif
+#ifndef S16_MAX
+#define S16_MAX  INT16_MAX
+#endif
+#ifndef S16_MIN
+#define S16_MIN  INT16_MIN
+#endif
+#ifndef S8_MAX
+#define S8_MAX   INT8_MAX
+#endif
+#ifndef S8_MIN
+#define S8_MIN   INT8_MIN
+#endif
+/* @} */
+
+/**
+ * @name GPU address types
+ * Types for integers which hold a GPU pointer or GPU pointer offsets.
+ * @{
+ */
+typedef u64      mali_addr64;
+typedef u32      mali_addr32;
+typedef u64      mali_size64;
+typedef s64      mali_offset64;
+/* 32 bit offsets and sizes are always for native types and so use ptrdiff_t and size_t respectively */
+/* @} */
+
+/**
+ * @name Mali error types
+ * @brief The common error type for the mali drivers
+ * The mali_error type, all driver error handling should be of this type unless
+ * it must deal with a specific APIs error type.
+ * @{
+ */
+typedef enum
+{
+	/**
+	 * @brief Common Mali errors for the entire driver
+	 * MALI_ERROR_NONE is guaranteed to be 0.
+	 * @{
+	 */
+	MALI_ERROR_NONE = 0,
+	MALI_ERROR_OUT_OF_GPU_MEMORY,
+	MALI_ERROR_OUT_OF_MEMORY,
+	MALI_ERROR_FUNCTION_FAILED,
+	/* @} */
+	/**
+	 * @brief Mali errors for Client APIs to pass to EGL when creating EGLImages
+	 * These errors must only be returned to EGL from one of the Client APIs as part of the
+	 * (clientapi)_egl_image_interface.h
+	 * @{
+	 */
+	MALI_ERROR_EGLP_BAD_ACCESS,
+	MALI_ERROR_EGLP_BAD_PARAMETER,
+	/* @} */
+	/**
+	 * @brief Mali errors for the MCL module.
+	 * These errors must only be used within the private components of the OpenCL implementation that report
+	 * directly to API functions for cases where errors cannot be detected in the entrypoints file. They must
+	 * not be passed between driver components.
+	 * These are errors in the mali error space specifically for the MCL module, hence the MCLP prefix.
+	 * @{
+	 */
+	MALI_ERROR_MCLP_DEVICE_NOT_FOUND,
+	MALI_ERROR_MCLP_DEVICE_NOT_AVAILABLE,
+	MALI_ERROR_MCLP_COMPILER_NOT_AVAILABLE,
+	MALI_ERROR_MCLP_MEM_OBJECT_ALLOCATION_FAILURE,
+	MALI_ERROR_MCLP_PROFILING_INFO_NOT_AVAILABLE,
+	MALI_ERROR_MCLP_MEM_COPY_OVERLAP,
+	MALI_ERROR_MCLP_IMAGE_FORMAT_MISMATCH,
+	MALI_ERROR_MCLP_IMAGE_FORMAT_NOT_SUPPORTED,
+	MALI_ERROR_MCLP_BUILD_PROGRAM_FAILURE,
+	MALI_ERROR_MCLP_MAP_FAILURE,
+	MALI_ERROR_MCLP_MISALIGNED_SUB_BUFFER_OFFSET,
+	MALI_ERROR_MCLP_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST,
+	MALI_ERROR_MCLP_INVALID_VALUE,
+	MALI_ERROR_MCLP_INVALID_DEVICE_TYPE,
+	MALI_ERROR_MCLP_INVALID_PLATFORM,
+	MALI_ERROR_MCLP_INVALID_DEVICE,
+	MALI_ERROR_MCLP_INVALID_CONTEXT,
+	MALI_ERROR_MCLP_INVALID_QUEUE_PROPERTIES,
+	MALI_ERROR_MCLP_INVALID_COMMAND_QUEUE,
+	MALI_ERROR_MCLP_INVALID_HOST_PTR,
+	MALI_ERROR_MCLP_INVALID_MEM_OBJECT,
+	MALI_ERROR_MCLP_INVALID_IMAGE_FORMAT_DESCRIPTOR,
+	MALI_ERROR_MCLP_INVALID_IMAGE_SIZE,
+	MALI_ERROR_MCLP_INVALID_SAMPLER,
+	MALI_ERROR_MCLP_INVALID_BINARY,
+	MALI_ERROR_MCLP_INVALID_BUILD_OPTIONS,
+	MALI_ERROR_MCLP_INVALID_PROGRAM,
+	MALI_ERROR_MCLP_INVALID_PROGRAM_EXECUTABLE,
+	MALI_ERROR_MCLP_INVALID_KERNEL_NAME,
+	MALI_ERROR_MCLP_INVALID_KERNEL_DEFINITION,
+	MALI_ERROR_MCLP_INVALID_KERNEL,
+	MALI_ERROR_MCLP_INVALID_ARG_INDEX,
+	MALI_ERROR_MCLP_INVALID_ARG_VALUE,
+	MALI_ERROR_MCLP_INVALID_ARG_SIZE,
+	MALI_ERROR_MCLP_INVALID_KERNEL_ARGS,
+	MALI_ERROR_MCLP_INVALID_WORK_DIMENSION,
+	MALI_ERROR_MCLP_INVALID_WORK_GROUP_SIZE,
+	MALI_ERROR_MCLP_INVALID_WORK_ITEM_SIZE,
+	MALI_ERROR_MCLP_INVALID_GLOBAL_OFFSET,
+	MALI_ERROR_MCLP_INVALID_EVENT_WAIT_LIST,
+	MALI_ERROR_MCLP_INVALID_EVENT,
+	MALI_ERROR_MCLP_INVALID_OPERATION,
+	MALI_ERROR_MCLP_INVALID_GL_OBJECT,
+	MALI_ERROR_MCLP_INVALID_BUFFER_SIZE,
+	MALI_ERROR_MCLP_INVALID_MIP_LEVEL,
+	MALI_ERROR_MCLP_INVALID_GLOBAL_WORK_SIZE,
+	MALI_ERROR_MCLP_INVALID_GL_SHAREGROUP_REFERENCE_KHR,
+	MALI_ERROR_MCLP_INVALID_EGL_OBJECT,
+	/* @} */
+	/**
+	 * @brief Mali errors for the BASE module
+	 * These errors must only be used within the private components of the Base implementation. They will not
+	 * passed to other modules by the base driver.
+	 * These are errors in the mali error space specifically for the BASE module, hence the BASEP prefix.
+	 * @{
+	 */
+	MALI_ERROR_BASEP_INVALID_FUNCTION,
+	/* @} */
+	/** A dependency exists upon a resource that the client application wants to modify, so the driver must either
+	 * create a copy of the resource (if possible) or block until the dependency has been satisfied.
+	 */
+	MALI_ERROR_RESOURCE_IN_USE,
+
+	/**
+	 * @brief A stride value was too big.
+	 *
+	 * A surface descriptor can store strides of up to 2<sup>31</sup>-1 bytes but strides greater than
+	 * 2<sup>28</sup>-1 bytes cannot be expressed in bits without overflow.
+	 */
+	MALI_ERROR_STRIDE_TOO_BIG
+
+} mali_error;
+/* @} */
+
+/* @} */
+
+/* @} */
+
+#endif /* _MALISW_STDTYPES_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/Kbuild b/drivers/gpu/mali-t6xx/r4p1/platform/Kbuild
new file mode 100644
index 0000000..558657b
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/Kbuild
@@ -0,0 +1,21 @@
+#
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+ifeq ($(CONFIG_MALI_PLATFORM_THIRDPARTY),y)
+# remove begin and end quotes from the Kconfig string type
+	platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+	obj-y += $(platform_name)/
+endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/Kconfig b/drivers/gpu/mali-t6xx/r4p1/platform/Kconfig
new file mode 100644
index 0000000..8fb4e91
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/Kconfig
@@ -0,0 +1,24 @@
+#
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+
+# Add your platform specific Kconfig file here
+#
+# "drivers/gpu/arm/midgard/platform/xxx/Kconfig"
+#
+# Where xxx is the platform name is the name set in MALI_PLATFORM_THIRDPARTY_NAME
+#
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/Kbuild b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/Kbuild
new file mode 100644
index 0000000..34e93b8
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/Kbuild
@@ -0,0 +1,23 @@
+#
+# (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+ifeq ($(CONFIG_MALI_MIDGARD),y)
+obj-y += mali_kbase_config_fujitsu_mb86s70.o
+obj-y += mali_kbase_cpu_fujitsu_mb86s70.o
+else
+platform_name := $(shell echo $(CONFIG_MALI_PLATFORM_THIRDPARTY_NAME))
+SRC += platform/$(platform_name)/mali_kbase_config_fujitsu_mb86s70.o
+SRC += platform/$(platform_name)/mali_kbase_cpu_fujitsu_mb86s70.o
+endif
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_config_fujitsu_mb86s70.c b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_config_fujitsu_mb86s70.c
new file mode 100644
index 0000000..b53130c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_config_fujitsu_mb86s70.c
@@ -0,0 +1,323 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/ioport.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_config.h>
+#include "mali_kbase_cpu_fujitsu_mb86s70.h"
+
+/* Versatile Express (VE) configuration defaults shared between config_attributes[]
+ * and config_attributes_hw_issue_8408[]. Settings are not shared for
+ * JS_HARD_STOP_TICKS_SS and JS_RESET_TICKS_SS.
+ */
+#define KBASE_VE_GPU_FREQ_KHZ_MAX               5000
+#define KBASE_VE_GPU_FREQ_KHZ_MIN               5000
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG    15000000u      /* 15ms, an agressive tick for testing purposes. This will reduce performance significantly */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG       1	/* between 15ms and 30ms before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG    1	/* between 15ms and 30ms before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG    333	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG 2000	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG    166	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG   100000	/* 1500s (25mins) before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS_DEBUG        500	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG   3000	/* 7.5s before resetting GPU - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL_DEBUG        500	/* 45s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS_DEBUG       100166	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS          1250000000u	/* 1.25s */
+#define KBASE_VE_JS_SOFT_STOP_TICKS             2	/* 2.5s before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL          1	/* 1.25s before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS          4	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401     24	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL          2	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS         1200	/* 1500s before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS              6	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401         36	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL              3	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS             1201	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_RESET_TIMEOUT_MS            3000	/* 3s before cancelling stuck jobs */
+#define KBASE_VE_JS_CTX_TIMESLICE_NS            1000000	/* 1ms - an agressive timeslice for testing purposes (causes lots of scheduling out for >4 ctxs) */
+#define KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE	((uintptr_t)MALI_FALSE)	/* By default we prefer performance over security on r0p0-15dev0 and KBASE_CONFIG_ATTR_ earlier */
+#define KBASE_VE_POWER_MANAGEMENT_CALLBACKS     ((uintptr_t)&pm_callbacks)
+#define KBASE_VE_CPU_SPEED_FUNC                 ((uintptr_t)&kbase_get_vexpress_cpu_clock_speed)
+
+#define HARD_RESET_AT_POWER_OFF 0
+
+#ifndef CONFIG_OF
+static kbase_io_resources io_resources = {
+	.job_irq_number = 68,
+	.mmu_irq_number = 69,
+	.gpu_irq_number = 70,
+	.io_memory_region = {
+			     .start = 0xFC010000,
+			     .end = 0xFC010000 + (4096 * 4) - 1}
+};
+#endif
+
+static int pm_callback_power_on(kbase_device *kbdev)
+{
+	/* Nothing is needed on VExpress, but we may have destroyed GPU state (if the below HARD_RESET code is active) */
+	return 1;
+}
+
+static void pm_callback_power_off(kbase_device *kbdev)
+{
+#if HARD_RESET_AT_POWER_OFF
+	/* Cause a GPU hard reset to test whether we have actually idled the GPU
+	 * and that we properly reconfigure the GPU on power up.
+	 * Usually this would be dangerous, but if the GPU is working correctly it should
+	 * be completely safe as the GPU should not be active at this point.
+	 * However this is disabled normally because it will most likely interfere with
+	 * bus logging etc.
+	 */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_os_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET);
+#endif
+}
+
+static kbase_pm_callback_conf pm_callbacks = {
+	.power_on_callback = pm_callback_power_on,
+	.power_off_callback = pm_callback_power_off,
+	.power_suspend_callback  = NULL,
+	.power_resume_callback = NULL
+};
+
+/* Please keep table config_attributes in sync with config_attributes_hw_issue_8408 */
+static kbase_attribute config_attributes[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US,
+	 20},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+/* as config_attributes array above except with different settings for
+ * JS_HARD_STOP_TICKS_SS, JS_RESET_TICKS_SS that
+ * are needed for BASE_HW_ISSUE_8408.
+ */
+kbase_attribute config_attributes_hw_issue_8408[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+static kbase_platform_config versatile_platform_config = {
+	.attributes = config_attributes,
+#ifndef CONFIG_OF
+	.io_resources = &io_resources
+#endif
+};
+
+kbase_platform_config *kbase_get_platform_config(void)
+{
+	return &versatile_platform_config;
+}
+
+int kbase_platform_early_init(void)
+{
+	/* Nothing needed at this stage */
+	return 0;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.c b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.c
new file mode 100644
index 0000000..a3e3362
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.c
@@ -0,0 +1,184 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/io.h>
+#include <mali_kbase.h>
+#include "mali_kbase_cpu_fujitsu_mb86s70.h"
+
+#define HZ_IN_MHZ					    (1000000)
+
+#define CORETILE_EXPRESS_A9X4_SCC_START	(0x100E2000)
+#define MOTHERBOARD_SYS_CFG_START		(0x10000000)
+#define SYS_CFGDATA_OFFSET				(0x000000A0)
+#define SYS_CFGCTRL_OFFSET				(0x000000A4)
+#define SYS_CFGSTAT_OFFSET				(0x000000A8)
+
+#define SYS_CFGCTRL_START_BIT_VALUE		  (1 << 31)
+#define READ_REG_BIT_VALUE				  (0 << 30)
+#define DCC_DEFAULT_BIT_VALUE			  (0 << 26)
+#define SYS_CFG_OSC_FUNC_BIT_VALUE		  (1 << 20)
+#define SITE_DEFAULT_BIT_VALUE			  (1 << 16)
+#define BOARD_STACK_POS_DEFAULT_BIT_VALUE (0 << 12)
+#define DEVICE_DEFAULT_BIT_VALUE	      (2 <<  0)
+#define SYS_CFG_COMPLETE_BIT_VALUE		  (1 <<  0)
+#define SYS_CFG_ERROR_BIT_VALUE			  (1 <<  1)
+
+#define FEED_REG_BIT_MASK				(0x0F)
+#define FCLK_PA_DIVIDE_BIT_SHIFT		(0x03)
+#define FCLK_PB_DIVIDE_BIT_SHIFT		(0x07)
+#define FCLK_PC_DIVIDE_BIT_SHIFT		(0x0B)
+#define AXICLK_PA_DIVIDE_BIT_SHIFT		(0x0F)
+#define AXICLK_PB_DIVIDE_BIT_SHIFT		(0x13)
+
+#define IS_SINGLE_BIT_SET(val, pos)		(val&(1<<pos))
+
+#define CPU_CLOCK_SPEED_UNDEFINED 0
+
+//static u32 cpu_clock_speed = CPU_CLOCK_SPEED_UNDEFINED;
+
+//static DEFINE_RAW_SPINLOCK(syscfg_lock);
+/**
+ * kbase_get_vendor_specific_cpu_clock_speed
+ * @brief  Retrieves the CPU clock speed.
+ *         The implementation is platform specific.
+ * @param[out]    cpu_clock - the value of CPU clock speed in MHz
+ * @return        0 on success, 1 otherwise
+*/
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock)
+{
+
+	*cpu_clock = 800;
+        return 0;
+#if 0
+
+	if (CPU_CLOCK_SPEED_UNDEFINED != cpu_clock_speed)
+	{
+		*cpu_clock = cpu_clock_speed;
+		return 0;
+	}
+	else
+	{
+		int result = 0;
+		u32 reg_val = 0;
+		u32 osc2_value = 0;
+		u32 pa_divide = 0;
+		u32 pb_divide = 0;
+		u32 pc_divide = 0;
+		void *volatile pSysCfgReg = 0;
+		void *volatile pSCCReg = 0;
+
+		/* Init the value case something goes wrong */
+		*cpu_clock = 0;
+
+		/* Map CPU register into virtual memory */
+		pSysCfgReg = ioremap(MOTHERBOARD_SYS_CFG_START, 0x1000);
+		if (pSysCfgReg == NULL) {
+			result = 1;
+
+			goto pSysCfgReg_map_failed;
+		}
+
+		pSCCReg = ioremap(CORETILE_EXPRESS_A9X4_SCC_START, 0x1000);
+		if (pSCCReg == NULL) {
+			result = 1;
+
+			goto pSCCReg_map_failed;
+		}
+
+		raw_spin_lock(&syscfg_lock);
+
+		/*Read SYS regs - OSC2 */
+		reg_val = readl(pSysCfgReg + SYS_CFGCTRL_OFFSET);
+
+		/*Verify if there is no other undergoing request */
+		if (!(reg_val & SYS_CFGCTRL_START_BIT_VALUE)) {
+			/*Reset the CGFGSTAT reg */
+			writel(0, (pSysCfgReg + SYS_CFGSTAT_OFFSET));
+
+			writel(SYS_CFGCTRL_START_BIT_VALUE | READ_REG_BIT_VALUE | DCC_DEFAULT_BIT_VALUE | SYS_CFG_OSC_FUNC_BIT_VALUE | SITE_DEFAULT_BIT_VALUE | BOARD_STACK_POS_DEFAULT_BIT_VALUE | DEVICE_DEFAULT_BIT_VALUE, (pSysCfgReg + SYS_CFGCTRL_OFFSET));
+			/* Wait for the transaction to complete */
+			while (!(readl(pSysCfgReg + SYS_CFGSTAT_OFFSET) & SYS_CFG_COMPLETE_BIT_VALUE))
+				;
+			/* Read SYS_CFGSTAT Register to get the status of submitted transaction */
+			reg_val = readl(pSysCfgReg + SYS_CFGSTAT_OFFSET);
+
+			/*------------------------------------------------------------------------------------------*/
+			/* Check for possible errors */
+			if (reg_val & SYS_CFG_ERROR_BIT_VALUE) {
+				/* Error while setting register */
+				result = 1;
+			} else {
+				osc2_value = readl(pSysCfgReg + SYS_CFGDATA_OFFSET);
+				/* Read the SCC CFGRW0 register */
+				reg_val = readl(pSCCReg);
+
+				/*
+				   Select the appropriate feed:
+				   CFGRW0[0] - CLKOB
+				   CFGRW0[1] - CLKOC
+				   CFGRW0[2] - FACLK (CLK)B FROM AXICLK PLL)
+				 */
+				/* Calculate the  FCLK */
+				if (IS_SINGLE_BIT_SET(reg_val, 0)) {	/*CFGRW0[0] - CLKOB */
+					/* CFGRW0[6:3] */
+					pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+					/* CFGRW0[10:7] */
+					pb_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PB_DIVIDE_BIT_SHIFT)) >> FCLK_PB_DIVIDE_BIT_SHIFT);
+					*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+				} else {
+					if (IS_SINGLE_BIT_SET(reg_val, 1)) {	/*CFGRW0[1] - CLKOC */
+						/* CFGRW0[6:3] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[14:11] */
+						pc_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PC_DIVIDE_BIT_SHIFT)) >> FCLK_PC_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pc_divide + 1);
+					} else if (IS_SINGLE_BIT_SET(reg_val, 2)) {	/*CFGRW0[2] - FACLK */
+						/* CFGRW0[18:15] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PA_DIVIDE_BIT_SHIFT)) >> AXICLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[22:19] */
+						pb_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PB_DIVIDE_BIT_SHIFT)) >> AXICLK_PB_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+					} else {
+						result = 1;
+					}
+				}
+			}
+		} else {
+			result = 1;
+		}
+		raw_spin_unlock(&syscfg_lock);
+		/* Convert result expressed in Hz to Mhz units. */
+		*cpu_clock /= HZ_IN_MHZ;
+		if(!result)
+		{
+			cpu_clock_speed = *cpu_clock;
+		}
+
+		/* Unmap memory */
+		iounmap(pSCCReg);
+
+	 pSCCReg_map_failed:
+		iounmap(pSysCfgReg);
+
+	 pSysCfgReg_map_failed:
+
+		return result;
+	}
+#endif
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.h b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.h
new file mode 100644
index 0000000..f607d18
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/fujitsu_mb86s70/mali_kbase_cpu_fujitsu_mb86s70.h
@@ -0,0 +1,28 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_CPU_VEXPRESS_H_
+#define _KBASE_CPU_VEXPRESS_H_
+
+/**
+ * Versatile Express implementation of @ref kbase_cpuprops_clock_speed_function.
+ */
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock);
+
+#endif				/* _KBASE_CPU_VEXPRESS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/mali_kbase_platform_common.h b/drivers/gpu/mali-t6xx/r4p1/platform/mali_kbase_platform_common.h
new file mode 100644
index 0000000..b0d8e32
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/mali_kbase_platform_common.h
@@ -0,0 +1,26 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+/**
+ * @brief Entry point to transfer control to a platform for early initialization
+ *
+ * This function is called early on in the initialization during execution of
+ * @ref kbase_driver_init.
+ *
+ * @return Zero to indicate success non-zero for failure.
+ */
+int kbase_platform_early_init(void);
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/Kbuild b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/Kbuild
new file mode 100644
index 0000000..084a156
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/Kbuild
@@ -0,0 +1,18 @@
+#
+# (C) COPYRIGHT 2012-2013 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+obj-y += mali_kbase_config_vexpress.o
+obj-y += mali_kbase_cpu_vexpress.o
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_config_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_config_vexpress.c
new file mode 100644
index 0000000..01d67fd
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_config_vexpress.c
@@ -0,0 +1,323 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/ioport.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_config.h>
+#include "mali_kbase_cpu_vexpress.h"
+
+/* Versatile Express (VE) configuration defaults shared between config_attributes[]
+ * and config_attributes_hw_issue_8408[]. Settings are not shared for
+ * JS_HARD_STOP_TICKS_SS and JS_RESET_TICKS_SS.
+ */
+#define KBASE_VE_GPU_FREQ_KHZ_MAX               5000
+#define KBASE_VE_GPU_FREQ_KHZ_MIN               5000
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG    15000000u      /* 15ms, an agressive tick for testing purposes. This will reduce performance significantly */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG       1	/* between 15ms and 30ms before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG    1	/* between 15ms and 30ms before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG    333	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG 2000	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG    166	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG   100000	/* 1500s (25mins) before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS_DEBUG        500	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG   3000	/* 7.5s before resetting GPU - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL_DEBUG        500	/* 45s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS_DEBUG       100166	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS          1250000000u	/* 1.25s */
+#define KBASE_VE_JS_SOFT_STOP_TICKS             2	/* 2.5s before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL          1	/* 1.25s before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS          4	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401     24	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL          2	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS         1200	/* 1500s before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS              6	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401         36	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL              3	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS             1201	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_RESET_TIMEOUT_MS            3000	/* 3s before cancelling stuck jobs */
+#define KBASE_VE_JS_CTX_TIMESLICE_NS            1000000	/* 1ms - an agressive timeslice for testing purposes (causes lots of scheduling out for >4 ctxs) */
+#define KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE	((uintptr_t)MALI_FALSE)	/* By default we prefer performance over security on r0p0-15dev0 and KBASE_CONFIG_ATTR_ earlier */
+#define KBASE_VE_POWER_MANAGEMENT_CALLBACKS     ((uintptr_t)&pm_callbacks)
+#define KBASE_VE_CPU_SPEED_FUNC                 ((uintptr_t)&kbase_get_vexpress_cpu_clock_speed)
+
+#define HARD_RESET_AT_POWER_OFF 0
+
+#ifndef CONFIG_OF
+static kbase_io_resources io_resources = {
+	.job_irq_number = 68,
+	.mmu_irq_number = 69,
+	.gpu_irq_number = 70,
+	.io_memory_region = {
+			     .start = 0xFC010000,
+			     .end = 0xFC010000 + (4096 * 4) - 1}
+};
+#endif
+
+static int pm_callback_power_on(kbase_device *kbdev)
+{
+	/* Nothing is needed on VExpress, but we may have destroyed GPU state (if the below HARD_RESET code is active) */
+	return 1;
+}
+
+static void pm_callback_power_off(kbase_device *kbdev)
+{
+#if HARD_RESET_AT_POWER_OFF
+	/* Cause a GPU hard reset to test whether we have actually idled the GPU
+	 * and that we properly reconfigure the GPU on power up.
+	 * Usually this would be dangerous, but if the GPU is working correctly it should
+	 * be completely safe as the GPU should not be active at this point.
+	 * However this is disabled normally because it will most likely interfere with
+	 * bus logging etc.
+	 */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_os_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET);
+#endif
+}
+
+static kbase_pm_callback_conf pm_callbacks = {
+	.power_on_callback = pm_callback_power_on,
+	.power_off_callback = pm_callback_power_off,
+	.power_suspend_callback  = NULL,
+	.power_resume_callback = NULL
+};
+
+/* Please keep table config_attributes in sync with config_attributes_hw_issue_8408 */
+static kbase_attribute config_attributes[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US,
+	 20},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+/* as config_attributes array above except with different settings for
+ * JS_HARD_STOP_TICKS_SS, JS_RESET_TICKS_SS that
+ * are needed for BASE_HW_ISSUE_8408.
+ */
+kbase_attribute config_attributes_hw_issue_8408[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+static kbase_platform_config versatile_platform_config = {
+	.attributes = config_attributes,
+#ifndef CONFIG_OF
+	.io_resources = &io_resources
+#endif
+};
+
+kbase_platform_config *kbase_get_platform_config(void)
+{
+	return &versatile_platform_config;
+}
+
+int kbase_platform_early_init(void)
+{
+	/* Nothing needed at this stage */
+	return 0;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.c
new file mode 100644
index 0000000..1b45d3c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.c
@@ -0,0 +1,180 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/io.h>
+#include <mali_kbase.h>
+#include "mali_kbase_cpu_vexpress.h"
+
+#define HZ_IN_MHZ					    (1000000)
+
+#define CORETILE_EXPRESS_A9X4_SCC_START	(0x100E2000)
+#define MOTHERBOARD_SYS_CFG_START		(0x10000000)
+#define SYS_CFGDATA_OFFSET				(0x000000A0)
+#define SYS_CFGCTRL_OFFSET				(0x000000A4)
+#define SYS_CFGSTAT_OFFSET				(0x000000A8)
+
+#define SYS_CFGCTRL_START_BIT_VALUE		  (1 << 31)
+#define READ_REG_BIT_VALUE				  (0 << 30)
+#define DCC_DEFAULT_BIT_VALUE			  (0 << 26)
+#define SYS_CFG_OSC_FUNC_BIT_VALUE		  (1 << 20)
+#define SITE_DEFAULT_BIT_VALUE			  (1 << 16)
+#define BOARD_STACK_POS_DEFAULT_BIT_VALUE (0 << 12)
+#define DEVICE_DEFAULT_BIT_VALUE	      (2 <<  0)
+#define SYS_CFG_COMPLETE_BIT_VALUE		  (1 <<  0)
+#define SYS_CFG_ERROR_BIT_VALUE			  (1 <<  1)
+
+#define FEED_REG_BIT_MASK				(0x0F)
+#define FCLK_PA_DIVIDE_BIT_SHIFT		(0x03)
+#define FCLK_PB_DIVIDE_BIT_SHIFT		(0x07)
+#define FCLK_PC_DIVIDE_BIT_SHIFT		(0x0B)
+#define AXICLK_PA_DIVIDE_BIT_SHIFT		(0x0F)
+#define AXICLK_PB_DIVIDE_BIT_SHIFT		(0x13)
+
+#define IS_SINGLE_BIT_SET(val, pos)		(val&(1<<pos))
+
+#define CPU_CLOCK_SPEED_UNDEFINED 0
+
+static u32 cpu_clock_speed = CPU_CLOCK_SPEED_UNDEFINED;
+
+static DEFINE_RAW_SPINLOCK(syscfg_lock);
+/**
+ * kbase_get_vendor_specific_cpu_clock_speed
+ * @brief  Retrieves the CPU clock speed.
+ *         The implementation is platform specific.
+ * @param[out]    cpu_clock - the value of CPU clock speed in MHz
+ * @return        0 on success, 1 otherwise
+*/
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock)
+{
+
+
+	if (CPU_CLOCK_SPEED_UNDEFINED != cpu_clock_speed)
+	{
+		*cpu_clock = cpu_clock_speed;
+		return 0;
+	}
+	else
+	{
+		int result = 0;
+		u32 reg_val = 0;
+		u32 osc2_value = 0;
+		u32 pa_divide = 0;
+		u32 pb_divide = 0;
+		u32 pc_divide = 0;
+		void *volatile pSysCfgReg = 0;
+		void *volatile pSCCReg = 0;
+
+		/* Init the value case something goes wrong */
+		*cpu_clock = 0;
+
+		/* Map CPU register into virtual memory */
+		pSysCfgReg = ioremap(MOTHERBOARD_SYS_CFG_START, 0x1000);
+		if (pSysCfgReg == NULL) {
+			result = 1;
+
+			goto pSysCfgReg_map_failed;
+		}
+
+		pSCCReg = ioremap(CORETILE_EXPRESS_A9X4_SCC_START, 0x1000);
+		if (pSCCReg == NULL) {
+			result = 1;
+
+			goto pSCCReg_map_failed;
+		}
+
+		raw_spin_lock(&syscfg_lock);
+
+		/*Read SYS regs - OSC2 */
+		reg_val = readl(pSysCfgReg + SYS_CFGCTRL_OFFSET);
+
+		/*Verify if there is no other undergoing request */
+		if (!(reg_val & SYS_CFGCTRL_START_BIT_VALUE)) {
+			/*Reset the CGFGSTAT reg */
+			writel(0, (pSysCfgReg + SYS_CFGSTAT_OFFSET));
+
+			writel(SYS_CFGCTRL_START_BIT_VALUE | READ_REG_BIT_VALUE | DCC_DEFAULT_BIT_VALUE | SYS_CFG_OSC_FUNC_BIT_VALUE | SITE_DEFAULT_BIT_VALUE | BOARD_STACK_POS_DEFAULT_BIT_VALUE | DEVICE_DEFAULT_BIT_VALUE, (pSysCfgReg + SYS_CFGCTRL_OFFSET));
+			/* Wait for the transaction to complete */
+			while (!(readl(pSysCfgReg + SYS_CFGSTAT_OFFSET) & SYS_CFG_COMPLETE_BIT_VALUE))
+				;
+			/* Read SYS_CFGSTAT Register to get the status of submitted transaction */
+			reg_val = readl(pSysCfgReg + SYS_CFGSTAT_OFFSET);
+
+			/*------------------------------------------------------------------------------------------*/
+			/* Check for possible errors */
+			if (reg_val & SYS_CFG_ERROR_BIT_VALUE) {
+				/* Error while setting register */
+				result = 1;
+			} else {
+				osc2_value = readl(pSysCfgReg + SYS_CFGDATA_OFFSET);
+				/* Read the SCC CFGRW0 register */
+				reg_val = readl(pSCCReg);
+
+				/*
+				   Select the appropriate feed:
+				   CFGRW0[0] - CLKOB
+				   CFGRW0[1] - CLKOC
+				   CFGRW0[2] - FACLK (CLK)B FROM AXICLK PLL)
+				 */
+				/* Calculate the  FCLK */
+				if (IS_SINGLE_BIT_SET(reg_val, 0)) {	/*CFGRW0[0] - CLKOB */
+					/* CFGRW0[6:3] */
+					pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+					/* CFGRW0[10:7] */
+					pb_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PB_DIVIDE_BIT_SHIFT)) >> FCLK_PB_DIVIDE_BIT_SHIFT);
+					*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+				} else {
+					if (IS_SINGLE_BIT_SET(reg_val, 1)) {	/*CFGRW0[1] - CLKOC */
+						/* CFGRW0[6:3] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[14:11] */
+						pc_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PC_DIVIDE_BIT_SHIFT)) >> FCLK_PC_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pc_divide + 1);
+					} else if (IS_SINGLE_BIT_SET(reg_val, 2)) {	/*CFGRW0[2] - FACLK */
+						/* CFGRW0[18:15] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PA_DIVIDE_BIT_SHIFT)) >> AXICLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[22:19] */
+						pb_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PB_DIVIDE_BIT_SHIFT)) >> AXICLK_PB_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+					} else {
+						result = 1;
+					}
+				}
+			}
+		} else {
+			result = 1;
+		}
+		raw_spin_unlock(&syscfg_lock);
+		/* Convert result expressed in Hz to Mhz units. */
+		*cpu_clock /= HZ_IN_MHZ;
+		if(!result)
+		{
+			cpu_clock_speed = *cpu_clock;
+		}
+
+		/* Unmap memory */
+		iounmap(pSCCReg);
+
+	 pSCCReg_map_failed:
+		iounmap(pSysCfgReg);
+
+	 pSysCfgReg_map_failed:
+
+		return result;
+	}
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.h b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.h
new file mode 100644
index 0000000..f607d18
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress/mali_kbase_cpu_vexpress.h
@@ -0,0 +1,28 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_CPU_VEXPRESS_H_
+#define _KBASE_CPU_VEXPRESS_H_
+
+/**
+ * Versatile Express implementation of @ref kbase_cpuprops_clock_speed_function.
+ */
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock);
+
+#endif				/* _KBASE_CPU_VEXPRESS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/Kbuild b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/Kbuild
new file mode 100644
index 0000000..0cb41ce
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/Kbuild
@@ -0,0 +1,18 @@
+#
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+
+obj-y += mali_kbase_config_vexpress.o
+obj-y += mali_kbase_cpu_vexpress.o
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_vexpress.c
new file mode 100644
index 0000000..57e86d5
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_config_vexpress.c
@@ -0,0 +1,324 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/ioport.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_config.h>
+#include "mali_kbase_cpu_vexpress.h"
+
+/* Versatile Express (VE) configuration defaults shared between config_attributes[]
+ * and config_attributes_hw_issue_8408[]. Settings are not shared for
+ * JS_HARD_STOP_TICKS_SS and JS_RESET_TICKS_SS.
+ */
+#define KBASE_VE_GPU_FREQ_KHZ_MAX               10000
+#define KBASE_VE_GPU_FREQ_KHZ_MIN               10000
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG    15000000u      /* 15ms, an agressive tick for testing purposes. This will reduce performance significantly */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG       1	/* between 15ms and 30ms before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG    1	/* between 15ms and 30ms before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG    333	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG 2000	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG    166	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG   100000	/* 1500s (25mins) before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS_DEBUG        500	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG   3000	/* 7.5s before resetting GPU - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL_DEBUG        500	/* 45s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS_DEBUG       100166	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS          1250000000u	/* 1.25s */
+#define KBASE_VE_JS_SOFT_STOP_TICKS             2	/* 2.5s before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL          1	/* 1.25s before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS          4	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401     24	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL          2	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS         1200	/* 1500s before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS              6	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401         36	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL              3	/* 3.75s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS             1201	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_RESET_TIMEOUT_MS            3000	/* 3s before cancelling stuck jobs */
+#define KBASE_VE_JS_CTX_TIMESLICE_NS            1000000	/* 1ms - an agressive timeslice for testing purposes (causes lots of scheduling out for >4 ctxs) */
+#define KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE	((uintptr_t)MALI_FALSE)	/* By default we prefer performance over security on r0p0-15dev0 and KBASE_CONFIG_ATTR_ earlier */
+#define KBASE_VE_POWER_MANAGEMENT_CALLBACKS     ((uintptr_t)&pm_callbacks)
+#define KBASE_VE_CPU_SPEED_FUNC                 ((uintptr_t)&kbase_get_vexpress_cpu_clock_speed)
+
+#define HARD_RESET_AT_POWER_OFF 0
+
+#ifndef CONFIG_OF
+static kbase_io_resources io_resources = {
+	.job_irq_number = 75,
+	.mmu_irq_number = 76,
+	.gpu_irq_number = 77,
+	.io_memory_region = {
+			     .start = 0x2F000000,
+			     .end = 0x2F000000 + (4096 * 4) - 1}
+};
+#endif
+
+static int pm_callback_power_on(kbase_device *kbdev)
+{
+	/* Nothing is needed on VExpress, but we may have destroyed GPU state (if the below HARD_RESET code is active) */
+	return 1;
+}
+
+static void pm_callback_power_off(kbase_device *kbdev)
+{
+#if HARD_RESET_AT_POWER_OFF
+	/* Cause a GPU hard reset to test whether we have actually idled the GPU
+	 * and that we properly reconfigure the GPU on power up.
+	 * Usually this would be dangerous, but if the GPU is working correctly it should
+	 * be completely safe as the GPU should not be active at this point.
+	 * However this is disabled normally because it will most likely interfere with
+	 * bus logging etc.
+	 */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_os_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET);
+#endif
+}
+
+static kbase_pm_callback_conf pm_callbacks = {
+	.power_on_callback = pm_callback_power_on,
+	.power_off_callback = pm_callback_power_off,
+	.power_suspend_callback  = NULL,
+	.power_resume_callback = NULL
+};
+
+/* Please keep table config_attributes in sync with config_attributes_hw_issue_8408 */
+static kbase_attribute config_attributes[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US,
+	 20},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+/* as config_attributes array above except with different settings for
+ * JS_HARD_STOP_TICKS_SS, JS_RESET_TICKS_SS that
+ * are needed for BASE_HW_ISSUE_8408.
+ */
+kbase_attribute config_attributes_hw_issue_8408[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+static kbase_platform_config versatile_platform_config = {
+	.attributes = config_attributes,
+#ifndef CONFIG_OF
+	.io_resources = &io_resources
+#endif
+};
+
+kbase_platform_config *kbase_get_platform_config(void)
+{
+	return &versatile_platform_config;
+}
+
+int kbase_platform_early_init(void)
+{
+	/* Nothing needed at this stage */
+	return 0;
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.c
new file mode 100644
index 0000000..1577f8c
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.c
@@ -0,0 +1,71 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#include <linux/io.h>
+#include <mali_kbase.h>
+#include "mali_kbase_cpu_vexpress.h"
+
+#define HZ_IN_MHZ					    (1000000)
+
+#define CORETILE_EXPRESS_A9X4_SCC_START	(0x100E2000)
+#define MOTHERBOARD_SYS_CFG_START		(0x10000000)
+#define SYS_CFGDATA_OFFSET				(0x000000A0)
+#define SYS_CFGCTRL_OFFSET				(0x000000A4)
+#define SYS_CFGSTAT_OFFSET				(0x000000A8)
+
+#define SYS_CFGCTRL_START_BIT_VALUE		  (1 << 31)
+#define READ_REG_BIT_VALUE				  (0 << 30)
+#define DCC_DEFAULT_BIT_VALUE			  (0 << 26)
+#define SYS_CFG_OSC_FUNC_BIT_VALUE		  (1 << 20)
+#define SITE_DEFAULT_BIT_VALUE			  (1 << 16)
+#define BOARD_STACK_POS_DEFAULT_BIT_VALUE (0 << 12)
+#define DEVICE_DEFAULT_BIT_VALUE	      (2 <<  0)
+#define SYS_CFG_COMPLETE_BIT_VALUE		  (1 <<  0)
+#define SYS_CFG_ERROR_BIT_VALUE			  (1 <<  1)
+
+#define FEED_REG_BIT_MASK				(0x0F)
+#define FCLK_PA_DIVIDE_BIT_SHIFT		(0x03)
+#define FCLK_PB_DIVIDE_BIT_SHIFT		(0x07)
+#define FCLK_PC_DIVIDE_BIT_SHIFT		(0x0B)
+#define AXICLK_PA_DIVIDE_BIT_SHIFT		(0x0F)
+#define AXICLK_PB_DIVIDE_BIT_SHIFT		(0x13)
+
+#define IS_SINGLE_BIT_SET(val, pos)		(val&(1<<pos))
+
+#define CPU_CLOCK_SPEED_UNDEFINED 0
+
+#define CPU_CLOCK_SPEED_6XV7 50
+
+static u32 cpu_clock_speed = CPU_CLOCK_SPEED_UNDEFINED;
+
+static DEFINE_RAW_SPINLOCK(syscfg_lock);
+/**
+ * kbase_get_vendor_specific_cpu_clock_speed
+ * @brief  Retrieves the CPU clock speed.
+ *         The implementation is platform specific.
+ * @param[out]    cpu_clock - the value of CPU clock speed in MHz
+ * @return        0 on success, 1 otherwise
+*/
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock)
+{
+	/* TODO: MIDBASE-2873 - Provide runtime detection of CPU clock freq for 6XV7 board */
+	*cpu_clock = CPU_CLOCK_SPEED_6XV7;
+
+	return 0;
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.h b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.h
new file mode 100644
index 0000000..f607d18
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_6xvirtex7_10mhz/mali_kbase_cpu_vexpress.h
@@ -0,0 +1,28 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+#ifndef _KBASE_CPU_VEXPRESS_H_
+#define _KBASE_CPU_VEXPRESS_H_
+
+/**
+ * Versatile Express implementation of @ref kbase_cpuprops_clock_speed_function.
+ */
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock);
+
+#endif				/* _KBASE_CPU_VEXPRESS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/Kbuild b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/Kbuild
new file mode 100644
index 0000000..32c7070
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/Kbuild
@@ -0,0 +1,17 @@
+#
+# (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+#
+# This program is free software and is provided to you under the terms of the
+# GNU General Public License version 2 as published by the Free Software
+# Foundation, and any use by you of this program is subject to the terms
+# of such GNU licence.
+#
+# A copy of the licence is included with the program, and can also be obtained
+# from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+# Boston, MA  02110-1301, USA.
+#
+#
+
+
+obj-y += mali_kbase_config_vexpress.o
+obj-y += mali_kbase_cpu_vexpress.o
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_config_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_config_vexpress.c
new file mode 100644
index 0000000..c403fde
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_config_vexpress.c
@@ -0,0 +1,323 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/ioport.h>
+#include <mali_kbase.h>
+#include <mali_kbase_defs.h>
+#include <mali_kbase_config.h>
+
+#include "mali_kbase_cpu_vexpress.h"
+
+/* Versatile Express (VE) configuration defaults shared between config_attributes[]
+ * and config_attributes_hw_issue_8408[]. Settings are not shared for
+ * JS_HARD_STOP_TICKS_SS and JS_RESET_TICKS_SS.
+ */
+#define KBASE_VE_GPU_FREQ_KHZ_MAX               40000
+#define KBASE_VE_GPU_FREQ_KHZ_MIN               40000
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG    15000000u      /* 15ms, an agressive tick for testing purposes. This will reduce performance significantly */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG       1	/* between 15ms and 30ms before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG    1	/* between 15ms and 30ms before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG    333	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG 2000	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG    166	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG   100000	/* 1500s (25mins) before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS_DEBUG        500	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG   3000	/* 7.5s before resetting GPU - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL_DEBUG        500	/* 45s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS_DEBUG       100166	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_SCHEDULING_TICK_NS          1250000000u	/* 1.25s */
+#define KBASE_VE_JS_SOFT_STOP_TICKS             2	/* 2.5s before soft-stop a job */
+#define KBASE_VE_JS_SOFT_STOP_TICKS_CL          1	/* 1.25s before soft-stop a CL job */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS          4	/* 5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_SS_8401     24	/* 30s before hard-stop, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_HARD_STOP_TICKS_CL          2	/* 2.5s before hard-stop */
+#define KBASE_VE_JS_HARD_STOP_TICKS_NSS         1200	/* 1500s before NSS hard-stop */
+#define KBASE_VE_JS_RESET_TICKS_SS              6	/* 7.5s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_SS_8401         36	/* 45s before resetting GPU, for a certain GLES2 test at 128x128 (bound by combined vertex+tiler job) - for issue 8401 */
+#define KBASE_VE_JS_RESET_TICKS_CL              3	/* 3.75s before resetting GPU */
+#define KBASE_VE_JS_RESET_TICKS_NSS             1201	/* 1502s before resetting GPU */
+
+#define KBASE_VE_JS_RESET_TIMEOUT_MS            3000	/* 3s before cancelling stuck jobs */
+#define KBASE_VE_JS_CTX_TIMESLICE_NS            1000000	/* 1ms - an agressive timeslice for testing purposes (causes lots of scheduling out for >4 ctxs) */
+#define KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE	((uintptr_t)MALI_FALSE)	/* By default we prefer performance over security on r0p0-15dev0 and KBASE_CONFIG_ATTR_ earlier */
+#define KBASE_VE_POWER_MANAGEMENT_CALLBACKS     ((uintptr_t)&pm_callbacks)
+#define KBASE_VE_CPU_SPEED_FUNC                 ((uintptr_t)&kbase_get_vexpress_cpu_clock_speed)
+
+#define HARD_RESET_AT_POWER_OFF 0
+
+#ifndef CONFIG_OF
+static kbase_io_resources io_resources = {
+	.job_irq_number = 68,
+	.mmu_irq_number = 69,
+	.gpu_irq_number = 70,
+	.io_memory_region = {
+			     .start = 0xFC010000,
+			     .end = 0xFC010000 + (4096 * 4) - 1}
+};
+#endif
+
+static int pm_callback_power_on(kbase_device *kbdev)
+{
+	/* Nothing is needed on VExpress, but we may have destroyed GPU state (if the below HARD_RESET code is active) */
+	return 1;
+}
+
+static void pm_callback_power_off(kbase_device *kbdev)
+{
+#if HARD_RESET_AT_POWER_OFF
+	/* Cause a GPU hard reset to test whether we have actually idled the GPU
+	 * and that we properly reconfigure the GPU on power up.
+	 * Usually this would be dangerous, but if the GPU is working correctly it should
+	 * be completely safe as the GPU should not be active at this point.
+	 * However this is disabled normally because it will most likely interfere with
+	 * bus logging etc.
+	 */
+	KBASE_TRACE_ADD(kbdev, CORE_GPU_HARD_RESET, NULL, NULL, 0u, 0);
+	kbase_os_reg_write(kbdev, GPU_CONTROL_REG(GPU_COMMAND), GPU_COMMAND_HARD_RESET);
+#endif
+}
+
+static kbase_pm_callback_conf pm_callbacks = {
+	.power_on_callback = pm_callback_power_on,
+	.power_off_callback = pm_callback_power_off,
+	.power_suspend_callback  = NULL,
+	.power_resume_callback = NULL
+};
+
+/* Please keep table config_attributes in sync with config_attributes_hw_issue_8408 */
+static kbase_attribute config_attributes[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS_CL,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_CL,
+	 KBASE_VE_JS_HARD_STOP_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_CL,
+	 KBASE_VE_JS_RESET_TICKS_CL},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_IRQ_THROTTLE_TIME_US,
+	 20},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+/* as config_attributes array above except with different settings for
+ * JS_HARD_STOP_TICKS_SS, JS_RESET_TICKS_SS that
+ * are needed for BASE_HW_ISSUE_8408.
+ */
+kbase_attribute config_attributes_hw_issue_8408[] = {
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MAX,
+	 KBASE_VE_GPU_FREQ_KHZ_MAX},
+
+	{
+	 KBASE_CONFIG_ATTR_GPU_FREQ_KHZ_MIN,
+	 KBASE_VE_GPU_FREQ_KHZ_MIN},
+
+#ifdef CONFIG_MALI_DEBUG
+/* Use more aggressive scheduling timeouts in debug builds for testing purposes */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401_DEBUG},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS_DEBUG},
+#else				/* CONFIG_MALI_DEBUG */
+/* In release builds same as the defaults but scaled for 5MHz FPGA */
+	{
+	 KBASE_CONFIG_ATTR_JS_SCHEDULING_TICK_NS,
+	 KBASE_VE_JS_SCHEDULING_TICK_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_SOFT_STOP_TICKS,
+	 KBASE_VE_JS_SOFT_STOP_TICKS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_SS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_HARD_STOP_TICKS_NSS,
+	 KBASE_VE_JS_HARD_STOP_TICKS_NSS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_SS,
+	 KBASE_VE_JS_RESET_TICKS_SS_8401},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TICKS_NSS,
+	 KBASE_VE_JS_RESET_TICKS_NSS},
+#endif				/* CONFIG_MALI_DEBUG */
+	{
+	 KBASE_CONFIG_ATTR_JS_RESET_TIMEOUT_MS,
+	 KBASE_VE_JS_RESET_TIMEOUT_MS},
+
+	{
+	 KBASE_CONFIG_ATTR_JS_CTX_TIMESLICE_NS,
+	 KBASE_VE_JS_CTX_TIMESLICE_NS},
+
+	{
+	 KBASE_CONFIG_ATTR_POWER_MANAGEMENT_CALLBACKS,
+	 KBASE_VE_POWER_MANAGEMENT_CALLBACKS},
+
+	{
+	 KBASE_CONFIG_ATTR_CPU_SPEED_FUNC,
+	 KBASE_VE_CPU_SPEED_FUNC},
+
+	{
+	 KBASE_CONFIG_ATTR_SECURE_BUT_LOSS_OF_PERFORMANCE,
+	 KBASE_VE_SECURE_BUT_LOSS_OF_PERFORMANCE},
+
+	{
+	 KBASE_CONFIG_ATTR_END,
+	 0}
+};
+
+static kbase_platform_config virtex7_platform_config = {
+	.attributes = config_attributes,
+#ifndef CONFIG_OF
+	.io_resources = &io_resources
+#endif
+};
+
+kbase_platform_config *kbase_get_platform_config(void)
+{
+	return &virtex7_platform_config;
+}
+
+int kbase_platform_early_init(void)
+{
+	/* Nothing needed at this stage */
+	return 0;
+}
+
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.c b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.c
new file mode 100644
index 0000000..47d45e2
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.c
@@ -0,0 +1,178 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#include <linux/io.h>
+#include <mali_kbase.h>
+#include "mali_kbase_cpu_vexpress.h"
+
+#define HZ_IN_MHZ					    (1000000)
+
+#define CORETILE_EXPRESS_A9X4_SCC_START	(0x100E2000)
+#define MOTHERBOARD_SYS_CFG_START		(0x10000000)
+#define SYS_CFGDATA_OFFSET				(0x000000A0)
+#define SYS_CFGCTRL_OFFSET				(0x000000A4)
+#define SYS_CFGSTAT_OFFSET				(0x000000A8)
+
+#define SYS_CFGCTRL_START_BIT_VALUE		  (1 << 31)
+#define READ_REG_BIT_VALUE				  (0 << 30)
+#define DCC_DEFAULT_BIT_VALUE			  (0 << 26)
+#define SYS_CFG_OSC_FUNC_BIT_VALUE		  (1 << 20)
+#define SITE_DEFAULT_BIT_VALUE			  (1 << 16)
+#define BOARD_STACK_POS_DEFAULT_BIT_VALUE (0 << 12)
+#define DEVICE_DEFAULT_BIT_VALUE	      (2 <<  0)
+#define SYS_CFG_COMPLETE_BIT_VALUE		  (1 <<  0)
+#define SYS_CFG_ERROR_BIT_VALUE			  (1 <<  1)
+
+#define FEED_REG_BIT_MASK				(0x0F)
+#define FCLK_PA_DIVIDE_BIT_SHIFT		(0x03)
+#define FCLK_PB_DIVIDE_BIT_SHIFT		(0x07)
+#define FCLK_PC_DIVIDE_BIT_SHIFT		(0x0B)
+#define AXICLK_PA_DIVIDE_BIT_SHIFT		(0x0F)
+#define AXICLK_PB_DIVIDE_BIT_SHIFT		(0x13)
+
+#define IS_SINGLE_BIT_SET(val, pos)		(val&(1<<pos))
+
+#define CPU_CLOCK_SPEED_UNDEFINED 0
+
+static u32 cpu_clock_speed = CPU_CLOCK_SPEED_UNDEFINED;
+
+static DEFINE_RAW_SPINLOCK(syscfg_lock);
+/**
+ * kbase_get_vendor_specific_cpu_clock_speed
+ * @brief  Retrieves the CPU clock speed.
+ *         The implementation is platform specific.
+ * @param[out]    cpu_clock - the value of CPU clock speed in MHz
+ * @return        0 on success, 1 otherwise
+*/
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock)
+{
+
+
+	if (CPU_CLOCK_SPEED_UNDEFINED != cpu_clock_speed)
+	{
+		*cpu_clock = cpu_clock_speed;
+		return 0;
+	}
+	else
+	{
+		int result = 0;
+		u32 reg_val = 0;
+		u32 osc2_value = 0;
+		u32 pa_divide = 0;
+		u32 pb_divide = 0;
+		u32 pc_divide = 0;
+		void *volatile pSysCfgReg = 0;
+		void *volatile pSCCReg = 0;
+
+		/* Init the value case something goes wrong */
+		*cpu_clock = 0;
+
+		/* Map CPU register into virtual memory */
+		pSysCfgReg = ioremap(MOTHERBOARD_SYS_CFG_START, 0x1000);
+		if (pSysCfgReg == NULL) {
+			result = 1;
+
+			goto pSysCfgReg_map_failed;
+		}
+
+		pSCCReg = ioremap(CORETILE_EXPRESS_A9X4_SCC_START, 0x1000);
+		if (pSCCReg == NULL) {
+			result = 1;
+
+			goto pSCCReg_map_failed;
+		}
+
+		raw_spin_lock(&syscfg_lock);
+
+		/*Read SYS regs - OSC2 */
+		reg_val = readl(pSysCfgReg + SYS_CFGCTRL_OFFSET);
+
+		/*Verify if there is no other undergoing request */
+		if (!(reg_val & SYS_CFGCTRL_START_BIT_VALUE)) {
+			/*Reset the CGFGSTAT reg */
+			writel(0, (pSysCfgReg + SYS_CFGSTAT_OFFSET));
+
+			writel(SYS_CFGCTRL_START_BIT_VALUE | READ_REG_BIT_VALUE | DCC_DEFAULT_BIT_VALUE | SYS_CFG_OSC_FUNC_BIT_VALUE | SITE_DEFAULT_BIT_VALUE | BOARD_STACK_POS_DEFAULT_BIT_VALUE | DEVICE_DEFAULT_BIT_VALUE, (pSysCfgReg + SYS_CFGCTRL_OFFSET));
+			/* Wait for the transaction to complete */
+			while (!(readl(pSysCfgReg + SYS_CFGSTAT_OFFSET) & SYS_CFG_COMPLETE_BIT_VALUE))
+				;
+			/* Read SYS_CFGSTAT Register to get the status of submitted transaction */
+			reg_val = readl(pSysCfgReg + SYS_CFGSTAT_OFFSET);
+
+			/*------------------------------------------------------------------------------------------*/
+			/* Check for possible errors */
+			if (reg_val & SYS_CFG_ERROR_BIT_VALUE) {
+				/* Error while setting register */
+				result = 1;
+			} else {
+				osc2_value = readl(pSysCfgReg + SYS_CFGDATA_OFFSET);
+				/* Read the SCC CFGRW0 register */
+				reg_val = readl(pSCCReg);
+
+				/*
+				   Select the appropriate feed:
+				   CFGRW0[0] - CLKOB
+				   CFGRW0[1] - CLKOC
+				   CFGRW0[2] - FACLK (CLK)B FROM AXICLK PLL)
+				 */
+				/* Calculate the  FCLK */
+				if (IS_SINGLE_BIT_SET(reg_val, 0)) {	/*CFGRW0[0] - CLKOB */
+					/* CFGRW0[6:3] */
+					pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+					/* CFGRW0[10:7] */
+					pb_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PB_DIVIDE_BIT_SHIFT)) >> FCLK_PB_DIVIDE_BIT_SHIFT);
+					*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+				} else {
+					if (IS_SINGLE_BIT_SET(reg_val, 1)) {	/*CFGRW0[1] - CLKOC */
+						/* CFGRW0[6:3] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PA_DIVIDE_BIT_SHIFT)) >> FCLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[14:11] */
+						pc_divide = ((reg_val & (FEED_REG_BIT_MASK << FCLK_PC_DIVIDE_BIT_SHIFT)) >> FCLK_PC_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pc_divide + 1);
+					} else if (IS_SINGLE_BIT_SET(reg_val, 2)) {	/*CFGRW0[2] - FACLK */
+						/* CFGRW0[18:15] */
+						pa_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PA_DIVIDE_BIT_SHIFT)) >> AXICLK_PA_DIVIDE_BIT_SHIFT);
+						/* CFGRW0[22:19] */
+						pb_divide = ((reg_val & (FEED_REG_BIT_MASK << AXICLK_PB_DIVIDE_BIT_SHIFT)) >> AXICLK_PB_DIVIDE_BIT_SHIFT);
+						*cpu_clock = osc2_value * (pa_divide + 1) / (pb_divide + 1);
+					} else {
+						result = 1;
+					}
+				}
+			}
+		} else {
+			result = 1;
+		}
+		raw_spin_unlock(&syscfg_lock);
+		/* Convert result expressed in Hz to Mhz units. */
+		*cpu_clock /= HZ_IN_MHZ;
+		if(!result)
+		{
+			cpu_clock_speed = *cpu_clock;
+		}
+
+		/* Unmap memory */
+		iounmap(pSCCReg);
+
+	 pSCCReg_map_failed:
+		iounmap(pSysCfgReg);
+
+	 pSysCfgReg_map_failed:
+
+		return result;
+	}
+}
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.h b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.h
new file mode 100644
index 0000000..3f6c68e
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform/vexpress_virtex7_40mhz/mali_kbase_cpu_vexpress.h
@@ -0,0 +1,26 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifndef _KBASE_CPU_VEXPRESS_H_
+#define _KBASE_CPU_VEXPRESS_H_
+
+/**
+ * Versatile Express implementation of @ref kbase_cpuprops_clock_speed_function.
+ */
+int kbase_get_vexpress_cpu_clock_speed(u32 *cpu_clock);
+
+#endif				/* _KBASE_CPU_VEXPRESS_H_ */
diff --git a/drivers/gpu/mali-t6xx/r4p1/platform_dummy/mali_ukk_os.h b/drivers/gpu/mali-t6xx/r4p1/platform_dummy/mali_ukk_os.h
new file mode 100644
index 0000000..daaa8c0
--- /dev/null
+++ b/drivers/gpu/mali-t6xx/r4p1/platform_dummy/mali_ukk_os.h
@@ -0,0 +1,54 @@
+/*
+ *
+ * (C) COPYRIGHT ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the
+ * GNU General Public License version 2 as published by the Free Software
+ * Foundation, and any use by you of this program is subject to the terms
+ * of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained
+ * from Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor,
+ * Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+
+
+/**
+ * @file mali_ukk_os.h
+ * Types and definitions that are common for Linux OSs for the kernel side of the
+ * User-Kernel interface.
+ */
+
+#ifndef _UKK_OS_H_ /* Linux version */
+#define _UKK_OS_H_
+
+#include <linux/fs.h>
+
+/**
+ * @addtogroup uk_api User-Kernel Interface API
+ * @{
+ */
+
+/**
+ * @addtogroup uk_api_kernel UKK (Kernel side)
+ * @{
+ */
+
+/**
+ * Internal OS specific data structure associated with each UKK session. Part
+ * of a ukk_session object.
+ */
+typedef struct ukkp_session
+{
+	int dummy;     /**< No internal OS specific data at this time */
+} ukkp_session;
+
+/** @} end group uk_api_kernel */
+
+/** @} end group uk_api */
+
+#endif /* _UKK_OS_H__ */
diff --git a/drivers/i2c/busses/Kconfig b/drivers/i2c/busses/Kconfig
index edafe15..0573576 100644
--- a/drivers/i2c/busses/Kconfig
+++ b/drivers/i2c/busses/Kconfig
@@ -788,6 +788,12 @@ config I2C_RCAR
 	  This driver can also be built as a module.  If so, the module
 	  will be called i2c-rcar.
 
+config I2C_F_I2C
+	tristate "Fujitsu Semiconductor I2C bus support"
+	depends on ARCH_MB8AC0300 || ARCH_MB86S70
+	help
+	  If you say yes to this option, support will be included for
+	  the Fujitsu Semiconductor's I2C interface.
 comment "External I2C/SMBus adapter drivers"
 
 config I2C_DIOLAN_U2C
diff --git a/drivers/i2c/busses/Makefile b/drivers/i2c/busses/Makefile
index dd80984..65e24d1 100644
--- a/drivers/i2c/busses/Makefile
+++ b/drivers/i2c/busses/Makefile
@@ -77,6 +77,7 @@ obj-$(CONFIG_I2C_OCTEON)	+= i2c-octeon.o
 obj-$(CONFIG_I2C_XILINX)	+= i2c-xiic.o
 obj-$(CONFIG_I2C_XLR)		+= i2c-xlr.o
 obj-$(CONFIG_I2C_RCAR)		+= i2c-rcar.o
+obj-$(CONFIG_I2C_F_I2C)		+= i2c-f_i2c.o
 
 # External I2C/SMBus adapter drivers
 obj-$(CONFIG_I2C_DIOLAN_U2C)	+= i2c-diolan-u2c.o
diff --git a/drivers/i2c/busses/i2c-f_i2c.c b/drivers/i2c/busses/i2c-f_i2c.c
new file mode 100644
index 0000000..dad4ee4
--- /dev/null
+++ b/drivers/i2c/busses/i2c-f_i2c.c
@@ -0,0 +1,734 @@
+/*
+ * linux/drivers/i2c/busses/i2c-f_i2c.c
+ *
+ * Copyright (C) 2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/sched.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/i2c.h>
+#include <linux/of_i2c.h>
+#include <linux/interrupt.h>
+#include <linux/delay.h>
+#include <linux/clk.h>
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/spinlock.h>
+#include <linux/io.h>
+#include <linux/platform_data/f_i2c.h>
+#include "i2c-f_i2c.h"
+
+#define DRV_NAME "f_i2c"
+
+enum f_i2c_state {
+	STATE_IDLE,
+	STATE_START,
+	STATE_READ,
+	STATE_WRITE
+};
+
+struct f_i2c {
+	struct completion completion;
+
+	struct i2c_msg *msg;
+	unsigned int msg_num;
+	unsigned int msg_idx;
+	unsigned int msg_ptr;
+
+	struct device *dev;
+	void __iomem *base;
+	unsigned int irq;
+	struct clk *clk;
+	unsigned long clkrate;
+	unsigned int speed_khz;
+	unsigned long timeout_ms;
+	enum f_i2c_state state;
+	struct i2c_adapter adapter;
+
+	bool is_suspended;
+};
+
+static inline int is_lastmsg(struct f_i2c *i2c)
+{
+	return i2c->msg_idx >= (i2c->msg_num - 1);
+}
+
+static inline int is_msglast(struct f_i2c *i2c)
+{
+	return i2c->msg_ptr == (i2c->msg->len - 1);
+}
+
+static inline int is_msgend(struct f_i2c *i2c)
+{
+	return i2c->msg_ptr >= i2c->msg->len;
+}
+
+static inline unsigned long calc_timeout_ms(struct f_i2c *i2c,
+					struct i2c_msg *msgs, int num)
+{
+	unsigned long bit_count = 0;
+	int i;
+
+	for (i = 0; i < num; i++, msgs++)
+		bit_count += msgs->len;
+
+	return DIV_ROUND_UP(((bit_count * 9) + (10 * num)) * 3, 200) + 10;
+}
+
+static void f_i2c_stop(struct f_i2c *i2c, int ret)
+{
+	dev_dbg(i2c->dev, "STOP\n");
+
+	/*
+	 * clear IRQ (INT=0, BER=0)
+	 * set Stop Condition (MSS=0)
+	 * Interrupt Disable
+	 */
+	writeb(0, i2c->base + F_I2C_REG_BCR);
+
+	i2c->state = STATE_IDLE;
+
+	i2c->msg_ptr = 0;
+	i2c->msg = NULL;
+	i2c->msg_idx++;
+	i2c->msg_num = 0;
+	if (ret)
+		i2c->msg_idx = ret;
+
+	complete(&i2c->completion);
+}
+
+static void f_i2c_hw_init(struct f_i2c *i2c)
+{
+	unsigned char ccr_cs, csr_cs;
+
+	/* Set own Address */
+	writeb(0, i2c->base + F_I2C_REG_ADR);
+
+	/* Set PCLK frequency */
+	writeb(F_I2C_BUS_CLK_FR(i2c->clkrate), i2c->base + F_I2C_REG_FSR);
+
+	switch (i2c->speed_khz) {
+	case F_I2C_SPEED_FM:
+		if (i2c->clkrate <= F_I2C_CLK_RATE_18M) {
+			ccr_cs = F_I2C_CCR_CS_FAST_MAX_18M(i2c->clkrate);
+			csr_cs = F_I2C_CSR_CS_FAST_MAX_18M(i2c->clkrate);
+		} else {
+			ccr_cs = F_I2C_CCR_CS_FAST_MIN_18M(i2c->clkrate);
+			csr_cs = F_I2C_CSR_CS_FAST_MIN_18M(i2c->clkrate);
+		}
+
+		/* Set Clock and enable, Set fast mode*/
+		writeb(ccr_cs | F_I2C_CCR_FM | F_I2C_CCR_EN,
+						i2c->base + F_I2C_REG_CCR);
+		writeb(csr_cs, i2c->base + F_I2C_REG_CSR);
+		break;
+	case F_I2C_SPEED_SM:
+		if (i2c->clkrate <= F_I2C_CLK_RATE_18M) {
+			ccr_cs = F_I2C_CCR_CS_STANDARD_MAX_18M(i2c->clkrate);
+			csr_cs = F_I2C_CSR_CS_STANDARD_MAX_18M(i2c->clkrate);
+		} else {
+			ccr_cs = F_I2C_CCR_CS_STANDARD_MIN_18M(i2c->clkrate);
+			csr_cs = F_I2C_CSR_CS_STANDARD_MIN_18M(i2c->clkrate);
+		}
+
+		/* Set Clock and enable, Set standard mode */
+		writeb(ccr_cs | F_I2C_CCR_EN, i2c->base + F_I2C_REG_CCR);
+		writeb(csr_cs, i2c->base + F_I2C_REG_CSR);
+		break;
+	default:
+		BUG();
+	}
+
+	/* clear IRQ (INT=0, BER=0), Interrupt Disable */
+	writeb(0, i2c->base + F_I2C_REG_BCR);
+	writeb(0, i2c->base + F_I2C_REG_BC2R);
+}
+
+static void f_i2c_hw_reset(struct f_i2c *i2c)
+{
+	/* Disable clock */
+	writeb(0, i2c->base + F_I2C_REG_CCR);
+	writeb(0, i2c->base + F_I2C_REG_CSR);
+
+	WAIT_PCLK(100, i2c->clkrate);
+
+	f_i2c_hw_init(i2c);
+}
+
+static int f_i2c_master_start(struct f_i2c *i2c, struct i2c_msg *pmsg)
+{
+	unsigned char bsr, bcr;
+
+	if (pmsg->flags & I2C_M_RD)
+		writeb((pmsg->addr << 1) | 1, i2c->base + F_I2C_REG_DAR);
+	else
+		writeb(pmsg->addr << 1, i2c->base + F_I2C_REG_DAR);
+
+	dev_dbg(i2c->dev, "%s slave:0x%02x\n", __func__, pmsg->addr);
+
+	/* Generate Start Condition */
+	bsr = readb(i2c->base + F_I2C_REG_BSR);
+	bcr = readb(i2c->base + F_I2C_REG_BCR);
+	dev_dbg(i2c->dev, "%s bsr:0x%08x, bcr:0x%08x\n", __func__, bsr, bcr);
+
+	if ((bsr & F_I2C_BSR_BB) && !(bcr & F_I2C_BCR_MSS)) {
+		dev_dbg(i2c->dev, "%s bus is busy", __func__);
+		return -EBUSY;
+	}
+
+	if (bsr & F_I2C_BSR_BB) { /* Bus is busy */
+		dev_dbg(i2c->dev, "%s Continuous Start", __func__);
+		writeb(bcr | F_I2C_BCR_SCC, i2c->base + F_I2C_REG_BCR);
+	} else {
+		if (bcr & F_I2C_BCR_MSS) {
+			dev_dbg(i2c->dev, "%s is not in master mode", __func__);
+			return -EAGAIN;
+		}
+		dev_dbg(i2c->dev, "%s Start Condition", __func__);
+		/* Start Condition + Enable Interrupts */
+		writeb(bcr | F_I2C_BCR_MSS | F_I2C_BCR_INTE | F_I2C_BCR_BEIE,
+						     i2c->base + F_I2C_REG_BCR);
+	}
+
+	WAIT_PCLK(10, i2c->clkrate);
+
+	/* get bsr&bcr register */
+	bsr = readb(i2c->base + F_I2C_REG_BSR);
+	bcr = readb(i2c->base + F_I2C_REG_BCR);
+	dev_dbg(i2c->dev, "%s bsr:0x%08x, bcr:0x%08x\n", __func__, bsr, bcr);
+
+	if ((bsr & F_I2C_BSR_AL) || !(bcr & F_I2C_BCR_MSS)) {
+		dev_dbg(i2c->dev, "%s arbitration lost\n", __func__);
+		return -EAGAIN;
+	}
+
+	return 0;
+}
+
+static int f_i2c_master_recover(struct f_i2c *i2c)
+{
+	unsigned int count = 0;
+	unsigned char bc2r;
+
+	/* Disable interrupts */
+	writeb(0, i2c->base + F_I2C_REG_BCR);
+
+	/* monitor SDA, SCL */
+	bc2r = readb(i2c->base + F_I2C_REG_BC2R);
+	dev_dbg(i2c->dev, "%s bc2r:0x%08x\n", __func__, (unsigned)bc2r);
+
+	while (!(bc2r & F_I2C_BC2R_SDAS) && (bc2r & F_I2C_BC2R_SCLS)) {
+		WAIT_PCLK(10, i2c->clkrate);
+		bc2r = readb(i2c->base + F_I2C_REG_BC2R);
+
+		/* another master is running */
+		if (++count >= 100) {
+			dev_dbg(i2c->dev, "%s: another master is running?\n",
+							__func__);
+			return -EAGAIN;
+		}
+	}
+
+	/* Force to make one clock pulse */
+	count = 0;
+	for (;;) {
+		/* SCL = L->H */
+		writeb(F_I2C_BC2R_SCLL, i2c->base + F_I2C_REG_BC2R);
+		WAIT_PCLK(10, i2c->clkrate);
+		writeb(0, i2c->base + F_I2C_REG_BC2R);
+
+		WAIT_PCLK(5, i2c->clkrate);
+
+		bc2r = readb(i2c->base + F_I2C_REG_BC2R);
+		if (bc2r & F_I2C_BC2R_SDAS)
+			break;
+		if (++count > 9) {
+			dev_err(i2c->dev, "%s: count: %i, bc2r: 0x%x\n",
+						__func__, count, bc2r);
+			return -EIO;
+		}
+	}
+
+	/* force to make bus-error phase */
+	/* SDA = L */
+	writeb(F_I2C_BC2R_SDAL, i2c->base + F_I2C_REG_BC2R);
+	WAIT_PCLK(1, i2c->clkrate);
+	/* SDA = H */
+	writeb(0, i2c->base + F_I2C_REG_BC2R);
+
+	/* Both SDA & SDL should be H */
+	bc2r = readb(i2c->base + F_I2C_REG_BC2R);
+	if (!(bc2r & F_I2C_BC2R_SDAS) || !(bc2r & F_I2C_BC2R_SCLS)) {
+		dev_err(i2c->dev, "%s: bc2r: 0x%x\n", __func__, bc2r);
+		return -EIO;
+	}
+
+	return 0;
+}
+
+static int f_i2c_doxfer(struct f_i2c *i2c,
+				struct i2c_msg *msgs, int num)
+{
+	unsigned char bsr;
+	unsigned long timeout, bb_timout;
+	int ret = 0;
+
+	if (i2c->is_suspended)
+		return -EBUSY;
+
+	f_i2c_hw_init(i2c);
+	bsr = readb(i2c->base + F_I2C_REG_BSR);
+	if (bsr & F_I2C_BSR_BB) {
+		dev_err(i2c->dev, "cannot get bus (bus busy)\n");
+		return -EBUSY;
+	}
+
+	init_completion(&i2c->completion);
+
+	i2c->msg = msgs;
+	i2c->msg_num = num;
+	i2c->msg_ptr = 0;
+	i2c->msg_idx = 0;
+	i2c->state = STATE_START;
+
+	ret = f_i2c_master_start(i2c, i2c->msg);
+	if (ret < 0) {
+		dev_dbg(i2c->dev, "Address failed: (0x%08x)\n", ret);
+		goto out;
+	}
+
+	timeout = wait_for_completion_timeout(&i2c->completion,
+						F_I2C_TIMEOUT(i2c->timeout_ms));
+	if (timeout <= 0) {
+		dev_dbg(i2c->dev, "timeout\n");
+		ret = -EAGAIN;
+		goto out;
+	}
+
+	ret = i2c->msg_idx;
+	if (ret != num) {
+		dev_dbg(i2c->dev, "incomplete xfer (%d)\n", ret);
+		ret = -EAGAIN;
+		goto out;
+	}
+
+	/* ensure the stop has been through the bus */
+	bb_timout = jiffies + HZ / 1000;
+	do {
+		bsr = readb(i2c->base + F_I2C_REG_BSR);
+	} while ((bsr & F_I2C_BSR_BB) && time_before(jiffies, bb_timout));
+out:
+	return ret;
+}
+
+static irqreturn_t f_i2c_isr(int irq, void *dev_id)
+{
+	struct f_i2c *i2c = dev_id;
+
+	unsigned char byte;
+	unsigned char bsr, bcr;
+	int ret = 0;
+
+	bcr = readb(i2c->base + F_I2C_REG_BCR);
+	bsr = readb(i2c->base + F_I2C_REG_BSR);
+	dev_dbg(i2c->dev, "%s bsr:0x%08x, bcr:0x%08x\n", __func__,
+					(unsigned)bsr, (unsigned)bcr);
+
+	if (bcr & F_I2C_BCR_BER) {
+		dev_err(i2c->dev, "%s: bus error\n", __func__);
+		f_i2c_stop(i2c, -EAGAIN);
+		goto out;
+	}
+	if ((bsr & F_I2C_BSR_AL) || !(bcr & F_I2C_BCR_MSS)) {
+		dev_dbg(i2c->dev, "%s arbitration lost\n", __func__);
+		f_i2c_stop(i2c, -EAGAIN);
+		goto out;
+	}
+
+	switch (i2c->state) {
+
+	case STATE_START:
+		if (bsr & F_I2C_BSR_LRB) {
+			dev_dbg(i2c->dev, "ack was not received\n");
+			f_i2c_stop(i2c, -EAGAIN);
+			goto out;
+		}
+
+		if (i2c->msg->flags & I2C_M_RD)
+			i2c->state = STATE_READ;
+		else
+			i2c->state = STATE_WRITE;
+
+		if (is_lastmsg(i2c) && i2c->msg->len == 0) {
+			f_i2c_stop(i2c, 0);
+			goto out;
+		}
+
+		if (i2c->state == STATE_READ)
+			goto prepare_read;
+
+		/* fallthru */
+
+	case STATE_WRITE:
+		if (bsr & F_I2C_BSR_LRB) {
+			dev_dbg(i2c->dev, "WRITE: No Ack\n");
+			f_i2c_stop(i2c, -EAGAIN);
+			goto out;
+		}
+
+		if (!is_msgend(i2c)) {
+			writeb(i2c->msg->buf[i2c->msg_ptr++],
+						i2c->base + F_I2C_REG_DAR);
+
+			/* clear IRQ, and continue */
+			writeb(F_I2C_BCR_BEIE | F_I2C_BCR_MSS |
+				F_I2C_BCR_INTE, i2c->base + F_I2C_REG_BCR);
+			break;
+		}
+		if (is_lastmsg(i2c)) {
+			f_i2c_stop(i2c, 0);
+			break;
+		}
+		dev_dbg(i2c->dev, "WRITE: Next Message\n");
+
+		i2c->msg_ptr = 0;
+		i2c->msg_idx++;
+		i2c->msg++;
+
+		/* send the new start */
+		ret = f_i2c_master_start(i2c, i2c->msg);
+		if (ret < 0) {
+			dev_dbg(i2c->dev, "restart err:0x%08x\n", ret);
+			f_i2c_stop(i2c, -EAGAIN);
+			break;
+		}
+		i2c->state = STATE_START;
+		break;
+
+	case STATE_READ:
+		if (!(bsr & F_I2C_BSR_FBT)) { /* data */
+			byte = readb(i2c->base + F_I2C_REG_DAR);
+			i2c->msg->buf[i2c->msg_ptr++] = byte;
+		} else /* address */
+			dev_dbg(i2c->dev, ", address:0x%08x. ignore it.\n",
+					readb(i2c->base + F_I2C_REG_DAR));
+
+prepare_read:
+		if (is_msglast(i2c)) {
+			writeb(F_I2C_BCR_MSS | F_I2C_BCR_BEIE | F_I2C_BCR_INTE,
+						     i2c->base + F_I2C_REG_BCR);
+			break;
+		}
+		if (!is_msgend(i2c)) {
+			writeb(F_I2C_BCR_MSS | F_I2C_BCR_BEIE |
+					F_I2C_BCR_INTE | F_I2C_BCR_ACK,
+						     i2c->base + F_I2C_REG_BCR);
+			break;
+		}
+		if (is_lastmsg(i2c)) {
+			/* last message, send stop and complete */
+			dev_dbg(i2c->dev, "READ: Send Stop\n");
+			f_i2c_stop(i2c, 0);
+			break;
+		}
+		dev_dbg(i2c->dev, "READ: Next Transfer\n");
+
+		i2c->msg_ptr = 0;
+		i2c->msg_idx++;
+		i2c->msg++;
+
+		ret = f_i2c_master_start(i2c, i2c->msg);
+		if (ret < 0) {
+			dev_dbg(i2c->dev, "restart err: 0x%08x\n", ret);
+			f_i2c_stop(i2c, -EAGAIN);
+		} else
+			i2c->state = STATE_START;
+		break;
+	default:
+		dev_err(i2c->dev, "%s: called in err STATE (%d)\n",
+			 __func__, i2c->state);
+		break;
+	}
+
+out:
+	WAIT_PCLK(10, i2c->clkrate);
+	return IRQ_HANDLED;
+}
+
+static int f_i2c_xfer(struct i2c_adapter *adap,
+				struct i2c_msg *msgs, int num)
+{
+	struct f_i2c *i2c;
+	int retry;
+	int ret = 0;
+
+	if (!msgs)
+		return -EINVAL;
+	if (num <= 0)
+		return -EINVAL;
+
+	i2c = i2c_get_adapdata(adap);
+	i2c->timeout_ms = calc_timeout_ms(i2c, msgs, num);
+
+	dev_dbg(i2c->dev, "calculated timeout %ld ms\n", i2c->timeout_ms);
+
+	for (retry = 0; retry < adap->retries; retry++) {
+
+		ret = f_i2c_doxfer(i2c, msgs, num);
+		if (ret != -EAGAIN)
+			return ret;
+
+		dev_dbg(i2c->dev, "Retrying transmission (%d)\n", retry);
+
+		f_i2c_master_recover(i2c);
+		f_i2c_hw_reset(i2c);
+	}
+
+	dev_err(i2c->dev, "transmission err: %d\n", retry);
+
+	return -EIO;
+}
+
+static u32 f_i2c_functionality(struct i2c_adapter *adap)
+{
+	return I2C_FUNC_I2C | I2C_FUNC_SMBUS_EMUL;
+}
+
+static const struct i2c_algorithm f_i2c_algo = {
+	.master_xfer   = f_i2c_xfer,
+	.functionality = f_i2c_functionality,
+};
+
+static struct i2c_adapter f_i2c_ops = {
+	.owner		= THIS_MODULE,
+	.name		= "f_i2c-adapter",
+	.algo		= &f_i2c_algo,
+	.retries	= 5,
+};
+
+static int f_i2c_probe(struct platform_device *pdev)
+{
+	struct f_i2c *i2c;
+	struct f_i2c_platform_data *pdata;
+	struct resource *r;
+	int ret = 0;
+	int speed_khz = 100;
+	const int *p;
+
+	pdata = pdev->dev.platform_data;
+	if (pdev->dev.of_node) {
+		p = of_get_property(pdev->dev.of_node, "clock-frequency", NULL);
+		if (!p) {
+			dev_err(&pdev->dev,
+					"Missing clock-frequency property\n");
+			return -EINVAL;
+		}
+		speed_khz = be32_to_cpu(*p) / 1000;
+	} else
+		if (pdata)
+			speed_khz = pdata->speed_khz;
+
+	i2c = kzalloc(sizeof(struct f_i2c), GFP_KERNEL);
+	if (!i2c) {
+		dev_err(&pdev->dev, "no memory for state\n");
+		return -ENOMEM;
+	}
+
+	i2c->clk = clk_get(&pdev->dev, NULL);
+	if (IS_ERR(i2c->clk)) {
+		dev_err(&pdev->dev, "cannot get clock\n");
+		ret = PTR_ERR(i2c->clk);
+		goto err_noclk;
+	}
+	dev_dbg(&pdev->dev, "clock source %p\n", i2c->clk);
+
+	i2c->clkrate = clk_get_rate(i2c->clk);
+	if ((i2c->clkrate < F_I2C_MIN_CLK_RATE) ||
+					(i2c->clkrate > F_I2C_MAX_CLK_RATE)) {
+		dev_err(&pdev->dev, "get clock rate err\n");
+		ret = -EINVAL;
+		goto err_noclk;
+	}
+	dev_dbg(&pdev->dev, "clock rate %ld\n", i2c->clkrate);
+	clk_prepare_enable(i2c->clk);
+
+	r = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!r) {
+		ret = -ENXIO;
+		goto err_clk;
+	}
+
+	i2c->base = ioremap(r->start, r->end - r->start + 1);
+	if (!i2c->base) {
+		ret = -ENOMEM;
+		goto err_clk;
+	}
+
+	dev_dbg(&pdev->dev, "registers %p (%p)\n", i2c->base, r);
+
+	i2c->irq = platform_get_irq(pdev, 0);
+	if (i2c->irq <= 0) {
+		ret = -ENXIO;
+		dev_err(&pdev->dev, "cannot find IRQ\n");
+		goto err_iomap;
+	}
+
+	ret = request_irq(i2c->irq, f_i2c_isr, 0, DRV_NAME, i2c);
+	if (ret != 0) {
+		dev_err(&pdev->dev, "cannot claim IRQ %d\n", i2c->irq);
+		goto err_iomap;
+	}
+
+	i2c->state = STATE_IDLE;
+	i2c->dev = &pdev->dev;
+	i2c->msg = NULL;
+	i2c->speed_khz = F_I2C_SPEED_SM;
+	if (speed_khz == F_I2C_SPEED_FM)
+		i2c->speed_khz = F_I2C_SPEED_FM;
+
+	f_i2c_hw_init(i2c);
+
+	i2c->adapter = f_i2c_ops;
+	i2c_set_adapdata(&i2c->adapter, i2c);
+	i2c->adapter.dev.parent = &pdev->dev;
+	i2c->adapter.dev.of_node = of_node_get(pdev->dev.of_node);
+	i2c->adapter.nr = pdev->id;
+
+	ret = i2c_add_numbered_adapter(&i2c->adapter);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "failed to add bus to i2c core\n");
+		goto err_irq;
+	}
+
+	platform_set_drvdata(pdev, i2c);
+	of_i2c_register_devices(&i2c->adapter);
+
+	dev_info(&pdev->dev, "%s: f_i2c adapter\n",
+				dev_name(&i2c->adapter.dev));
+
+	return 0;
+
+err_irq:
+	of_node_put(pdev->dev.of_node);
+	free_irq(i2c->irq, i2c);
+
+err_iomap:
+	iounmap(i2c->base);
+
+err_clk:
+	clk_disable_unprepare(i2c->clk);
+	clk_put(i2c->clk);
+
+err_noclk:
+	kfree(i2c);
+
+	return ret;
+}
+
+static int f_i2c_remove(struct platform_device *pdev)
+{
+	struct f_i2c *i2c = platform_get_drvdata(pdev);
+
+	platform_set_drvdata(pdev, NULL);
+	i2c_del_adapter(&i2c->adapter);
+	clk_disable_unprepare(i2c->clk);
+	clk_put(i2c->clk);
+	free_irq(i2c->irq, i2c);
+	iounmap(i2c->base);
+	kfree(i2c);
+	of_node_put(pdev->dev.of_node);
+
+	return 0;
+};
+
+
+#ifdef CONFIG_PM_SLEEP
+static int f_i2c_suspend(struct device *dev)
+{
+	struct f_i2c *i2c = dev_get_drvdata(dev);
+
+	i2c_lock_adapter(&i2c->adapter);
+	i2c->is_suspended = true;
+	i2c_unlock_adapter(&i2c->adapter);
+
+	clk_disable_unprepare(i2c->clk);
+
+	return 0;
+}
+
+static int f_i2c_resume(struct device *dev)
+{
+	struct f_i2c *i2c = dev_get_drvdata(dev);
+	int ret;
+
+	i2c_lock_adapter(&i2c->adapter);
+
+	ret = clk_prepare_enable(i2c->clk);
+
+	if (!ret)
+		i2c->is_suspended = false;
+
+	i2c_unlock_adapter(&i2c->adapter);
+
+	return ret;
+}
+
+static SIMPLE_DEV_PM_OPS(f_i2c_pm, f_i2c_suspend, f_i2c_resume);
+#define F_I2C_PM	(&f_i2c_pm)
+#else
+#define F_I2C_PM	NULL
+#endif
+
+static const struct of_device_id f_i2c_dt_ids[] = {
+	{ .compatible = "fujitsu,f_i2c" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, f_i2c_dt_ids);
+
+static struct platform_driver f_i2c_driver = {
+	.probe   = f_i2c_probe,
+	.remove  = f_i2c_remove,
+	.driver  = {
+		.owner = THIS_MODULE,
+		.name = DRV_NAME,
+		.of_match_table = f_i2c_dt_ids,
+		.pm = F_I2C_PM,
+	},
+};
+
+static int __init f_i2c_init(void)
+{
+	return platform_driver_register(&f_i2c_driver);
+}
+
+static void __exit f_i2c_exit(void)
+{
+	platform_driver_unregister(&f_i2c_driver);
+}
+
+module_init(f_i2c_init);
+module_exit(f_i2c_exit);
+
+MODULE_AUTHOR("Fujitsu Semiconductor Ltd");
+MODULE_DESCRIPTION("Fujitsu Semiconductor I2C Driver");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:"DRV_NAME);
diff --git a/drivers/i2c/busses/i2c-f_i2c.h b/drivers/i2c/busses/i2c-f_i2c.h
new file mode 100644
index 0000000..5ad4493
--- /dev/null
+++ b/drivers/i2c/busses/i2c-f_i2c.h
@@ -0,0 +1,109 @@
+/*
+ * linux/drivers/i2c/busses/i2c-f_i2c.h
+ *
+ * Copyright (C) 2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __I2C_F_I2C_H__
+#define __I2C_F_I2C_H__
+
+#define WAIT_PCLK(n, clkrate) ndelay((((1000000000 +  clkrate - 1) / \
+						clkrate + n - 1) / n) + 10)
+#define F_I2C_TIMEOUT(x) (msecs_to_jiffies(x))
+
+/* I2C register adress definitions */
+#define F_I2C_REG_BSR		(0x00 << 2) /* Bus Status Regster */
+#define F_I2C_REG_BCR		(0x01 << 2) /* Bus Control Register */
+#define F_I2C_REG_CCR		(0x02 << 2) /* Clock Control Register */
+#define F_I2C_REG_ADR		(0x03 << 2) /* Address Register */
+#define F_I2C_REG_DAR		(0x04 << 2) /* Data Register */
+#define F_I2C_REG_CSR		(0x05 << 2) /* Expansion CS Register */
+#define F_I2C_REG_FSR		(0x06 << 2) /* Bus Clock Frequency Register */
+#define F_I2C_REG_BC2R		(0x07 << 2) /* Bus Control 2 Register */
+
+/* I2C register bit definitions */
+#define F_I2C_BSR_FBT		(1 << 0)  /* First Byte Transfer */
+#define F_I2C_BSR_GCA		(1 << 1)  /* General Call Address */
+#define F_I2C_BSR_AAS		(1 << 2)  /* Address as Slave */
+#define F_I2C_BSR_TRX		(1 << 3)  /* Transfer/Receive */
+#define F_I2C_BSR_LRB		(1 << 4)  /* Last Received Bit */
+#define F_I2C_BSR_AL		(1 << 5)  /* Arbitration Lost */
+#define F_I2C_BSR_RSC		(1 << 6)  /* Repeated Start Condition */
+#define F_I2C_BSR_BB		(1 << 7)  /* Bus Busy */
+
+#define F_I2C_BCR_INT		(1 << 0)  /* Interrupt */
+#define F_I2C_BCR_INTE		(1 << 1)  /* Interrupt Enable */
+#define F_I2C_BCR_GCAA		(1 << 2)  /* General Call Access Acknowledge */
+#define F_I2C_BCR_ACK		(1 << 3)  /* Acknowledge */
+#define F_I2C_BCR_MSS		(1 << 4)  /* Master Slave Select */
+#define F_I2C_BCR_SCC		(1 << 5)  /* Start Condition Continue */
+#define F_I2C_BCR_BEIE		(1 << 6)  /* Bus Error Interrupt Enable */
+#define F_I2C_BCR_BER		(1 << 7)  /* Bus Error */
+
+#define F_I2C_CCR_CS_MASK	(0x1f)  /* CCR Clock Period Select */
+#define F_I2C_CCR_EN		(1 << 5)  /* Enable */
+#define F_I2C_CCR_FM		(1 << 6)  /* Speed Mode Select */
+
+#define F_I2C_CSR_CS_MASK	(0x3f)  /* CSR Clock Period Select */
+
+#define F_I2C_BC2R_SCLL		(1 << 0)  /* SCL Low Drive */
+#define F_I2C_BC2R_SDAL		(1 << 1)  /* SDA Low Drive */
+#define F_I2C_BC2R_SCLS		(1 << 4)  /* SCL Status */
+#define F_I2C_BC2R_SDAS		(1 << 5)  /* SDA Status */
+
+/* PCLK frequency */
+#define F_I2C_BUS_CLK_FR(clkrate)		((clkrate / 20000000) + 1)
+
+/* STANDARD MODE frequency */
+#define F_I2C_CLK_MASTER_STANDARD(clkrate) \
+	DIV_ROUND_UP(DIV_ROUND_UP(clkrate, 100000) - 2, 2)
+/* FAST MODE frequency */
+#define F_I2C_CLK_MASTER_FAST(clkrate) \
+	DIV_ROUND_UP((DIV_ROUND_UP(clkrate, 400000) - 2) * 2, 3)
+
+/* (clkrate <= 18000000) */
+/* calculate the value of CS bits in CCR register on standard mode */
+#define F_I2C_CCR_CS_STANDARD_MAX_18M(clkrate) \
+	   ((F_I2C_CLK_MASTER_STANDARD(clkrate) - 65) & F_I2C_CCR_CS_MASK)
+/* calculate the value of CS bits in CSR register on standard mode */
+#define F_I2C_CSR_CS_STANDARD_MAX_18M(clkrate)	0x00
+/* calculate the value of CS bits in CCR register on fast mode */
+#define F_I2C_CCR_CS_FAST_MAX_18M(clkrate) \
+	   ((F_I2C_CLK_MASTER_FAST(clkrate) - 1)  & F_I2C_CCR_CS_MASK)
+/* calculate the value of CS bits in CSR register on fast mode */
+#define F_I2C_CSR_CS_FAST_MAX_18M(clkrate)	0x00
+
+/* (clkrate > 18000000) */
+/* calculate the value of CS bits in CCR register on standard mode */
+#define F_I2C_CCR_CS_STANDARD_MIN_18M(clkrate) \
+	   ((F_I2C_CLK_MASTER_STANDARD(clkrate) - 1) & F_I2C_CCR_CS_MASK)
+/* calculate the value of CS bits in CSR register on standard mode */
+#define F_I2C_CSR_CS_STANDARD_MIN_18M(clkrate) \
+	   (((F_I2C_CLK_MASTER_STANDARD(clkrate) - 1) >> 5) & F_I2C_CSR_CS_MASK)
+/* calculate the value of CS bits in CCR register on fast mode */
+#define F_I2C_CCR_CS_FAST_MIN_18M(clkrate) \
+	   ((F_I2C_CLK_MASTER_FAST(clkrate) - 1) & F_I2C_CCR_CS_MASK)
+/* calculate the value of CS bits in CSR register on fast mode */
+#define F_I2C_CSR_CS_FAST_MIN_18M(clkrate) \
+	   (((F_I2C_CLK_MASTER_FAST(clkrate) - 1) >> 5) & F_I2C_CSR_CS_MASK)
+
+/* min I2C clock frequency 14M */
+#define F_I2C_MIN_CLK_RATE	(14 * 1000000)
+/* max I2C clock frequency 200M */
+#define F_I2C_MAX_CLK_RATE	(200 * 1000000)
+/* I2C clock frequency 18M */
+#define F_I2C_CLK_RATE_18M	(18 * 1000000)
+
+#endif /* __I2C_F_I2C_H__ */
diff --git a/drivers/input/mouse/gpio_mouse.c b/drivers/input/mouse/gpio_mouse.c
index 532eaca..32d1fe2 100644
--- a/drivers/input/mouse/gpio_mouse.c
+++ b/drivers/input/mouse/gpio_mouse.c
@@ -13,6 +13,7 @@
 #include <linux/platform_device.h>
 #include <linux/input-polldev.h>
 #include <linux/gpio.h>
+#include <linux/slab.h>
 #include <linux/gpio_mouse.h>
 
 
@@ -49,13 +50,46 @@ static void gpio_mouse_scan(struct input_polled_dev *dev)
 static int gpio_mouse_probe(struct platform_device *pdev)
 {
 	struct gpio_mouse_platform_data *pdata = pdev->dev.platform_data;
+	struct device_node *np = pdev->dev.of_node;
+	const unsigned int *iprop;
+	u32 len;
 	struct input_polled_dev *input_poll;
 	struct input_dev *input;
 	int pin, i;
 	int error;
 
+	if (np) {
+		/* create a pdata struct from device tree properties */
+		pdata = kmalloc(sizeof(*pdata), GFP_KERNEL);
 	if (!pdata) {
-		dev_err(&pdev->dev, "no platform data\n");
+			dev_err(&pdev->dev, "Out of memory\n");
+			return -ENOMEM;
+		}
+		iprop = of_get_property(np, "scan_ms", NULL);
+		if (!iprop) {
+			dev_err(&pdev->dev, "Missing scan_ms property\n");
+			return -EINVAL;
+		}
+		pdata->scan_ms = be32_to_cpu(*iprop);
+		iprop = of_get_property(np, "polarity", NULL);
+		if (!iprop) {
+			dev_err(&pdev->dev, "Missing polarity property\n");
+			return -EINVAL;
+		}
+		pdata->polarity = be32_to_cpu(*iprop);
+		iprop = of_get_property(np, "pins", &len);
+		if (len != sizeof(pdata->pins)) {
+			dev_err(&pdev->dev, "Wrong pin gpio count\n");
+			return -EINVAL;
+		}
+		i = 0;
+		while (len) {
+			pdata->pins[i++] = be32_to_cpu(*iprop++);
+			len -= sizeof(int);
+		}
+	} else
+		if (!pdata) {
+			dev_err(&pdev->dev, "no platform data or DT node\n");
 		error = -ENXIO;
 		goto out;
 	}
@@ -147,6 +181,9 @@ static int gpio_mouse_probe(struct platform_device *pdev)
 			gpio_free(pin);
 	}
  out:
+	if (np)
+		kfree(pdata);
+
 	return error;
 }
 
@@ -167,15 +204,26 @@ static int gpio_mouse_remove(struct platform_device *pdev)
 
 	platform_set_drvdata(pdev, NULL);
 
+	if (pdev->dev.of_node)
+		kfree(pdata);
+
 	return 0;
 }
 
+static const struct of_device_id mb8ac0300_gpiomouse_dt_ids[] = {
+	{ .compatible = "fujitsu,mb8ac0300-gpiomouse" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, mb8ac0300_gpiomouse_dt_ids);
+
 static struct platform_driver gpio_mouse_device_driver = {
 	.probe		= gpio_mouse_probe,
 	.remove		= gpio_mouse_remove,
 	.driver		= {
 		.name	= "gpio_mouse",
 		.owner	= THIS_MODULE,
+		.of_match_table = mb8ac0300_gpiomouse_dt_ids,
 	}
 };
 module_platform_driver(gpio_mouse_device_driver);
diff --git a/drivers/irqchip/Kconfig b/drivers/irqchip/Kconfig
index 4a33351..4f4dfec 100644
--- a/drivers/irqchip/Kconfig
+++ b/drivers/irqchip/Kconfig
@@ -41,3 +41,7 @@ config VERSATILE_FPGA_IRQ_NR
        int
        default 4
        depends on VERSATILE_FPGA_IRQ
+
+config MB8AC0300_EXIU
+	bool
+	depends on ARCH_MB8AC0300 || ARCH_MB86S70
diff --git a/drivers/irqchip/Makefile b/drivers/irqchip/Makefile
index f0954d4..68c9c33 100644
--- a/drivers/irqchip/Makefile
+++ b/drivers/irqchip/Makefile
@@ -16,5 +16,6 @@ obj-$(CONFIG_RENESAS_INTC_IRQPIN)	+= irq-renesas-intc-irqpin.o
 obj-$(CONFIG_RENESAS_IRQC)		+= irq-renesas-irqc.o
 obj-$(CONFIG_VERSATILE_FPGA_IRQ)	+= irq-versatile-fpga.o
 obj-$(CONFIG_ARCH_VT8500)		+= irq-vt8500.o
+obj-$(CONFIG_ARCH_MB8AC0300)		+= irq-mb8ac0300.o
 obj-$(CONFIG_ARCH_KEYSTONE)		+= irq-keystone-ipc.o
 obj-$(CONFIG_ARCH_KEYSTONE)		+= irq-tci6614.o
diff --git a/drivers/irqchip/irq-gic.c b/drivers/irqchip/irq-gic.c
index 1275013..5ed5eae 100644
--- a/drivers/irqchip/irq-gic.c
+++ b/drivers/irqchip/irq-gic.c
@@ -41,6 +41,7 @@
 #include <linux/slab.h>
 #include <linux/irqchip/chained_irq.h>
 #include <linux/irqchip/arm-gic.h>
+//#include <trace/events/arm-ipi.h>
 
 #include <asm/cputype.h>
 #include <asm/irq.h>
@@ -693,7 +694,28 @@ void gic_send_sgi(unsigned int cpu_id, unsigned int irq)
 }
 
 /*
- * gic_migrate_target - migrate IRQs to another CPU interface
+ * gic_get_cpu_id - get the CPU interface ID for the specified CPU
+ *
+ * @cpu: the logical CPU number to get the GIC ID for.
+ *
+ * Return the CPU interface ID for the given logical CPU number,
+ * or -1 if the CPU number is too large or the interface ID is
+ * unknown (more than one bit set).
+ */
+int gic_get_cpu_id(unsigned int cpu)
+{
+	unsigned int cpu_bit;
+
+	if (cpu >= NR_GIC_CPU_IF)
+		return -1;
+	cpu_bit = gic_cpu_map[cpu];
+	if (cpu_bit & (cpu_bit - 1))
+		return -1;
+	return __ffs(cpu_bit);
+}
+
+/*
+ * gic_migrate_target - migrate IRQs to another PU interface
  *
  * @new_cpu_id: the CPU target ID to migrate IRQs to
  *
@@ -704,10 +726,10 @@ void gic_send_sgi(unsigned int cpu_id, unsigned int irq)
  */
 void gic_migrate_target(unsigned int new_cpu_id)
 {
-	unsigned int cur_cpu_id, gic_irqs, gic_nr = 0;
+	unsigned int old_cpu_id, gic_irqs, gic_nr = 0;
 	void __iomem *dist_base;
 	int i, ror_val, cpu = smp_processor_id();
-	u32 val, cur_target_mask, active_mask;
+	u32 val, old_mask, active_mask;
 
 	if (gic_nr >= MAX_GIC_NR)
 		BUG();
@@ -717,23 +739,17 @@ void gic_migrate_target(unsigned int new_cpu_id)
 		return;
 	gic_irqs = gic_data[gic_nr].gic_irqs;
 
-	cur_cpu_id = __ffs(gic_cpu_map[cpu]);
-	cur_target_mask = 0x01010101 << cur_cpu_id;
-	ror_val = (cur_cpu_id - new_cpu_id) & 31;
+	old_cpu_id = __ffs(gic_cpu_map[cpu]);
+	old_mask = 0x01010101 << old_cpu_id;
+	ror_val = (old_cpu_id - new_cpu_id) & 31;
 
 	raw_spin_lock(&irq_controller_lock);
 
-	/* Update the target interface for this logical CPU */
 	gic_cpu_map[cpu] = 1 << new_cpu_id;
 
-	/*
-	 * Find all the peripheral interrupts targetting the current
-	 * CPU interface and migrate them to the new CPU interface.
-	 * We skip DIST_TARGET 0 to 7 as they are read-only.
-	 */
 	for (i = 8; i < DIV_ROUND_UP(gic_irqs, 4); i++) {
 		val = readl_relaxed(dist_base + GIC_DIST_TARGET + i * 4);
-		active_mask = val & cur_target_mask;
+		active_mask = val & old_mask;
 		if (active_mask) {
 			val &= ~active_mask;
 			val |= ror32(active_mask, ror_val);
@@ -745,7 +761,7 @@ void gic_migrate_target(unsigned int new_cpu_id)
 
 	/*
 	 * Now let's migrate and clear any potential SGIs that might be
-	 * pending for us (cur_cpu_id).  Since GIC_DIST_SGI_PENDING_SET
+	 * pending for us (old_cpu_id).  Since GIC_DIST_SGI_PENDING_SET
 	 * is a banked register, we can only forward the SGI using
 	 * GIC_DIST_SOFTINT.  The original SGI source is lost but Linux
 	 * doesn't use that information anyway.
diff --git a/drivers/irqchip/irq-mb8ac0300.c b/drivers/irqchip/irq-mb8ac0300.c
new file mode 100644
index 0000000..e060cdf
--- /dev/null
+++ b/drivers/irqchip/irq-mb8ac0300.c
@@ -0,0 +1,268 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/exiu.c
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/irqchip/arm-gic.h>
+#include <linux/irqchip/irq-mb8ac0300.h>
+
+#define DRIVER_NAME		"mb8ac0300-exiu"
+#define DRIVER_DESC		"MB8AC0300 External Interrupt Unit"
+
+/* two banks */
+#define BANKLEN 16
+
+static void __iomem *exiu_base;
+static spinlock_t exiu_lock;
+static int exiu_irq_base[2]; /* 0-15 and 16-31 base irq */
+static u32 exiu_state[EXIU_REG_EXTENT / sizeof(u32)];
+
+static int exiu_irq_to_shift(int irq)
+{
+	if ((irq < exiu_irq_base[0]) ||
+			(irq >= (exiu_irq_base[1] + BANKLEN)) ||
+			(((irq >= (exiu_irq_base[0] + BANKLEN))) &&
+						(irq < exiu_irq_base[1])))
+		return -1;
+
+	if (irq < (exiu_irq_base[0] + BANKLEN))
+		return irq - exiu_irq_base[0];
+
+	return irq - exiu_irq_base[1] + EXTINT16_OFFSET;
+}
+
+
+static void exiu_irq_eoi(struct irq_data *d)
+{
+	int shift;
+
+	shift = exiu_irq_to_shift(d->hwirq);
+	if (shift < 0)
+		return;
+
+	__raw_writel(1 << shift, exiu_base + EXIU_REG_EIREQCLR);
+}
+
+static void exiu_irq_mask(struct irq_data *d)
+{
+	unsigned long val;
+	int shift;
+
+	shift = exiu_irq_to_shift(d->hwirq);
+	if (shift < 0)
+		return;
+
+	/* disable interrupt request */
+	val = __raw_readl(exiu_base + EXIU_REG_EIMASK);
+	val |= (0x1 << shift);
+	__raw_writel(val, exiu_base + EXIU_REG_EIMASK);
+}
+
+static void exiu_irq_unmask(struct irq_data *d)
+{
+	unsigned long val;
+	int shift;
+
+	shift = exiu_irq_to_shift(d->hwirq);
+	if (shift < 0)
+		return;
+
+	/* enable an interrupt request */
+	val = __raw_readl(exiu_base + EXIU_REG_EIMASK);
+	val &= ~(0x1 << shift);
+	__raw_writel(val, exiu_base + EXIU_REG_EIMASK);
+}
+
+int exiu_irq_set_type(unsigned long irq_num, unsigned long type)
+{
+	unsigned long eilvl, eiedg, flags;
+	int shift;
+
+	shift = exiu_irq_to_shift(irq_num);
+	if (shift < 0) {
+		pr_err("%s(): Bad exiu irq num %d.", __func__, (int)irq_num);
+		return -EINVAL;
+	}
+
+	spin_lock_irqsave(&exiu_lock, flags);
+	eilvl = __raw_readl(exiu_base + EXIU_REG_EILVL);
+	eiedg = __raw_readl(exiu_base + EXIU_REG_EIEDG);
+	eilvl &= ~(EXIU_EILVL_MASK << shift);
+	eiedg &= ~(EXIU_EIEDG_MASK << shift);
+
+	switch (type) {
+	case IRQ_TYPE_LEVEL_HIGH:
+		eilvl |= EXIU_EILVL_HIGH << shift;
+		eiedg |= EXIU_EIEDG_LEVEL << shift;
+		break;
+	case IRQ_TYPE_LEVEL_LOW:
+		eilvl |= EXIU_EILVL_LOW << shift;
+		eiedg |= EXIU_EIEDG_LEVEL << shift;
+		break;
+	case IRQ_TYPE_EDGE_RISING:
+		eilvl |= EXIU_EILVL_HIGH << shift;
+		eiedg |= EXIU_EIEDG_EDGE << shift;
+		break;
+	case IRQ_TYPE_EDGE_FALLING:
+		eilvl |= EXIU_EILVL_LOW << shift;
+		eiedg |= EXIU_EIEDG_EDGE << shift;
+		break;
+	default:
+		spin_unlock_irqrestore(&exiu_lock, flags);
+		pr_err("%s(): Bad exiu irq type %lu.", __func__, type);
+		return -EINVAL;
+	}
+	__raw_writel(eilvl, exiu_base + EXIU_REG_EILVL);
+	__raw_writel(eiedg, exiu_base + EXIU_REG_EIEDG);
+	spin_unlock_irqrestore(&exiu_lock, flags);
+
+	return 0;
+}
+EXPORT_SYMBOL(exiu_irq_set_type);
+
+#ifdef CONFIG_PM
+static int exiu_irq_set_wake(struct irq_data *d, unsigned int on)
+{
+	return 0;
+}
+#endif
+
+static int mb8ac0300_exiu_probe(struct platform_device *pdev)
+{
+	struct resource *res;
+
+	spin_lock_init(&exiu_lock);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -EINVAL;
+
+	exiu_base = ioremap(res->start, res->end - res->start + 1);
+	if (!exiu_base) {
+		dev_err(&pdev->dev, "unable to map mem region\n");
+		return -EBUSY;
+	}
+
+	of_property_read_u32(pdev->dev.of_node, "exiu_irq_base0",
+							&exiu_irq_base[0]);
+	of_property_read_u32(pdev->dev.of_node, "exiu_irq_base1",
+							&exiu_irq_base[1]);
+
+	/* Disable all external interrupts */
+	__raw_writel(EXIU_EIMASK_ALL, exiu_base + EXIU_REG_EIMASK);
+
+	/* Set interrupt factor be external interrupt request */
+	__raw_writel(EXIU_EISRCSEL_ALL_EXINT, exiu_base + EXIU_REG_EISRCSEL);
+
+	/* Set all interrupts be edge triggered, active rising */
+	__raw_writel(EXIU_EILVL_ALL_HIGH, exiu_base + EXIU_REG_EILVL);
+	__raw_writel(EXIU_EIEDG_ALL_EDGE, exiu_base + EXIU_REG_EIEDG);
+
+	/* Clear all interrupts */
+	__raw_writel(EXIU_EIREQCLR_ALL, exiu_base + EXIU_REG_EIREQCLR);
+
+	gic_arch_extn.irq_eoi = exiu_irq_eoi;
+	gic_arch_extn.irq_mask = exiu_irq_mask;
+	gic_arch_extn.irq_unmask = exiu_irq_unmask;
+	/* use exiu_irq_set_type directly */
+	gic_arch_extn.irq_set_type = NULL;
+#ifdef CONFIG_PM
+	gic_arch_extn.irq_set_wake = exiu_irq_set_wake;
+#endif
+	return 0;
+}
+
+static int mb8ac0300_exiu_remove(struct platform_device *pdev)
+{
+	__raw_writel(EXIU_EIMASK_ALL, exiu_base + EXIU_REG_EIMASK);
+
+	iounmap(exiu_base);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+
+int mb8ac0300_exiu_suspend_noirq(struct device *dev)
+{
+	int n;
+
+	dev_info(dev, "saving exiu state\n");
+	for (n = 0; n < EXIU_REG_EXTENT; n += 4)
+		exiu_state[n >> 2] = readl(exiu_base + n);
+
+        return 0;
+}
+
+int mb8ac0300_exiu_resume_noirq(struct device *dev)
+{
+	dev_info(dev, "restoring exiu state\n");
+
+	writel(exiu_state[EXIU_REG_EILVL >> 2], exiu_base + EXIU_REG_EILVL);
+	writel(exiu_state[EXIU_REG_EIEDG >> 2], exiu_base + EXIU_REG_EIEDG);
+	writel(EXIU_EIREQCLR_ALL, exiu_base + EXIU_REG_EIREQCLR);
+	writel(exiu_state[EXIU_REG_EIMASK >> 2], exiu_base + EXIU_REG_EIMASK);
+
+        return 0;
+}
+
+static const struct dev_pm_ops mb8ac0300_exiu_pm_ops = {
+	.suspend_noirq = mb8ac0300_exiu_suspend_noirq,
+        .resume_noirq = mb8ac0300_exiu_resume_noirq,
+};
+
+#endif /* CONFIG_PM */
+
+#ifdef CONFIG_OF
+static const struct of_device_id mb8ac0300_exiu_dt_ids[] = {
+	{ .compatible = "fujitsu,mb8ac0300-exiu" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, mb8ac0300_hdmac_dt_ids);
+#else
+#define mb8ac0300_hdmac_dt_ids NULL
+#endif
+
+static struct platform_driver mb8ac0300_exiu_driver = {
+	.probe     = mb8ac0300_exiu_probe,
+	.remove    = mb8ac0300_exiu_remove,
+	.driver    = {
+		.name  = DRIVER_NAME,
+		.owner = THIS_MODULE,
+#ifdef CONFIG_PM
+		.pm = &mb8ac0300_exiu_pm_ops,
+#endif /* CONFIG_PM */
+		.of_match_table = mb8ac0300_exiu_dt_ids,
+	},
+};
+
+static int __init mb8ac0300_exiu_driver_init(void)
+{
+	return platform_driver_register(&mb8ac0300_exiu_driver);
+}
+subsys_initcall(mb8ac0300_exiu_driver_init);
+
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_ALIAS("platform:" DRIVER_NAME);
+MODULE_LICENSE("GPL");
diff --git a/drivers/mailbox/Kconfig b/drivers/mailbox/Kconfig
index c8b5c13..3a55530 100644
--- a/drivers/mailbox/Kconfig
+++ b/drivers/mailbox/Kconfig
@@ -5,6 +5,10 @@ menuconfig MAILBOX
 	  on-chip processors through queued messages and interrupt driven
 	  signals. Say Y if your platform supports hardware mailboxes.
 
+config DEBUG_MBOX
+	bool "Debug Mailbox API"
+	depends on MAILBOX
+
 if MAILBOX
 config PL320_MBOX
 	bool "ARM PL320 Mailbox"
@@ -47,7 +51,25 @@ config OMAP_MBOX_KFIFO_SIZE
 	depends on OMAP2PLUS_MBOX || OMAP1_MBOX
 	default 256
 	help
-	  Specify the default size of mailbox's kfifo buffers (bytes).
+	  Specify the default size of mailbox's associated data buffer
+	  (bytes)
 	  This can also be changed at runtime (via the mbox_kfifo_size
 	  module parameter).
+
+config MBOX_F_IPCU
+	tristate "Fujitsu IPCU Mailbox"
+ 	depends on ARCH_MB8AC0300 || ARCH_MB86S70
+ 	help
+ 	  Fujitsu Inter-Processor Communication Unit driver
+
+config MBOX_F_MHU
+	bool "Fujitsu MHU Controller"
+	depends on ARCH_MB86S70
+	help
+	  Say Y here if you want to use the F_MHU IPCM support.
+
+config MBOX_F_MHU
+	bool "Fujitsu MHU Controller"
+	help
+	  Say Y here if you want to use the F_MHU IPCM support.
 endif
diff --git a/drivers/mailbox/Makefile b/drivers/mailbox/Makefile
index e0facb3..16c5e52 100644
--- a/drivers/mailbox/Makefile
+++ b/drivers/mailbox/Makefile
@@ -1,4 +1,10 @@
-obj-$(CONFIG_PL320_MBOX)	+= pl320-ipc.o
+# Generic MAILBOX API
+
+obj-$(CONFIG_MAILBOX)		+= mailbox.o
+
+obj-$(CONFIG_MBOX_F_IPCU)	+= f_ipcu.o
+obj-$(CONFIG_PL320_MBOX)	+= pl320.o
+obj-$(CONFIG_MBOX_F_MHU)	+= f_mhu.o
 
 obj-$(CONFIG_OMAP_MBOX)		+= omap-mailbox.o
 obj-$(CONFIG_OMAP1_MBOX)	+= mailbox_omap1.o
diff --git a/drivers/mailbox/f_ipcu.c b/drivers/mailbox/f_ipcu.c
new file mode 100644
index 0000000..aed6bd7
--- /dev/null
+++ b/drivers/mailbox/f_ipcu.c
@@ -0,0 +1,397 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/types.h>
+#include <linux/err.h>
+#include <linux/delay.h>
+#include <linux/export.h>
+#include <linux/io.h>
+#include <linux/interrupt.h>
+#include <linux/completion.h>
+#include <linux/mutex.h>
+#include <linux/notifier.h>
+#include <linux/spinlock.h>
+#include <linux/device.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/of.h>
+#include <linux/module.h>
+#include <linux/clk.h>
+#include <linux/f_ipcu.h>
+#include <linux/mailbox_controller.h>
+
+#define MAR(i)		(0x80 + (i) * 4)
+#define OFFSET_MBOX(m)	(0x100 + 0x80 * (m))
+#define SRC_REG(m)	(OFFSET_MBOX(m) + 0x0)
+#define MODE_REG(m)	(OFFSET_MBOX(m) + 0x4)
+#define SEND_REG(m)	(OFFSET_MBOX(m) + 0x8)
+#define DST_SET(m)	(OFFSET_MBOX(m) + 0x10)
+#define DST_CLR(m)	(OFFSET_MBOX(m) + 0x14)
+#define DST_STAT(m)	(OFFSET_MBOX(m) + 0x18)
+#define ACK_SET(m)	(OFFSET_MBOX(m) + 0x30)
+#define ACK_CLR(m)	(OFFSET_MBOX(m) + 0x34)
+#define ACK_STAT(m)	(OFFSET_MBOX(m) + 0x38)
+#define DAT_REG(m)	(OFFSET_MBOX(m) + 0x40)
+
+#define IPCU_DSTMASK	0xffff /* 16 max cpu interfaces */
+
+struct ipcu_mbox {
+	/* The CPU i/f that owns this mailbox */
+	int cpuif;
+	/* Physical mailbox this structure represents */
+	int id;
+	/* Parent IPCU controller */
+	struct ipcu *ipcu;
+	struct ipc_link link;
+};
+
+struct ipcu {
+	struct device *dev;
+	void __iomem *base;
+	struct clk *clk;
+	/* Number of mailboxes */
+	int mboxes;
+	/* Array of mailboxes */
+	struct ipcu_mbox *mbox;
+	/* Number of irq interfaces */
+	int ifaces;
+	/* Array of irqs */
+	int *irq;
+	struct ipc_controller ipc_con;
+};
+
+static inline struct ipcu_mbox *to_mbox(struct ipc_link *l)
+{
+	if (!l)
+		return NULL;
+
+	return container_of(l, struct ipcu_mbox, link);
+}
+
+static irqreturn_t ipcu_interrupt(int irq, void *data)
+{
+	struct ipcu_mbox *mbox = data;
+	struct ipcu *ipcu = mbox->ipcu;
+	u32 ack, mar;
+	int i, src;
+
+	mar = readl(ipcu->base + MAR(mbox->cpuif));
+	src = ((mar & 0xfff) - 0x100) / 0x80;
+
+	if (mar & (1 << 13)) { /* ACK */
+		ack = readl(ipcu->base + ACK_STAT(src));
+		writel(ack, ipcu->base + ACK_CLR(src));
+		writel(0, ipcu->base + SEND_REG(src));
+		ipc_link_txdone(&mbox->link, XFER_OK);
+
+		return IRQ_HANDLED;
+	}
+
+	if (mar & (1 << 12)) { /* REQ */
+		struct ipcu_mssg mssg;
+
+		/* Read the data */
+		for (i = 0; i < ARRAY_SIZE(mssg.data); i++)
+			mssg.data[i] = readl(ipcu->base +
+					DAT_REG(src) + i * 4);
+		mssg.mask = readl(ipcu->base + SRC_REG(src));
+
+		/* Handover the message to client */
+		ipc_link_received_data(&mbox->link, (void *)&mssg);
+
+		writel(1 << mbox->cpuif, ipcu->base + DST_CLR(src));
+		writel(1 << mbox->cpuif, ipcu->base + ACK_SET(src));
+
+		return IRQ_HANDLED;
+	}
+
+	mbox_dbg("%s:%d !!!\n", __func__, __LINE__);
+
+	return IRQ_HANDLED;
+}
+
+static int ipcu_send_data(struct ipc_link *link, void *data)
+{
+	struct ipcu_mssg *mssg = data;
+	struct ipcu_mbox *mbox = to_mbox(link);
+	struct ipcu *ipcu = mbox->ipcu;
+	int i;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+
+	/* Reject if the mailbox is readonly */
+	if (mbox->id == ipcu->mboxes)
+		return -EIO;
+
+	/* Reject if there is any invalid destination */
+	if (mssg->mask & ~IPCU_DSTMASK)
+		return -EINVAL;
+
+	/* Reject if busy */
+	if (readl(ipcu->base + SEND_REG(mbox->id)))
+		return -EBUSY;
+
+	writel(mssg->mask, ipcu->base + DST_SET(mbox->id));
+
+	/* Fill the data */
+	for (i = 0; i < 9; i++)
+		writel(mssg->data[i],
+				ipcu->base + DAT_REG(mbox->id) + i * 4);
+
+	/* Trigger */
+	writel(1, ipcu->base + SEND_REG(mbox->id));
+
+	return 0;
+}
+
+static int ipcu_startup(struct ipc_link *link, void *params)
+{
+	struct ipcu_mbox *mbox = to_mbox(link);
+	struct ipcu_client *cl = params;
+	struct ipcu *ipcu = mbox->ipcu;
+	int i, ret;
+	u32 val;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+
+	if (cl->iface >= ipcu->ifaces)
+		return -EINVAL;
+
+	/* Fail if the cpu i/f already owns some other mailbox */
+	for (i = 0; i < ipcu->mboxes; i++)
+		if (ipcu->mbox[i].cpuif == cl->iface)
+			return -EBUSY;
+
+	/* The mbox structure doesn't own a physical mbox yet */
+	mbox->id = ipcu->mboxes;
+
+	/* If the mailbox is not going to be used as read-only */
+	if (!cl->ro) {
+		/* Try to exclusively own a mailbox */
+		for (i = 0; i < ipcu->mboxes; i++) {
+			writel(1 << cl->iface, ipcu->base + SRC_REG(i));
+			mb();
+			val = readl(ipcu->base + SRC_REG(i));
+			if (val == (1 << cl->iface)) {
+				/* Set to Mode1 */
+				writel(0, ipcu->base + MODE_REG(i));
+				break;
+			}
+		}
+		/* If no mailbox is available */
+		if (i == ipcu->mboxes)
+			return -EAGAIN;
+
+		/* The mbox structure owns i'th physical mbox */
+		mbox->id = i;
+	}
+
+	/* Assign the mailbox to the cpu i/f */
+	mbox->cpuif = cl->iface;
+
+	ret = request_irq(ipcu->irq[cl->iface], ipcu_interrupt,
+				IRQF_SHARED, mbox->link.link_name, mbox);
+	if (unlikely(ret)) {
+		printk("Unable to aquire IRQ\n");
+		if (mbox->id != ipcu->mboxes)
+			writel(0, ipcu->base + SRC_REG(mbox->id));
+		mbox->cpuif = -1;
+		mbox->id = ipcu->mboxes;
+	}
+
+	return ret;
+}
+
+static void ipcu_shutdown(struct ipc_link *link)
+{
+	struct ipcu_mbox *mbox = to_mbox(link);
+	struct ipcu *ipcu = mbox->ipcu;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+
+	free_irq(ipcu->irq[mbox->cpuif], mbox);
+
+	if (mbox->id != ipcu->mboxes)
+		writel(0, ipcu->base + SRC_REG(mbox->id));
+
+	mbox->id = ipcu->mboxes;
+	mbox->cpuif = -1;
+}
+
+static struct ipc_link_ops ipcu_ops = {
+	.send_data = ipcu_send_data,
+	.startup = ipcu_startup,
+	.shutdown = ipcu_shutdown,
+};
+
+static int f_ipcu_probe(struct platform_device *pdev)
+{
+	struct ipcu_mbox *mbox;
+	int i, mboxes, ifaces;
+	struct resource	*res;
+	struct ipc_link **l;
+	struct ipcu *ipcu;
+	const int *p;
+	int ret;
+
+	if (!pdev->dev.of_node) {
+		dev_err(&pdev->dev, "Requires DT node\n");
+		return -ENODEV;
+	}
+
+	p = of_get_property(pdev->dev.of_node, "mboxes", NULL);
+	if (!p) {
+		dev_err(&pdev->dev, "Requires DT \"mboxes\" property\n");
+		return -ENODEV;
+	}
+	mboxes = be32_to_cpu(*p);
+
+	/* Calculate number of local cpu interfaces */
+	ifaces = 0;
+	do {
+		res = platform_get_resource(pdev, IORESOURCE_IRQ, ifaces++);
+	} while (res);
+	ifaces--;
+
+	if (!ifaces) {
+		dev_err(&pdev->dev, "Requires DT \"interrupts\" property\n");
+		return -ENODEV;
+	}
+
+	/* Allocate memory for device */
+	ipcu = kzalloc(sizeof(struct ipcu), GFP_KERNEL);
+	if (!ipcu)
+		return -ENOMEM;
+
+	ipcu->dev = &pdev->dev;
+	ipcu->mboxes = mboxes;
+	ipcu->ifaces = ifaces;
+	ipcu->irq = kmalloc(sizeof(int) * ifaces, GFP_KERNEL);
+	if (!ipcu->irq) {
+		ret = -ENOMEM;
+		goto fail1;
+	}
+	ipcu->mbox = kmalloc(sizeof(struct ipcu_mbox) * mboxes, GFP_KERNEL);
+	if (!ipcu->mbox) {
+		ret = -ENOMEM;
+		goto fail2;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "no mmio resource defined.\n");
+		ret = -ENXIO;
+		goto fail3;
+	}
+	ipcu->base = ioremap(res->start, resource_size(res));
+	if (!ipcu->base) {
+		dev_err(&pdev->dev, "ioremap failed.\n");
+		ret = -ENXIO;
+		goto fail3;
+	}
+
+	for (i = 0; i < ifaces; i++) {
+		res = platform_get_resource(pdev, IORESOURCE_IRQ, i);
+		ipcu->irq[i] = res->start;
+	}
+
+	l = kzalloc((mboxes + 1) * sizeof(struct ipc_link *), GFP_KERNEL);
+	if (!l) {
+		ret = -ENOMEM;
+		goto l_fail;
+	}
+
+	for (i = 0; i < mboxes; i++) {
+		mbox = &ipcu->mbox[i];
+		mbox->id = ipcu->mboxes;
+		mbox->cpuif = -1;
+		mbox->ipcu = ipcu;
+		l[i] = &mbox->link;
+		snprintf(mbox->link.link_name,
+			 sizeof(mbox->link.link_name), "mbox");
+		mbox_dbg("%s:%d link=%p\n", __func__, __LINE__, l[i]);
+	}
+
+	l[mboxes] = NULL;
+	ipcu->ipc_con.links = l;
+	ipcu->ipc_con.ops = &ipcu_ops;
+	ipcu->ipc_con.txdone_irq = true;
+	/* Every mailbox is named "f_ipcu:mbox" */
+	snprintf(ipcu->ipc_con.controller_name,
+		 sizeof(ipcu->ipc_con.controller_name), "f_ipcu");
+
+	platform_set_drvdata(pdev, ipcu);
+
+	ret = ipc_links_register(&ipcu->ipc_con);
+	if (ret) {
+		dev_err(&pdev->dev, "Failed to register mailboxes %d\n", ret);
+		goto fail;
+	}
+	kfree(l);
+
+	dev_info(&pdev->dev, "IPCU Mailbox registered with %d interfaces\n",
+		 ifaces);
+
+	return 0;
+
+fail:
+	kfree(l);
+l_fail:
+	iounmap(ipcu->base);
+fail3:
+	kfree(ipcu->mbox);
+fail2:
+	kfree(ipcu->irq);
+fail1:
+	kfree(ipcu);
+
+	return ret;
+}
+
+static int __exit f_ipcu_remove(struct platform_device *pdev)
+{
+	struct ipcu *ipcu = platform_get_drvdata(pdev);
+
+	ipc_links_unregister(&ipcu->ipc_con);
+	iounmap(ipcu->base);
+	kfree(ipcu->mbox);
+	kfree(ipcu->irq);
+	kfree(ipcu);
+
+	return 0;
+}
+
+static const struct of_device_id f_ipcu_dt_ids[] = {
+	{ .compatible = "fujitsu,ipcu" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, f_ipcu_dt_ids);
+
+static struct platform_driver f_ipcu_driver = {
+	.driver		= {
+		.name	= "f_ipcu",
+		.owner = THIS_MODULE,
+		.of_match_table = f_ipcu_dt_ids,
+	},
+	.probe		= f_ipcu_probe,
+	.remove		= __exit_p(f_ipcu_remove),
+};
+
+static int __init f_ipcu_init(void)
+{
+	return platform_driver_register(&f_ipcu_driver);
+}
+module_init(f_ipcu_init);
+
+static void __exit f_ipcu_exit(void)
+{
+	platform_driver_unregister(&f_ipcu_driver);
+}
+module_exit(f_ipcu_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Fujitsu IPCU Driver");
+MODULE_AUTHOR("Jassi Brar <jaswinder.singh@linaro.org>");
diff --git a/drivers/mailbox/f_mhu.c b/drivers/mailbox/f_mhu.c
new file mode 100644
index 0000000..61965f1
--- /dev/null
+++ b/drivers/mailbox/f_mhu.c
@@ -0,0 +1,244 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/clk.h>
+#include <linux/module.h>
+#include <linux/mailbox_controller.h>
+#include <linux/platform_device.h>
+
+#define INTR_STAT_OFS	0x0
+#define INTR_SET_OFS	0x8
+#define INTR_CLR_OFS	0x10
+
+#define MHU_SCFG	0x400
+
+struct mhu_link {
+	unsigned irq;
+	spinlock_t lock;
+	void __iomem *tx_reg;
+	void __iomem *rx_reg;
+	struct ipc_link link;
+};
+
+struct f_mhu {
+	struct device *dev;
+	void __iomem *base;
+	struct clk *clk;
+	struct mhu_link mlink[3];
+	struct ipc_controller ipc_con;
+};
+
+static char *ch_name[] = {"LP_NonSec", "HP_NonSec", "Secure"};
+
+static void __iomem *__hackbase;
+
+static inline struct mhu_link *to_mlink(struct ipc_link *l)
+{
+	if (!l)
+		return NULL;
+
+	return container_of(l, struct mhu_link, link);
+}
+
+static irqreturn_t mhu_rx_interrupt(int irq, void *p)
+{
+	struct mhu_link *mlink = to_mlink(p);
+	u32 val;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+	/* See NOTE_RX_DONE */
+	val = __raw_readl(mlink->rx_reg + INTR_STAT_OFS);
+	ipc_link_received_data(&mlink->link, (void *)val);
+
+	/*
+	 * It is agreed with the remote firmware that the receiver
+	 * will clear the STAT register indicating it is ready to
+	 * receive next data - NOTE_RX_DONE
+	 */
+	__raw_writel(val, mlink->rx_reg + INTR_CLR_OFS);
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * MHU doesn't get "remote RTR" interrupt, so we can't call
+ * ipc_link_txdone() instead we provide this callback for
+ * the API to poll the controller for status
+ */
+static bool mhu_last_tx_done(struct ipc_link *link)
+{
+	struct mhu_link *mlink = to_mlink(link);
+	unsigned long flags;
+	u32 val;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+	spin_lock_irqsave(&mlink->lock, flags);
+	/* See NOTE_RX_DONE */
+	val = __raw_readl(mlink->tx_reg + INTR_STAT_OFS);
+	spin_unlock_irqrestore(&mlink->lock, flags);
+
+	return (val == 0);
+}
+
+static int mhu_send_data(struct ipc_link *link, void *data)
+{
+	struct mhu_link *mlink = to_mlink(link);
+	unsigned long flags;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+	if (!mhu_last_tx_done(link)) {
+		printk("%s:%d Shouldn't have seen the day!\n",
+			__func__, __LINE__);
+		return -EBUSY;
+	}
+
+	spin_lock_irqsave(&mlink->lock, flags);
+	__raw_writel((u32)data, mlink->tx_reg + INTR_SET_OFS);
+	spin_unlock_irqrestore(&mlink->lock, flags);
+
+	return 0;
+}
+
+static int mhu_startup(struct ipc_link *link, void *ignored)
+{
+	struct mhu_link *mlink = to_mlink(link);
+	unsigned long flags;
+	u32 val;
+	int ret;
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+	spin_lock_irqsave(&mlink->lock, flags);
+	val = __raw_readl(mlink->tx_reg + INTR_STAT_OFS);
+	__raw_writel(val, mlink->tx_reg + INTR_CLR_OFS);
+	spin_unlock_irqrestore(&mlink->lock, flags);
+
+	ret = request_irq(mlink->irq, mhu_rx_interrupt,
+		IRQF_SHARED | IRQF_NO_THREAD, mlink->link.link_name, link);
+	if (unlikely(ret)) {
+		printk("Unable to aquire IRQ\n");
+		return ret;
+	}
+
+	return 0;
+}
+
+static void mhu_shutdown(struct ipc_link *link)
+{
+	struct mhu_link *mlink = to_mlink(link);
+
+	mbox_dbg("%s:%d\n", __func__, __LINE__);
+	free_irq(mlink->irq, link);
+}
+
+static struct ipc_link_ops mhu_ops = {
+	.send_data = mhu_send_data,
+	.startup = mhu_startup,
+	.shutdown = mhu_shutdown,
+	.last_tx_done = mhu_last_tx_done,
+};
+
+static int f_mhu_probe(struct platform_device *pdev)
+{
+	int i, err;
+	struct f_mhu *mhu;
+	struct resource *res;
+	struct ipc_link *l[4];
+	struct mhu_link *mlink;
+	int mhu_reg[3] = {0x0, 0x20, 0x200};
+
+	/* Allocate memory for device */
+	mhu = kzalloc(sizeof(struct f_mhu), GFP_KERNEL);
+	if (!mhu) {
+		dev_err(&pdev->dev, "failed to allocate memory.\n");
+		return -EBUSY;
+	}
+
+	mhu->clk = clk_get(&pdev->dev, "clk");
+	if (unlikely(IS_ERR(mhu->clk))) {
+		dev_err(&pdev->dev, "unable to init clock\n");
+		kfree(mhu);
+		return -EINVAL;
+	}
+	clk_prepare_enable(mhu->clk);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	mhu->base = ioremap(res->start, resource_size(res));
+	if (!mhu->base) {
+		dev_err(&pdev->dev, "ioremap failed.\n");
+		kfree(mhu);
+		return -EBUSY;
+	}
+	mhu->dev = &pdev->dev;
+	__hackbase = mhu->base;
+
+	/* Let UnTrustedOS's access violations don't bother us */
+	__raw_writel(0, mhu->base + MHU_SCFG);
+
+	for (i = 0; i < 3; i++) {
+		mlink = &mhu->mlink[i];
+		spin_lock_init(&mlink->lock);
+		snprintf(mlink->link.link_name, 16, ch_name[i]);
+		res = platform_get_resource(pdev, IORESOURCE_IRQ, i);
+		mlink->irq = res->start;
+		mlink->rx_reg = mhu->base + mhu_reg[i];
+		mlink->tx_reg = mlink->rx_reg + 0x100;
+		l[i] = &mlink->link;
+		mbox_dbg("%s:%d link=%p\n", __func__, __LINE__, l[i]);
+	}
+
+	l[3] = NULL;
+	mhu->ipc_con.links = l;
+	mhu->ipc_con.ops = &mhu_ops;
+	mhu->ipc_con.txdone_irq = false;
+	mhu->ipc_con.txdone_poll = true;
+	mhu->ipc_con.txpoll_period = 10;
+	snprintf(mhu->ipc_con.controller_name, 16, "f_mhu");
+
+	platform_set_drvdata(pdev, mhu);
+
+	err = ipc_links_register(&mhu->ipc_con);
+	if (err) {
+		dev_err(&pdev->dev, "Failed to register mailboxes %d\n", err);
+		iounmap(mhu->base);
+		kfree(mhu);
+	} else {
+		dev_info(&pdev->dev, "Fujitsu MHU Mailbox registered\n");
+	}
+
+	return 0;
+}
+
+static const struct of_device_id f_mhu_dt_ids[] = {
+	{ .compatible = "fujitsu,mhu" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, f_mhu_dt_ids);
+
+static struct platform_driver f_mhu_driver = {
+	.driver		= {
+		.name	= "f_mhu",
+		.owner = THIS_MODULE,
+		.of_match_table = f_mhu_dt_ids,
+	},
+	.probe		= f_mhu_probe,
+};
+
+static int __init f_mhu_init(void)
+{
+	return platform_driver_register(&f_mhu_driver);
+}
+module_init(f_mhu_init);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("Fujitsu MHU Driver");
+MODULE_AUTHOR("Jassi Brar <jaswinder.singh@linaro.org>");
diff --git a/drivers/mailbox/mailbox.c b/drivers/mailbox/mailbox.c
new file mode 100644
index 0000000..004a51d
--- /dev/null
+++ b/drivers/mailbox/mailbox.c
@@ -0,0 +1,475 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/interrupt.h>
+#include <linux/spinlock.h>
+#include <linux/mutex.h>
+#include <linux/delay.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/module.h>
+#include <linux/mailbox_client.h>
+#include <linux/mailbox_controller.h>
+
+/*
+ * The length of circular buffer for queuing messages from a client.
+ * 'msg_count' tracks the number of buffered messages while 'msg_free'
+ * is the index where the next message would be buffered.
+ * We shouldn't need it too big because every transferr is interrupt
+ * triggered and if we have lots of data to transfer, the interrupt
+ * latencies are going to be the bottleneck, not the buffer length.
+ * Besides, ipc_send_message could be called from atomic context and
+ * the client could also queue another message from the notifier 'txcb'
+ * of the last transfer done.
+ */
+#define MBOX_TX_QUEUE_LEN	10
+
+#define TXDONE_BY_IRQ	(1 << 0) /* controller has remote RTR irq */
+#define TXDONE_BY_POLL 	(1 << 1) /* controller can read status of last TX */
+#define TXDONE_BY_ACK	(1 << 2) /* S/W ACK recevied by Client ticks the TX */
+
+struct ipc_chan {
+	char chan_name[32]; /* controller_name:link_name */
+	unsigned txdone_method;
+
+	/* Cached values from controller */
+	struct ipc_link *link;
+	struct ipc_link_ops *link_ops;
+
+	/* Cached values from client */
+	void (*rxcb)(void *data);
+	void (*txcb)(request_token_t t, enum xfer_result r);
+	bool tx_block;
+	unsigned long tx_tout;
+	struct completion tx_complete;
+
+	request_token_t active_token;
+	unsigned msg_count, msg_free;
+	void *msg_data[MBOX_TX_QUEUE_LEN];
+	/* Timer shared by all links of a controller */
+	struct tx_poll_timer *timer;
+	bool assigned;
+	/* Serialize access to the channel */
+	spinlock_t lock;
+	/* Hook to add to the global list of channels */
+	struct list_head node;
+	/* Notifier to all clients waiting on aquiring this channel */
+	struct blocking_notifier_head avail;
+};
+
+/*
+ * If the controller supports only TXDONE_BY_POLL, this
+ * timer polls all the links for txdone.
+ */
+struct tx_poll_timer {
+	struct timer_list poll;
+	unsigned period;
+};
+
+static LIST_HEAD(ipc_channels);
+static DEFINE_MUTEX(chpool_mutex);
+
+static request_token_t _add_to_rbuf(struct ipc_chan *chan, void *data)
+{
+	request_token_t idx;
+	unsigned long flags;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	/* See if there is any space left */
+	if (chan->msg_count == MBOX_TX_QUEUE_LEN) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return 0;
+	}
+
+	idx = chan->msg_free;
+	chan->msg_data[idx] = data;
+	chan->msg_count++;
+
+	if (idx == MBOX_TX_QUEUE_LEN - 1)
+		chan->msg_free = 0;
+	else
+		chan->msg_free++;
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	return idx + 1;
+}
+
+static void _msg_submit(struct ipc_chan *chan)
+{
+	struct ipc_link *link = chan->link;
+	unsigned count, idx;
+	unsigned long flags;
+	void *data;
+	int err;
+
+	spin_lock_irqsave(&chan->lock, flags);
+
+	if (!chan->msg_count || chan->active_token) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return;
+	}
+
+	count = chan->msg_count;
+	idx = chan->msg_free;
+	if (idx >= count)
+		idx -= count;
+	else
+		idx += MBOX_TX_QUEUE_LEN - count;
+
+	data = chan->msg_data[idx];
+
+	/* Try to submit a message to the IPC controller */
+	err = chan->link_ops->send_data(link, data);
+	if (!err) {
+		chan->active_token = idx + 1;
+		chan->msg_count--;
+	}
+
+	spin_unlock_irqrestore(&chan->lock, flags);
+}
+
+static void tx_tick(struct ipc_chan *chan, enum xfer_result r)
+{
+	unsigned long flags;
+	request_token_t t;
+
+	spin_lock_irqsave(&chan->lock, flags);
+	t = chan->active_token;
+	chan->active_token = 0;
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	/* Submit next message */
+	_msg_submit(chan);
+
+	/* Notify the client */
+	if (chan->tx_block)
+		complete(&chan->tx_complete);
+	else if (t && chan->txcb)
+		chan->txcb(t, r);
+}
+
+static void poll_txdone(unsigned long data)
+{
+	struct tx_poll_timer *timer = (struct tx_poll_timer *)data;
+	bool txdone, resched = false;
+	struct ipc_chan *chan;
+
+	list_for_each_entry(chan, &ipc_channels, node) {
+		if (chan->timer == timer && chan->active_token) {
+			resched = true;
+			txdone = chan->link_ops->last_tx_done(chan->link);
+			if (txdone)
+				tx_tick(chan, XFER_OK);
+		}
+	}
+
+	if (resched)
+		mod_timer(&timer->poll,
+			jiffies + msecs_to_jiffies(timer->period));
+}
+
+/*
+ * After 'startup' and before 'shutdown', the IPC controller driver
+ * notifies the API of data received over the link.
+ * The controller driver should make sure the 'RTR' is de-asserted since
+ * reception of the packet and until after this call returns.
+ * This call could be made from atomic context.
+ */
+void ipc_link_received_data(struct ipc_link *link, void *data)
+{
+	struct ipc_chan *chan = (struct ipc_chan *)link->api_priv;
+
+	mbox_dbg("%s:%d link=%p chan=%p\n", __func__, __LINE__, link, chan);
+	/* No buffering the received data */
+	if (chan->rxcb)
+		chan->rxcb(data);
+}
+EXPORT_SYMBOL(ipc_link_received_data);
+
+/*
+ * The IPC controller driver notifies the API that the remote has
+ * asserted RTR and it could now send another message on the link.
+ */
+void ipc_link_txdone(struct ipc_link *link, enum xfer_result r)
+{
+	struct ipc_chan *chan = (struct ipc_chan *)link->api_priv;
+
+	if (unlikely(!(chan->txdone_method & TXDONE_BY_IRQ))) {
+		printk("Controller can't run the TX ticker\n");
+		return;
+	}
+
+	tx_tick(chan, r);
+}
+EXPORT_SYMBOL(ipc_link_txdone);
+
+/*
+ * The client/protocol had received some 'ACK' packet and it notifies
+ * the API that the last packet was sent successfully. This only works
+ * if the controller doesn't get IRQ for TX done.
+ */
+void ipc_client_txdone(void *channel, enum xfer_result r)
+{
+	struct ipc_chan *chan = (struct ipc_chan *)channel;
+	bool txdone = true;
+
+	if (unlikely(!(chan->txdone_method & TXDONE_BY_ACK))) {
+		printk("Client can't run the TX ticker\n");
+		return;
+	}
+
+	if (chan->txdone_method & TXDONE_BY_POLL)
+		txdone = chan->link_ops->last_tx_done(chan->link);
+
+	if (txdone)
+		tx_tick(chan, r);
+}
+EXPORT_SYMBOL(ipc_client_txdone);
+
+/*
+ * Called by a client to "put data on the h/w channel" so that if
+ * everything else is fine we don't need to do anything more locally
+ * for the remote to receive the data intact.
+ * In reality, the remote may receive it intact, corrupted or not at all.
+ * This could be called from atomic context as it simply
+ * queues the data and returns a token (request_token_t)
+ * against the request.
+ * The client is later notified of successful transmission of
+ * data over the channel via the 'txcb'. The client could in
+ * turn queue more messages from txcb.
+ */
+request_token_t ipc_send_message(void *channel, void *data)
+{
+	struct ipc_chan *chan = (struct ipc_chan *)channel;
+	request_token_t t;
+
+	if (!chan) {
+		pr_err("%s:%d!!!\n", __func__, __LINE__);
+		return 0;
+	}
+
+	if (chan->tx_block)
+		init_completion(&chan->tx_complete);
+
+	t = _add_to_rbuf(chan, data);
+	if (!t)
+		printk("Try increasing MBOX_TX_QUEUE_LEN\n");
+
+	_msg_submit(chan);
+
+	if (chan->txdone_method	& TXDONE_BY_POLL)
+		poll_txdone((unsigned long)chan->timer);
+
+	if (chan->tx_block && chan->active_token) {
+		int ret;
+		ret = wait_for_completion_timeout(&chan->tx_complete,
+			chan->tx_tout);
+		if (ret == 0) {
+			t = 0;
+			pr_err("%s:%d!!!\n", __func__, __LINE__);
+			tx_tick(chan, XFER_ERR);
+		}
+	}
+
+	return t;
+}
+EXPORT_SYMBOL(ipc_send_message);
+
+/*
+ * A client driver asks for exclusive use of a channel/mailbox.
+ * If assigned, the channel has to be 'freed' before it could
+ * be assigned to some other client.
+ * After assignment, any packet received on this channel will be
+ * handed over to the client via the 'rxcb' callback.
+ * The 'txcb' callback is used to notify client upon sending the
+ * packet over the channel, which may or may not have been yet
+ * read by the remote processor.
+ */
+void *ipc_request_channel(struct ipc_client *cl)
+{
+	struct ipc_chan *chan;
+	unsigned long flags;
+	int ret = 0;
+
+	mutex_lock(&chpool_mutex);
+
+	list_for_each_entry(chan, &ipc_channels, node) {
+		spin_lock_irqsave(&chan->lock, flags);
+		if(!chan->assigned
+				&& !strcmp(cl->chan_name, chan->chan_name)) {
+			chan->msg_free = 0;
+			chan->msg_count = 0;
+			chan->active_token = 0;
+			chan->rxcb = cl->rxcb;
+			chan->txcb = cl->txcb;
+			chan->assigned = true;
+			chan->tx_block = cl->tx_block;
+			mbox_dbg("%s:%d chan=%p rxcb=%p\n",
+				__func__, __LINE__, chan, chan->rxcb);
+			if (!cl->tx_tout)
+				chan->tx_tout = msecs_to_jiffies(500); /* 500ms default */
+			else
+				chan->tx_tout = msecs_to_jiffies(cl->tx_tout);
+			if (chan->txdone_method	== TXDONE_BY_POLL
+					&& cl->knows_txdone)
+				chan->txdone_method |= TXDONE_BY_ACK;
+			ret = 1;
+		}
+		spin_unlock_irqrestore(&chan->lock, flags);
+		if (ret)
+			break;
+	}
+
+	mutex_unlock(&chpool_mutex);
+
+	if (!ret) {
+		mbox_dbg("Unable to assign mailbox(%s)\n", cl->chan_name);
+		return NULL;
+	}
+
+	ret = chan->link_ops->startup(chan->link, cl->cntlr_data);
+	if (ret) {
+		printk("Unable to startup the link\n");
+		ipc_free_channel((void *)chan);
+		return NULL;
+	}
+
+	return (void *)chan;
+}
+EXPORT_SYMBOL(ipc_request_channel);
+
+/* Drop any messages queued and release the channel */
+void ipc_free_channel(void *ch)
+{
+	struct ipc_chan *chan = (struct ipc_chan *)ch;
+	unsigned long flags;
+
+	if (!chan)
+		return;
+
+	spin_lock_irqsave(&chan->lock, flags);
+	if (!chan->assigned) {
+		spin_unlock_irqrestore(&chan->lock, flags);
+		return;
+	}
+
+	/* The queued TX requests are simply aborted, no callbacks are made */
+	chan->assigned = false;
+	if (chan->txdone_method == (TXDONE_BY_POLL | TXDONE_BY_ACK))
+		chan->txdone_method = TXDONE_BY_POLL;
+	spin_unlock_irqrestore(&chan->lock, flags);
+
+	chan->link_ops->shutdown(chan->link);
+	blocking_notifier_call_chain(&chan->avail, 0, NULL);
+}
+EXPORT_SYMBOL(ipc_free_channel);
+
+int ipc_notify_chan_register(const char *name, struct notifier_block *nb)
+{
+	struct ipc_chan *chan;
+
+	if (!nb)
+		return -EINVAL;
+
+	mutex_lock(&chpool_mutex);
+
+	list_for_each_entry(chan, &ipc_channels, node)
+		if (!strcmp(name, chan->chan_name))
+			blocking_notifier_chain_register(&chan->avail, nb);
+
+	mutex_unlock(&chpool_mutex);
+
+	return 0;
+}
+EXPORT_SYMBOL(ipc_notify_chan_register);
+
+void ipc_notify_chan_unregister(const char *name, struct notifier_block *nb)
+{
+	struct ipc_chan *chan;
+
+	if (!nb)
+		return;
+
+	mutex_lock(&chpool_mutex);
+
+	list_for_each_entry(chan, &ipc_channels, node)
+		if (!strcmp(name, chan->chan_name))
+			blocking_notifier_chain_unregister(&chan->avail, nb);
+
+	mutex_unlock(&chpool_mutex);
+}
+EXPORT_SYMBOL(ipc_notify_chan_unregister);
+
+/*
+ * Call for IPC controller drivers to register a controller, adding
+ * its channels/mailboxes to the global pool.
+ */
+int ipc_links_register(struct ipc_controller *ipc_con)
+{
+	struct tx_poll_timer *timer = NULL;
+	struct ipc_chan *channel;
+	int i, num_links, txdone;
+
+	/* Are you f***ing with us, sir? */
+	if (!ipc_con || !ipc_con->ops)
+		return -EINVAL;
+
+	for (i = 0; ipc_con->links[i]; i++)
+		;
+	if (!i)
+		return -EINVAL;
+	num_links = i;
+
+	if (ipc_con->txdone_irq)
+		txdone = TXDONE_BY_IRQ;
+	else if (ipc_con->txdone_poll)
+		txdone = TXDONE_BY_POLL;
+	else /* It has to be at least ACK */
+		txdone = TXDONE_BY_ACK;
+
+	if (txdone == TXDONE_BY_POLL) {
+		timer = kzalloc(sizeof(struct tx_poll_timer), GFP_KERNEL);
+		timer->period = ipc_con->txpoll_period;
+		timer->poll.function = &poll_txdone;
+		timer->poll.data = (unsigned long)timer;
+		init_timer(&timer->poll);
+	}
+
+	channel = kzalloc(sizeof(struct ipc_chan) * num_links, GFP_KERNEL);
+
+	for (i = 0; i < num_links; i++) {
+		channel[i].timer = timer;
+		channel[i].assigned = false;
+		channel[i].txdone_method = txdone;
+		channel[i].link_ops = ipc_con->ops;
+		channel[i].link = ipc_con->links[i];
+		channel[i].link->api_priv = &channel[i];
+		snprintf(channel[i].chan_name, 32, "%s:%s",
+			ipc_con->controller_name,
+			ipc_con->links[i]->link_name);
+		spin_lock_init(&channel[i].lock);
+		BLOCKING_INIT_NOTIFIER_HEAD(&channel[i].avail);
+		INIT_LIST_HEAD(&channel[i].node);
+		mutex_lock(&chpool_mutex);
+		list_add_tail(&channel[i].node, &ipc_channels);
+		mutex_unlock(&chpool_mutex);
+		mbox_dbg("%s:%d chan=%p-link=%p\n",
+			__func__, __LINE__, &channel[i], ipc_con->links[i]);
+	}
+
+	return 0;
+}
+EXPORT_SYMBOL(ipc_links_register);
+
+/* Free any occupied channels */
+void ipc_links_unregister(struct ipc_controller *ipc_con)
+{
+	/* TBD */
+}
+EXPORT_SYMBOL(ipc_links_unregister);
+
diff --git a/drivers/misc/f_media-shell.c b/drivers/misc/f_media-shell.c
new file mode 100644
index 0000000..e56ea86
--- /dev/null
+++ b/drivers/misc/f_media-shell.c
@@ -0,0 +1,789 @@
+/*
+ * Generic media shell driver for userland drivers
+ * Copyright (C) 2013 Fujitsu Semiconductor, Ltd
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/mm.h>
+#include <linux/interrupt.h>
+#include <linux/ioport.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/wait.h>
+#include <linux/list.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/uaccess.h>
+#include <linux/slab.h>
+#include <linux/fs.h>
+#include <linux/cdev.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/firmware.h>
+#include <linux/sched.h>
+#include <linux/signal.h>
+#include <linux/pm_runtime.h>
+#include <uapi/linux/f_media-shell.h>
+#include <asm/dma-iommu.h>
+#include <linux/iommu.h>
+
+#define MS_PLATFORM_DEVICE_NAME		"media-shell"
+
+#define MAX_MS_INSTANCES			4
+/* #define MS_SUPPORT_RESERVED_VIDEO_MEMORY */
+#ifdef MS_SUPPORT_RESERVED_VIDEO_MEMORY
+#define MS_DRAM_PHYSICAL_BASE			0x70000000
+#endif
+
+/* REGISTER BASE */
+#define BIT_BASE		0x0000
+/* HARDWARE REGISTER */
+#define BIT_CODE_RUN		(BIT_BASE + 0x000)
+#define BIT_CODE_DOWN		(BIT_BASE + 0x004)
+#define BIT_INT_CLEAR		(BIT_BASE + 0x00C)
+#define BIT_CODE_RESET		(BIT_BASE + 0x014)
+/* GLOBAL REGISTER */
+#define BIT_BUSY_FLAG		(BIT_BASE + 0x160)
+#define MS_BIT_CODE_HEAD	(1 * 1024 / 2) /* Write 2 bytes 1 time*/
+
+#define WAVE320_FW_NAME		"wave320-firmware.bin"
+/* #define SUPPORT_DRM_GEM */
+
+MODULE_FIRMWARE(WAVE320_FW_NAME);
+
+enum {
+	IP_TYPE_F_JPEGTX,
+	IP_TYPE_WAVE320,
+
+	/* always last */
+	IP_TYPE_COUNT
+};
+
+static char *ip_type_name[] = {
+	"f_jpegtx",
+	"f_wave320",
+};
+
+static char *ip_type_devname[] = {
+	"jpu%d",
+	"vpu%d",
+};
+
+
+#define MS_REG_STORE_NUM	64
+
+struct ms {
+	struct device *dev;
+	struct cdev *cdev;
+	struct fasync_struct *async_queue;
+	void __iomem *base;
+	int irq;
+	u32 phy_base;
+	struct clk *pclk;
+	struct clk *aclk;
+	bool clocks_enabled;
+	u32 open_count;
+	struct mutex lock;
+	spinlock_t spinlock;
+	struct list_head jbp_head;
+	struct ms_buffer instance_pool;
+	struct ms_buffer common_memory;
+	int interrupt_flag;
+	wait_queue_head_t interrupt_wait_q;
+	int driver_type;
+	u32 ms_reg_store[MS_REG_STORE_NUM];
+	struct dma_attrs dma_attrs;
+};
+
+/* To track the allocated memory buffer */
+struct ms_buffer_pool {
+	struct list_head list;
+	struct ms_buffer vb;
+};
+
+static dev_t dev_node[IP_TYPE_COUNT];
+
+static const struct of_device_id media_shell_dt_ids[] = {
+	{
+		.compatible = "fujitsu,jpegtx",
+		.data = (void *)IP_TYPE_F_JPEGTX,
+	},
+	{
+		.compatible = "fujitsu,wave320",
+		.data = (void *)IP_TYPE_WAVE320,
+	},
+	{ /* sentinel */ }
+};
+
+static struct class *ms_class;
+static struct ms *mss[IP_TYPE_COUNT][MAX_MS_INSTANCES];
+#define ms_reg_read(_ms, addr) __raw_readl(_ms->base + addr)
+#define ms_reg_write(_ms, addr, val) __raw_writel(val, _ms->base + addr)
+
+int ms_hw_reset(struct ms *ms)
+{
+	dev_info(ms->dev, "request ms reset from applciation.\n");
+	return 0;
+}
+
+
+static int ms_alloc_dma_buffer(struct ms *ms, struct ms_buffer *vb)
+{
+	vb->base = dma_alloc_attrs(ms->dev,
+		PAGE_ALIGN(vb->size),
+		(dma_addr_t *)(&vb->phys_addr),
+		GFP_DMA | GFP_KERNEL, &ms->dma_attrs);
+
+	if (vb->base)
+		return 0;
+	dev_err(ms->dev, "Physmem alloc error size=%d\n", vb->size);
+	return -ENOMEM;
+}
+
+static void ms_free_dma_buffer(struct ms_buffer *jb, struct ms *ms)
+{
+	if (!jb->base || !ms)
+		return;
+
+	dma_free_attrs(ms->dev, PAGE_ALIGN(jb->size),
+		jb->base, jb->phys_addr, &ms->dma_attrs);
+}
+
+static int ms_free_buffers(struct ms *ms)
+{
+	struct ms_buffer_pool *pool, *n;
+	struct ms_buffer vb;
+
+	list_for_each_entry_safe(pool, n, &ms->jbp_head, list) {
+		vb = pool->vb;
+		if (!vb.base)
+			continue;
+
+		ms_free_dma_buffer(&vb, ms);
+		list_del(&pool->list);
+		kfree(pool);
+	}
+	return 0;
+}
+
+static irqreturn_t irq_handler(int irq, void *dev_id)
+{
+	struct ms *ms = dev_id;
+
+	if (ms->driver_type == IP_TYPE_WAVE320)
+		ms_reg_write(ms, BIT_INT_CLEAR, 0x1);
+
+	if (ms->async_queue)
+		kill_fasync(&ms->async_queue, SIGIO, POLL_IN);
+
+	spin_lock(&ms->spinlock);
+	ms->interrupt_flag = 1;
+
+	dev_dbg(ms->dev, "irq_handler\n");
+	wake_up_interruptible(&ms->interrupt_wait_q);
+
+	disable_irq_nosync(ms->irq);
+	spin_unlock(&ms->spinlock);
+	return IRQ_HANDLED;
+}
+
+static int ms_open(struct inode *inode, struct file *filp)
+{
+	int minor;
+	struct ms *ms;
+	int n;
+
+	minor = iminor(inode);
+	if (minor > MAX_MS_INSTANCES)
+		return -ENODEV;
+
+	if (!mss[minor])
+		return -ENODEV;
+
+	for (n = 0; n < IP_TYPE_COUNT; n++)
+		if (imajor(inode) == MAJOR(dev_node[n]))
+			break;
+	if (n == IP_TYPE_COUNT) {
+		pr_err("Unknown dev mapping\n");
+		return -ENODEV;
+	}
+
+	ms = mss[n][minor];
+
+	mutex_lock(&ms->lock);
+
+	ms->open_count++;
+
+	filp->private_data = ms;
+	mutex_unlock(&ms->lock);
+
+	pm_runtime_get_sync(ms->dev);
+
+	return 0;
+}
+
+static long ms_ioctl(struct file *filp, u_int cmd, u_long arg)
+{
+	struct ms *ms = filp->private_data;
+	int ret = 0;
+	struct ms_buffer_pool *vbp = NULL, *n;
+	struct ms_buffer vb;
+	u32 timeout;
+	u32 clkgate;
+	unsigned long flags;
+
+	switch (cmd) {
+	case JDI_IOCTL_GET_RESERVED_VIDEO_MEMORY_INFO:
+		mutex_lock(&ms->lock);
+		if (ms->common_memory.base != 0) {
+			ret = copy_to_user(
+				(void __user *)arg, &ms->common_memory,
+				sizeof(struct ms_buffer));
+
+			if (ret != 0)
+				ret = -EFAULT;
+		} else
+			ret = -EFAULT;
+
+		mutex_unlock(&ms->lock);
+		break;
+
+	case JDI_IOCTL_ALLOCATE_PHYSICAL_MEMORY:
+		vbp = kzalloc(sizeof(*vbp), GFP_KERNEL);
+		if (!vbp)
+			return -ENOMEM;
+		ret = copy_from_user(&(vbp->vb), (struct ms_buffer *)arg,
+						sizeof(struct ms_buffer));
+		if (ret) {
+			kfree(vbp);
+			return -EFAULT;
+		}
+
+		ret = ms_alloc_dma_buffer(ms, &vbp->vb);
+		if (ret == -1) {
+			kfree(vbp);
+			break;
+		}
+
+		ret = copy_to_user((void __user *)arg, &vbp->vb,
+						sizeof(struct ms_buffer));
+
+		if (ret) {
+			kfree(vbp);
+			ret = -EFAULT;
+			break;
+		}
+
+		mutex_lock(&ms->lock);
+		list_add(&vbp->list, &ms->jbp_head);
+		mutex_unlock(&ms->lock);
+		break;
+
+	case JDI_IOCTL_FREE_PHYSICALMEMORY:
+		ret = copy_from_user(&vb, (struct ms_buffer *)arg,
+						sizeof(struct ms_buffer));
+
+		if (ret)
+			return -EACCES;
+
+		if (vb.base)
+			ms_free_dma_buffer(&vb, ms);
+
+		mutex_lock(&ms->lock);
+		list_for_each_entry_safe(vbp, n, &ms->jbp_head, list) {
+			if (vbp->vb.base != vb.base)
+				continue;
+			list_del(&vbp->list);
+			kfree(vbp);
+			break;
+		}
+		mutex_unlock(&ms->lock);
+		break;
+
+	case JDI_IOCTL_WAIT_INTERRUPT:
+		timeout = (u32)arg;
+		if (!wait_event_interruptible_timeout(
+			ms->interrupt_wait_q, ms->interrupt_flag != 0,
+					msecs_to_jiffies(timeout))) {
+			ret = -ETIME;
+			break;
+		}
+
+		if (signal_pending(current)) {
+			ret = -ERESTARTSYS;
+			break;
+		}
+
+		spin_lock_irqsave(&ms->spinlock, flags);
+		ms->interrupt_flag = 0;
+		enable_irq(ms->irq);
+		spin_unlock_irqrestore(&ms->spinlock, flags);
+		break;
+
+	case JDI_IOCTL_SET_CLOCK_GATE:
+		if (get_user(clkgate, (u32 __user *) arg))
+			return -EFAULT;
+
+		ms->clocks_enabled = clkgate;
+
+		if (clkgate) {
+			clk_prepare_enable(ms->pclk);
+			clk_prepare_enable(ms->aclk);
+		} else {
+			clk_disable_unprepare(ms->pclk);
+			clk_disable_unprepare(ms->aclk);
+		}
+		break;
+
+	case JDI_IOCTL_GET_INSTANCE_POOL:
+		mutex_lock(&ms->lock);
+
+		if (ms->instance_pool.base) {
+			ret = copy_to_user((void __user *)arg,
+				&ms->instance_pool, sizeof(struct ms_buffer));
+			if (ret)
+				ret = -EFAULT;
+			goto bail;
+		}
+
+
+		ret = copy_from_user(&ms->instance_pool,
+			(struct ms_buffer *)arg, sizeof(struct ms_buffer));
+		if (ret)
+			goto bail;
+
+		if (ms_alloc_dma_buffer(ms, &ms->instance_pool) == -1)
+			goto bail;
+
+		ret = copy_to_user((void __user *)arg,
+					&ms->instance_pool,
+					sizeof(struct ms_buffer));
+
+		if (!ret) {
+			/* success to get memory for instance pool */
+			mutex_unlock(&ms->lock);
+			break;
+		}
+
+		ret = -EFAULT;
+bail:
+		mutex_unlock(&ms->lock);
+		break;
+
+	case JDI_IOCTL_RESET:
+		ms_hw_reset(ms);
+		break;
+
+	default:
+		pr_err("Unknown IOCTL %d\n", cmd);
+		break;
+	}
+	return ret;
+}
+
+static ssize_t ms_read(struct file *filp, char __user *buf,
+						size_t len, loff_t *ppos)
+{
+	return -1;
+}
+
+static ssize_t ms_write(struct file *filp, const char __user *buf,
+						size_t len, loff_t *ppos)
+{
+	return -1;
+}
+
+static int ms_release(struct inode *inode, struct file *filp)
+{
+	struct ms *ms = filp->private_data;
+
+	mutex_lock(&ms->lock);
+	ms->open_count--;
+
+	if (ms->open_count <= 0)
+		/* found and free the not handled buffer by user applications */
+		ms_free_buffers(ms);
+
+	mutex_unlock(&ms->lock);
+
+	pm_runtime_put_sync_suspend(ms->dev);
+
+	return 0;
+}
+
+
+static int ms_fasync(int fd, struct file *filp, int mode)
+{
+	struct ms *dev = filp->private_data;
+
+	return fasync_helper(fd, filp, mode, &dev->async_queue);
+}
+
+
+static int ms_map_to_register(struct ms *ms, struct file *fp,
+						struct vm_area_struct *vm)
+{
+	unsigned long pfn;
+
+	vm->vm_flags |= VM_IO;
+	vm->vm_page_prot = pgprot_noncached(vm->vm_page_prot);
+	pfn = (unsigned long)ms->phy_base >> PAGE_SHIFT;
+
+	return remap_pfn_range(vm, vm->vm_start, pfn,
+		vm->vm_end-vm->vm_start, vm->vm_page_prot) ? -EAGAIN : 0;
+}
+
+static int ms_map_to_physical_memory(struct file *fp,
+					struct vm_area_struct *vm)
+{
+	struct ms *ms;
+	struct dma_iommu_mapping *mapping;
+	phys_addr_t physaddr;
+	ms = fp->private_data;
+	vm->vm_flags |= VM_IO;
+	vm->vm_page_prot = pgprot_writecombine(vm->vm_page_prot);
+
+	mapping = to_dma_iommu_mapping(ms->dev);
+
+	if(mapping) {
+		physaddr = iommu_iova_to_phys(
+					mapping->domain, vm->vm_pgoff << PAGE_SHIFT);
+		vm->vm_pgoff = physaddr >> PAGE_SHIFT;
+	}
+
+	return remap_pfn_range(vm, vm->vm_start, vm->vm_pgoff,
+		vm->vm_end-vm->vm_start, vm->vm_page_prot) ? -EAGAIN : 0;
+}
+
+static int ms_mmap(struct file *filp, struct vm_area_struct *vm)
+{
+	struct ms *ms = filp->private_data;
+
+	if (vm->vm_pgoff)
+		return ms_map_to_physical_memory(filp, vm);
+	else
+		return ms_map_to_register(ms, filp, vm);
+}
+
+static const struct file_operations ms_fops = {
+	.owner = THIS_MODULE,
+	.open = ms_open,
+	.read = ms_read,
+	.write = ms_write,
+	.unlocked_ioctl = ms_ioctl,
+	.release = ms_release,
+	.fasync = ms_fasync,
+	.mmap = ms_mmap,
+};
+
+static int ms_probe(struct platform_device *pdev)
+{
+	int err = 0;
+	struct resource *res;
+	struct ms *ms;
+	const u32 *p;
+	const struct of_device_id *of_id =
+		of_match_device(media_shell_dt_ids, &pdev->dev);
+	int id;
+	struct dma_iommu_mapping *mapping;
+
+	if (!pdev->dev.of_node) {
+		dev_err(&pdev->dev, "DT required\n");
+		return -EINVAL;
+	}
+
+	ms = kzalloc(sizeof(*ms), GFP_KERNEL);
+	if (!ms)
+		return -ENOMEM;
+
+	ms->driver_type = (int)of_id->data;
+	ms->dev = &pdev->dev;
+	mutex_init(&ms->lock);
+	spin_lock_init(&ms->spinlock);
+	INIT_LIST_HEAD(&ms->jbp_head);
+	init_waitqueue_head(&ms->interrupt_wait_q);
+
+	dev_set_drvdata(&pdev->dev, ms);
+	init_dma_attrs(&ms->dma_attrs);
+	mapping = to_dma_iommu_mapping(ms->dev);
+	if (mapping) {
+		dev_info(ms->dev,
+			"Support IOMMU and get contiguous DMA memory\n");
+		dma_set_attr(DMA_ATTR_FORCE_CONTIGUOUS, &ms->dma_attrs);
+	}
+	
+        p = of_get_property(pdev->dev.of_node, "memory-size", NULL);
+        if (!p) {
+                dev_err(&pdev->dev, "Missing id property\n");
+                return -EINVAL;
+	}
+        ms->common_memory.size = be32_to_cpu(*p);
+	dev_info(&pdev->dev, "Reserved %d bytes\n", ms->common_memory.size);
+
+#ifndef SUPPORT_DRM_GEM
+#ifdef MS_SUPPORT_RESERVED_VIDEO_MEMORY
+	ms->common_memory.phys_addr = MS_DRAM_PHYSICAL_BASE;
+	ms->common_memory.base = ioremap(ms->common_memory.phys_addr,
+					PAGE_ALIGN(ms->common_memory.size));
+	if (!ms->common_memory.base) {
+		err = -ENOMEM;
+		goto bail_free;
+	}
+#else
+	if (ms_alloc_dma_buffer(ms, &ms->common_memory) == -1) {
+		err = -ENOMEM;
+		goto bail_free;
+	}
+#endif
+#endif /* SUPPORT_DRM_GEM */
+
+	p = of_get_property(pdev->dev.of_node, "id", NULL);
+	if (!p) {
+		dev_err(&pdev->dev, "Missing id property\n");
+		goto bail_dma_unalloc;
+	}
+	id = be32_to_cpu(*p);
+	mss[ms->driver_type][id] = ms;
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "Missing resource\n");
+		err = -EINVAL;
+		goto bail_dma_unalloc;
+	}
+
+	ms->phy_base = res->start;
+	ms->base = ioremap(res->start, res->end - res->start);
+	if (!ms->base) {
+		err = -ENOMEM;
+		goto bail_dma_unalloc;
+	}
+
+	ms->pclk = of_clk_get(pdev->dev.of_node, 0);
+	if (IS_ERR(ms->pclk)) {
+		dev_err(&pdev->dev, "Failed to get pclock");
+		err = -EINVAL;
+		goto bail_iounmap;
+	}
+
+	ms->aclk = of_clk_get(pdev->dev.of_node, 1);
+	if (IS_ERR(ms->aclk)) {
+		dev_err(&pdev->dev, "Failed to get aclock");
+		err = -EINVAL;
+		goto bail_clk_put;
+	}
+
+	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
+	if (!res) {
+		dev_err(&pdev->dev, "Missing resource\n");
+		err = -EINVAL;
+		goto bail_clk_put;
+	}
+
+	ms->irq = res->start;
+
+	err = request_irq(ms->irq, irq_handler, 0, "MS_CODEC_IRQ", ms);
+	if (err) {
+		dev_err(&pdev->dev, "failed to register IRQ\n");
+		goto bail_clk_put;
+	}
+
+	ms->cdev = cdev_alloc();
+	cdev_init(ms->cdev, &ms_fops);
+
+	if (cdev_add(ms->cdev, dev_node[ms->driver_type] + id, 1)) {
+		dev_err(&pdev->dev, "failed to create char device\n");
+		goto bail_irq;
+	}
+	if (IS_ERR(device_create(ms_class, NULL, dev_node[ms->driver_type] + id,
+				 NULL, ip_type_devname[ms->driver_type], id))) {
+		dev_err(&pdev->dev, "failed to create char device\n");
+		goto bail_cdev;
+	}
+
+	pm_runtime_enable(&pdev->dev);
+
+	dev_info(&pdev->dev, "%s at pa=0x%x, va=0x%p, IRQ %d, node: %d:%d\n",
+			ip_type_name[ms->driver_type],
+			ms->phy_base , ms->base, ms->irq,
+				MAJOR(dev_node[ms->driver_type]),
+				MINOR(dev_node[ms->driver_type]) + id);
+
+	return 0;
+
+bail_cdev:
+	cdev_del(ms->cdev);
+bail_irq:
+	free_irq(ms->irq, ms);
+bail_clk_put:
+	clk_put(ms->pclk);
+	clk_put(ms->aclk);
+bail_iounmap:
+	iounmap(ms->base);
+bail_dma_unalloc:
+#ifndef SUPPORT_DRM_GEM
+	if (ms->common_memory.base) {
+#ifdef MS_SUPPORT_RESERVED_VIDEO_MEMORY
+			iounmap((void *)ms->common_memory.base);
+#else
+			ms_free_dma_buffer(&ms->common_memory, ms);
+#endif
+	}
+#endif /* SUPPORT_DRM_GEM */
+bail_free:
+	kfree(ms);
+
+	return err;
+}
+
+static int ms_remove(struct platform_device *pdev)
+{
+	struct ms *ms = dev_get_drvdata(&pdev->dev);
+
+	if (ms->instance_pool.base)
+		ms_free_dma_buffer(&ms->instance_pool, ms);
+
+	pm_runtime_disable(&pdev->dev);
+
+	cdev_del(ms->cdev);
+	free_irq(ms->irq, ms);
+
+#ifndef SUPPORT_DRM_GEM
+	if (ms->common_memory.base) {
+#ifdef MS_SUPPORT_RESERVED_VIDEO_MEMORY
+			iounmap((void *)ms->common_memory.base);
+#else
+			ms_free_dma_buffer(&ms->common_memory, ms);
+#endif
+	}
+#endif /* SUPPORT_DRM_GEM */
+
+	clk_disable_unprepare(ms->pclk);
+	clk_disable_unprepare(ms->aclk);
+	clk_put(ms->pclk);
+	clk_put(ms->aclk);
+	iounmap(ms->base);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM
+
+static int ms_device_runtime_idle(struct device *dev)
+{
+	return 1; /* no runtime_suspend */
+}
+
+static int ms_device_runtime_suspend(struct device *dev)
+{
+	struct ms *ms = dev_get_drvdata(dev);
+
+	dev_info(dev, "%s\n", __func__);
+
+	/* disable_irq(ms->irq); */
+
+	if (ms->clocks_enabled) {
+		clk_disable_unprepare(ms->pclk);
+		clk_disable_unprepare(ms->aclk);
+	}
+
+	return 0;
+}
+
+static int ms_device_runtime_resume(struct device *dev)
+{
+	struct  ms *ms = dev_get_drvdata(dev);
+
+	dev_info(dev, "%s\n", __func__);
+
+	if (ms->clocks_enabled) {
+		clk_prepare_enable(ms->pclk);
+		clk_prepare_enable(ms->aclk);
+	}
+
+	/* enable_irq(ms->irq); */
+
+	return 0;
+}
+
+static int ms_suspend(struct device *dev)
+{
+	return ms_device_runtime_suspend(dev);
+}
+static int ms_resume(struct device *dev)
+{
+	return ms_device_runtime_resume(dev);
+}
+
+static const struct dev_pm_ops media_shell_pm_ops = {
+	.suspend = ms_suspend,
+	.resume = ms_resume,
+	.runtime_suspend = ms_device_runtime_suspend,
+	.runtime_resume = ms_device_runtime_resume,
+	.runtime_idle = ms_device_runtime_idle,
+};
+#endif				/* CONFIG_PM */
+
+
+MODULE_DEVICE_TABLE(of, media_shell_dt_ids);
+
+static struct platform_driver ms_driver = {
+	.driver = {
+			.name = MS_PLATFORM_DEVICE_NAME,
+			.of_match_table = media_shell_dt_ids,
+#ifdef CONFIG_PM
+			.pm = &media_shell_pm_ops,
+#endif
+		   },
+	.probe = ms_probe,
+	.remove = ms_remove,
+};
+
+static int __init ms_init(void)
+{
+	int err;
+	int n;
+
+	ms_class = class_create(THIS_MODULE, "mediashell");
+
+	for (n = 0; n < IP_TYPE_COUNT; n++) {
+		err = alloc_chrdev_region(&dev_node[n], 0, MAX_MS_INSTANCES,
+							      ip_type_name[n]);
+		if (err < 0) {
+			pr_err("%s: Failed to get chrdev region\n",
+							      ip_type_name[n]);
+			return -ENODEV;
+		}
+
+		pr_info(
+		   "%s: allocated char region starting major %d, minor %d\n",
+		      ip_type_name[n], MAJOR(dev_node[n]), MINOR(dev_node[n]));
+	}
+	err = platform_driver_register(&ms_driver);
+	if (err < 0)
+		goto bail;
+	return err;
+
+bail:
+	while (--n >= 0)
+		unregister_chrdev_region(dev_node[n], MAX_MS_INSTANCES);
+	return err;
+}
+
+static void __exit ms_exit(void)
+{
+	int n;
+	for (n = 0; n < IP_TYPE_COUNT; n++)
+		unregister_chrdev_region(dev_node[n], MAX_MS_INSTANCES);
+
+	class_destroy(ms_class);
+	platform_driver_unregister(&ms_driver);
+}
+
+MODULE_AUTHOR("Fujitsu Semiconductor Ltd");
+MODULE_DESCRIPTION("Driver for media-shell units");
+MODULE_LICENSE("GPL");
+
+module_init(ms_init);
+module_exit(ms_exit);
diff --git a/drivers/misc/gpio-revision.c b/drivers/misc/gpio-revision.c
new file mode 100644
index 0000000..e3832db
--- /dev/null
+++ b/drivers/misc/gpio-revision.c
@@ -0,0 +1,114 @@
+#include <linux/platform_device.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/of.h>
+#include <linux/of_device.h>
+#include <linux/seq_file.h>
+#include <linux/module.h>
+
+static int gpio_revision = -EPROBE_DEFER;
+
+int gpio_revision_get(void)
+{
+	return gpio_revision;
+}
+EXPORT_SYMBOL_GPL(gpio_revision_get);
+
+bool gpio_revision_of_bitmap(struct device_node *of_node, const char *of_name)
+{
+	u32 val;
+
+	if (of_property_read_u32(of_node, of_name, &val))
+		return false;
+
+	return (1 << gpio_revision) & val;
+}
+EXPORT_SYMBOL_GPL(gpio_revision_of_bitmap);
+
+static ssize_t show_gpio_revision(struct device *dev,
+				struct device_attribute *attr,
+				char *buf)
+{
+	return sprintf(buf, "%u\n", gpio_revision);
+}
+
+static DEVICE_ATTR(gpio_revision, 0444, show_gpio_revision, NULL);
+
+static struct attribute *gpio_revision_attributes[] = {
+	&dev_attr_gpio_revision.attr,
+	NULL
+};
+
+static struct attribute_group gpio_revision_attribute_group = {
+	.attrs = gpio_revision_attributes,
+};
+
+static int gpio_revision_probe(struct platform_device *pdev)
+{
+	int err, rev = 31, i, n;
+
+	/* if no gpios present in DT, will set revision 31 */
+
+	if (of_gpio_count(pdev->dev.of_node) > 0)
+		rev = 0;
+
+	for (i = 0; i < of_gpio_count(pdev->dev.of_node); i++) {
+
+		n = of_get_gpio(pdev->dev.of_node, i);
+
+		err = gpio_request(n, "gpio-revision");
+		if (unlikely(err)) {
+			dev_err(&pdev->dev, " gpio %d request failed ", n);
+			return err;
+		}
+
+		err = gpio_direction_input(n);
+		if (unlikely(err < 0)) {
+			dev_err(&pdev->dev, "failed to set gpio as input\n");
+			gpio_free(n);
+			return err;
+		}
+
+		err = gpio_get_value(n);
+		if (unlikely(err < 0)) {
+			dev_err(&pdev->dev, "failed to read gpio\n");
+			gpio_free(n);
+			return err;
+		}
+		gpio_free(n);
+
+		rev = (rev << 1) | (err & 1);
+	}
+
+	gpio_revision = rev;
+	dev_info(&pdev->dev, "Using Revision: 0x%x\n", gpio_revision);
+
+	return sysfs_create_group(&pdev->dev.kobj,
+				    &gpio_revision_attribute_group);
+}
+
+static int gpio_revision_remove(struct platform_device *pdev)
+{
+	return sysfs_create_group(&pdev->dev.kobj,
+				&gpio_revision_attribute_group);
+}
+
+static const struct of_device_id gpio_revision_dt_ids[] = {
+	{
+		.compatible = "fujitsu,gpio-revision",
+	},
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, gpio_revision_dt_ids);
+
+static struct platform_driver gpio_revision_driver = {
+	.driver = {
+			.name = "gpio-revision",
+			.of_match_table = gpio_revision_dt_ids,
+		   },
+	.probe = gpio_revision_probe,
+	.remove = gpio_revision_remove,
+};
+
+module_platform_driver(gpio_revision_driver);
diff --git a/drivers/mmc/core/core.c b/drivers/mmc/core/core.c
index bf18b6b..4120a46 100644
--- a/drivers/mmc/core/core.c
+++ b/drivers/mmc/core/core.c
@@ -1677,6 +1677,36 @@ static inline void mmc_bus_put(struct mmc_host *host)
 	spin_unlock_irqrestore(&host->lock, flags);
 }
 
+int mmc_resume_bus(struct mmc_host *host)
+{
+	unsigned long flags;
+
+	if (!mmc_bus_needs_resume(host))
+		return -EINVAL;
+
+	printk("%s: Starting deferred resume\n", mmc_hostname(host));
+	spin_lock_irqsave(&host->lock, flags);
+	host->bus_resume_flags &= ~MMC_BUSRESUME_NEEDS_RESUME;
+	host->rescan_disable = 0;
+	spin_unlock_irqrestore(&host->lock, flags);
+
+	mmc_bus_get(host);
+	if (host->bus_ops && !host->bus_dead) {
+		mmc_power_up(host);
+		BUG_ON(!host->bus_ops->resume);
+		host->bus_ops->resume(host);
+	}
+
+	if (host->bus_ops->detect && !host->bus_dead)
+		host->bus_ops->detect(host);
+
+	mmc_bus_put(host);
+	printk("%s: Deferred resume completed\n", mmc_hostname(host));
+	return 0;
+}
+
+EXPORT_SYMBOL(mmc_resume_bus);
+
 /*
  * Assign a mmc bus handler to a host. Only one bus handler may control a
  * host at any given time.
@@ -2722,6 +2752,10 @@ int mmc_pm_notify(struct notifier_block *notify_block,
 	case PM_POST_RESTORE:
 
 		spin_lock_irqsave(&host->lock, flags);
+		if (mmc_bus_manual_resume(host)) {
+			spin_unlock_irqrestore(&host->lock, flags);
+			break;
+		}
 		host->rescan_disable = 0;
 		spin_unlock_irqrestore(&host->lock, flags);
 		mmc_detect_change(host, 0);
diff --git a/drivers/mmc/host/Kconfig b/drivers/mmc/host/Kconfig
index f4c52d0..41b23d6 100644
--- a/drivers/mmc/host/Kconfig
+++ b/drivers/mmc/host/Kconfig
@@ -283,6 +283,13 @@ config MMC_SDHCI_BCM2835
 	  This selects the BCM2835 SD/MMC controller. If you have a BCM2835
 	  platform with SD or MMC devices, say Y or M here.
 
+config MMC_SDHCI_F_SDH30
+	tristate "SDHCI support for Fujitsu Semiconductor F_SDH30"
+	depends on MMC_SDHCI && (ARCH_MB8AC0300 || ARCH_MB86S70)
+	help
+	  This selects the Secure Digital Host Controller Interface (SDHCI)
+	  Needed by some Fujitsu SoC for MMC / SD / SDIO support.
+	  If you have a controller with this interface, say Y or M here.
 	  If unsure, say N.
 
 config MMC_OMAP
diff --git a/drivers/mmc/host/Makefile b/drivers/mmc/host/Makefile
index 7671bf9..f148309 100644
--- a/drivers/mmc/host/Makefile
+++ b/drivers/mmc/host/Makefile
@@ -14,6 +14,7 @@ obj-$(CONFIG_MMC_SDHCI_PXAV3)	+= sdhci-pxav3.o
 obj-$(CONFIG_MMC_SDHCI_PXAV2)	+= sdhci-pxav2.o
 obj-$(CONFIG_MMC_SDHCI_S3C)	+= sdhci-s3c.o
 obj-$(CONFIG_MMC_SDHCI_SIRF)   	+= sdhci-sirf.o
+obj-$(CONFIG_MMC_SDHCI_F_SDH30)	+= sdhci_f_sdh30.o
 obj-$(CONFIG_MMC_SDHCI_SPEAR)	+= sdhci-spear.o
 obj-$(CONFIG_MMC_WBSD)		+= wbsd.o
 obj-$(CONFIG_MMC_AU1X)		+= au1xmmc.o
diff --git a/drivers/mmc/host/sdhci.c b/drivers/mmc/host/sdhci.c
index 6ea7d41..6c06550 100644
--- a/drivers/mmc/host/sdhci.c
+++ b/drivers/mmc/host/sdhci.c
@@ -77,6 +77,37 @@ static void sdhci_runtime_pm_bus_off(struct sdhci_host *host)
 }
 #endif
 
+/*****************************************************************************\
+ *                                                                           *
+ * Resume detection functions                                                *
+ *                                                                           *
+\*****************************************************************************/
+
+#define RESUME_DETECT_TIME	50
+#define RESUME_DETECT_JIFFIES	msecs_to_jiffies(RESUME_DETECT_TIME)
+static void sdhci_resume_detect_work_func(struct work_struct *work)
+{
+	struct sdhci_host *host = container_of(work, struct sdhci_host, resume_detect_work.work);
+	int err = 0;
+	if (mmc_bus_manual_resume(host->mmc))
+		mmc_set_bus_resume_policy(host->mmc, 0);
+	err = mmc_resume_host(host->mmc);
+	if (host->resume_detect_count-- && err)
+		queue_delayed_work(host->resume_detect_wq, &host->resume_detect_work, RESUME_DETECT_JIFFIES);
+	else
+		pr_info("%s: resume detection done (count:%d, err:%d)\n",
+			mmc_hostname(host->mmc),host->resume_detect_count, err);
+}
+
+static int sdhci_start_resume_detection(struct mmc_host *mmc, int count)
+{
+	struct sdhci_host *host = mmc_priv(mmc);
+	mmc_set_bus_resume_policy(mmc, 1);
+	host->resume_detect_count = count;
+	queue_delayed_work(host->resume_detect_wq, &host->resume_detect_work, RESUME_DETECT_JIFFIES);
+	return 0;
+}
+
 static void sdhci_dumpregs(struct sdhci_host *host)
 {
 	pr_debug(DRIVER_NAME ": =========== REGISTER DUMP (%s)===========\n",
@@ -896,7 +927,7 @@ static void sdhci_prepare_data(struct sdhci_host *host, struct mmc_command *cmd)
 static void sdhci_set_transfer_mode(struct sdhci_host *host,
 	struct mmc_command *cmd)
 {
-	u16 mode;
+	u16 mode = 0;
 	struct mmc_data *data = cmd->data;
 
 	if (data == NULL)
@@ -904,9 +935,11 @@ static void sdhci_set_transfer_mode(struct sdhci_host *host,
 
 	WARN_ON(!host->data);
 
+	if (!(host->quirks2 & SDHCI_QUIRK2_SUPPORT_SINGLE))
 	mode = SDHCI_TRNS_BLK_CNT_EN;
+
 	if (mmc_op_multi(cmd->opcode) || data->blocks > 1) {
-		mode |= SDHCI_TRNS_MULTI;
+		mode = SDHCI_TRNS_BLK_CNT_EN | SDHCI_TRNS_MULTI;
 		/*
 		 * If we are sending CMD23, CMD12 never gets sent
 		 * on successful completion (so no Auto-CMD12).
@@ -1262,8 +1295,10 @@ static int sdhci_set_power(struct sdhci_host *host, unsigned short power)
 			break;
 		case MMC_VDD_29_30:
 		case MMC_VDD_30_31:
+		if (!(host->quirks2 & SDHCI_QUIRK2_UNSUPPORT_3_0_V)) {
 			pwr = SDHCI_POWER_300;
 			break;
+		}
 		case MMC_VDD_32_33:
 		case MMC_VDD_33_34:
 			pwr = SDHCI_POWER_330;
@@ -1333,6 +1368,8 @@ static void sdhci_request(struct mmc_host *mmc, struct mmc_request *mrq)
 
 	sdhci_runtime_pm_get(host);
 
+	present = mmc_gpio_get_cd(host->mmc);
+
 	spin_lock_irqsave(&host->lock, flags);
 
 	WARN_ON(host->mrq != NULL);
@@ -1361,7 +1398,6 @@ static void sdhci_request(struct mmc_host *mmc, struct mmc_request *mrq)
 	 *     zero: cd-gpio is used, and card is removed
 	 *     one: cd-gpio is used, and card is present
 	 */
-	present = mmc_gpio_get_cd(host->mmc);
 	if (present < 0) {
 		/* If polling, assume that the card is always present. */
 		if (host->quirks & SDHCI_QUIRK_BROKEN_CARD_DETECTION)
@@ -1780,6 +1816,11 @@ static int sdhci_do_start_signal_voltage_switch(struct sdhci_host *host,
 		ctrl |= SDHCI_CTRL_VDD_180;
 		sdhci_writew(host, ctrl, SDHCI_HOST_CONTROL2);
 
+		/* Some controller need to do more when switching */
+		if ((host->quirks2 & SDHCI_QUIRK2_VOLTAGE_SWITCH) &&
+						host->ops->voltage_switch)
+			host->ops->voltage_switch(host);
+
 		/* Wait for 5ms */
 		usleep_range(5000, 5500);
 
@@ -1866,9 +1907,12 @@ static int sdhci_execute_tuning(struct mmc_host *mmc, u32 opcode)
 		requires_tuning_nonuhs = true;
 
 	if (((ctrl & SDHCI_CTRL_UHS_MASK) == SDHCI_CTRL_UHS_SDR104) ||
-	    requires_tuning_nonuhs)
+		((ctrl & SDHCI_CTRL_UHS_MASK) == SDHCI_CTRL_HS_SDR200) ||
+			requires_tuning_nonuhs) {
 		ctrl |= SDHCI_CTRL_EXEC_TUNING;
-	else {
+		if (host->quirks2 & SDHCI_QUIRK2_TUNING_WORK_AROUND)
+			ctrl |= SDHCI_CTRL_TUNED_CLK;
+	} else {
 		spin_unlock(&host->lock);
 		enable_irq(host->irq);
 		sdhci_runtime_pm_put(host);
@@ -1894,7 +1938,7 @@ static int sdhci_execute_tuning(struct mmc_host *mmc, u32 opcode)
 	 * Issue CMD19 repeatedly till Execute Tuning is set to 0 or the number
 	 * of loops reaches 40 times or a timeout of 150ms occurs.
 	 */
-	timeout = 150;
+	timeout = 75;
 	do {
 		struct mmc_command cmd = {0};
 		struct mmc_request mrq = {NULL};
@@ -1971,7 +2015,7 @@ static int sdhci_execute_tuning(struct mmc_host *mmc, u32 opcode)
 		ctrl = sdhci_readw(host, SDHCI_HOST_CONTROL2);
 		tuning_loop_counter--;
 		timeout--;
-		mdelay(1);
+		mdelay(2);
 	} while (ctrl & SDHCI_CTRL_EXEC_TUNING);
 
 	/*
@@ -2228,7 +2272,7 @@ static void sdhci_cmd_irq(struct sdhci_host *host, u32 intmask)
 	BUG_ON(intmask == 0);
 
 	if (!host->cmd) {
-		pr_err("%s: Got command interrupt 0x%08x even "
+		pr_info("%s: Got command interrupt 0x%08x even "
 			"though no command operation was in progress.\n",
 			mmc_hostname(host->mmc), (unsigned)intmask);
 		sdhci_dumpregs(host);
@@ -2582,6 +2626,7 @@ int sdhci_suspend_host(struct sdhci_host *host)
 
 EXPORT_SYMBOL_GPL(sdhci_suspend_host);
 
+#define RESUME_DETECT_COUNT	16
 int sdhci_resume_host(struct sdhci_host *host)
 {
 	int ret;
@@ -2614,6 +2659,10 @@ int sdhci_resume_host(struct sdhci_host *host)
 	}
 
 	ret = mmc_resume_host(host->mmc);
+	if (host->quirks2 & SDHCI_QUIRK2_RESUME_DETECT_RETRY && ret) {
+		sdhci_start_resume_detection(host->mmc, RESUME_DETECT_COUNT);
+		ret = 0;
+	}
 	sdhci_enable_card_detection(host);
 
 	if (host->ops->platform_resume)
@@ -3217,6 +3266,19 @@ int sdhci_add_host(struct sdhci_host *host)
 
 	sdhci_init(host, 0);
 
+	/*
+	 * Init workqueue.
+	 */
+	if (host->quirks2 & SDHCI_QUIRK2_RESUME_DETECT_RETRY) {
+		host->resume_detect_wq = create_workqueue("sdhci_resume_detection");
+		if (host->resume_detect_wq == NULL) {
+			ret = -ENOMEM;
+			pr_err("%s: Failed to create resume detection workqueue: %d\n",
+			       mmc_hostname(mmc), ret);
+		}
+	}
+	INIT_DELAYED_WORK(&host->resume_detect_work, sdhci_resume_detect_work_func);
+
 	ret = request_irq(host->irq, sdhci_irq, IRQF_SHARED,
 		mmc_hostname(mmc), host);
 	if (ret) {
@@ -3267,6 +3329,8 @@ reset:
 untasklet:
 	tasklet_kill(&host->card_tasklet);
 	tasklet_kill(&host->finish_tasklet);
+	if (host->quirks2 & SDHCI_QUIRK2_RESUME_DETECT_RETRY)
+		destroy_workqueue(host->resume_detect_wq);
 
 	return ret;
 }
@@ -3312,6 +3376,9 @@ void sdhci_remove_host(struct sdhci_host *host, int dead)
 	tasklet_kill(&host->card_tasklet);
 	tasklet_kill(&host->finish_tasklet);
 
+	if (host->quirks2 & SDHCI_QUIRK2_RESUME_DETECT_RETRY)
+		destroy_workqueue(host->resume_detect_wq);
+
 	if (host->vmmc) {
 		regulator_disable(host->vmmc);
 		regulator_put(host->vmmc);
diff --git a/drivers/mmc/host/sdhci.h b/drivers/mmc/host/sdhci.h
index b037f18..ccda5f4 100644
--- a/drivers/mmc/host/sdhci.h
+++ b/drivers/mmc/host/sdhci.h
@@ -295,6 +295,7 @@ struct sdhci_ops {
 	void    (*adma_workaround)(struct sdhci_host *host, u32 intmask);
 	void	(*platform_init)(struct sdhci_host *host);
 	void    (*card_event)(struct sdhci_host *host);
+	void	(*voltage_switch)(struct sdhci_host *host);
 };
 
 #ifdef CONFIG_MMC_SDHCI_IO_ACCESSORS
diff --git a/drivers/mmc/host/sdhci_f_sdh30.c b/drivers/mmc/host/sdhci_f_sdh30.c
new file mode 100644
index 0000000..80468b0
--- /dev/null
+++ b/drivers/mmc/host/sdhci_f_sdh30.c
@@ -0,0 +1,374 @@
+/*
+ * linux/drivers/mmc/host/sdhci_f_sdh30.c
+ *
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+
+#include <linux/err.h>
+#include <linux/delay.h>
+#include <linux/module.h>
+#include <linux/mmc/sd.h>
+#include <linux/mmc/host.h>
+#include <linux/gpio.h>
+#include <linux/of_gpio.h>
+#include <linux/platform_device.h>
+#include <linux/gpio-revision.h>
+#include <linux/pm.h>
+#include <linux/pm_runtime.h>
+
+#include "sdhci.h"
+#include "sdhci-pltfm.h"
+#include "sdhci_f_sdh30.h"
+
+#define DRIVER_NAME "f_sdh30"
+
+#define SDCHI_F_SD30_POWER_CONTROL 0x124
+
+
+struct f_sdhost_priv {
+	struct clk *clk_sd4;
+	struct clk *clk_b;
+	u32 vendor_hs200;
+	struct device *dev;
+};
+
+void sdhci_f_sdh30_soft_voltage_switch(struct sdhci_host *host)
+{
+	struct f_sdhost_priv *priv = sdhci_priv(host);
+	u32 ctrl = 0;
+
+	usleep_range(2500, 3000);
+	ctrl = sdhci_readl(host, F_SDH30_IO_CONTROL2);
+	ctrl |= F_SDH30_CRES_O_DN;
+	sdhci_writel(host, ctrl, F_SDH30_IO_CONTROL2);
+	ctrl |= F_SDH30_MSEL_O_1_8;
+	sdhci_writel(host, ctrl, F_SDH30_IO_CONTROL2);
+
+	ctrl &= ~F_SDH30_CRES_O_DN;
+	sdhci_writel(host, ctrl, F_SDH30_IO_CONTROL2);
+	usleep_range(2500, 3000);
+
+	if (priv->vendor_hs200) {
+		dev_info(priv->dev, "%s: setting hs200\n", __func__);
+		ctrl = sdhci_readl(host, F_SDH30_ESD_CONTROL);
+		ctrl |= priv->vendor_hs200;
+		sdhci_writel(host, ctrl, F_SDH30_ESD_CONTROL);
+	}
+
+	ctrl= sdhci_readl(host, F_SDH30_TUNING_SETTING);
+	ctrl |= F_SDH30_CMD_CHK_DIS;
+	sdhci_writel(host, ctrl, F_SDH30_TUNING_SETTING);
+}
+
+unsigned int sdhci_f_sdh30_get_min_clock(struct sdhci_host *host)
+{
+	return F_SDH30_MIN_CLOCK;
+}
+
+void sdhci_f_sdh30_reset_enter(struct sdhci_host *host, u8 mask)
+{
+	if (sdhci_readw(host, SDHCI_CLOCK_CONTROL) == 0) {
+		sdhci_writew(host, 0xBC01, SDHCI_CLOCK_CONTROL);
+		mmiowb();
+	}
+}
+
+static const struct sdhci_ops sdhci_f_sdh30_ops = {
+	.voltage_switch = sdhci_f_sdh30_soft_voltage_switch,
+	.get_min_clock = sdhci_f_sdh30_get_min_clock,
+	.platform_reset_enter = sdhci_f_sdh30_reset_enter,
+};
+
+static int sdhci_f_sdh30_probe(struct platform_device *pdev)
+{
+	struct sdhci_host *host;
+	struct resource *iomem;
+	struct device *dev = &pdev->dev;
+	int irq, ctrl = 0, ret = 0;
+	struct f_sdhost_priv *priv;
+	u32 reg, caps2, bus_width;
+
+	iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!iomem) {
+		dev_err(dev, "%s: resource get error\n", __func__);
+		ret = -ENOENT;
+		goto err;
+	}
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(dev, "%s: no irq specified\n", __func__);
+		ret = irq;
+		goto err;
+	}
+
+	host = sdhci_alloc_host(dev, sizeof(struct sdhci_host) + sizeof(struct f_sdhost_priv));
+	if (IS_ERR(host)) {
+		dev_err(dev, "%s: host allocate error\n", __func__);
+		ret = -ENOMEM;
+		goto err;
+	}
+	priv = sdhci_priv(host); 
+	priv->dev = dev;
+
+	host->quirks =  SDHCI_QUIRK_NO_ENDATTR_IN_NOPDESC |
+		        SDHCI_QUIRK_INVERTED_WRITE_PROTECT;
+	host->quirks2 = SDHCI_QUIRK2_SUPPORT_SINGLE |
+			SDHCI_QUIRK2_UNSUPPORT_3_0_V |
+			SDHCI_QUIRK2_VOLTAGE_SWITCH |
+			SDHCI_QUIRK2_TUNING_WORK_AROUND |
+			SDHCI_QUIRK2_IGNORE_UNEXPECTED_IRQ;
+
+	if (gpio_revision_of_bitmap(pdev->dev.of_node, "revs-no-1v8")) {
+		dev_info(dev, "Applying no 1.8V quirk\n");
+		host->quirks2 |= SDHCI_QUIRK2_NO_1_8_V;
+	}
+
+	if (!of_property_read_u32(pdev->dev.of_node, "mmc-caps2", &caps2)) {
+		dev_info(dev, "Applying mmc capabilities 2\n");
+		host->mmc->caps2 |= caps2;
+	}
+
+	if (!of_property_read_u32(pdev->dev.of_node, "vendor-hs200", &priv->vendor_hs200))
+		dev_info(dev, "Applying vendor-hs200 setting\n");
+	else
+		priv->vendor_hs200 = 0;
+
+	if (!of_property_read_u32(pdev->dev.of_node, "bus-width", &bus_width)) {
+		if (bus_width == 8) {
+			dev_info(dev, "Applying 8 bit bus width\n");
+			host->mmc->caps |= MMC_CAP_8_BIT_DATA;
+		}
+	}
+
+	if (of_find_property(pdev->dev.of_node, "resume-detect-retry", NULL)){
+		dev_info(dev, "Applying resume detect retry quirk\n");
+		host->quirks2 |= SDHCI_QUIRK2_RESUME_DETECT_RETRY;
+	}
+
+	if (of_find_property(pdev->dev.of_node, "no-dma", NULL)){
+		dev_info(dev, "Applying no dma quirk\n");
+		host->quirks |= SDHCI_QUIRK_BROKEN_DMA;
+		host->quirks |= SDHCI_QUIRK_BROKEN_ADMA;
+	}
+
+	host->hw_name = DRIVER_NAME;
+	host->ops = &sdhci_f_sdh30_ops;
+	host->irq = irq;
+
+	if (!request_mem_region(iomem->start, resource_size(iomem),
+		mmc_hostname(host->mmc))) {
+		dev_err(dev, "%s: cannot request region\n", __func__);
+		ret = -ENXIO;
+		goto err_request;
+	}
+
+	host->ioaddr = ioremap_nocache(iomem->start, resource_size(iomem));
+	if (!host->ioaddr) {
+		dev_err(dev, "%s: failed to remap registers\n", __func__);
+		ret = -ENXIO;
+		goto err_remap;
+	}
+
+	priv->clk_sd4 = clk_get(&pdev->dev, "sd_sd4clk");
+	if (!IS_ERR(priv->clk_sd4)) {
+		ret = clk_prepare_enable(priv->clk_sd4);
+		if (ret < 0) {
+			dev_err(dev, "Failed to enable sd4 clock: %d\n", ret);
+			goto err_clk1;
+		}
+	}
+	priv->clk_b = clk_get(&pdev->dev, "sd_bclk");
+	if (!IS_ERR(priv->clk_b)) {
+		ret = clk_prepare_enable(priv->clk_b);
+		if (ret < 0) {
+			dev_err(dev, "Failed to enable sd4 clock: %d\n", ret);
+			goto err_clk2;
+		}
+	}
+
+	platform_set_drvdata(pdev, host);
+
+#ifdef CONFIG_PM_RUNTIME
+	pm_runtime_set_active(&pdev->dev);
+	pm_runtime_enable(&pdev->dev);
+	ret = pm_runtime_get_sync(&pdev->dev);
+	if (ret < 0) {
+		dev_err(dev, "Failed to pm_runtime_get_sync: %d\n", ret);
+	}
+#endif
+
+	ret = sdhci_add_host(host);
+	if (ret) {
+		dev_err(dev, "%s: host add error\n", __func__);
+		goto err_add_host;
+	}
+
+	/* init vendor specific regs */
+	ctrl = sdhci_readw(host, F_SDH30_AHB_CONFIG);
+	ctrl |= F_SDH30_SIN |
+		F_SDH30_AHB_INCR_16 | F_SDH30_AHB_INCR_8 |
+		F_SDH30_AHB_INCR_4;
+	ctrl &= ~F_SDH30_AHB_BIGED;
+	ctrl &= ~F_SDH30_BUSLOCK_EN;
+
+	sdhci_writew(host, ctrl, F_SDH30_AHB_CONFIG);
+
+	mmiowb();
+
+	reg = readl(host->ioaddr + SDCHI_F_SD30_POWER_CONTROL);
+	dev_dbg(priv->dev, "%s: 0x%llx read 0x%x\n", __func__,
+						(u64)iomem->start, reg);
+	writel(reg & ~2, host->ioaddr + SDCHI_F_SD30_POWER_CONTROL);
+
+	msleep(10);
+
+	writel(reg | 2, host->ioaddr + SDCHI_F_SD30_POWER_CONTROL);
+	dev_dbg(priv->dev, "%s: write 0x%x\n", __func__,
+			readl(host->ioaddr + SDCHI_F_SD30_POWER_CONTROL));
+
+	mmiowb();
+
+	return 0;
+
+err_add_host:
+	clk_put(priv->clk_sd4);
+err_clk2:
+	clk_put(priv->clk_b);	
+err_clk1:
+	iounmap(host->ioaddr);
+err_remap:
+	release_mem_region(iomem->start, resource_size(iomem));
+err_request:
+	sdhci_free_host(host);
+err:
+	return ret;
+}
+
+static int sdhci_f_sdh30_remove(struct platform_device *pdev)
+{
+	struct sdhci_host *host = platform_get_drvdata(pdev);
+	struct f_sdhost_priv *priv = sdhci_priv(host);
+	struct resource *iomem = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	int dead;
+	u32 scratch;
+
+	dead = 0;
+	scratch = readl(host->ioaddr + SDHCI_INT_STATUS);
+	if (scratch == (u32)-1)
+		dead = 1;
+
+	sdhci_remove_host(host, dead);
+	iounmap(host->ioaddr);
+	release_mem_region(iomem->start, resource_size(iomem));
+
+	clk_disable_unprepare(priv->clk_sd4);
+	clk_disable_unprepare(priv->clk_b);
+
+	clk_put(priv->clk_b);
+	clk_put(priv->clk_sd4);
+
+	sdhci_free_host(host);
+	platform_set_drvdata(pdev, NULL);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int sdhci_f_sdh30_suspend(struct device *dev)
+{
+	struct sdhci_host *host = dev_get_drvdata(dev);
+	int ret;
+	dev_info(dev, "%s start \n", __func__);
+	ret = sdhci_suspend_host(host);
+	dev_info(dev, "%s end \n", __func__);
+	return ret;
+}
+
+static int sdhci_f_sdh30_resume(struct device *dev)
+{
+	struct sdhci_host *host = dev_get_drvdata(dev);
+	int ret;
+	dev_info(dev, "%s start \n", __func__);
+	ret = sdhci_resume_host(host);
+	dev_info(dev, "%s end \n", __func__);
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_PM_RUNTIME
+static int sdhci_f_sdh30_runtime_suspend(struct device *dev)
+{
+	struct sdhci_host *host = dev_get_drvdata(dev);
+	int ret;
+
+	dev_info(dev, "%s start \n", __func__);
+	ret = sdhci_runtime_suspend_host(host);
+	dev_info(dev, "%s end \n", __func__);
+	return ret;
+}
+
+static int sdhci_f_sdh30_runtime_resume(struct device *dev)
+{
+	struct sdhci_host *host = dev_get_drvdata(dev);
+	int ret;
+
+	dev_info(dev, "%s start \n", __func__);
+	ret = sdhci_runtime_resume_host(host);
+	dev_info(dev, "%s end \n", __func__);
+	return ret;
+}
+#endif
+
+#ifdef CONFIG_PM
+static const struct dev_pm_ops sdhci_f_sdh30_pmops = {
+	SET_SYSTEM_SLEEP_PM_OPS(sdhci_f_sdh30_suspend, sdhci_f_sdh30_resume)
+	SET_RUNTIME_PM_OPS(sdhci_f_sdh30_runtime_suspend, sdhci_f_sdh30_runtime_resume,
+			   NULL)
+};
+
+#define SDHCI_F_SDH30_PMOPS (&sdhci_f_sdh30_pmops)
+
+#else
+#define SDHCI_F_SDH30_PMOPS NULL
+#endif
+
+static const struct of_device_id f_sdh30_dt_ids[] = {
+	{ .compatible = "fujitsu,f_sdh30" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, f_sdh30_dt_ids);
+
+static struct platform_driver sdhci_f_sdh30_driver = {
+	.driver = {
+		.name = DRIVER_NAME,
+		.owner = THIS_MODULE,
+		.of_match_table = f_sdh30_dt_ids,
+#ifdef CONFIG_PM_SLEEP
+		.pm	= SDHCI_F_SDH30_PMOPS,
+#endif
+	},
+	.probe	= sdhci_f_sdh30_probe,
+	.remove	= sdhci_f_sdh30_remove,
+};
+
+static int __init sdhci_f_sdh30_init(void)
+{
+	return platform_driver_register(&sdhci_f_sdh30_driver);
+}
+module_init(sdhci_f_sdh30_init);
+
+static void __exit sdhci_f_sdh30_exit(void)
+{
+	platform_driver_unregister(&sdhci_f_sdh30_driver);
+}
+module_exit(sdhci_f_sdh30_exit);
+
+MODULE_DESCRIPTION("F_SDH30 SD Card Controller driver");
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("FUJITSU SEMICONDUCTOR LTD.");
+MODULE_ALIAS("platform: " DRIVER_NAME);
diff --git a/drivers/mmc/host/sdhci_f_sdh30.h b/drivers/mmc/host/sdhci_f_sdh30.h
new file mode 100644
index 0000000..0088e3f
--- /dev/null
+++ b/drivers/mmc/host/sdhci_f_sdh30.h
@@ -0,0 +1,36 @@
+/*
+ * linux/drivers/mmc/host/sdhci_f_sdh30.h
+ *
+ * Copyright (C) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ */
+
+#ifndef F_SDH30_H
+#define F_SDH30_H
+
+/* F_SDH30 extended Controller registers */
+#define F_SDH30_AHB_CONFIG		0x100
+#define  F_SDH30_AHB_BIGED		0x00000040
+#define  F_SDH30_BUSLOCK_DMA		0x00000020
+#define  F_SDH30_BUSLOCK_EN		0x00000010
+#define  F_SDH30_SIN			0x00000008
+#define  F_SDH30_AHB_INCR_16		0x00000004
+#define  F_SDH30_AHB_INCR_8		0x00000002
+#define  F_SDH30_AHB_INCR_4		0x00000001
+
+#define F_SDH30_TUNING_SETTING		0x108
+#define  F_SDH30_CMD_CHK_DIS		0x00010000
+
+#define F_SDH30_IO_CONTROL2		0x114
+#define  F_SDH30_CRES_O_DN		0x00080000
+#define  F_SDH30_MSEL_O_1_8		0x00040000
+
+#define F_SDH30_ESD_CONTROL		0x124
+#define	F_SDH30_CMD_DAT_DELAY		0x200
+
+#define F_SDH30_MIN_CLOCK		400000
+
+#endif
diff --git a/drivers/mtd/Kconfig b/drivers/mtd/Kconfig
index 5fab4e6..78cc1c0 100644
--- a/drivers/mtd/Kconfig
+++ b/drivers/mtd/Kconfig
@@ -158,9 +158,7 @@ config MTD_BCM47XX_PARTS
 comment "User Modules And Translation Layers"
 
 config MTD_BLKDEVS
-	tristate "Common interface to block layer for MTD 'translation layers'"
-	depends on BLOCK
-	default n
+	tristate
 
 config MTD_BLOCK
 	tristate "Caching block device access to MTD devices"
@@ -320,6 +318,8 @@ source "drivers/mtd/onenand/Kconfig"
 
 source "drivers/mtd/lpddr/Kconfig"
 
+source "drivers/mtd/spi-nor/Kconfig"
+
 source "drivers/mtd/ubi/Kconfig"
 
 endif # MTD
diff --git a/drivers/mtd/Makefile b/drivers/mtd/Makefile
index 4cfb31e..99bb9a1 100644
--- a/drivers/mtd/Makefile
+++ b/drivers/mtd/Makefile
@@ -32,4 +32,5 @@ inftl-objs		:= inftlcore.o inftlmount.o
 
 obj-y		+= chips/ lpddr/ maps/ devices/ nand/ onenand/ tests/
 
+obj-$(CONFIG_MTD_SPI_NOR)	+= spi-nor/
 obj-$(CONFIG_MTD_UBI)		+= ubi/
diff --git a/drivers/mtd/devices/Kconfig b/drivers/mtd/devices/Kconfig
index d8ed2fd..ab9065d 100644
--- a/drivers/mtd/devices/Kconfig
+++ b/drivers/mtd/devices/Kconfig
@@ -80,7 +80,7 @@ config MTD_DATAFLASH_OTP
 
 config MTD_M25P80
 	tristate "Support most SPI Flash chips (AT26DF, M25P, W25X, ...)"
-	depends on SPI_MASTER
+	depends on SPI_MASTER && MTD_SPI_NOR
 	help
 	  This enables access to most modern SPI flash chips, used for
 	  program and data storage.   Series supported include Atmel AT26DF,
@@ -288,4 +288,16 @@ config MTD_DOCPROBE_55AA
 	  LinuxBIOS or if you need to recover a DiskOnChip Millennium on which
 	  you have managed to wipe the first block.
 
+config MTD_HS_SPI_FLASH
+	tristate "HS SPI flash driver"
+	depends on HS_SPI
+	help
+	  HS SPI flash driver for most SPI Flash chips support Dual/Quad
+	  Read in command sequencer mode.
+
+config MTD_GPIO_NOR
+	tristate "Entirely GPIO NOR access"
+	help
+	  Hack to allow access to a parallel NOR by GPIO
+
 endmenu
diff --git a/drivers/mtd/devices/Makefile b/drivers/mtd/devices/Makefile
index d83bd73..488c660 100644
--- a/drivers/mtd/devices/Makefile
+++ b/drivers/mtd/devices/Makefile
@@ -16,6 +16,7 @@ obj-$(CONFIG_MTD_NAND_OMAP_BCH)	+= elm.o
 obj-$(CONFIG_MTD_SPEAR_SMI)	+= spear_smi.o
 obj-$(CONFIG_MTD_SST25L)	+= sst25l.o
 obj-$(CONFIG_MTD_BCM47XXSFLASH)	+= bcm47xxsflash.o
-
+obj-$(CONFIG_MTD_HS_SPI_FLASH)	+= hs_spi_flash.o
+obj-$(CONFIG_MTD_GPIO_NOR)	+= gpio-nor.o
 
 CFLAGS_docg3.o			+= -I$(src)
diff --git a/drivers/mtd/devices/gpio-nor.c b/drivers/mtd/devices/gpio-nor.c
new file mode 100644
index 0000000..d47ee50
--- /dev/null
+++ b/drivers/mtd/devices/gpio-nor.c
@@ -0,0 +1,427 @@
+/*
+ * drivers/mtd/devices/gpio-nor.c
+ * (C) Copyright 2013 Linaro, Ltd
+ * Andy Green <andy.green@linaro.org>
+ *
+ * This driver is a bit unusual... it's a bitbang gpio driver to access
+ * a parallel NOR flash entirely by GPIO.
+ *
+ * based on -->
+ *
+ * Updated, and converted to generic GPIO based driver by Russell King.
+ *
+ * Written by Ben Dooks <ben@simtec.co.uk>
+ *   Based on 2.4 version by Mark Whittaker
+ *
+ * © 2004 Simtec Electronics
+ *
+ * Device driver for NAND connected via GPIO
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/gpio.h>
+#include <linux/io.h>
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/nand.h>
+#include <linux/mtd/partitions.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_gpio.h>
+#include <linux/mtd/cfi.h>
+#include <linux/of_platform.h>
+#include <linux/err.h>
+
+#define MAX_ADS_BITS 32
+#define MAX_DATA_BITS 16
+
+struct gpio_mtd {
+	void __iomem		*io_sync;
+
+	struct mtd_info fake_mtd_info;
+	struct map_info map;
+
+	int data_bits;
+	int ads_bits;
+
+	int	gpio_nce;
+	int	gpio_nwe;
+	int	gpio_noe;
+	int	gpio_rdy;
+	int	gpio_ads[MAX_ADS_BITS];
+	int	gpio_data[MAX_DATA_BITS];
+
+	struct mtd_partition *parts;
+	unsigned int num_parts;
+	unsigned int options;
+};
+
+#define gpio_nor_getpriv(x) \
+	((struct gpio_mtd *)((struct map_info *)(x)->priv)->map_priv_1)
+#define map_to_mtd_info(x) ((struct mtd_info *)(x)->map_priv_2)
+
+static const struct of_device_id gpio_nor_id_table[] = {
+	{ .compatible = "gpio-control-nor" },
+	{}
+};
+MODULE_DEVICE_TABLE(of, gpio_nor_id_table);
+
+static int gpio_nor_get_config_of(const struct device *dev, struct gpio_mtd *gm)
+{
+	u32 val;
+	int n;
+	int gpio = 1;
+	int ret;
+
+	gm->data_bits = 8;
+	if (!of_property_read_u32(dev->of_node, "bank-width", &val)) {
+		if (val == 2) {
+			gm->options |= NAND_BUSWIDTH_16;
+			gm->data_bits = 16;
+		} else if (val != 1) {
+			dev_err(dev, "invalid bank-width %u\n", val);
+			return -EINVAL;
+		}
+	}
+
+	/* first 4 are in this fixed order */
+	gm->gpio_rdy = of_get_gpio(dev->of_node, 0);
+	gm->gpio_nce = of_get_gpio(dev->of_node, 1);
+	gm->gpio_noe = of_get_gpio(dev->of_node, 2);
+	gm->gpio_nwe = of_get_gpio(dev->of_node, 3);
+
+	if (gpio_is_valid(gm->gpio_rdy)) {
+		ret = gpio_request(gm->gpio_rdy, "NOR_rdy");
+		if (!ret)
+			gpio_direction_input(gm->gpio_rdy);
+	}
+
+	ret = gpio_request(gm->gpio_nce, "NOR_nCE");
+	if (ret) {
+		dev_err(dev, "failed to request nCE gpio %d\n", gm->gpio_nce);
+		return -EINVAL;
+	}
+	gpio_direction_output(gm->gpio_nce, 1);
+
+	ret = gpio_request(gm->gpio_noe, "NOR_nOE");
+	if (ret) {
+		dev_err(dev, "failed to request nOE gpio\n");
+		return -EINVAL;
+	}
+	gpio_direction_output(gm->gpio_noe, 1);
+
+	ret = gpio_request(gm->gpio_nwe, "NOR_nWE");
+	if (ret) {
+		dev_err(dev, "failed to request nWE gpio\n");
+		return -EINVAL;
+	}
+	gpio_direction_output(gm->gpio_nwe, 1);
+
+	/* then the next 8 * bank-width are data */
+	for (n = 0; n < gm->data_bits; n++) {
+		gm->gpio_data[n] = of_get_gpio(dev->of_node, n + 4);
+		if (IS_ERR((void *)gm->gpio_data[n])) {
+			dev_err(dev, " invalid gpio %d\n", gm->gpio_data[n]);
+			return -EINVAL;
+		}
+		ret = gpio_request(gm->gpio_data[n] , "NOR_data");
+		if (ret) {
+			dev_err(dev, "failed to request D%d gpio %d: %d\n",
+						     n, gm->gpio_data[n], ret);
+			return -EINVAL;
+		}
+	}
+
+	/* and the remaining ones are address */
+	gm->ads_bits = 0;
+	while (!IS_ERR((void *)gpio)) {
+		gpio = of_get_gpio(dev->of_node, gm->ads_bits +
+							gm->data_bits + 4);
+		if (IS_ERR((void *)gpio))
+			continue;
+		ret = gpio_request(gpio, "NOR_ads");
+		if (ret) {
+			dev_err(dev, "failed to request A%d gpio %d: %d\n",
+						      gm->ads_bits, gpio, ret);
+			return -EINVAL;
+		}
+		gm->gpio_ads[gm->ads_bits++] = gpio;
+		gpio_direction_output(gpio, 0);
+	}
+
+	return 0;
+}
+
+static void gpio_nor_set_address(struct gpio_mtd *gm, unsigned long ads)
+{
+	int n;
+
+	for (n = 0; n < gm->ads_bits; n++)
+		gpio_set_value(gm->gpio_ads[n], !!(ads & (1 << n)));
+}
+
+static void gpio_nor_write_data(struct gpio_mtd *gm, unsigned int data)
+{
+	int n;
+
+	for (n = 0; n < gm->data_bits; n++)
+		gpio_set_value(gm->gpio_data[n], !!(data & (1 << n)));
+}
+
+static unsigned int gpio_nor_read_data(struct gpio_mtd *gm)
+{
+	int n;
+	unsigned int data = 0;
+
+	for (n = 0; n < gm->data_bits; n++)
+		if (gpio_get_value(gm->gpio_data[n]))
+			data |= 1 << n;
+
+	return data;
+}
+
+static unsigned int gpio_nor_data_bus_write_mode(struct gpio_mtd *gm)
+{
+	int n;
+
+	for (n = 0; n < gm->data_bits; n++)
+		gpio_direction_output(gm->gpio_data[n], 0);
+
+	return 0;
+}
+
+static unsigned int gpio_nor_data_bus_read_mode(struct gpio_mtd *gm)
+{
+	int n;
+
+	for (n = 0; n < gm->data_bits; n++)
+		gpio_direction_input(gm->gpio_data[n]);
+
+	return 0;
+}
+
+
+static int gpio_nor_read(struct mtd_info *mtd, loff_t from, size_t len,
+						   size_t *retlen, u_char *buf)
+{
+	struct gpio_mtd *gm = gpio_nor_getpriv(mtd);
+	int val;
+
+	*retlen = len;
+	gpio_nor_data_bus_read_mode(gm);
+
+	while (len) {
+
+		gpio_nor_set_address(gm, from >> 1);
+
+		gpio_set_value(gm->gpio_noe, 0);
+		gpio_set_value(gm->gpio_nwe, 1);
+		gpio_set_value(gm->gpio_nce, 0);
+
+		val = gpio_nor_read_data(gm);
+		*buf++ = val;
+		len--;
+		if (len) {
+			*buf++ = val >> 8;
+			len--;
+		}
+
+		gpio_set_value(gm->gpio_nce, 1);
+		gpio_set_value(gm->gpio_nwe, 1);
+		gpio_set_value(gm->gpio_nce, 1);
+
+		from += 2;
+	}
+
+	return 0;
+}
+
+map_word gpio_nor_map_read(struct map_info *map, unsigned long ads)
+{
+	struct mtd_info *mtd_info = map_to_mtd_info(map);
+	map_word result = { {0} };
+	size_t len = map->bankwidth;
+
+	gpio_nor_read(mtd_info, ads, len, &len, (char *)&result);
+
+	return result;
+}
+void gpio_nor_copy_from(struct map_info *map, void *source, unsigned long ads,
+								ssize_t len)
+{
+	struct mtd_info *mtd_info = map_to_mtd_info(map);
+
+	gpio_nor_read(mtd_info, ads, len, &len, (char *)source);
+}
+
+
+static int gpio_nor_write(struct mtd_info *mtd, loff_t to, size_t len,
+					     size_t *retlen, const u_char *buf)
+{
+	struct gpio_mtd *gm = gpio_nor_getpriv(mtd);
+	int val;
+
+	*retlen = len;
+
+	if (gm->data_bits == 16)
+		if (len & 1)
+			len++;
+
+	gpio_nor_data_bus_write_mode(gm);
+
+	while (len) {
+
+		gpio_nor_set_address(gm, to >> 1);
+		gpio_set_value(gm->gpio_noe, 1);
+		gpio_set_value(gm->gpio_nwe, 0);
+		gpio_set_value(gm->gpio_nce, 0);
+
+		val = *buf++;
+		val |= (*buf++) << 8;
+
+		gpio_nor_write_data(gm, val);
+
+		gpio_set_value(gm->gpio_nce, 1);
+		gpio_set_value(gm->gpio_nwe, 1);
+		gpio_set_value(gm->gpio_nce, 1);
+
+		len -= 2;
+		to += 2;
+	}
+
+	return 0;
+}
+
+void gpio_nor_map_write(struct map_info *map, map_word data, unsigned long ads)
+{
+	struct mtd_info *mtd_info = map_to_mtd_info(map);
+	size_t len = map->bankwidth;
+
+	gpio_nor_write(mtd_info, ads, len, &len, (char *)&data);
+}
+
+static int gpio_nor_remove(struct platform_device *dev)
+{
+	struct gpio_mtd *gm = platform_get_drvdata(dev);
+	struct mtd_info *mtd_info = map_to_mtd_info(&gm->map);
+	int n;
+
+	if (mtd_info) {
+		mtd_device_unregister(mtd_info);
+		dev_info(&dev->dev, "Removing\n");
+		map_destroy(mtd_info);
+	}
+
+	if (gpio_is_valid(gm->gpio_nce))
+		gpio_free(gm->gpio_nce);
+	if (gpio_is_valid(gm->gpio_noe))
+		gpio_free(gm->gpio_noe);
+	if (gpio_is_valid(gm->gpio_nwe))
+		gpio_free(gm->gpio_nwe);
+	if (gpio_is_valid(gm->gpio_rdy))
+		gpio_free(gm->gpio_rdy);
+
+	for (n = 0; n < gm->data_bits; n++)
+		if (gpio_is_valid(gm->gpio_data[n]))
+			gpio_free(gm->gpio_data[n]);
+
+	for (n = 0; n < gm->ads_bits; n++)
+		if (gpio_is_valid(gm->gpio_ads[n]))
+			gpio_free(gm->gpio_ads[n]);
+
+	kfree(gm);
+
+	return 0;
+}
+
+static int gpio_nor_probe(struct platform_device *dev)
+{
+	struct gpio_mtd *gm;
+	struct mtd_info *mtd_info;
+	struct mtd_part_parser_data ppdata = {};
+	int ret = 0;
+
+	if (!dev->dev.of_node)
+		return -EINVAL;
+
+	gm = devm_kzalloc(&dev->dev, sizeof(*gm), GFP_KERNEL);
+	if (gm == NULL) {
+		dev_err(&dev->dev, "failed to create NOR MTD\n");
+		return -ENOMEM;
+	}
+	platform_set_drvdata(dev, gm);
+
+	ret = gpio_nor_get_config_of(&dev->dev, gm);
+	if (ret) {
+		dev_err(&dev->dev, "failed get_config_of\n");
+		goto bail;
+	}
+
+	/* the real mtd_info->priv also points to map_info */
+	gm->fake_mtd_info.priv = &gm->map;
+
+	gm->map.name = "gpio-nor";
+	gm->map.map_priv_1 = (unsigned long)gm;
+	gm->map.map_priv_2 = (unsigned long)&gm->fake_mtd_info;
+
+	gm->map.bankwidth = gm->data_bits >> 3;
+	gm->map.read = gpio_nor_map_read;
+	gm->map.write = gpio_nor_map_write;
+	gm->map.copy_from = gpio_nor_copy_from;
+
+	/* there's no mapping region, limited only by ads bits x width */
+	gm->map.size = 1 << (gm->ads_bits + gm->map.bankwidth - 1);
+
+	mtd_info = do_map_probe("cfi_probe", &gm->map);
+	if (!mtd_info) {
+		dev_err(&dev->dev, "map probe failed\n");
+		return -ENODEV;
+	}
+
+	mtd_info->dev.parent = &dev->dev;
+	mtd_info->owner = THIS_MODULE;
+	/* point to the real one */
+	gm->map.map_priv_2 = (unsigned long)mtd_info;
+	mtd_info->_read = gpio_nor_read;
+
+	ppdata.of_node = dev->dev.of_node;
+	ret = mtd_device_parse_register(mtd_info, NULL, &ppdata,
+						     gm->parts, gm->num_parts);
+	if (ret) {
+		dev_err(&dev->dev, "mtd_device_parse_register: %d\n", ret);
+		goto bail;
+	}
+
+	dev_info(&dev->dev, "Usable: %dMiB\n", 1 << (gm->ads_bits - 20));
+
+	return 0;
+
+bail:
+	dev_err(&dev->dev, "failed to probe %d\n", ret);
+	gpio_nor_remove(dev);
+
+	return ret;
+}
+
+static struct platform_driver gpio_nor_driver = {
+	.probe		= gpio_nor_probe,
+	.remove		= gpio_nor_remove,
+	.driver		= {
+		.name	= "gpio-nor",
+		.of_match_table = of_match_ptr(gpio_nor_id_table),
+	},
+};
+
+module_platform_driver(gpio_nor_driver);
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Andy Green <andy.green@linaro.org>");
+MODULE_DESCRIPTION("GPIO NOR Driver");
diff --git a/drivers/mtd/devices/hs_spi_flash.c b/drivers/mtd/devices/hs_spi_flash.c
new file mode 100644
index 0000000..296b61c
--- /dev/null
+++ b/drivers/mtd/devices/hs_spi_flash.c
@@ -0,0 +1,234 @@
+/*
+ * MTD SPI driver for HS SPI Controller
+ *
+ * Copyright (C) 2010-2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/init.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/io.h>
+
+#include <linux/mtd/mtd.h>
+#include <linux/mtd/partitions.h>
+
+#include <linux/spi/spi.h>
+#include <linux/spi/flash.h>
+
+#include <linux/platform_data/dma-mb8ac0300-xdmac.h>
+#include <linux/platform_data/mb8ac0300-hs_spi.h>
+
+/*
+ * MTD driver for HS SPI Controller operation in command sequencer mode
+ */
+#define	DRV_NAME		"hsspi_cs"
+
+struct command_sequencer {
+	struct spi_device	*spi;
+	struct mutex		lock;
+	struct mtd_info		mtd;
+	void __iomem		*base;
+
+};
+
+static inline struct command_sequencer *mtd_to_cs(struct mtd_info *mtd)
+{
+	return container_of(mtd, struct command_sequencer, mtd);
+}
+
+/*
+ * MTD implementation
+ */
+
+/*
+ * hs_spi_flash_read - Read data from Serial Flash Area
+ * @mtd:	Mtd device data.
+ * @from:	Start address to read.
+ * @len:	Data length to read.
+ * @retlen:	Actually read length.
+ * @buf:	Buffer for read data.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_flash_read(struct mtd_info *mtd, loff_t from, size_t len,
+	size_t *retlen, unsigned char *buf)
+{
+	struct command_sequencer	*cs = mtd_to_cs(mtd);
+	unsigned int		addr_virt = (unsigned int)cs->base
+							 + (unsigned int)from;
+
+	mutex_lock(&cs->lock);
+	memcpy(buf, (const void *)addr_virt, len);
+	*retlen = len;
+	mutex_unlock(&cs->lock);
+
+	return 0;
+}
+
+/*
+ * hs_spi_flash_write - Write data to Serial Flash Area
+ * @mtd:	Mtd device data.
+ * @to:		Start address to write.
+ * @len:	Data length to write.
+ * @retlen:	Actually write length.
+ * @buf:	Buffer for write data.
+ *
+ * Error code will be returned because write operation is not supported
+ * by HS SPI Driver in command sequencer mode
+ *
+ * Returns negative errno on failure
+ */
+static int hs_spi_flash_write(struct mtd_info *mtd, loff_t to, size_t len,
+	size_t *retlen, const u_char *buf)
+{
+	struct command_sequencer	*cs = mtd_to_cs(mtd);
+
+	dev_err(&cs->spi->dev, "write operation is not supported by HS SPI Driver in command sequencer mode\n");
+	return -EINVAL;
+}
+
+/*
+ * hs_spi_flash_erase - Erase the data of Serial Flash Area
+ * @mtd:	Mtd device data.
+ * @instr:	Erase information.
+ *
+ * Error code will be returned because erase operation is not supported
+ * by HS SPI Driver in command sequencer mode
+ *
+ * Returns negative errno on failure
+ */
+static int hs_spi_flash_erase(struct mtd_info *mtd, struct erase_info *instr)
+{
+	struct command_sequencer	*cs = mtd_to_cs(mtd);
+
+	dev_err(&cs->spi->dev, "erase operation is not supported by HS SPI Driver in command sequencer mode\n");
+	return -EINVAL;
+}
+
+/****************************************************************************/
+/*
+ * hs_spi_probe - probe the flash driver for HS SPI command sequencer mode
+ * @spi:	Spi device data.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_flash_probe(struct spi_device *spi)
+{
+	struct hs_spi			*hs;
+	struct command_sequencer	*cs;
+	struct flash_platform_data	*data;
+	struct mtd_partition		*parts = NULL;
+	int				nr_parts = 0;
+	hs = spi_master_get_devdata(spi->master);
+
+	if (!hs) {
+		pr_err("%s: No hs spi driver data\n",
+			dev_name(&spi->dev));
+		return -ENODEV;
+	}
+
+	data = spi->dev.platform_data;
+
+	if (!data || !data->type) {
+		pr_err("%s: No flash type designated\n",
+			dev_name(&spi->dev));
+		return -ENODEV;
+	}
+
+	cs = kzalloc(sizeof(*cs), GFP_KERNEL);
+	if (!cs)
+		return -ENOMEM;
+
+	cs->base	= hs->csa;
+	cs->spi		= spi;
+	mutex_init(&cs->lock);
+	dev_set_drvdata(&spi->dev, cs);
+	cs->mtd.name	= data->name;
+
+	/* setup mtd information */
+	cs->mtd.type		= MTD_NORFLASH;
+	cs->mtd.writesize	= 1;
+	cs->mtd.flags		= MTD_CAP_NORFLASH;
+	cs->mtd.size		= hs->csa_size;
+	/*
+	 * only support read operation in command sequencer mode.
+	 * error code will be returned when write or erase.
+	 */
+	cs->mtd._read		= hs_spi_flash_read;
+	cs->mtd._write		= hs_spi_flash_write;
+	cs->mtd._erase		= hs_spi_flash_erase;
+
+	cs->mtd.erasesize	= SZ_64K; /* do not support erase */
+
+	cs->mtd.dev.parent = &spi->dev;
+
+	parts = data->parts;
+	nr_parts = data->nr_parts;
+
+	return mtd_device_register(&cs->mtd, parts, nr_parts) == 0 ?
+		0 : -ENODEV;
+}
+
+/*
+ * hs_spi_flash_remove - remove the flash driver
+ * @spi:	Spi device data.
+ *
+ * Returns 0 on success
+ */
+static int hs_spi_flash_remove(struct spi_device *spi)
+{
+	struct command_sequencer	*cs = dev_get_drvdata(&spi->dev);
+	int				status;
+
+	/* Clean up MTD stuff. */
+	status = mtd_device_unregister(&cs->mtd);
+	if (status == 0)
+		kfree(cs);
+	return 0;
+}
+
+static struct spi_driver hs_spi_flash_driver = {
+	.driver = {
+		.name	= DRV_NAME,
+		.owner	= THIS_MODULE,
+	},
+	.probe	= hs_spi_flash_probe,
+	.remove	= hs_spi_flash_remove,
+};
+
+/*
+ * hs_spi_init - initialize module
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int __init hs_spi_flash_init(void)
+{
+	return spi_register_driver(&hs_spi_flash_driver);
+}
+
+/*
+ * hs_spi_init - exit module
+ */
+static void __exit hs_spi_flash_exit(void)
+{
+	spi_unregister_driver(&hs_spi_flash_driver);
+}
+
+
+module_init(hs_spi_flash_init);
+module_exit(hs_spi_flash_exit);
+
+MODULE_DESCRIPTION("MTD SPI driver for HS SPI Controller");
+MODULE_AUTHOR("Fujitsu Semiconductor Limitd");
+MODULE_LICENSE("GPL");
diff --git a/drivers/mtd/devices/m25p80.c b/drivers/mtd/devices/m25p80.c
index 4e29a30..27ae5ab 100644
--- a/drivers/mtd/devices/m25p80.c
+++ b/drivers/mtd/devices/m25p80.c
@@ -15,76 +15,32 @@
  *
  */
 
-#include <linux/init.h>
 #include <linux/err.h>
 #include <linux/errno.h>
 #include <linux/module.h>
 #include <linux/device.h>
-#include <linux/interrupt.h>
-#include <linux/mutex.h>
-#include <linux/math64.h>
-#include <linux/slab.h>
-#include <linux/sched.h>
-#include <linux/mod_devicetable.h>
-#include <linux/of_device.h>
-
-#include <linux/mtd/cfi.h>
 #include <linux/mtd/mtd.h>
 #include <linux/mtd/partitions.h>
-#include <linux/of_platform.h>
 
 #include <linux/spi/spi.h>
 #include <linux/spi/flash.h>
+#include <linux/mtd/spi-nor.h>
 
 /* Flash opcodes. */
-#define	OPCODE_WREN		0x06	/* Write enable */
-#define	OPCODE_RDSR		0x05	/* Read status register */
-#define	OPCODE_WRSR		0x01	/* Write status register 1 byte */
-#define	OPCODE_NORM_READ	0x03	/* Read data bytes (low frequency) */
-#define	OPCODE_FAST_READ	0x0b	/* Read data bytes (high frequency) */
 #define OPCODE_QUAD_READ	0x6b	/* Quad read command */
-#define	OPCODE_PP		0x02	/* Page program (up to 256 bytes) */
 #define OPCODE_QPP		0x32	/* Quad page program */
-#define	OPCODE_BE_4K		0x20	/* Erase 4KiB block */
-#define	OPCODE_BE_32K		0x52	/* Erase 32KiB block */
-#define	OPCODE_CHIP_ERASE	0xc7	/* Erase whole flash chip */
-#define	OPCODE_SE		0xd8	/* Sector erase (usually 64KiB) */
-#define	OPCODE_RDID		0x9f	/* Read JEDEC ID */
 #define OPCODE_RDFSR		0x70	/* Read Flag Status Register */
 #define OPCODE_WREAR		0xc5	/* Write Extended Address Register */
 
-/* Used for SST flashes only. */
-#define	OPCODE_BP		0x02	/* Byte program */
-#define	OPCODE_WRDI		0x04	/* Write disable */
-#define	OPCODE_AAI_WP		0xad	/* Auto address increment word program */
-
-/* Used for Macronix flashes only. */
-#define	OPCODE_EN4B		0xb7	/* Enter 4-byte mode */
-#define	OPCODE_EX4B		0xe9	/* Exit 4-byte mode */
-
 /* Used for Spansion flashes only. */
-#define	OPCODE_BRWR		0x17	/* Bank register write */
 #define	OPCODE_BRRD		0x16	/* Bank register read */
 
-/* Status Register bits. */
-#define	SR_WIP			1	/* Write in progress */
-#define	SR_WEL			2	/* Write enable latch */
-/* meaning of other SR_* bits may differ between vendors */
-#define	SR_BP0			4	/* Block protect 0 */
-#define	SR_BP1			8	/* Block protect 1 */
-#define	SR_BP2			0x10	/* Block protect 2 */
-#define	SR_SRWD			0x80	/* SR write protect */
-
 /* Flag Status Register bits. */
 #define FSR_RDY			0x80	/* Ready/Busy program erase
 					controller */
 /* Define max times to check status register before we give up. */
 #define	MAX_READY_WAIT_JIFFIES	(480 * HZ) /* N25Q specs 480s max chip erase */
-#define	MAX_CMD_SIZE		5
-
-#define JEDEC_MFR(_jedec_id)	((_jedec_id) >> 16)
-
-/****************************************************************************/
+#define	MAX_CMD_SIZE		6
 
 struct m25p {
 	struct spi_device	*spi;
@@ -1390,18 +1346,60 @@ static int m25p_probe(struct spi_device *spi)
 
 static int m25p_remove(struct spi_device *spi)
 {
-	struct m25p	*flash = dev_get_drvdata(&spi->dev);
-	int		status;
+	struct m25p	*flash = spi_get_drvdata(spi);
 
 	/* Clean up MTD stuff. */
-	status = mtd_device_unregister(&flash->mtd);
-	if (status == 0) {
-		kfree(flash->command);
-		kfree(flash);
-	}
-	return 0;
+	return mtd_device_unregister(&flash->mtd);
 }
 
+/*
+ * XXX This needs to be kept in sync with spi_nor_ids.  We can't share
+ * it with spi-nor, because if this is built as a module then modpost
+ * won't be able to read it and add appropriate aliases.
+ */
+static const struct spi_device_id m25p_ids[] = {
+	{"at25fs010"},	{"at25fs040"},	{"at25df041a"},	{"at25df321a"},
+	{"at25df641"},	{"at26f004"},	{"at26df081a"},	{"at26df161a"},
+	{"at26df321"},	{"at45db081d"},
+	{"en25f32"},	{"en25p32"},	{"en25q32b"},	{"en25p64"},
+	{"en25q64"},	{"en25qh128"},	{"en25qh256"},
+	{"f25l32pa"},
+	{"mr25h256"},	{"mr25h10"},
+	{"gd25q32"},	{"gd25q64"},
+	{"160s33b"},	{"320s33b"},	{"640s33b"},
+	{"mx25l2005a"},	{"mx25l4005a"},	{"mx25l8005"},	{"mx25l1606e"},
+	{"mx25l3205d"},	{"mx25l3255e"},	{"mx25l6405d"},	{"mx25l12805d"},
+	{"mx25l12855e"},{"mx25l25635e"},{"mx25l25655e"},{"mx66l51235l"},
+	{"mx66l1g55g"},
+	{"n25q064"},	{"n25q128a11"},	{"n25q128a13"},	{"n25q256a"},
+	{"n25q512a"},	{"n25q512ax3"},	{"n25q00"},
+	{"pm25lv512"},	{"pm25lv010"},	{"pm25lq032"},
+	{"s25sl032p"},	{"s25sl064p"},	{"s25fl256s0"},	{"s25fl256s1"},
+	{"s25fl512s"},	{"s70fl01gs"},	{"s25sl12800"},	{"s25sl12801"},
+	{"s25fl129p0"},	{"s25fl129p1"},	{"s25sl004a"},	{"s25sl008a"},
+	{"s25sl016a"},	{"s25sl032a"},	{"s25sl064a"},	{"s25fl008k"},
+	{"s25fl016k"},	{"s25fl064k"},	{"s25fl132k"},
+	{"sst25vf040b"},{"sst25vf080b"},{"sst25vf016b"},{"sst25vf032b"},
+	{"sst25vf064c"},{"sst25wf512"},	{"sst25wf010"},	{"sst25wf020"},
+	{"sst25wf040"},
+	{"m25p05"},	{"m25p10"},	{"m25p20"},	{"m25p40"},
+	{"m25p80"},	{"m25p16"},	{"m25p32"},	{"m25p64"},
+	{"m25p128"},	{"n25q032"},
+	{"m25p05-nonjedec"},	{"m25p10-nonjedec"},	{"m25p20-nonjedec"},
+	{"m25p40-nonjedec"},	{"m25p80-nonjedec"},	{"m25p16-nonjedec"},
+	{"m25p32-nonjedec"},	{"m25p64-nonjedec"},	{"m25p128-nonjedec"},
+	{"m45pe10"},	{"m45pe80"},	{"m45pe16"},
+	{"m25pe20"},	{"m25pe80"},	{"m25pe16"},
+	{"m25px16"},	{"m25px32"},	{"m25px32-s0"},	{"m25px32-s1"},
+	{"m25px64"},	{"m25px80"},
+	{"w25x10"},	{"w25x20"},	{"w25x40"},	{"w25x80"},
+	{"w25x16"},	{"w25x32"},	{"w25q32"},	{"w25q32dw"},
+	{"w25x64"},	{"w25q64"},	{"w25q80"},	{"w25q80bl"},
+	{"w25q128"},	{"w25q256"},	{"cat25c11"},
+	{"cat25c03"},	{"cat25c09"},	{"cat25c17"},	{"cat25128"},
+	{ },
+};
+MODULE_DEVICE_TABLE(spi, m25p_ids);
 
 static struct spi_driver m25p80_driver = {
 	.driver = {
diff --git a/drivers/mtd/maps/plat-ram.c b/drivers/mtd/maps/plat-ram.c
index 71fdda2..c8deeeb 100644
--- a/drivers/mtd/maps/plat-ram.c
+++ b/drivers/mtd/maps/plat-ram.c
@@ -30,6 +30,7 @@
 #include <linux/device.h>
 #include <linux/slab.h>
 #include <linux/platform_device.h>
+#include <linux/of.h>
 
 #include <linux/mtd/mtd.h>
 #include <linux/mtd/map.h>
@@ -128,15 +129,40 @@ static int platram_probe(struct platform_device *pdev)
 	struct resource *res;
 	int err = 0;
 
+#ifdef CONFIG_OF
+	const int *p;
+	
 	dev_dbg(&pdev->dev, "probe entered\n");
 
-	if (pdev->dev.platform_data == NULL) {
-		dev_err(&pdev->dev, "no platform data supplied\n");
-		err = -ENOENT;
+	if (pdev->dev.of_node) {
+		pdata = kzalloc(sizeof(*pdata), GFP_KERNEL);
+		if (pdata == NULL) {
+			dev_err(&pdev->dev, "Out of memory\n");
+			err = -ENOMEM;
 		goto exit_error;
 	}
 
+		p = of_get_property(pdev->dev.of_node, "bank-width", NULL);
+		if(p)
+			pdata->bankwidth = be32_to_cpu(*p);
+
+		/* FIXME: not sure these are correct */
+		pdata->mapname = NULL;
+		pdata->map_probes = NULL;
+		pdata->probes = NULL;
+		pdata->nr_partitions = 0;
+
+		/* TODO: get other properties */
+	} else
+#endif
+	{
 	pdata = pdev->dev.platform_data;
+		 if (pdata == NULL) {
+		  	 dev_err(&pdev->dev, "no platform data supplied\n");
+			 err = -ENOENT;
+			 goto exit_error;
+		 }
+ 	}
 
 	info = kzalloc(sizeof(*info), GFP_KERNEL);
 	if (info == NULL) {
@@ -247,6 +273,13 @@ static int platram_probe(struct platform_device *pdev)
 
 /* device driver info */
 
+static const struct of_device_id mtd_ram_dt_ids[] = {
+	{ .compatible = "mtd-ram" },
+	{ /* sentinel */ }
+};
+
+
+
 /* work with hotplug and coldplug */
 MODULE_ALIAS("platform:mtd-ram");
 
@@ -256,9 +289,15 @@ static struct platform_driver platram_driver = {
 	.driver		= {
 		.name	= "mtd-ram",
 		.owner	= THIS_MODULE,
+		.of_match_table = mtd_ram_dt_ids,
+
 	},
 };
 
+MODULE_DEVICE_TABLE(of, mtd_ram_dt_ids);
+
+
+
 /* module init/exit */
 
 static int __init platram_init(void)
diff --git a/drivers/mtd/spi-nor/Kconfig b/drivers/mtd/spi-nor/Kconfig
new file mode 100644
index 0000000..64a4f0e
--- /dev/null
+++ b/drivers/mtd/spi-nor/Kconfig
@@ -0,0 +1,31 @@
+menuconfig MTD_SPI_NOR
+	tristate "SPI-NOR device support"
+	depends on MTD
+	help
+	  This is the framework for the SPI NOR which can be used by the SPI
+	  device drivers and the SPI-NOR device driver.
+
+if MTD_SPI_NOR
+
+config MTD_SPI_NOR_USE_4K_SECTORS
+	bool "Use small 4096 B erase sectors"
+	default y
+	help
+	  Many flash memories support erasing small (4096 B) sectors. Depending
+	  on the usage this feature may provide performance gain in comparison
+	  to erasing whole blocks (32/64 KiB).
+	  Changing a small part of the flash's contents is usually faster with
+	  small sectors. On the other hand erasing should be faster when using
+	  64 KiB block instead of 16 × 4 KiB sectors.
+
+	  Please note that some tools/drivers/filesystems may not work with
+	  4096 B erase size (e.g. UBIFS requires 15 KiB as a minimum).
+
+config SPI_FSL_QUADSPI
+	tristate "Freescale Quad SPI controller"
+	depends on ARCH_MXC
+	help
+	  This enables support for the Quad SPI controller in master mode.
+	  We only connect the NOR to this controller now.
+
+endif # MTD_SPI_NOR
diff --git a/drivers/mtd/spi-nor/Makefile b/drivers/mtd/spi-nor/Makefile
new file mode 100644
index 0000000..50d190a
--- /dev/null
+++ b/drivers/mtd/spi-nor/Makefile
@@ -0,0 +1 @@
+obj-$(CONFIG_MTD_SPI_NOR)	+= spi-nor.o
diff --git a/drivers/mtd/spi-nor/spi-nor.c b/drivers/mtd/spi-nor/spi-nor.c
new file mode 100644
index 0000000..95f3143
--- /dev/null
+++ b/drivers/mtd/spi-nor/spi-nor.c
@@ -0,0 +1,1155 @@
+/*
+ * Based on m25p80.c, by Mike Lavender (mike@steroidmicros.com), with
+ * influence from lart.c (Abraham Van Der Merwe) and mtd_dataflash.c
+ *
+ * Copyright (C) 2005, Intec Automation Inc.
+ * Copyright (C) 2014, Freescale Semiconductor, Inc.
+ *
+ * This code is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#include <linux/err.h>
+#include <linux/errno.h>
+#include <linux/module.h>
+#include <linux/device.h>
+#include <linux/mutex.h>
+#include <linux/math64.h>
+
+#include <linux/mtd/cfi.h>
+#include <linux/mtd/mtd.h>
+#include <linux/of_platform.h>
+#include <linux/spi/flash.h>
+#include <linux/mtd/spi-nor.h>
+
+/* Define max times to check status register before we give up. */
+#define	MAX_READY_WAIT_JIFFIES	(40 * HZ) /* M25P16 specs 40s max chip erase */
+
+#define SPI_NOR_MAX_ID_LEN	6
+
+struct flash_info {
+	/*
+	 * This array stores the ID bytes.
+	 * The first three bytes are the JEDIC ID.
+	 * JEDEC ID zero means "no ID" (mostly older chips).
+	 */
+	u8		id[SPI_NOR_MAX_ID_LEN];
+	u8		id_len;
+
+	/* The size listed here is what works with SPINOR_OP_SE, which isn't
+	 * necessarily called a "sector" by the vendor.
+	 */
+	unsigned	sector_size;
+	u16		n_sectors;
+
+	u16		page_size;
+	u16		addr_width;
+
+	u16		flags;
+#define	SECT_4K			0x01	/* SPINOR_OP_BE_4K works uniformly */
+#define	SPI_NOR_NO_ERASE	0x02	/* No erase command needed */
+#define	SST_WRITE		0x04	/* use SST byte programming */
+#define	SPI_NOR_NO_FR		0x08	/* Can't do fastread */
+#define	SECT_4K_PMC		0x10	/* SPINOR_OP_BE_4K_PMC works uniformly */
+#define	SPI_NOR_DUAL_READ	0x20    /* Flash supports Dual Read */
+#define	SPI_NOR_QUAD_READ	0x40    /* Flash supports Quad Read */
+#define	USE_FSR			0x80	/* use flag status register */
+};
+
+#define JEDEC_MFR(info)	((info)->id[0])
+
+static const struct spi_device_id *spi_nor_match_id(const char *name);
+
+/*
+ * Read the status register, returning its value in the location
+ * Return the status register value.
+ * Returns negative if error occurred.
+ */
+static int read_sr(struct spi_nor *nor)
+{
+	int ret;
+	u8 val;
+
+	ret = nor->read_reg(nor, SPINOR_OP_RDSR, &val, 1);
+	if (ret < 0) {
+		pr_err("error %d reading SR\n", (int) ret);
+		return ret;
+	}
+
+	return val;
+}
+
+/*
+ * Read the flag status register, returning its value in the location
+ * Return the status register value.
+ * Returns negative if error occurred.
+ */
+static int read_fsr(struct spi_nor *nor)
+{
+	int ret;
+	u8 val;
+
+	ret = nor->read_reg(nor, SPINOR_OP_RDFSR, &val, 1);
+	if (ret < 0) {
+		pr_err("error %d reading FSR\n", ret);
+		return ret;
+	}
+
+	return val;
+}
+
+/*
+ * Read configuration register, returning its value in the
+ * location. Return the configuration register value.
+ * Returns negative if error occured.
+ */
+static int read_cr(struct spi_nor *nor)
+{
+	int ret;
+	u8 val;
+
+	ret = nor->read_reg(nor, SPINOR_OP_RDCR, &val, 1);
+	if (ret < 0) {
+		dev_err(nor->dev, "error %d reading CR\n", ret);
+		return ret;
+	}
+
+	return val;
+}
+
+/*
+ * Dummy Cycle calculation for different type of read.
+ * It can be used to support more commands with
+ * different dummy cycle requirements.
+ */
+static inline int spi_nor_read_dummy_cycles(struct spi_nor *nor)
+{
+	switch (nor->flash_read) {
+	case SPI_NOR_FAST:
+	case SPI_NOR_DUAL:
+	case SPI_NOR_QUAD:
+		return 8;
+	case SPI_NOR_NORMAL:
+		return 0;
+	}
+	return 0;
+}
+
+/*
+ * Write status register 1 byte
+ * Returns negative if error occurred.
+ */
+static inline int write_sr(struct spi_nor *nor, u8 val)
+{
+	nor->cmd_buf[0] = val;
+	return nor->write_reg(nor, SPINOR_OP_WRSR, nor->cmd_buf, 1, 0);
+}
+
+/*
+ * Set write enable latch with Write Enable command.
+ * Returns negative if error occurred.
+ */
+static inline int write_enable(struct spi_nor *nor)
+{
+	return nor->write_reg(nor, SPINOR_OP_WREN, NULL, 0, 0);
+}
+
+/*
+ * Send write disble instruction to the chip.
+ */
+static inline int write_disable(struct spi_nor *nor)
+{
+	return nor->write_reg(nor, SPINOR_OP_WRDI, NULL, 0, 0);
+}
+
+static inline struct spi_nor *mtd_to_spi_nor(struct mtd_info *mtd)
+{
+	return mtd->priv;
+}
+
+/* Enable/disable 4-byte addressing mode. */
+static inline int set_4byte(struct spi_nor *nor, struct flash_info *info,
+			    int enable)
+{
+	int status;
+	bool need_wren = false;
+	u8 cmd;
+
+	switch (JEDEC_MFR(info)) {
+	case CFI_MFR_ST: /* Micron, actually */
+		/* Some Micron need WREN command; all will accept it */
+		need_wren = true;
+	case CFI_MFR_MACRONIX:
+	case 0xEF /* winbond */:
+		if (need_wren)
+			write_enable(nor);
+
+		cmd = enable ? SPINOR_OP_EN4B : SPINOR_OP_EX4B;
+		status = nor->write_reg(nor, cmd, NULL, 0, 0);
+		if (need_wren)
+			write_disable(nor);
+
+		return status;
+	default:
+		/* Spansion style */
+		nor->cmd_buf[0] = enable << 7;
+		return nor->write_reg(nor, SPINOR_OP_BRWR, nor->cmd_buf, 1, 0);
+	}
+}
+static inline int spi_nor_sr_ready(struct spi_nor *nor)
+{
+	int sr = read_sr(nor);
+	if (sr < 0)
+		return sr;
+	else
+		return !(sr & SR_WIP);
+}
+
+static inline int spi_nor_fsr_ready(struct spi_nor *nor)
+{
+	int fsr = read_fsr(nor);
+	if (fsr < 0)
+		return fsr;
+	else
+		return fsr & FSR_READY;
+}
+
+static int spi_nor_ready(struct spi_nor *nor)
+{
+	int sr, fsr;
+	sr = spi_nor_sr_ready(nor);
+	if (sr < 0)
+		return sr;
+	fsr = nor->flags & SNOR_F_USE_FSR ? spi_nor_fsr_ready(nor) : 1;
+	if (fsr < 0)
+		return fsr;
+	return sr && fsr;
+}
+
+/*
+ * Service routine to read status register until ready, or timeout occurs.
+ * Returns non-zero if error.
+ */
+static int spi_nor_wait_till_ready(struct spi_nor *nor)
+{
+	unsigned long deadline;
+	int timeout = 0, ret;
+
+	deadline = jiffies + MAX_READY_WAIT_JIFFIES;
+
+	while (!timeout) {
+		if (time_after_eq(jiffies, deadline))
+			timeout = 1;
+
+		ret = spi_nor_ready(nor);
+		if (ret < 0)
+			return ret;
+		if (ret)
+			return 0;
+
+		cond_resched();
+	}
+
+	dev_err(nor->dev, "flash operation timed out\n");
+
+	return -ETIMEDOUT;
+}
+
+/*
+ * Erase the whole flash memory
+ *
+ * Returns 0 if successful, non-zero otherwise.
+ */
+static int erase_chip(struct spi_nor *nor)
+{
+	dev_dbg(nor->dev, " %lldKiB\n", (long long)(nor->mtd->size >> 10));
+
+	return nor->write_reg(nor, SPINOR_OP_CHIP_ERASE, NULL, 0, 0);
+}
+
+static int spi_nor_lock_and_prep(struct spi_nor *nor, enum spi_nor_ops ops)
+{
+	int ret = 0;
+
+	mutex_lock(&nor->lock);
+
+	if (nor->prepare) {
+		ret = nor->prepare(nor, ops);
+		if (ret) {
+			dev_err(nor->dev, "failed in the preparation.\n");
+			mutex_unlock(&nor->lock);
+			return ret;
+		}
+	}
+	return ret;
+}
+
+static void spi_nor_unlock_and_unprep(struct spi_nor *nor, enum spi_nor_ops ops)
+{
+	if (nor->unprepare)
+		nor->unprepare(nor, ops);
+	mutex_unlock(&nor->lock);
+}
+
+/*
+ * Erase an address range on the nor chip.  The address range may extend
+ * one or more erase sectors.  Return an error is there is a problem erasing.
+ */
+static int spi_nor_erase(struct mtd_info *mtd, struct erase_info *instr)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	u32 addr, len;
+	uint32_t rem;
+	int ret;
+
+	dev_dbg(nor->dev, "at 0x%llx, len %lld\n", (long long)instr->addr,
+			(long long)instr->len);
+
+	div_u64_rem(instr->len, mtd->erasesize, &rem);
+	if (rem)
+		return -EINVAL;
+
+	addr = instr->addr;
+	len = instr->len;
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_ERASE);
+	if (ret)
+		return ret;
+
+	/* whole-chip erase? */
+	if (len == mtd->size) {
+		write_enable(nor);
+
+		if (erase_chip(nor)) {
+			ret = -EIO;
+			goto erase_err;
+		}
+
+		ret = spi_nor_wait_till_ready(nor);
+		if (ret)
+			goto erase_err;
+
+	/* REVISIT in some cases we could speed up erasing large regions
+	 * by using SPINOR_OP_SE instead of SPINOR_OP_BE_4K.  We may have set up
+	 * to use "small sector erase", but that's not always optimal.
+	 */
+
+	/* "sector"-at-a-time erase */
+	} else {
+		while (len) {
+			write_enable(nor);
+
+			if (nor->erase(nor, addr)) {
+				ret = -EIO;
+				goto erase_err;
+			}
+
+			addr += mtd->erasesize;
+			len -= mtd->erasesize;
+
+			ret = spi_nor_wait_till_ready(nor);
+			if (ret)
+				goto erase_err;
+		}
+	}
+
+	write_disable(nor);
+
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_ERASE);
+
+	instr->state = MTD_ERASE_DONE;
+	mtd_erase_callback(instr);
+
+	return ret;
+
+erase_err:
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_ERASE);
+	instr->state = MTD_ERASE_FAILED;
+	return ret;
+}
+
+static int spi_nor_lock(struct mtd_info *mtd, loff_t ofs, uint64_t len)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	uint32_t offset = ofs;
+	uint8_t status_old, status_new;
+	int ret = 0;
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_LOCK);
+	if (ret)
+		return ret;
+
+	status_old = read_sr(nor);
+
+	if (offset < mtd->size - (mtd->size / 2))
+		status_new = status_old | SR_BP2 | SR_BP1 | SR_BP0;
+	else if (offset < mtd->size - (mtd->size / 4))
+		status_new = (status_old & ~SR_BP0) | SR_BP2 | SR_BP1;
+	else if (offset < mtd->size - (mtd->size / 8))
+		status_new = (status_old & ~SR_BP1) | SR_BP2 | SR_BP0;
+	else if (offset < mtd->size - (mtd->size / 16))
+		status_new = (status_old & ~(SR_BP0 | SR_BP1)) | SR_BP2;
+	else if (offset < mtd->size - (mtd->size / 32))
+		status_new = (status_old & ~SR_BP2) | SR_BP1 | SR_BP0;
+	else if (offset < mtd->size - (mtd->size / 64))
+		status_new = (status_old & ~(SR_BP2 | SR_BP0)) | SR_BP1;
+	else
+		status_new = (status_old & ~(SR_BP2 | SR_BP1)) | SR_BP0;
+
+	/* Only modify protection if it will not unlock other areas */
+	if ((status_new & (SR_BP2 | SR_BP1 | SR_BP0)) >
+				(status_old & (SR_BP2 | SR_BP1 | SR_BP0))) {
+		write_enable(nor);
+		ret = write_sr(nor, status_new);
+		if (ret)
+			goto err;
+	}
+
+err:
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_LOCK);
+	return ret;
+}
+
+static int spi_nor_unlock(struct mtd_info *mtd, loff_t ofs, uint64_t len)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	uint32_t offset = ofs;
+	uint8_t status_old, status_new;
+	int ret = 0;
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_UNLOCK);
+	if (ret)
+		return ret;
+
+	status_old = read_sr(nor);
+
+	if (offset+len > mtd->size - (mtd->size / 64))
+		status_new = status_old & ~(SR_BP2 | SR_BP1 | SR_BP0);
+	else if (offset+len > mtd->size - (mtd->size / 32))
+		status_new = (status_old & ~(SR_BP2 | SR_BP1)) | SR_BP0;
+	else if (offset+len > mtd->size - (mtd->size / 16))
+		status_new = (status_old & ~(SR_BP2 | SR_BP0)) | SR_BP1;
+	else if (offset+len > mtd->size - (mtd->size / 8))
+		status_new = (status_old & ~SR_BP2) | SR_BP1 | SR_BP0;
+	else if (offset+len > mtd->size - (mtd->size / 4))
+		status_new = (status_old & ~(SR_BP0 | SR_BP1)) | SR_BP2;
+	else if (offset+len > mtd->size - (mtd->size / 2))
+		status_new = (status_old & ~SR_BP1) | SR_BP2 | SR_BP0;
+	else
+		status_new = (status_old & ~SR_BP0) | SR_BP2 | SR_BP1;
+
+	/* Only modify protection if it will not lock other areas */
+	if ((status_new & (SR_BP2 | SR_BP1 | SR_BP0)) <
+				(status_old & (SR_BP2 | SR_BP1 | SR_BP0))) {
+		write_enable(nor);
+		ret = write_sr(nor, status_new);
+		if (ret)
+			goto err;
+	}
+
+err:
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_UNLOCK);
+	return ret;
+}
+
+/* Used when the "_ext_id" is two bytes at most */
+#define INFO(_jedec_id, _ext_id, _sector_size, _n_sectors, _flags)	\
+	((kernel_ulong_t)&(struct flash_info) {				\
+		.id = {							\
+			((_jedec_id) >> 16) & 0xff,			\
+			((_jedec_id) >> 8) & 0xff,			\
+			(_jedec_id) & 0xff,				\
+			((_ext_id) >> 8) & 0xff,			\
+			(_ext_id) & 0xff,				\
+			},						\
+		.id_len = (!(_jedec_id) ? 0 : (3 + ((_ext_id) ? 2 : 0))),	\
+		.sector_size = (_sector_size),				\
+		.n_sectors = (_n_sectors),				\
+		.page_size = 256,					\
+		.flags = (_flags),					\
+	})
+
+#define INFO6(_jedec_id, _ext_id, _sector_size, _n_sectors, _flags)	\
+	((kernel_ulong_t)&(struct flash_info) {				\
+		.id = {							\
+			((_jedec_id) >> 16) & 0xff,			\
+			((_jedec_id) >> 8) & 0xff,			\
+			(_jedec_id) & 0xff,				\
+			((_ext_id) >> 16) & 0xff,			\
+			((_ext_id) >> 8) & 0xff,			\
+			(_ext_id) & 0xff,				\
+			},						\
+		.id_len = 6,						\
+		.sector_size = (_sector_size),				\
+		.n_sectors = (_n_sectors),				\
+		.page_size = 256,					\
+		.flags = (_flags),					\
+	})
+
+#define CAT25_INFO(_sector_size, _n_sectors, _page_size, _addr_width, _flags)	\
+	((kernel_ulong_t)&(struct flash_info) {				\
+		.sector_size = (_sector_size),				\
+		.n_sectors = (_n_sectors),				\
+		.page_size = (_page_size),				\
+		.addr_width = (_addr_width),				\
+		.flags = (_flags),					\
+	})
+
+/* NOTE: double check command sets and memory organization when you add
+ * more nor chips.  This current list focusses on newer chips, which
+ * have been converging on command sets which including JEDEC ID.
+ */
+static const struct spi_device_id spi_nor_ids[] = {
+	/* Atmel -- some are (confusingly) marketed as "DataFlash" */
+	{ "at25fs010",  INFO(0x1f6601, 0, 32 * 1024,   4, SECT_4K) },
+	{ "at25fs040",  INFO(0x1f6604, 0, 64 * 1024,   8, SECT_4K) },
+
+	{ "at25df041a", INFO(0x1f4401, 0, 64 * 1024,   8, SECT_4K) },
+	{ "at25df321a", INFO(0x1f4701, 0, 64 * 1024,  64, SECT_4K) },
+	{ "at25df641",  INFO(0x1f4800, 0, 64 * 1024, 128, SECT_4K) },
+
+	{ "at26f004",   INFO(0x1f0400, 0, 64 * 1024,  8, SECT_4K) },
+	{ "at26df081a", INFO(0x1f4501, 0, 64 * 1024, 16, SECT_4K) },
+	{ "at26df161a", INFO(0x1f4601, 0, 64 * 1024, 32, SECT_4K) },
+	{ "at26df321",  INFO(0x1f4700, 0, 64 * 1024, 64, SECT_4K) },
+
+	{ "at45db081d", INFO(0x1f2500, 0, 64 * 1024, 16, SECT_4K) },
+
+	/* EON -- en25xxx */
+	{ "en25f32",    INFO(0x1c3116, 0, 64 * 1024,   64, SECT_4K) },
+	{ "en25p32",    INFO(0x1c2016, 0, 64 * 1024,   64, 0) },
+	{ "en25q32b",   INFO(0x1c3016, 0, 64 * 1024,   64, 0) },
+	{ "en25p64",    INFO(0x1c2017, 0, 64 * 1024,  128, 0) },
+	{ "en25q64",    INFO(0x1c3017, 0, 64 * 1024,  128, SECT_4K) },
+	{ "en25qh128",  INFO(0x1c7018, 0, 64 * 1024,  256, 0) },
+	{ "en25qh256",  INFO(0x1c7019, 0, 64 * 1024,  512, 0) },
+
+	/* ESMT */
+	{ "f25l32pa", INFO(0x8c2016, 0, 64 * 1024, 64, SECT_4K) },
+
+	/* Everspin */
+	{ "mr25h256", CAT25_INFO( 32 * 1024, 1, 256, 2, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ "mr25h10",  CAT25_INFO(128 * 1024, 1, 256, 3, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+
+	/* Fujitsu */
+	{ "mb85rs1mt", INFO(0x047f27, 0, 128 * 1024, 1, SPI_NOR_NO_ERASE) },
+
+	/* GigaDevice */
+	{ "gd25q32", INFO(0xc84016, 0, 64 * 1024,  64, SECT_4K) },
+	{ "gd25q64", INFO(0xc84017, 0, 64 * 1024, 128, SECT_4K) },
+
+	/* Intel/Numonyx -- xxxs33b */
+	{ "160s33b",  INFO(0x898911, 0, 64 * 1024,  32, 0) },
+	{ "320s33b",  INFO(0x898912, 0, 64 * 1024,  64, 0) },
+	{ "640s33b",  INFO(0x898913, 0, 64 * 1024, 128, 0) },
+
+	/* Macronix */
+	{ "mx25l2005a",  INFO(0xc22012, 0, 64 * 1024,   4, SECT_4K) },
+	{ "mx25l4005a",  INFO(0xc22013, 0, 64 * 1024,   8, SECT_4K) },
+	{ "mx25l8005",   INFO(0xc22014, 0, 64 * 1024,  16, 0) },
+	{ "mx25l1606e",  INFO(0xc22015, 0, 64 * 1024,  32, SECT_4K) },
+	{ "mx25l3205d",  INFO(0xc22016, 0, 64 * 1024,  64, 0) },
+	{ "mx25l3255e",  INFO(0xc29e16, 0, 64 * 1024,  64, SECT_4K) },
+	{ "mx25l6405d",  INFO(0xc22017, 0, 64 * 1024, 128, 0) },
+	{ "mx25l12805d", INFO(0xc22018, 0, 64 * 1024, 256, 0) },
+	{ "mx25l12855e", INFO(0xc22618, 0, 64 * 1024, 256, 0) },
+	{ "mx25l25635e", INFO(0xc22019, 0, 64 * 1024, 512, 0) },
+	{ "mx25l25655e", INFO(0xc22619, 0, 64 * 1024, 512, 0) },
+	{ "mx66l51235l", INFO(0xc2201a, 0, 64 * 1024, 1024, SPI_NOR_QUAD_READ) },
+	{ "mx66l1g55g",  INFO(0xc2261b, 0, 64 * 1024, 2048, SPI_NOR_QUAD_READ) },
+
+	/* Micron */
+	{ "n25q032",	 INFO(0x20ba16, 0, 64 * 1024,   64, 0) },
+	{ "n25q064",     INFO(0x20ba17, 0, 64 * 1024,  128, 0) },
+	{ "n25q128a11",  INFO(0x20bb18, 0, 64 * 1024,  256, 0) },
+	{ "n25q128a13",  INFO(0x20ba18, 0, 64 * 1024,  256, 0) },
+	{ "n25q256a",    INFO(0x20ba19, 0, 64 * 1024,  512, SECT_4K) },
+	{ "n25q512a",    INFO(0x20bb20, 0, 64 * 1024, 1024, SECT_4K | USE_FSR) },
+	{ "n25q512ax3",  INFO(0x20ba20, 0, 64 * 1024, 1024, USE_FSR) },
+	{ "n25q00",      INFO(0x20ba21, 0, 64 * 1024, 2048, USE_FSR) },
+
+	/* PMC */
+	{ "pm25lv512",   INFO(0,        0, 32 * 1024,    2, SECT_4K_PMC) },
+	{ "pm25lv010",   INFO(0,        0, 32 * 1024,    4, SECT_4K_PMC) },
+	{ "pm25lq032",   INFO(0x7f9d46, 0, 64 * 1024,   64, SECT_4K) },
+
+	/* Spansion -- single (large) sector size only, at least
+	 * for the chips listed here (without boot sectors).
+	 */
+	{ "s25sl032p",  INFO(0x010215, 0x4d00,  64 * 1024,  64, SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+	{ "s25sl064p",  INFO(0x010216, 0x4d00,  64 * 1024, 128, 0) },
+	{ "s25fl256s0", INFO(0x010219, 0x4d00, 256 * 1024, 128, 0) },
+	{ "s25fl256s1", INFO(0x010219, 0x4d01,  64 * 1024, 512, SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+	{ "s25fl512s",  INFO(0x010220, 0x4d00, 256 * 1024, 256, SPI_NOR_DUAL_READ | SPI_NOR_QUAD_READ) },
+	{ "s70fl01gs",  INFO(0x010221, 0x4d00, 256 * 1024, 256, 0) },
+	{ "s25sl12800", INFO(0x012018, 0x0300, 256 * 1024,  64, 0) },
+	{ "s25sl12801", INFO(0x012018, 0x0301,  64 * 1024, 256, 0) },
+	{ "s25fl128s",	INFO6(0x012018, 0x4d0180, 64 * 1024, 256, SPI_NOR_QUAD_READ) },
+	{ "s25fl129p0", INFO(0x012018, 0x4d00, 256 * 1024,  64, 0) },
+	{ "s25fl129p1", INFO(0x012018, 0x4d01,  64 * 1024, 256, 0) },
+	{ "s25sl004a",  INFO(0x010212,      0,  64 * 1024,   8, 0) },
+	{ "s25sl008a",  INFO(0x010213,      0,  64 * 1024,  16, 0) },
+	{ "s25sl016a",  INFO(0x010214,      0,  64 * 1024,  32, 0) },
+	{ "s25sl032a",  INFO(0x010215,      0,  64 * 1024,  64, 0) },
+	{ "s25sl064a",  INFO(0x010216,      0,  64 * 1024, 128, 0) },
+	{ "s25fl008k",  INFO(0xef4014,      0,  64 * 1024,  16, SECT_4K) },
+	{ "s25fl016k",  INFO(0xef4015,      0,  64 * 1024,  32, SECT_4K) },
+	{ "s25fl064k",  INFO(0xef4017,      0,  64 * 1024, 128, SECT_4K) },
+	{ "s25fl132k",  INFO(0x014016,      0,  64 * 1024,  64, 0) },
+
+	/* SST -- large erase sizes are "overlays", "sectors" are 4K */
+	{ "sst25vf040b", INFO(0xbf258d, 0, 64 * 1024,  8, SECT_4K | SST_WRITE) },
+	{ "sst25vf080b", INFO(0xbf258e, 0, 64 * 1024, 16, SECT_4K | SST_WRITE) },
+	{ "sst25vf016b", INFO(0xbf2541, 0, 64 * 1024, 32, SECT_4K | SST_WRITE) },
+	{ "sst25vf032b", INFO(0xbf254a, 0, 64 * 1024, 64, SECT_4K | SST_WRITE) },
+	{ "sst25vf064c", INFO(0xbf254b, 0, 64 * 1024, 128, SECT_4K) },
+	{ "sst25wf512",  INFO(0xbf2501, 0, 64 * 1024,  1, SECT_4K | SST_WRITE) },
+	{ "sst25wf010",  INFO(0xbf2502, 0, 64 * 1024,  2, SECT_4K | SST_WRITE) },
+	{ "sst25wf020",  INFO(0xbf2503, 0, 64 * 1024,  4, SECT_4K | SST_WRITE) },
+	{ "sst25wf040",  INFO(0xbf2504, 0, 64 * 1024,  8, SECT_4K | SST_WRITE) },
+	{ "sst25wf080",  INFO(0xbf2505, 0, 64 * 1024, 16, SECT_4K | SST_WRITE) },
+
+	/* ST Microelectronics -- newer production may have feature updates */
+	{ "m25p05",  INFO(0x202010,  0,  32 * 1024,   2, 0) },
+	{ "m25p10",  INFO(0x202011,  0,  32 * 1024,   4, 0) },
+	{ "m25p20",  INFO(0x202012,  0,  64 * 1024,   4, 0) },
+	{ "m25p40",  INFO(0x202013,  0,  64 * 1024,   8, 0) },
+	{ "m25p80",  INFO(0x202014,  0,  64 * 1024,  16, 0) },
+	{ "m25p16",  INFO(0x202015,  0,  64 * 1024,  32, 0) },
+	{ "m25p32",  INFO(0x202016,  0,  64 * 1024,  64, 0) },
+	{ "m25p64",  INFO(0x202017,  0,  64 * 1024, 128, 0) },
+	{ "m25p128", INFO(0x202018,  0, 256 * 1024,  64, 0) },
+
+	{ "m25p05-nonjedec",  INFO(0, 0,  32 * 1024,   2, 0) },
+	{ "m25p10-nonjedec",  INFO(0, 0,  32 * 1024,   4, 0) },
+	{ "m25p20-nonjedec",  INFO(0, 0,  64 * 1024,   4, 0) },
+	{ "m25p40-nonjedec",  INFO(0, 0,  64 * 1024,   8, 0) },
+	{ "m25p80-nonjedec",  INFO(0, 0,  64 * 1024,  16, 0) },
+	{ "m25p16-nonjedec",  INFO(0, 0,  64 * 1024,  32, 0) },
+	{ "m25p32-nonjedec",  INFO(0, 0,  64 * 1024,  64, 0) },
+	{ "m25p64-nonjedec",  INFO(0, 0,  64 * 1024, 128, 0) },
+	{ "m25p128-nonjedec", INFO(0, 0, 256 * 1024,  64, 0) },
+
+	{ "m45pe10", INFO(0x204011,  0, 64 * 1024,    2, 0) },
+	{ "m45pe80", INFO(0x204014,  0, 64 * 1024,   16, 0) },
+	{ "m45pe16", INFO(0x204015,  0, 64 * 1024,   32, 0) },
+
+	{ "m25pe20", INFO(0x208012,  0, 64 * 1024,  4,       0) },
+	{ "m25pe80", INFO(0x208014,  0, 64 * 1024, 16,       0) },
+	{ "m25pe16", INFO(0x208015,  0, 64 * 1024, 32, SECT_4K) },
+
+	{ "m25px16",    INFO(0x207115,  0, 64 * 1024, 32, SECT_4K) },
+	{ "m25px32",    INFO(0x207116,  0, 64 * 1024, 64, SECT_4K) },
+	{ "m25px32-s0", INFO(0x207316,  0, 64 * 1024, 64, SECT_4K) },
+	{ "m25px32-s1", INFO(0x206316,  0, 64 * 1024, 64, SECT_4K) },
+	{ "m25px64",    INFO(0x207117,  0, 64 * 1024, 128, 0) },
+	{ "m25px80",    INFO(0x207114,  0, 64 * 1024, 16, 0) },
+
+	/* Winbond -- w25x "blocks" are 64K, "sectors" are 4KiB */
+	{ "w25x10", INFO(0xef3011, 0, 64 * 1024,  2,  SECT_4K) },
+	{ "w25x20", INFO(0xef3012, 0, 64 * 1024,  4,  SECT_4K) },
+	{ "w25x40", INFO(0xef3013, 0, 64 * 1024,  8,  SECT_4K) },
+	{ "w25x80", INFO(0xef3014, 0, 64 * 1024,  16, SECT_4K) },
+	{ "w25x16", INFO(0xef3015, 0, 64 * 1024,  32, SECT_4K) },
+	{ "w25x32", INFO(0xef3016, 0, 64 * 1024,  64, SECT_4K) },
+	{ "w25q32", INFO(0xef4016, 0, 64 * 1024,  64, SECT_4K) },
+	{ "w25q32dw", INFO(0xef6016, 0, 64 * 1024,  64, SECT_4K) },
+	{ "w25x64", INFO(0xef3017, 0, 64 * 1024, 128, SECT_4K) },
+	{ "w25q64", INFO(0xef4017, 0, 64 * 1024, 128, SECT_4K) },
+	{ "w25q80", INFO(0xef5014, 0, 64 * 1024,  16, SECT_4K) },
+	{ "w25q80bl", INFO(0xef4014, 0, 64 * 1024,  16, SECT_4K) },
+	{ "w25q128", INFO(0xef4018, 0, 64 * 1024, 256, SECT_4K) },
+	{ "w25q256", INFO(0xef4019, 0, 64 * 1024, 512, SECT_4K) },
+
+	/* Catalyst / On Semiconductor -- non-JEDEC */
+	{ "cat25c11", CAT25_INFO(  16, 8, 16, 1, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ "cat25c03", CAT25_INFO(  32, 8, 16, 2, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ "cat25c09", CAT25_INFO( 128, 8, 32, 2, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ "cat25c17", CAT25_INFO( 256, 8, 32, 2, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ "cat25128", CAT25_INFO(2048, 8, 64, 2, SPI_NOR_NO_ERASE | SPI_NOR_NO_FR) },
+	{ },
+};
+
+static const struct spi_device_id *spi_nor_read_id(struct spi_nor *nor)
+{
+	int			tmp;
+	u8			id[SPI_NOR_MAX_ID_LEN];
+	struct flash_info	*info;
+
+	tmp = nor->read_reg(nor, SPINOR_OP_RDID, id, SPI_NOR_MAX_ID_LEN);
+	if (tmp < 0) {
+		dev_dbg(nor->dev, " error %d reading JEDEC ID\n", tmp);
+		return ERR_PTR(tmp);
+	}
+
+	for (tmp = 0; tmp < ARRAY_SIZE(spi_nor_ids) - 1; tmp++) {
+		info = (void *)spi_nor_ids[tmp].driver_data;
+		if (info->id_len) {
+			if (!memcmp(info->id, id, info->id_len))
+				return &spi_nor_ids[tmp];
+		}
+	}
+	dev_err(nor->dev, "unrecognized JEDEC id bytes: %02x, %2x, %2x\n",
+		id[0], id[1], id[2]);
+	return ERR_PTR(-ENODEV);
+}
+
+static int spi_nor_read(struct mtd_info *mtd, loff_t from, size_t len,
+			size_t *retlen, u_char *buf)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	int ret;
+
+	dev_dbg(nor->dev, "from 0x%08x, len %zd\n", (u32)from, len);
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_READ);
+	if (ret)
+		return ret;
+
+	ret = nor->read(nor, from, len, retlen, buf);
+
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_READ);
+	return ret;
+}
+
+static int sst_write(struct mtd_info *mtd, loff_t to, size_t len,
+		size_t *retlen, const u_char *buf)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	size_t actual;
+	int ret;
+
+	dev_dbg(nor->dev, "to 0x%08x, len %zd\n", (u32)to, len);
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_WRITE);
+	if (ret)
+		return ret;
+
+	write_enable(nor);
+
+	nor->sst_write_second = false;
+
+	actual = to % 2;
+	/* Start write from odd address. */
+	if (actual) {
+		nor->program_opcode = SPINOR_OP_BP;
+
+		/* write one byte. */
+		nor->write(nor, to, 1, retlen, buf);
+		ret = spi_nor_wait_till_ready(nor);
+		if (ret)
+			goto time_out;
+	}
+	to += actual;
+
+	/* Write out most of the data here. */
+	for (; actual < len - 1; actual += 2) {
+		nor->program_opcode = SPINOR_OP_AAI_WP;
+
+		/* write two bytes. */
+		nor->write(nor, to, 2, retlen, buf + actual);
+		ret = spi_nor_wait_till_ready(nor);
+		if (ret)
+			goto time_out;
+		to += 2;
+		nor->sst_write_second = true;
+	}
+	nor->sst_write_second = false;
+
+	write_disable(nor);
+	ret = spi_nor_wait_till_ready(nor);
+	if (ret)
+		goto time_out;
+
+	/* Write out trailing byte if it exists. */
+	if (actual != len) {
+		write_enable(nor);
+
+		nor->program_opcode = SPINOR_OP_BP;
+		nor->write(nor, to, 1, retlen, buf + actual);
+
+		ret = spi_nor_wait_till_ready(nor);
+		if (ret)
+			goto time_out;
+		write_disable(nor);
+	}
+time_out:
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_WRITE);
+	return ret;
+}
+
+/*
+ * Write an address range to the nor chip.  Data must be written in
+ * FLASH_PAGESIZE chunks.  The address range may be any size provided
+ * it is within the physical boundaries.
+ */
+static int spi_nor_write(struct mtd_info *mtd, loff_t to, size_t len,
+	size_t *retlen, const u_char *buf)
+{
+	struct spi_nor *nor = mtd_to_spi_nor(mtd);
+	u32 page_offset, page_size, i;
+	int ret;
+
+	dev_dbg(nor->dev, "to 0x%08x, len %zd\n", (u32)to, len);
+
+	ret = spi_nor_lock_and_prep(nor, SPI_NOR_OPS_WRITE);
+	if (ret)
+		return ret;
+
+	write_enable(nor);
+
+	page_offset = to & (nor->page_size - 1);
+
+	/* do all the bytes fit onto one page? */
+	if (page_offset + len <= nor->page_size) {
+		nor->write(nor, to, len, retlen, buf);
+	} else {
+		/* the size of data remaining on the first page */
+		page_size = nor->page_size - page_offset;
+		nor->write(nor, to, page_size, retlen, buf);
+
+		/* write everything in nor->page_size chunks */
+		for (i = page_size; i < len; i += page_size) {
+			page_size = len - i;
+			if (page_size > nor->page_size)
+				page_size = nor->page_size;
+
+			ret = spi_nor_wait_till_ready(nor);
+			if (ret)
+				goto write_err;
+
+			write_enable(nor);
+
+			nor->write(nor, to + i, page_size, retlen, buf + i);
+		}
+	}
+
+	ret = spi_nor_wait_till_ready(nor);
+write_err:
+	spi_nor_unlock_and_unprep(nor, SPI_NOR_OPS_WRITE);
+	return ret;
+}
+
+static int macronix_quad_enable(struct spi_nor *nor)
+{
+	int ret, val;
+
+	val = read_sr(nor);
+	write_enable(nor);
+
+	nor->cmd_buf[0] = val | SR_QUAD_EN_MX;
+	nor->write_reg(nor, SPINOR_OP_WRSR, nor->cmd_buf, 1, 0);
+
+	if (spi_nor_wait_till_ready(nor))
+		return 1;
+
+	ret = read_sr(nor);
+	if (!(ret > 0 && (ret & SR_QUAD_EN_MX))) {
+		dev_err(nor->dev, "Macronix Quad bit not set\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+/*
+ * Write status Register and configuration register with 2 bytes
+ * The first byte will be written to the status register, while the
+ * second byte will be written to the configuration register.
+ * Return negative if error occured.
+ */
+static int write_sr_cr(struct spi_nor *nor, u16 val)
+{
+	nor->cmd_buf[0] = val & 0xff;
+	nor->cmd_buf[1] = (val >> 8);
+
+	return nor->write_reg(nor, SPINOR_OP_WRSR, nor->cmd_buf, 2, 0);
+}
+
+static int spansion_quad_enable(struct spi_nor *nor)
+{
+	int ret;
+	int quad_en = CR_QUAD_EN_SPAN << 8;
+
+	write_enable(nor);
+
+	ret = write_sr_cr(nor, quad_en);
+	if (ret < 0) {
+		dev_err(nor->dev,
+			"error while writing configuration register\n");
+		return -EINVAL;
+	}
+
+	/* read back and check it */
+	ret = read_cr(nor);
+	if (!(ret > 0 && (ret & CR_QUAD_EN_SPAN))) {
+		dev_err(nor->dev, "Spansion Quad bit not set\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+static int set_quad_mode(struct spi_nor *nor, struct flash_info *info)
+{
+	int status;
+
+	switch (JEDEC_MFR(info)) {
+	case CFI_MFR_MACRONIX:
+		status = macronix_quad_enable(nor);
+		if (status) {
+			dev_err(nor->dev, "Macronix quad-read not enabled\n");
+			return -EINVAL;
+		}
+		return status;
+	default:
+		status = spansion_quad_enable(nor);
+		if (status) {
+			dev_err(nor->dev, "Spansion quad-read not enabled\n");
+			return -EINVAL;
+		}
+		return status;
+	}
+}
+
+static int spi_nor_check(struct spi_nor *nor)
+{
+	if (!nor->dev || !nor->read || !nor->write ||
+		!nor->read_reg || !nor->write_reg || !nor->erase) {
+		pr_err("spi-nor: please fill all the necessary fields!\n");
+		return -EINVAL;
+	}
+
+	return 0;
+}
+
+int spi_nor_scan(struct spi_nor *nor, const char *name, enum read_mode mode)
+{
+	const struct spi_device_id	*id = NULL;
+	struct flash_info		*info;
+	struct device *dev = nor->dev;
+	struct mtd_info *mtd = nor->mtd;
+	struct device_node *np = dev->of_node;
+	int ret;
+	int i;
+
+	ret = spi_nor_check(nor);
+	if (ret)
+		return ret;
+
+	/* Try to auto-detect if chip name wasn't specified */
+	if (!name)
+		id = spi_nor_read_id(nor);
+	else
+		id = spi_nor_match_id(name);
+	if (IS_ERR_OR_NULL(id))
+		return -ENOENT;
+
+	info = (void *)id->driver_data;
+
+	/*
+	 * If caller has specified name of flash model that can normally be
+	 * detected using JEDEC, let's verify it.
+	 */
+	if (name && info->id_len) {
+		const struct spi_device_id *jid;
+
+		jid = spi_nor_read_id(nor);
+		if (IS_ERR(jid)) {
+			return PTR_ERR(jid);
+		} else if (jid != id) {
+			/*
+			 * JEDEC knows better, so overwrite platform ID. We
+			 * can't trust partitions any longer, but we'll let
+			 * mtd apply them anyway, since some partitions may be
+			 * marked read-only, and we don't want to lose that
+			 * information, even if it's not 100% accurate.
+			 */
+			dev_warn(dev, "found %s, expected %s\n",
+				 jid->name, id->name);
+			id = jid;
+			info = (void *)jid->driver_data;
+		}
+	}
+
+	mutex_init(&nor->lock);
+
+	/*
+	 * Atmel, SST and Intel/Numonyx serial nor tend to power
+	 * up with the software protection bits set
+	 */
+
+	if (JEDEC_MFR(info) == CFI_MFR_ATMEL ||
+	    JEDEC_MFR(info) == CFI_MFR_INTEL ||
+	    JEDEC_MFR(info) == CFI_MFR_SST) {
+		write_enable(nor);
+		write_sr(nor, 0);
+	}
+
+	if (!mtd->name)
+		mtd->name = dev_name(dev);
+	mtd->type = MTD_NORFLASH;
+	mtd->writesize = 1;
+	mtd->flags = MTD_CAP_NORFLASH;
+	mtd->size = info->sector_size * info->n_sectors;
+	mtd->_erase = spi_nor_erase;
+	mtd->_read = spi_nor_read;
+
+	/* nor protection support for STmicro chips */
+	if (JEDEC_MFR(info) == CFI_MFR_ST) {
+		mtd->_lock = spi_nor_lock;
+		mtd->_unlock = spi_nor_unlock;
+	}
+
+	/* sst nor chips use AAI word program */
+	if (info->flags & SST_WRITE)
+		mtd->_write = sst_write;
+	else
+		mtd->_write = spi_nor_write;
+
+	if (info->flags & USE_FSR)
+		nor->flags |= SNOR_F_USE_FSR;
+
+#ifdef CONFIG_MTD_SPI_NOR_USE_4K_SECTORS
+	/* prefer "small sector" erase if possible */
+	if (info->flags & SECT_4K) {
+		nor->erase_opcode = SPINOR_OP_BE_4K;
+		mtd->erasesize = 4096;
+	} else if (info->flags & SECT_4K_PMC) {
+		nor->erase_opcode = SPINOR_OP_BE_4K_PMC;
+		mtd->erasesize = 4096;
+	} else
+#endif
+	{
+		nor->erase_opcode = SPINOR_OP_SE;
+		mtd->erasesize = info->sector_size;
+	}
+
+	if (info->flags & SPI_NOR_NO_ERASE)
+		mtd->flags |= MTD_NO_ERASE;
+
+	mtd->dev.parent = dev;
+	nor->page_size = info->page_size;
+	mtd->writebufsize = nor->page_size;
+
+	if (np) {
+		/* If we were instantiated by DT, use it */
+		if (of_property_read_bool(np, "m25p,fast-read"))
+			nor->flash_read = SPI_NOR_FAST;
+		else
+			nor->flash_read = SPI_NOR_NORMAL;
+	} else {
+		/* If we weren't instantiated by DT, default to fast-read */
+		nor->flash_read = SPI_NOR_FAST;
+	}
+
+	/* Some devices cannot do fast-read, no matter what DT tells us */
+	if (info->flags & SPI_NOR_NO_FR)
+		nor->flash_read = SPI_NOR_NORMAL;
+
+	/* Quad/Dual-read mode takes precedence over fast/normal */
+	if (mode == SPI_NOR_QUAD && info->flags & SPI_NOR_QUAD_READ) {
+		ret = set_quad_mode(nor, info);
+		if (ret) {
+			dev_err(dev, "quad mode not supported\n");
+			return ret;
+		}
+		nor->flash_read = SPI_NOR_QUAD;
+	} else if (mode == SPI_NOR_DUAL && info->flags & SPI_NOR_DUAL_READ) {
+		nor->flash_read = SPI_NOR_DUAL;
+	}
+
+	/* Default commands */
+	switch (nor->flash_read) {
+	case SPI_NOR_QUAD:
+		nor->read_opcode = SPINOR_OP_READ_1_1_4;
+		break;
+	case SPI_NOR_DUAL:
+		nor->read_opcode = SPINOR_OP_READ_1_1_2;
+		break;
+	case SPI_NOR_FAST:
+		nor->read_opcode = SPINOR_OP_READ_FAST;
+		break;
+	case SPI_NOR_NORMAL:
+		nor->read_opcode = SPINOR_OP_READ;
+		break;
+	default:
+		dev_err(dev, "No Read opcode defined\n");
+		return -EINVAL;
+	}
+
+	nor->program_opcode = SPINOR_OP_PP;
+
+	if (info->addr_width)
+		nor->addr_width = info->addr_width;
+	else if (mtd->size > 0x1000000) {
+		/* enable 4-byte addressing if the device exceeds 16MiB */
+		nor->addr_width = 4;
+		if (JEDEC_MFR(info) == CFI_MFR_AMD) {
+			/* Dedicated 4-byte command set */
+			switch (nor->flash_read) {
+			case SPI_NOR_QUAD:
+				nor->read_opcode = SPINOR_OP_READ4_1_1_4;
+				break;
+			case SPI_NOR_DUAL:
+				nor->read_opcode = SPINOR_OP_READ4_1_1_2;
+				break;
+			case SPI_NOR_FAST:
+				nor->read_opcode = SPINOR_OP_READ4_FAST;
+				break;
+			case SPI_NOR_NORMAL:
+				nor->read_opcode = SPINOR_OP_READ4;
+				break;
+			}
+			nor->program_opcode = SPINOR_OP_PP_4B;
+			/* No small sector erase for 4-byte command set */
+			nor->erase_opcode = SPINOR_OP_SE_4B;
+			mtd->erasesize = info->sector_size;
+		} else
+			set_4byte(nor, info, 1);
+	} else {
+		nor->addr_width = 3;
+	}
+
+	nor->read_dummy = spi_nor_read_dummy_cycles(nor);
+
+	dev_info(dev, "%s (%lld Kbytes)\n", id->name,
+			(long long)mtd->size >> 10);
+
+	dev_dbg(dev,
+		"mtd .name = %s, .size = 0x%llx (%lldMiB), "
+		".erasesize = 0x%.8x (%uKiB) .numeraseregions = %d\n",
+		mtd->name, (long long)mtd->size, (long long)(mtd->size >> 20),
+		mtd->erasesize, mtd->erasesize / 1024, mtd->numeraseregions);
+
+	if (mtd->numeraseregions)
+		for (i = 0; i < mtd->numeraseregions; i++)
+			dev_dbg(dev,
+				"mtd.eraseregions[%d] = { .offset = 0x%llx, "
+				".erasesize = 0x%.8x (%uKiB), "
+				".numblocks = %d }\n",
+				i, (long long)mtd->eraseregions[i].offset,
+				mtd->eraseregions[i].erasesize,
+				mtd->eraseregions[i].erasesize / 1024,
+				mtd->eraseregions[i].numblocks);
+	return 0;
+}
+EXPORT_SYMBOL_GPL(spi_nor_scan);
+
+static const struct spi_device_id *spi_nor_match_id(const char *name)
+{
+	const struct spi_device_id *id = spi_nor_ids;
+
+	while (id->name[0]) {
+		if (!strcmp(name, id->name))
+			return id;
+		id++;
+	}
+	return NULL;
+}
+
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("Huang Shijie <shijie8@gmail.com>");
+MODULE_AUTHOR("Mike Lavender");
+MODULE_DESCRIPTION("framework for SPI NOR");
diff --git a/drivers/net/phy/phy.c b/drivers/net/phy/phy.c
index 663d2d0..8b13727 100644
--- a/drivers/net/phy/phy.c
+++ b/drivers/net/phy/phy.c
@@ -1157,3 +1157,84 @@ void phy_ethtool_get_wol(struct phy_device *phydev, struct ethtool_wolinfo *wol)
 		phydev->drv->get_wol(phydev, wol);
 }
 EXPORT_SYMBOL(phy_ethtool_get_wol);
+
+/**
+ * phy_init_eee_private - init and check the EEE feature
+ * @bus: the target MII bus
+ * @phy_addr: PHY address on the MII bus
+ * @speed: PHY speed
+ * @duplex: PHY duplex
+ * @clk_stop_enable: PHY may stop the clock during LPI
+ *
+ * Description: it checks if the Energy-Efficient Ethernet (EEE)
+ * is supported by looking at the MMD registers 3.20 and 7.60/61
+ * and it programs the MMD register 3.0 setting the "Clock stop enable"
+ * bit if required.
+ */
+int phy_init_eee_private(struct mii_bus *bus, unsigned int phy_addr, int speed,
+		      int duplex, bool clk_stop_enable)
+{
+	int ret = -EPROTONOSUPPORT;
+
+	/* According to 802.3az,the EEE is supported only in full duplex-mode.
+	 * Also EEE feature is active when core is operating with MII, GMII
+	 * or RGMII.
+	 */
+	if (duplex == DUPLEX_FULL) {
+		int eee_lp, eee_cap, eee_adv;
+		u32 lp, cap, adv;
+		int idx;
+
+		/* Check if the EEE ability is supported */
+		eee_cap = phy_read_mmd_indirect(bus, MDIO_PCS_EEE_ABLE,
+						MDIO_MMD_PCS, phy_addr);
+		if (eee_cap < 0)
+			return eee_cap;
+
+		cap = mmd_eee_cap_to_ethtool_sup_t(eee_cap);
+		if (!cap)
+			goto eee_exit;
+
+		/* Check which link settings negotiated and verify it in
+		 * the EEE advertising registers.
+		 */
+		eee_lp = phy_read_mmd_indirect(bus, MDIO_AN_EEE_LPABLE,
+					       MDIO_MMD_AN, phy_addr);
+		if (eee_lp < 0)
+			return eee_lp;
+
+		eee_adv = phy_read_mmd_indirect(bus, MDIO_AN_EEE_ADV,
+						MDIO_MMD_AN, phy_addr);
+		if (eee_adv < 0)
+			return eee_adv;
+
+		adv = mmd_eee_adv_to_ethtool_adv_t(eee_adv);
+		lp = mmd_eee_adv_to_ethtool_adv_t(eee_lp);
+		idx = phy_find_setting(speed, duplex);
+
+		if (!(lp & adv & settings[idx].setting))
+			goto eee_exit;
+
+		if (clk_stop_enable) {
+			/* Configure the PHY to stop receiving xMII
+			 * clock while it is signaling LPI.
+			 */
+			int val = phy_read_mmd_indirect(bus, MDIO_CTRL1,
+							MDIO_MMD_PCS,
+							phy_addr);
+			if (val < 0)
+				return val;
+
+			val |= MDIO_PCS_CTRL1_CLKSTOP_EN;
+			phy_write_mmd_indirect(bus, MDIO_CTRL1,
+					       MDIO_MMD_PCS, phy_addr, val);
+		}
+
+		ret = 0; /* EEE supported */
+	}
+
+eee_exit:
+	return ret;
+}
+EXPORT_SYMBOL(phy_init_eee_private);
+
diff --git a/drivers/of/Kconfig b/drivers/of/Kconfig
index 78cc760..30caafa 100644
--- a/drivers/of/Kconfig
+++ b/drivers/of/Kconfig
@@ -27,6 +27,7 @@ config OF_SELFTEST
 config OF_FLATTREE
 	bool
 	select DTC
+	select LIBFDT
 
 config OF_EARLY_FLATTREE
 	bool
@@ -74,4 +75,10 @@ config OF_MTD
 	depends on MTD
 	def_bool y
 
+config OF_RESERVED_MEM
+	depends on OF_EARLY_FLATTREE
+	bool
+	help
+	  Helpers to allow for reservation of memory regions
+
 endmenu # OF
diff --git a/drivers/of/Makefile b/drivers/of/Makefile
index efd0510..9891232 100644
--- a/drivers/of/Makefile
+++ b/drivers/of/Makefile
@@ -9,3 +9,6 @@ obj-$(CONFIG_OF_MDIO)	+= of_mdio.o
 obj-$(CONFIG_OF_PCI)	+= of_pci.o
 obj-$(CONFIG_OF_PCI_IRQ)  += of_pci_irq.o
 obj-$(CONFIG_OF_MTD)	+= of_mtd.o
+obj-$(CONFIG_OF_RESERVED_MEM) += of_reserved_mem.o
+
+CFLAGS_fdt.o = -I$(src)/../../scripts/dtc/libfdt
diff --git a/drivers/of/address.c b/drivers/of/address.c
index 8fb2b57..f5582f3 100644
--- a/drivers/of/address.c
+++ b/drivers/of/address.c
@@ -224,6 +224,73 @@ int of_pci_address_to_resource(struct device_node *dev, int bar,
 	return __of_address_to_resource(dev, addrp, size, flags, NULL, r);
 }
 EXPORT_SYMBOL_GPL(of_pci_address_to_resource);
+
+int of_pci_range_parser_init(struct of_pci_range_parser *parser,
+				struct device_node *node)
+{
+	const int na = 3, ns = 2;
+	int rlen;
+
+	parser->node = node;
+	parser->pna = of_n_addr_cells(node);
+	parser->np = parser->pna + na + ns;
+
+	parser->range = of_get_property(node, "ranges", &rlen);
+	if (parser->range == NULL)
+		return -ENOENT;
+
+	parser->end = parser->range + rlen / sizeof(__be32);
+
+	return 0;
+}
+EXPORT_SYMBOL_GPL(of_pci_range_parser_init);
+
+struct of_pci_range *of_pci_range_parser_one(struct of_pci_range_parser *parser,
+						struct of_pci_range *range)
+{
+	const int na = 3, ns = 2;
+
+	if (!range)
+		return NULL;
+
+	if (!parser->range || parser->range + parser->np > parser->end)
+		return NULL;
+
+	range->pci_space = parser->range[0];
+	range->flags = of_bus_pci_get_flags(parser->range);
+	range->pci_addr = of_read_number(parser->range + 1, ns);
+	range->cpu_addr = of_translate_address(parser->node,
+				parser->range + na);
+	range->size = of_read_number(parser->range + parser->pna + na, ns);
+
+	parser->range += parser->np;
+
+	/* Now consume following elements while they are contiguous */
+	while (parser->range + parser->np <= parser->end) {
+		u32 flags, pci_space;
+		u64 pci_addr, cpu_addr, size;
+
+		pci_space = be32_to_cpup(parser->range);
+		flags = of_bus_pci_get_flags(parser->range);
+		pci_addr = of_read_number(parser->range + 1, ns);
+		cpu_addr = of_translate_address(parser->node,
+				parser->range + na);
+		size = of_read_number(parser->range + parser->pna + na, ns);
+
+		if (flags != range->flags)
+			break;
+		if (pci_addr != range->pci_addr + range->size ||
+		    cpu_addr != range->cpu_addr + range->size)
+			break;
+
+		range->size += size;
+		parser->range += parser->np;
+	}
+
+	return range;
+}
+EXPORT_SYMBOL_GPL(of_pci_range_parser_one);
+
 #endif /* CONFIG_PCI */
 
 /*
diff --git a/drivers/of/fdt.c b/drivers/of/fdt.c
index 1187737..02ab384 100644
--- a/drivers/of/fdt.c
+++ b/drivers/of/fdt.c
@@ -14,9 +14,13 @@
 #include <linux/module.h>
 #include <linux/of.h>
 #include <linux/of_fdt.h>
+#include <linux/of_reserved_mem.h>
+#include <linux/sizes.h>
 #include <linux/string.h>
 #include <linux/errno.h>
 #include <linux/slab.h>
+#include <linux/memblock.h>
+#include <linux/libfdt.h>
 
 #include <asm/setup.h>  /* for COMMAND_LINE_SIZE */
 #ifdef CONFIG_PPC
@@ -25,54 +29,6 @@
 
 #include <asm/page.h>
 
-char *of_fdt_get_string(struct boot_param_header *blob, u32 offset)
-{
-	return ((char *)blob) +
-		be32_to_cpu(blob->off_dt_strings) + offset;
-}
-
-/**
- * of_fdt_get_property - Given a node in the given flat blob, return
- * the property ptr
- */
-void *of_fdt_get_property(struct boot_param_header *blob,
-		       unsigned long node, const char *name,
-		       unsigned long *size)
-{
-	unsigned long p = node;
-
-	do {
-		u32 tag = be32_to_cpup((__be32 *)p);
-		u32 sz, noff;
-		const char *nstr;
-
-		p += 4;
-		if (tag == OF_DT_NOP)
-			continue;
-		if (tag != OF_DT_PROP)
-			return NULL;
-
-		sz = be32_to_cpup((__be32 *)p);
-		noff = be32_to_cpup((__be32 *)(p + 4));
-		p += 8;
-		if (be32_to_cpu(blob->version) < 0x10)
-			p = ALIGN(p, sz >= 8 ? 8 : 4);
-
-		nstr = of_fdt_get_string(blob, noff);
-		if (nstr == NULL) {
-			pr_warning("Can't find property index name !\n");
-			return NULL;
-		}
-		if (strcmp(name, nstr) == 0) {
-			if (size)
-				*size = sz;
-			return (void *)p;
-		}
-		p += sz;
-		p = ALIGN(p, 4);
-	} while (1);
-}
-
 /**
  * of_fdt_is_compatible - Return true if given node from the given blob has
  * compat in its compatible list
@@ -87,9 +43,10 @@ int of_fdt_is_compatible(struct boot_param_header *blob,
 		      unsigned long node, const char *compat)
 {
 	const char *cp;
-	unsigned long cplen, l, score = 0;
+	int cplen;
+	unsigned long l, score = 0;
 
-	cp = of_fdt_get_property(blob, node, "compatible", &cplen);
+	cp = fdt_getprop(blob, node, "compatible", &cplen);
 	if (cp == NULL)
 		return 0;
 	while (cplen > 0) {
@@ -125,12 +82,12 @@ int of_fdt_match(struct boot_param_header *blob, unsigned long node,
 	return score;
 }
 
-static void *unflatten_dt_alloc(unsigned long *mem, unsigned long size,
+static void *unflatten_dt_alloc(void **mem, unsigned long size,
 				       unsigned long align)
 {
 	void *res;
 
-	*mem = ALIGN(*mem, align);
+	*mem = PTR_ALIGN(*mem, align);
 	res = (void *)*mem;
 	*mem += size;
 
@@ -146,30 +103,29 @@ static void *unflatten_dt_alloc(unsigned long *mem, unsigned long size,
  * @allnextpp: pointer to ->allnext from last allocated device_node
  * @fpsize: Size of the node path up at the current depth.
  */
-static unsigned long unflatten_dt_node(struct boot_param_header *blob,
-				unsigned long mem,
-				unsigned long *p,
+static void * unflatten_dt_node(struct boot_param_header *blob,
+				void *mem,
+				int *poffset,
 				struct device_node *dad,
 				struct device_node ***allnextpp,
 				unsigned long fpsize)
 {
+	const __be32 *p;
 	struct device_node *np;
 	struct property *pp, **prev_pp = NULL;
-	char *pathp;
-	u32 tag;
+	const char *pathp;
 	unsigned int l, allocl;
+	static int depth = 0;
+	int old_depth;
+	int offset;
 	int has_name = 0;
 	int new_format = 0;
 
-	tag = be32_to_cpup((__be32 *)(*p));
-	if (tag != OF_DT_BEGIN_NODE) {
-		pr_err("Weird tag at start of node: %x\n", tag);
+	pathp = fdt_get_name(blob, *poffset, &l);
+	if (!pathp)
 		return mem;
-	}
-	*p += 4;
-	pathp = (char *)*p;
-	l = allocl = strlen(pathp) + 1;
-	*p = ALIGN(*p + l, 4);
+
+	allocl = l++;
 
 	/* version 0x10 has a more compact unit name here instead of the full
 	 * path. we accumulate the full path size using "fpsize", we'll rebuild
@@ -187,7 +143,7 @@ static unsigned long unflatten_dt_node(struct boot_param_header *blob,
 			fpsize = 1;
 			allocl = 2;
 			l = 1;
-			*pathp = '\0';
+			pathp = "";
 		} else {
 			/* account for '/' and path size minus terminal 0
 			 * already in 'l'
@@ -235,32 +191,23 @@ static unsigned long unflatten_dt_node(struct boot_param_header *blob,
 		kref_init(&np->kref);
 	}
 	/* process properties */
-	while (1) {
-		u32 sz, noff;
-		char *pname;
-
-		tag = be32_to_cpup((__be32 *)(*p));
-		if (tag == OF_DT_NOP) {
-			*p += 4;
-			continue;
-		}
-		if (tag != OF_DT_PROP)
+	for (offset = fdt_first_property_offset(blob, *poffset);
+	     (offset >= 0);
+	     (offset = fdt_next_property_offset(blob, offset))) {
+		const char *pname;
+		u32 sz;
+
+		if (!(p = fdt_getprop_by_offset(blob, offset, &pname, &sz))) {
+			offset = -FDT_ERR_INTERNAL;
 			break;
-		*p += 4;
-		sz = be32_to_cpup((__be32 *)(*p));
-		noff = be32_to_cpup((__be32 *)((*p) + 4));
-		*p += 8;
-		if (be32_to_cpu(blob->version) < 0x10)
-			*p = ALIGN(*p, sz >= 8 ? 8 : 4);
-
-		pname = of_fdt_get_string(blob, noff);
+		}
+
 		if (pname == NULL) {
 			pr_info("Can't find property name in list !\n");
 			break;
 		}
 		if (strcmp(pname, "name") == 0)
 			has_name = 1;
-		l = strlen(pname) + 1;
 		pp = unflatten_dt_alloc(&mem, sizeof(struct property),
 					__alignof__(struct property));
 		if (allnextpp) {
@@ -272,26 +219,25 @@ static unsigned long unflatten_dt_node(struct boot_param_header *blob,
 			if ((strcmp(pname, "phandle") == 0) ||
 			    (strcmp(pname, "linux,phandle") == 0)) {
 				if (np->phandle == 0)
-					np->phandle = be32_to_cpup((__be32*)*p);
+					np->phandle = be32_to_cpup(p);
 			}
 			/* And we process the "ibm,phandle" property
 			 * used in pSeries dynamic device tree
 			 * stuff */
 			if (strcmp(pname, "ibm,phandle") == 0)
-				np->phandle = be32_to_cpup((__be32 *)*p);
-			pp->name = pname;
+				np->phandle = be32_to_cpup(p);
+			pp->name = (char *)pname;
 			pp->length = sz;
-			pp->value = (void *)*p;
+			pp->value = (__be32 *)p;
 			*prev_pp = pp;
 			prev_pp = &pp->next;
 		}
-		*p = ALIGN((*p) + sz, 4);
 	}
 	/* with version 0x10 we may not have the name property, recreate
 	 * it here from the unit name if absent
 	 */
 	if (!has_name) {
-		char *p1 = pathp, *ps = pathp, *pa = NULL;
+		const char *p1 = pathp, *ps = pathp, *pa = NULL;
 		int sz;
 
 		while (*p1) {
@@ -328,19 +274,18 @@ static unsigned long unflatten_dt_node(struct boot_param_header *blob,
 		if (!np->type)
 			np->type = "<NULL>";
 	}
-	while (tag == OF_DT_BEGIN_NODE || tag == OF_DT_NOP) {
-		if (tag == OF_DT_NOP)
-			*p += 4;
-		else
-			mem = unflatten_dt_node(blob, mem, p, np, allnextpp,
+
+	old_depth = depth;
+	*poffset = fdt_next_node(blob, *poffset, &depth);
+	if (depth < 0)
+		depth = 0;
+	while (*poffset > 0 && depth > old_depth)
+		mem = unflatten_dt_node(blob, mem, poffset, np, allnextpp,
 						fpsize);
-		tag = be32_to_cpup((__be32 *)(*p));
-	}
-	if (tag != OF_DT_END_NODE) {
-		pr_err("Weird tag at end of node: %x\n", tag);
-		return mem;
-	}
-	*p += 4;
+
+	if (*poffset < 0 && *poffset != -FDT_ERR_NOTFOUND)
+		pr_err("unflatten: error %d processing FDT\n", *poffset);
+
 	return mem;
 }
 
@@ -360,7 +305,9 @@ static void __unflatten_device_tree(struct boot_param_header *blob,
 			     struct device_node **mynodes,
 			     void * (*dt_alloc)(u64 size, u64 align))
 {
-	unsigned long start, mem, size;
+	unsigned long size;
+	int start;
+	void *mem;
 	struct device_node **allnextp = mynodes;
 
 	pr_debug(" -> unflatten_device_tree()\n");
@@ -381,30 +328,25 @@ static void __unflatten_device_tree(struct boot_param_header *blob,
 	}
 
 	/* First pass, scan for size */
-	start = ((unsigned long)blob) +
-		be32_to_cpu(blob->off_dt_struct);
-	size = unflatten_dt_node(blob, 0, &start, NULL, NULL, 0);
-	size = (size | 3) + 1;
+	start = 0;
+	size = (unsigned long)unflatten_dt_node(blob, 0, &start, NULL, NULL, 0);
+	size = ALIGN(size, 4);
 
 	pr_debug("  size is %lx, allocating...\n", size);
 
 	/* Allocate memory for the expanded device tree */
-	mem = (unsigned long)
-		dt_alloc(size + 4, __alignof__(struct device_node));
+	mem = dt_alloc(size + 4, __alignof__(struct device_node));
 
 	memset((void *)mem, 0, size);
 
 	((__be32 *)mem)[size / 4] = cpu_to_be32(0xdeadbeef);
 
-	pr_debug("  unflattening %lx...\n", mem);
+	pr_debug("  unflattening %p...\n", mem);
 
 	/* Second pass, do actual unflattening */
-	start = ((unsigned long)blob) +
-		be32_to_cpu(blob->off_dt_struct);
+	start = 0;
 	unflatten_dt_node(blob, mem, &start, NULL, &allnextp, 0);
-	if (be32_to_cpup((__be32 *)start) != OF_DT_END)
-		pr_warning("Weird tag at end of tree: %08x\n", *((u32 *)start));
-	if (be32_to_cpu(((__be32 *)mem)[size / 4]) != 0xdeadbeef)
+	if (be32_to_cpup(mem + size) != 0xdeadbeef)
 		pr_warning("End of tree marker overwritten: %08x\n",
 			   be32_to_cpu(((__be32 *)mem)[size / 4]));
 	*allnextp = NULL;
@@ -443,6 +385,129 @@ struct boot_param_header *initial_boot_params;
 #ifdef CONFIG_OF_EARLY_FLATTREE
 
 /**
+ * res_mem_reserve_reg() - reserve all memory described in 'reg' property
+ */
+static int __init __reserved_mem_reserve_reg(unsigned long node,
+					     const char *uname)
+{
+	int t_len = (dt_root_addr_cells + dt_root_size_cells) * sizeof(__be32);
+	phys_addr_t base, size;
+	int len;
+	const __be32 *prop;
+	int nomap, first = 1;
+
+	prop = of_get_flat_dt_prop(node, "reg", &len);
+	if (!prop)
+		return -ENOENT;
+
+	if (len && len % t_len != 0) {
+		pr_err("Reserved memory: invalid reg property in '%s', skipping node.\n",
+		       uname);
+		return -EINVAL;
+	}
+
+	nomap = of_get_flat_dt_prop(node, "no-map", NULL) != NULL;
+
+	while (len >= t_len) {
+		base = dt_mem_next_cell(dt_root_addr_cells, &prop);
+		size = dt_mem_next_cell(dt_root_size_cells, &prop);
+
+		if (base && size &&
+		    early_init_dt_reserve_memory_arch(base, size, nomap) == 0)
+			pr_debug("Reserved memory: reserved region for node '%s': base %pa, size %ld MiB\n",
+				uname, &base, (unsigned long)size / SZ_1M);
+		else
+			pr_info("Reserved memory: failed to reserve memory for node '%s': base %pa, size %ld MiB\n",
+				uname, &base, (unsigned long)size / SZ_1M);
+
+		len -= t_len;
+		if (first) {
+			fdt_reserved_mem_save_node(node, uname, base, size);
+			first = 0;
+		}
+	}
+	return 0;
+}
+
+/**
+ * __reserved_mem_check_root() - check if #size-cells, #address-cells provided
+ * in /reserved-memory matches the values supported by the current implementation,
+ * also check if ranges property has been provided
+ */
+static int __init __reserved_mem_check_root(unsigned long node)
+{
+	const __be32 *prop;
+
+	prop = of_get_flat_dt_prop(node, "#size-cells", NULL);
+	if (!prop || be32_to_cpup(prop) != dt_root_size_cells)
+		return -EINVAL;
+
+	prop = of_get_flat_dt_prop(node, "#address-cells", NULL);
+	if (!prop || be32_to_cpup(prop) != dt_root_addr_cells)
+		return -EINVAL;
+
+	prop = of_get_flat_dt_prop(node, "ranges", NULL);
+	if (!prop)
+		return -EINVAL;
+	return 0;
+}
+
+/**
+ * fdt_scan_reserved_mem() - scan a single FDT node for reserved memory
+ */
+static int __init __fdt_scan_reserved_mem(unsigned long node, const char *uname,
+					  int depth, void *data)
+{
+	static int found;
+	const char *status;
+	int err;
+
+	if (!found && depth == 1 && strcmp(uname, "reserved-memory") == 0) {
+		if (__reserved_mem_check_root(node) != 0) {
+			pr_err("Reserved memory: unsupported node format, ignoring\n");
+			/* break scan */
+			return 1;
+		}
+		found = 1;
+		/* scan next node */
+		return 0;
+	} else if (!found) {
+		/* scan next node */
+		return 0;
+	} else if (found && depth < 2) {
+		/* scanning of /reserved-memory has been finished */
+		return 1;
+	}
+
+	status = of_get_flat_dt_prop(node, "status", NULL);
+	if (status && strcmp(status, "okay") != 0 && strcmp(status, "ok") != 0)
+		return 0;
+
+	err = __reserved_mem_reserve_reg(node, uname);
+	if (err == -ENOENT && of_get_flat_dt_prop(node, "size", NULL))
+		fdt_reserved_mem_save_node(node, uname, 0, 0);
+
+	/* scan next node */
+	return 0;
+}
+
+/**
+ * early_init_fdt_scan_reserved_mem() - create reserved memory regions
+ *
+ * This function grabs memory from early allocator for device exclusive use
+ * defined in device tree structures. It should be called by arch specific code
+ * once the early allocator (i.e. memblock) has been fully activated.
+ */
+void __init early_init_fdt_scan_reserved_mem(void)
+{
+	if (!initial_boot_params)
+		return;
+
+	of_scan_flat_dt(__fdt_scan_reserved_mem, NULL);
+	fdt_init_reserved_mem();
+}
+
+/**
  * of_scan_flat_dt - scan flattened tree blob and call callback on each.
  * @it: callback function
  * @data: context data pointer
@@ -456,47 +521,19 @@ int __init of_scan_flat_dt(int (*it)(unsigned long node,
 				     void *data),
 			   void *data)
 {
-	unsigned long p = ((unsigned long)initial_boot_params) +
-		be32_to_cpu(initial_boot_params->off_dt_struct);
-	int rc = 0;
-	int depth = -1;
-
-	do {
-		u32 tag = be32_to_cpup((__be32 *)p);
+	const void *blob = initial_boot_params;
 		const char *pathp;
+	int offset, rc = 0, depth = -1;
 
-		p += 4;
-		if (tag == OF_DT_END_NODE) {
-			depth--;
-			continue;
-		}
-		if (tag == OF_DT_NOP)
-			continue;
-		if (tag == OF_DT_END)
-			break;
-		if (tag == OF_DT_PROP) {
-			u32 sz = be32_to_cpup((__be32 *)p);
-			p += 8;
-			if (be32_to_cpu(initial_boot_params->version) < 0x10)
-				p = ALIGN(p, sz >= 8 ? 8 : 4);
-			p += sz;
-			p = ALIGN(p, 4);
-			continue;
-		}
-		if (tag != OF_DT_BEGIN_NODE) {
-			pr_err("Invalid tag %x in flat device tree!\n", tag);
-			return -EINVAL;
-		}
-		depth++;
-		pathp = (char *)p;
-		p = ALIGN(p + strlen(pathp) + 1, 4);
+        for (offset = fdt_next_node(blob, -1, &depth);
+             offset >= 0 && depth >= 0 && !rc;
+             offset = fdt_next_node(blob, offset, &depth)) {
+
+		pathp = fdt_get_name(blob, offset, NULL);
 		if (*pathp == '/')
 			pathp = kbasename(pathp);
-		rc = it(p, pathp, depth, data);
-		if (rc != 0)
-			break;
-	} while (1);
-
+		rc = it(offset, pathp, depth, data);
+	}
 	return rc;
 }
 
@@ -505,14 +542,7 @@ int __init of_scan_flat_dt(int (*it)(unsigned long node,
  */
 unsigned long __init of_get_flat_dt_root(void)
 {
-	unsigned long p = ((unsigned long)initial_boot_params) +
-		be32_to_cpu(initial_boot_params->off_dt_struct);
-
-	while (be32_to_cpup((__be32 *)p) == OF_DT_NOP)
-		p += 4;
-	BUG_ON(be32_to_cpup((__be32 *)p) != OF_DT_BEGIN_NODE);
-	p += 4;
-	return ALIGN(p + strlen((char *)p) + 1, 4);
+	return 0;
 }
 
 /**
@@ -521,10 +551,10 @@ unsigned long __init of_get_flat_dt_root(void)
  * This function can be used within scan_flattened_dt callback to get
  * access to properties
  */
-void *__init of_get_flat_dt_prop(unsigned long node, const char *name,
-				 unsigned long *size)
+const void *__init of_get_flat_dt_prop(unsigned long node, const char *name,
+				       int *size)
 {
-	return of_fdt_get_property(initial_boot_params, node, name, size);
+	return fdt_getprop(initial_boot_params, node, name, size);
 }
 
 /**
@@ -545,6 +575,15 @@ int __init of_flat_dt_match(unsigned long node, const char *const *compat)
 	return of_fdt_match(initial_boot_params, node, compat);
 }
 
+struct fdt_scan_status {
+	const char *name;
+	int namelen;
+	int depth;
+	int found;
+	int (*iterator)(unsigned long node, const char *uname, int depth, void *data);
+	void *data;
+};
+
 #ifdef CONFIG_BLK_DEV_INITRD
 /**
  * early_init_dt_check_for_initrd - Decode initrd location from flat tree
@@ -552,23 +591,25 @@ int __init of_flat_dt_match(unsigned long node, const char *const *compat)
  */
 void __init early_init_dt_check_for_initrd(unsigned long node)
 {
-	unsigned long start, end, len;
-	__be32 *prop;
+	u64 start, end;
+	int len;
+	const __be32 *prop;
 
 	pr_debug("Looking for initrd properties... ");
 
 	prop = of_get_flat_dt_prop(node, "linux,initrd-start", &len);
 	if (!prop)
 		return;
-	start = of_read_ulong(prop, len/4);
+	start = of_read_number(prop, len/4);
 
 	prop = of_get_flat_dt_prop(node, "linux,initrd-end", &len);
 	if (!prop)
 		return;
-	end = of_read_ulong(prop, len/4);
+	end = of_read_number(prop, len/4);
 
 	early_init_dt_setup_initrd_arch(start, end);
-	pr_debug("initrd_start=0x%lx  initrd_end=0x%lx\n", start, end);
+	pr_debug("initrd_start=0x%llx  initrd_end=0x%llx\n",
+		 (unsigned long long)start, (unsigned long long)end);
 }
 #else
 inline void early_init_dt_check_for_initrd(unsigned long node)
@@ -582,7 +623,7 @@ inline void early_init_dt_check_for_initrd(unsigned long node)
 int __init early_init_dt_scan_root(unsigned long node, const char *uname,
 				   int depth, void *data)
 {
-	__be32 *prop;
+	const __be32 *prop;
 
 	if (depth != 0)
 		return 0;
@@ -604,9 +645,9 @@ int __init early_init_dt_scan_root(unsigned long node, const char *uname,
 	return 1;
 }
 
-u64 __init dt_mem_next_cell(int s, __be32 **cellp)
+u64 __init dt_mem_next_cell(int s, const __be32 **cellp)
 {
-	__be32 *p = *cellp;
+	const __be32 *p = *cellp;
 
 	*cellp = p + s;
 	return of_read_number(p, s);
@@ -618,9 +659,9 @@ u64 __init dt_mem_next_cell(int s, __be32 **cellp)
 int __init early_init_dt_scan_memory(unsigned long node, const char *uname,
 				     int depth, void *data)
 {
-	char *type = of_get_flat_dt_prop(node, "device_type", NULL);
-	__be32 *reg, *endp;
-	unsigned long l;
+	const char *type = of_get_flat_dt_prop(node, "device_type", NULL);
+	const __be32 *reg, *endp;
+	int l;
 
 	/* We are scanning "memory" nodes only */
 	if (type == NULL) {
@@ -641,7 +682,7 @@ int __init early_init_dt_scan_memory(unsigned long node, const char *uname,
 
 	endp = reg + (l / sizeof(__be32));
 
-	pr_debug("memory scan node %s, reg size %ld, data: %x %x %x %x,\n",
+	pr_debug("memory scan node %s, reg size %d, data: %x %x %x %x,\n",
 	    uname, l, reg[0], reg[1], reg[2], reg[3]);
 
 	while ((endp - reg) >= (dt_root_addr_cells + dt_root_size_cells)) {
@@ -664,8 +705,8 @@ int __init early_init_dt_scan_memory(unsigned long node, const char *uname,
 int __init early_init_dt_scan_chosen(unsigned long node, const char *uname,
 				     int depth, void *data)
 {
-	unsigned long l;
-	char *p;
+	int l;
+	const char *p;
 
 	pr_debug("search \"chosen\", depth: %d, uname: %s\n", depth, uname);
 
@@ -698,6 +739,80 @@ int __init early_init_dt_scan_chosen(unsigned long node, const char *uname,
 	return 1;
 }
 
+#ifdef CONFIG_HAVE_MEMBLOCK
+void __init __weak early_init_dt_add_memory_arch(u64 base, u64 size)
+{
+	const u64 phys_offset = __pa(PAGE_OFFSET);
+	base &= PAGE_MASK;
+	size &= PAGE_MASK;
+	if (base + size < phys_offset) {
+		pr_warning("Ignoring memory block 0x%llx - 0x%llx\n",
+			   base, base + size);
+		return;
+	}
+	if (base < phys_offset) {
+		pr_warning("Ignoring memory range 0x%llx - 0x%llx\n",
+			   base, phys_offset);
+		size -= phys_offset - base;
+		base = phys_offset;
+	}
+	memblock_add(base, size);
+}
+
+int __init __weak early_init_dt_reserve_memory_arch(phys_addr_t base,
+					phys_addr_t size, bool nomap)
+{
+	if (memblock_is_region_reserved(base, size))
+		return -EBUSY;
+	if (nomap)
+		return memblock_remove(base, size);
+	return memblock_reserve(base, size);
+}
+
+/*
+ * called from unflatten_device_tree() to bootstrap devicetree itself
+ * Architectures can override this definition if memblock isn't used
+ */
+void * __init __weak early_init_dt_alloc_memory_arch(u64 size, u64 align)
+{
+	return __va(memblock_alloc(size, align));
+}
+#else
+int __init __weak early_init_dt_reserve_memory_arch(phys_addr_t base,
+					phys_addr_t size, bool nomap)
+{
+	pr_err("Reserved memory not supported, ignoring range 0x%llx - 0x%llx%s\n",
+		  base, size, nomap ? " (nomap)" : "");
+	return -ENOSYS;
+}
+#endif
+
+bool __init early_init_dt_scan(void *params)
+{
+	if (!params)
+		return false;
+
+	/* Setup flat device-tree pointer */
+	initial_boot_params = params;
+
+	/* check device tree validity */
+	if (be32_to_cpu(initial_boot_params->magic) != OF_DT_HEADER) {
+		initial_boot_params = NULL;
+		return false;
+	}
+
+	/* Retrieve various information from the /chosen node */
+	of_scan_flat_dt(early_init_dt_scan_chosen, boot_command_line);
+
+	/* Initialize {size,address}-cells info */
+	of_scan_flat_dt(early_init_dt_scan_root, NULL);
+
+	/* Setup memory, calling early_init_dt_add_memory_arch */
+	of_scan_flat_dt(early_init_dt_scan_memory, NULL);
+
+	return true;
+}
+
 /**
  * unflatten_device_tree - create tree of device_nodes from flat blob
  *
diff --git a/drivers/of/of_reserved_mem.c b/drivers/of/of_reserved_mem.c
new file mode 100644
index 0000000..e420eb5
--- /dev/null
+++ b/drivers/of/of_reserved_mem.c
@@ -0,0 +1,217 @@
+/*
+ * Device tree based initialization code for reserved memory.
+ *
+ * Copyright (c) 2013, The Linux Foundation. All Rights Reserved.
+ * Copyright (c) 2013,2014 Samsung Electronics Co., Ltd.
+ *		http://www.samsung.com
+ * Author: Marek Szyprowski <m.szyprowski@samsung.com>
+ * Author: Josh Cartwright <joshc@codeaurora.org>
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License as
+ * published by the Free Software Foundation; either version 2 of the
+ * License or (at your optional) any later version of the license.
+ */
+
+#include <linux/err.h>
+#include <linux/of.h>
+#include <linux/of_fdt.h>
+#include <linux/of_platform.h>
+#include <linux/mm.h>
+#include <linux/sizes.h>
+#include <linux/of_reserved_mem.h>
+
+#define MAX_RESERVED_REGIONS	16
+static struct reserved_mem reserved_mem[MAX_RESERVED_REGIONS];
+static int reserved_mem_count;
+
+#if defined(CONFIG_HAVE_MEMBLOCK)
+#include <linux/memblock.h>
+int __init __weak early_init_dt_alloc_reserved_memory_arch(phys_addr_t size,
+	phys_addr_t align, phys_addr_t start, phys_addr_t end, bool nomap,
+	phys_addr_t *res_base)
+{
+	/*
+	 * We use __memblock_alloc_base() because memblock_alloc_base()
+	 * panic()s on allocation failure.
+	 */
+	phys_addr_t base = __memblock_alloc_base(size, align, end);
+	if (!base)
+		return -ENOMEM;
+
+	/*
+	 * Check if the allocated region fits in to start..end window
+	 */
+	if (base < start) {
+		memblock_free(base, size);
+		return -ENOMEM;
+	}
+
+	*res_base = base;
+	if (nomap)
+		return memblock_remove(base, size);
+	return 0;
+}
+#else
+int __init __weak early_init_dt_alloc_reserved_memory_arch(phys_addr_t size,
+	phys_addr_t align, phys_addr_t start, phys_addr_t end, bool nomap,
+	phys_addr_t *res_base)
+{
+	pr_err("Reserved memory not supported, ignoring region 0x%llx%s\n",
+		  size, nomap ? " (nomap)" : "");
+	return -ENOSYS;
+}
+#endif
+
+/**
+ * res_mem_save_node() - save fdt node for second pass initialization
+ */
+void __init fdt_reserved_mem_save_node(unsigned long node, const char *uname,
+				      phys_addr_t base, phys_addr_t size)
+{
+	struct reserved_mem *rmem = &reserved_mem[reserved_mem_count];
+
+	if (reserved_mem_count == ARRAY_SIZE(reserved_mem)) {
+		pr_err("Reserved memory: not enough space all defined regions.\n");
+		return;
+	}
+
+	rmem->fdt_node = node;
+	rmem->name = uname;
+	rmem->base = base;
+	rmem->size = size;
+
+	reserved_mem_count++;
+	return;
+}
+
+/**
+ * res_mem_alloc_size() - allocate reserved memory described by 'size', 'align'
+ *			  and 'alloc-ranges' properties
+ */
+static int __init __reserved_mem_alloc_size(unsigned long node,
+	const char *uname, phys_addr_t *res_base, phys_addr_t *res_size)
+{
+	int t_len = (dt_root_addr_cells + dt_root_size_cells) * sizeof(__be32);
+	phys_addr_t start = 0, end = 0;
+	phys_addr_t base = 0, align = 0, size;
+	int len;
+	const __be32 *prop;
+	int nomap;
+	int ret;
+
+	prop = of_get_flat_dt_prop(node, "size", &len);
+	if (!prop)
+		return -EINVAL;
+
+	if (len != dt_root_size_cells * sizeof(__be32)) {
+		pr_err("Reserved memory: invalid size property in '%s' node.\n",
+				uname);
+		return -EINVAL;
+	}
+	size = dt_mem_next_cell(dt_root_size_cells, &prop);
+
+	nomap = of_get_flat_dt_prop(node, "no-map", NULL) != NULL;
+
+	prop = of_get_flat_dt_prop(node, "alignment", &len);
+	if (prop) {
+		if (len != dt_root_addr_cells * sizeof(__be32)) {
+			pr_err("Reserved memory: invalid alignment property in '%s' node.\n",
+				uname);
+			return -EINVAL;
+		}
+		align = dt_mem_next_cell(dt_root_addr_cells, &prop);
+	}
+
+	prop = of_get_flat_dt_prop(node, "alloc-ranges", &len);
+	if (prop) {
+
+		if (len % t_len != 0) {
+			pr_err("Reserved memory: invalid alloc-ranges property in '%s', skipping node.\n",
+			       uname);
+			return -EINVAL;
+		}
+
+		base = 0;
+
+		while (len > 0) {
+			start = dt_mem_next_cell(dt_root_addr_cells, &prop);
+			end = start + dt_mem_next_cell(dt_root_size_cells,
+						       &prop);
+
+			ret = early_init_dt_alloc_reserved_memory_arch(size,
+					align, start, end, nomap, &base);
+			if (ret == 0) {
+				pr_debug("Reserved memory: allocated memory for '%s' node: base %pa, size %ld MiB\n",
+					uname, &base,
+					(unsigned long)size / SZ_1M);
+				break;
+			}
+			len -= t_len;
+		}
+
+	} else {
+		ret = early_init_dt_alloc_reserved_memory_arch(size, align,
+							0, 0, nomap, &base);
+		if (ret == 0)
+			pr_debug("Reserved memory: allocated memory for '%s' node: base %pa, size %ld MiB\n",
+				uname, &base, (unsigned long)size / SZ_1M);
+	}
+
+	if (base == 0) {
+		pr_info("Reserved memory: failed to allocate memory for node '%s'\n",
+			uname);
+		return -ENOMEM;
+	}
+
+	*res_base = base;
+	*res_size = size;
+
+	return 0;
+}
+
+static const struct of_device_id __rmem_of_table_sentinel
+	__used __section(__reservedmem_of_table_end);
+
+/**
+ * res_mem_init_node() - call region specific reserved memory init code
+ */
+static int __init __reserved_mem_init_node(struct reserved_mem *rmem)
+{
+	extern const struct of_device_id __reservedmem_of_table[];
+	const struct of_device_id *i;
+
+	for (i = __reservedmem_of_table; i < &__rmem_of_table_sentinel; i++) {
+		reservedmem_of_init_fn initfn = i->data;
+		const char *compat = i->compatible;
+
+		if (!of_flat_dt_is_compatible(rmem->fdt_node, compat))
+			continue;
+
+		if (initfn(rmem, rmem->fdt_node, rmem->name) == 0) {
+			pr_info("Reserved memory: initialized node %s, compatible id %s\n",
+				rmem->name, compat);
+			return 0;
+		}
+	}
+	return -ENOENT;
+}
+
+/**
+ * fdt_init_reserved_mem - allocate and init all saved reserved memory regions
+ */
+void __init fdt_init_reserved_mem(void)
+{
+	int i;
+	for (i = 0; i < reserved_mem_count; i++) {
+		struct reserved_mem *rmem = &reserved_mem[i];
+		unsigned long node = rmem->fdt_node;
+		int err = 0;
+
+		if (rmem->size == 0)
+			err = __reserved_mem_alloc_size(node, rmem->name,
+						 &rmem->base, &rmem->size);
+		if (err == 0)
+			__reserved_mem_init_node(rmem);
+	}
+}
diff --git a/drivers/of/platform.c b/drivers/of/platform.c
index e0a6514..0d728c1 100644
--- a/drivers/of/platform.c
+++ b/drivers/of/platform.c
@@ -11,6 +11,8 @@
  *  2 of the License, or (at your option) any later version.
  *
  */
+/* #define DEBUG */
+
 #include <linux/errno.h>
 #include <linux/module.h>
 #include <linux/amba/bus.h>
@@ -214,7 +216,7 @@ struct platform_device *of_platform_device_create_pdata(
 #if defined(CONFIG_MICROBLAZE)
 	dev->archdata.dma_mask = 0xffffffffUL;
 #endif
-	dev->dev.coherent_dma_mask = DMA_BIT_MASK(32);
+	dev->dev.coherent_dma_mask = DMA_BIT_MASK(64);
 	dev->dev.bus = &platform_bus_type;
 	dev->dev.platform_data = platform_data;
 
diff --git a/drivers/pci/Kconfig b/drivers/pci/Kconfig
index 6d51aa6..74026b2 100644
--- a/drivers/pci/Kconfig
+++ b/drivers/pci/Kconfig
@@ -2,7 +2,7 @@
 # PCI configuration
 #
 config ARCH_SUPPORTS_MSI
-	bool
+	bool "ARCH_SUPPORTS_MSI"
 
 config PCI_MSI
 	bool "Message Signaled Interrupts (MSI and MSI-X)"
@@ -119,3 +119,5 @@ config PCI_IOAPIC
 config PCI_LABEL
 	def_bool y if (DMI || ACPI)
 	select NLS
+
+source "drivers/pci/host/Kconfig"
diff --git a/drivers/pci/Makefile b/drivers/pci/Makefile
index 0c3efcf..b483a78 100644
--- a/drivers/pci/Makefile
+++ b/drivers/pci/Makefile
@@ -67,3 +67,7 @@ obj-$(CONFIG_XEN_PCIDEV_FRONTEND) += xen-pcifront.o
 obj-$(CONFIG_OF) += of.o
 
 ccflags-$(CONFIG_PCI_DEBUG) := -DDEBUG
+
+# PCI host controller drivers
+obj-y += host/
+
diff --git a/drivers/pci/host/Kconfig b/drivers/pci/host/Kconfig
new file mode 100644
index 0000000..a77b1a6
--- /dev/null
+++ b/drivers/pci/host/Kconfig
@@ -0,0 +1,36 @@
+menu "PCI host controller drivers"
+	depends on PCI
+
+config PCIE_F_PCIE2_DMXH
+	tristate "Fujitsu f_pcie2_dmxh Support"
+	depends on PCI && (ARCH_MB8AC0300 || ARCH_MB86S70)
+	default y
+	help
+	  Say Y here if you want to enable
+	  Fujitsu F_PCIE2_DMXH PCIe root complex
+	  support.
+	  If unsure say Y.
+
+config SYSOC_F_PCIE_PCIE2_DMXH
+	tristate "Fujitsu F_PCIE2_DMXH System control Support"
+	depends on PCIE_F_PCIE2_DMXH
+	default y
+	help
+	  Say Y here if you want to enable
+	  Fujitsu F_PCIE2_DMXH System control Support.
+	  If unsure say Y.
+
+config PCI_MVEBU
+	bool "Marvell EBU PCIe controller"
+	depends on ARCH_MVEBU || ARCH_KIRKWOOD
+
+config PCIE_DW
+	bool
+
+config PCI_EXYNOS
+	bool "Samsung Exynos PCIe controller"
+	depends on SOC_EXYNOS5440
+	select PCIEPORTBUS
+	select PCIE_DW
+
+endmenu
diff --git a/drivers/pci/host/Makefile b/drivers/pci/host/Makefile
new file mode 100644
index 0000000..2f3976e
--- /dev/null
+++ b/drivers/pci/host/Makefile
@@ -0,0 +1,5 @@
+obj-$(CONFIG_PCIE_F_PCIE2_DMXH) += pcie_f_pcie2_dme.o
+obj-$(CONFIG_SYSOC_F_PCIE_PCIE2_DMXH) += sysoc_pcie.o
+obj-$(CONFIG_PCI_MVEBU) += pci-mvebu.o
+obj-$(CONFIG_PCIE_DW) += pcie-designware.o
+obj-$(CONFIG_PCI_MSI) += pcie_f_pcie2_dme_msi.o
diff --git a/drivers/pci/host/pcie_f_pcie2_dme.c b/drivers/pci/host/pcie_f_pcie2_dme.c
new file mode 100644
index 0000000..9ec792e
--- /dev/null
+++ b/drivers/pci/host/pcie_f_pcie2_dme.c
@@ -0,0 +1,2474 @@
+/*
+ * Copyright (C) 2013-2014 Fujitsu Semiconductor Ltd
+ *
+ * F_PCIE2_DME functions for Fujitsu SoC
+ *
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/skbuff.h>
+#include <linux/platform_device.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
+#include <linux/of_pci.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/pm_runtime.h>
+#include <linux/pm_domain.h>
+#include <asm/dma-iommu.h>
+#include <linux/sched.h>
+#include <linux/msi.h>
+#include <linux/iommu.h>
+#include "pcie_f_pcie2_dme.h"
+
+#define PCIE_TRANS_STAT				0x844
+#define PCIE_TRANS_STAT_DL_ACT			(1 << 6)
+#define PCIE_PWR_CSR				0x408
+#define PCIE_PWR_CTL_PERST			1
+#define BRIDGE_MODE				0x880
+#define DL_WRITE_MODE				(1 << 3)
+#define SLA_TRANS_EN				0x884
+#define ICS_IF_ENABLE				(1 << 8)
+#define TRS_IF_ENABLE				(1 << 0)
+#define BAR_EN_REG				0x820
+#define EN_BAR(x)				(1 << x)
+#define SLA_CFG_BASE_ADDR			0x1100
+#define SLA_IO_CFG_STA				0x100C
+#define CFG_BZ					1
+#define SLA_CONF_TAR_ID0			0x1110
+#define SLA_CONF_TAR_ID(a)			(SLA_CONF_TAR_ID0 + a * 4)
+#define CFG_TLP_TYPE(a)				(a << 16)
+#define BUS_NUM(a)				(a << 8)
+#define DEV_NUM(a)				(a << 3)
+#define FUN_NUM(a)				(a << 0)
+#define TYPE0_ISSUE				0
+#define TYPE1_ISSUE				1
+#define SLA_MEM_BAR(a)				(0x1820 + a * 16)
+#define SLA_MEM_BAR_REMAP_SZ(a)			(0x1824 + a * 16)
+#define SLA_MEM_REMAP_SZ			0xF0000000
+#define SLA_MEM_BAR_REMAP_L(a)			(0x1828 + a * 16)
+#define SLA_MEM_BAR_REMAP_U(a)			(0x182C + a * 16)
+#define SLA_IO_BAR(a)				(0x1200 + a * 16)
+#define SLA_IO_BAR_REMAP_SZ(a)			(0x1204 + a * 16)
+#define SLA_IO_BAR_REMAP_ADD(a)			(0x1208 + a * 16)
+#define SLA_IO_REMAP_SZ				0xF0000000
+#define BAR_RES_SET(x)				(0x804 + x * 4)
+#define SLA_MEM_INT_ST				0x1800
+#define SLA_MEM_INT_MASK			0x1804
+#define MEM_RD_COMPLE_ABORT			(1 << 0)
+#define MEM_RD_UNSUPPORT_REQ			(1 << 1)
+#define MEM_RD_COMPLE_TIMEOUT			(1 << 2)
+#define MEM_RD_POISONED_TLP			(1 << 3)
+#define MEM_RD_TRANS_REQ			(1 << 24)
+#define SLA_IO_CFG_INT_ST			0x1000
+#define SLA_IO_CFG_INT_MASK			0x1004
+#define IO_CFG_INT_ST_CLR			0xffffffff
+#define CFG_RD_COMPLETION_ABORT			(1 << 0)
+#define CFG_RD_UNSUPPORTED_REQ			(1 << 1)
+#define CFG_RD_COMPLETE_TIMEOUT			(1 << 2)
+#define CFG_RD_POISONED_TLP			(1 << 3)
+#define CFG_RD_CFG_RETRY_STATUS			(1 << 4)
+#define CFG_WT_COMPLETION_ABORT			(1 << 5)
+#define CFG_WT_UNSUPPORTED_REQ			(1 << 6)
+#define CFG_WT_COMPLETE_TIMEOUT			(1 << 7)
+#define CFG_WT_CFG_RETRY_STATUS			(1 << 8)
+#define DT_CONF_SPACE				0
+#define BAR_SET_PREFETCHABLE			(1 << 3)
+#define BAR_SET_MEMORY_TYPE_32BIT		(0 << 1)
+#define BAR_SET_MEMORY_TYPE_64BIT		(2 << 1)
+#define BAR_SET_IO_SPACE			(1 << 0)
+#define BAR_SET_MEM_SPACE			(0 << 0)
+#define EROMR_SETTING				0x81C
+#define FUNC_SEL_BAR_REMAP			0x08A0
+#define AXI_MASTER_REMAP_ADDR(x)		(0x8A4 + x * 4)
+#define EROM_EN					(1 << 6)
+#define BAR_EN(x)				(1 << x)
+#define PCI_SLAVE_BUFFER			(512 * SZ_1K)
+#define MSG_INT_A_D(x)				(x)
+#define SLA_MEM_INT				5
+#define DMAC_INSTR				0xC00
+#define DMA_STOP				(0 << 4)
+#define DMA_START				(1 << 4)
+#define DMA_INT_CLR				(2 << 4)
+#define DMA_NUM(x)				(x)
+#define DMA_MODE_CONL				0xC04
+#define DMA_STATUS				0xC08
+#define DMA_NORMAL				0x00
+#define DMA_TRANS_BUSY				0x01
+#define DMA_ABORT_ENDERR			0x10
+#define DMA_INT_MASK				0xC0C
+#define DEM_DMA_PCIE_L_ADDR_CH(x)		(0xE00 + x * 0x20)
+#define DEM_DMA_PCIE_U_ADDR_CH(x)		(0xE04 + x * 0x20)
+#define DEM_DMA_AXI_ADDR_CH(x)			(0xE08 + x * 0x20)
+#define DEM_DMA_TRANS_SZ_CH(x)			(0xE0C + x * 0x20)
+#define DEM_DMA_TRANS_SET_CH(x)			(0xE10 + x * 0x20)
+#define MULTI_R_REQ_1_MULTI			(1 << 20)
+#define TAB_ADDR(x)				(0xE14 + x * 0x20)
+#define DEM_DMA_TRAN_SAT_CH(x)			(0xE18 + x * 0x20)
+#define DMA_TRAN_SPL_SZ(x)			(x)
+#define DMA_TRANS_ABORT_BY_USER			(1 << 7)
+#define DMA_TRANS_ABORT_RESET_DET		(1 << 6)
+#define DMA_TRANS_ABORT_POISO_DET		(1 << 5)
+#define DMA_TRANS_ABORT_BY_PCIE			(0x111 << 2)
+#define COMPLETION_TIMEOUT(x)			(x & 0x100)
+#define UNSUPPORTED_REQ(x)			(x & 0x10)
+#define COMPLETION_ABORT(x)			(x & 0x1)
+#define AXI_ERROR_DETECT			(1 << 1)
+#define TRANS_COMPLETE				(1 << 0)
+/* (PCIe: Read, AXI: Write) */
+#define DMA_TRANS_PCIE2AXI			(0 << 19)
+ /* (AXI: Read, PCIe Write) */
+#define DMA_TRANS_AXI2PCIE			(1 << 19)
+#define DMA_TYPE_DEMAND				0
+#define DMA_TYPE_DESCRIPT			1
+#define FUNC_SEL_BAR_REMAP			0x08A0
+#define AXI_MASTER_REMAP_ADDR(x)		(0x8A4 + x * 4)
+#define AXI_MASTER_ERR_INT_MASK			0x904
+#define WAIT_DMA_TIMEOUT			msecs_to_jiffies(100)
+#define WAIT_MEM_TIMEOUT			usecs_to_jiffies(5)
+#define SAT_DMA_WAIT				0
+#define SAT_DMA_DONE				1
+#define DMXH_DMA_CH				4
+#define DES_DMA_LAST_DESC			31
+#define DES_DMA_DESC_ID				16
+#define DES_DMA_TB_L_ADDR			0x00
+#define DES_DMA_TB_U_ADDR			0x01
+#define DES_DMA_TB_AXI_ADDR			0x02
+#define DES_DMA_TB_TRANS_SZ			0x03
+#define DES_DMA_TB_TRANS_SET			0x04
+#define DES_DMA_TB_ADDR_SET			0x05
+#define DES_DMA_TB_LDID_SET			0x06
+#define TEST_NUM				6
+#define DMA_INT_NUM				4
+#define ST_DMA_INT_ENB				0x28
+#define EN_DMA_INT				0xffff
+#define LINK_STATUS				0x92
+#define LINK_SPEED_MASK				0x0F
+#define LINK_WIDTH_MASK				0xF0
+#define LINK_WIDTH_1				(1 << 0)
+#define LINK_WIDTH_2				(1 << 1)
+#define LINK_WIDTH_4				(1 << 2)
+#define LINK_WIDTH_8				(1 << 3)
+#define LINK_SPEED_2_5				1
+#define LINK_SPEED_5_0				2
+#define PCIE_WAP_LIG_INT			10
+#define PCIE_MSIX_CTL				0xE2
+#define PCIE_MSIX_TABLE				0xE4
+#define PCIE_MSIX_PBA				0xE8
+#define PCIE_MSI_CTL				0xC2
+#define PCIE_MSI_ADDR_LO			0xC4
+#define PCIE_MSI_ADDR_HI			0xC8
+#define PCIE_MSI_DATA				0xCC
+#define MSIX_ADDR_L(a)				(0x700 + a * 0x10)
+#define MSIX_ADDR_H(a)				(0x704 + a * 0x10)
+#define MSIX_DATA(a)				(0x708 + a * 0x10)
+#define PCIE_LIG				0x1200
+#define LIG_INT_REQ				0X00
+#define LIG_INT_STS				0X04
+#define LIG_INT_EN				0X08
+#define LIG_INT_CLR				0X0C
+#define SET_LIG_ENABLE				0xffffffff
+#define INT_INTX				0
+#define INT_MSI				1
+#define INT_MSIX				2
+#define MSI_HW_INT_N				16
+#define EP_NAME_LEN				20
+
+struct desc_tab {
+	dma_addr_t dma_table_phys;
+	dma_addr_t pcie_lower_addr;
+	dma_addr_t pcie_upper_addr;
+	dma_addr_t axi_addr;
+	dma_addr_t next_tab_addr;
+	u32 *dma_tab;
+	u32 *pcie_buf;
+	u32 *axi_buf;
+	u32 traf_size;
+	u32 traf_set;
+	u32 ld_descrid;
+	u32 descr_cont;
+};
+
+struct dme_ep {
+	void __iomem *pcie_addr;
+	void __iomem *regs;
+	struct device *dev;
+	struct clk **clk;
+	struct msix_entry *msix_entries;
+	int dma_irq[4];
+	int dma_done;
+	int index;
+	dma_addr_t *remap_addr;
+	dma_addr_t remap_addr_phys;
+	u32 device_id;
+	u32 vendor_id;
+	u32 clk_num;
+	u32 dma_type; /* Demand mode:0, Descript mode:1 */
+	u32 remap_addr_len;
+	u32 act_dma_number;
+	wait_queue_head_t dma_wait;
+	u32 dma_wait_timeout;
+	u32 loop_coned;
+	u32 inited;
+	u32 msix_count;
+	u32 int_type;
+};
+
+
+u32 test_patten[TEST_NUM] = {
+	0xFFFFFFFF,
+	0xF0F0F0F0,
+	0x0F0F0F0F,
+	0x55555555,
+	0x50505050,
+	0x05050505
+};
+
+struct pcie_port pcie_port = {.rc_cnt = 0, .dme_pcie = NULL};
+
+
+void f_pcie_dev_set_platdata(struct device *dev, void *data)
+{
+	dev->platform_data = data;
+}
+
+struct f_pcie_port *f_get_pcie_port(int index)
+{
+	struct f_pcie_port *port = NULL, *tmp;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcie_port.lock, flags);
+	list_for_each_entry_safe(port, tmp, &pcie_port.list, ports) {
+		if (port->index == index) {
+			spin_unlock_irqrestore(&pcie_port.lock, flags);
+			return port;
+		}
+	}
+
+	spin_unlock_irqrestore(&pcie_port.lock, flags);
+	pr_info("can't find pcie port\n");
+	return NULL;
+}
+
+struct f_pcie_port *f_get_pcie_ep_port(u32 device_id)
+{
+	struct f_pcie_port *port, *tmp;
+	struct dme_ep *ep;
+	unsigned long flags;
+
+	spin_lock_irqsave(&pcie_port.lock, flags);
+	list_for_each_entry_safe(port, tmp, &pcie_port.list, ports) {
+		ep = port->ep;
+		if (!ep)
+			continue;
+
+		if (ep->device_id == device_id) {
+			spin_unlock_irqrestore(&pcie_port.lock, flags);
+			return port;
+		}
+	}
+	spin_unlock_irqrestore(&pcie_port.lock, flags);
+	pr_info("can't find EP port (device : 0x%x)\n", device_id);
+
+	return NULL;
+}
+
+int f_pcie_host_link_up(struct dme_rc *rc)
+{
+	int retries = 20, link_up;
+
+	while (retries) {
+		link_up = readb(rc->rc_cfg_base + PCIE_TRANS_STAT);
+		link_up &= PCIE_TRANS_STAT_DL_ACT;
+
+		if (!link_up)
+			retries--;
+		else
+			break;
+
+		usleep_range(10000, 20000);
+	}
+
+	rc->link_up = link_up;
+	if (rc->link_up)
+		dev_info(rc->dev, "PCIe port %d link up\n", rc->index);
+	else
+		dev_info(rc->dev, "PCIe port %d link down\n", rc->index);
+
+	return 0;
+}
+
+
+void f_pcie_host_init_rc(struct dme_rc *rc)
+{
+	void __iomem *base = rc->rc_cfg_base;
+	u32 dl_sat, i, res_low, res_hi, res_mask, msi_multi;
+	u32 now_link_lan, now_link_sp, link_sat, addr;
+
+	f_pcie_host_link_up(rc);
+	if (!rc->link_up)
+		return;
+
+	/* device DL write enable */
+	dl_sat = readl(base + BRIDGE_MODE);
+	dl_sat |= DL_WRITE_MODE;
+	writel(dl_sat, base + BRIDGE_MODE);
+
+	/* init pcie vendor class number */
+	writew(PCI_CLASS_BRIDGE_PCI, base + PCI_CLASS_DEVICE);
+
+	/* init pcie vendor number */
+	writew(PCI_VENDOR_ID_FUJITSU_ME, base + PCI_VENDOR_ID);
+
+	/*
+	 * The numbers possessed by MSI Table are indicated
+	 * S73 supply (1-16)
+	 * 000h: 1 / 100: 16
+	 */
+	if (of_property_read_u32(rc->dev->of_node, "msi-multi",
+		&msi_multi)) {
+		dev_warn(rc->dev, "Missing msi-multi in dt\n");
+		msi_multi = 0;
+	}
+
+	res_mask = readb(base + PCIE_MSI_CTL);
+	res_mask &= ~0x0e;
+	res_mask |= (msi_multi << 1);
+	writeb(res_mask, base + PCIE_MSI_CTL);
+
+	/*
+	 * MSI-X
+	 * S73 supply (1-32)
+	 * 0x0h: 1 / 0x1F: 32
+	 * setting  MSIX table count 32
+	 * setting  MSIX address table offset in 0x1000 of base 0
+	 * setting  MSIX Pending table offset in 0x1008 of base 0
+	 */
+	res_mask = readb(base + PCIE_MSIX_CTL);
+	res_mask &= ~(PCI_MSIX_FLAGS_QSIZE);
+	res_mask |= 0x1f;
+	writeb(res_mask, base + PCIE_MSIX_CTL);
+	writel(0x1000, base + PCIE_MSIX_TABLE);
+	writel(0x1008, base + PCIE_MSIX_PBA);
+
+	/* 512K DDR */
+	addr = 0xFFF80000;
+	res_mask = addr | BAR_SET_MEMORY_TYPE_32BIT | BAR_SET_MEM_SPACE;
+
+	/* BAR 0 enable  */
+	writel(res_mask, base + BAR_RES_SET(0));
+
+	/* device DL write disenable */
+	dl_sat = dl_sat & ~DL_WRITE_MODE;
+	writel(dl_sat, base + BRIDGE_MODE);
+
+	writel(0, base + BAR_RES_SET(1));
+	writel(1, base + BAR_EN_REG);
+
+	/* Enable AXI Slave IO/Config Interrupt */
+	writel(0, base + SLA_MEM_INT_MASK);
+
+	/* Enable AXI Slave Memory Interrupt Status */
+	writel(0, base + SLA_IO_CFG_INT_MASK);
+
+	/* Configure space */
+	writel(rc->cfg_res.start, base + SLA_CFG_BASE_ADDR);
+	writel(0, base + SLA_CONF_TAR_ID0);
+
+	/* I/O memory */
+	for (i = 0; i <= rc->mem_num; i++) {
+		res_low = (u32)rc->mem_res[i].start;
+		writel(rc->mem_offset[i], base + SLA_IO_BAR(i));
+		writel(SLA_IO_REMAP_SZ, base + SLA_IO_BAR_REMAP_SZ(i));
+		writel(res_low, base + SLA_IO_BAR_REMAP_ADD(i));
+	}
+
+	/* slave memory */
+	for (i = 0; i <= rc->mem_num; i++) {
+		res_hi = (u64)rc->mem_res[i].start >> 32;
+		res_low = (u32)rc->mem_res[i].start;
+		res_mask = (u32)rc->mem_offset[i];
+		writel(rc->mem_offset[i], base + SLA_MEM_BAR(i));
+		writel(SLA_MEM_REMAP_SZ, base + SLA_MEM_BAR_REMAP_SZ(i));
+		writel(res_low, base + SLA_MEM_BAR_REMAP_L(i));
+		writel(res_hi, base + SLA_MEM_BAR_REMAP_U(i));
+	}
+
+	link_sat = readw(base + LINK_STATUS);
+	now_link_lan = ((link_sat & LINK_WIDTH_MASK) >> 4);
+	switch (now_link_lan) {
+	case LINK_WIDTH_1:
+		dev_info(rc->dev, "pcie Host:now link lan x1\n");
+	break;
+	case LINK_WIDTH_2:
+		dev_info(rc->dev, "pcie Host:now link lan x2\n");
+	break;
+	case LINK_WIDTH_4:
+		dev_info(rc->dev, "pcie Host:now link lan x4\n");
+	break;
+	case LINK_WIDTH_8:
+		dev_info(rc->dev, "pcie Host:now link lan x8\n");
+	break;
+	default:
+		dev_info(rc->dev, "pcie Host:now link lan not support!\n");
+	break;
+	}
+
+	now_link_sp = (link_sat & LINK_SPEED_MASK);
+	switch (now_link_sp) {
+	case LINK_SPEED_2_5:
+		dev_info(rc->dev, "now link speed 2.5GT/s\n");
+	break;
+
+	case LINK_SPEED_5_0:
+		dev_info(rc->dev, "now link speed 5.0GT/s\n");
+	break;
+
+	default:
+		dev_info(rc->dev, "not support!\n");
+	break;
+	}
+
+	/* Enable ICS */
+	writel(ICS_IF_ENABLE | TRS_IF_ENABLE, base + SLA_TRANS_EN);
+
+	/* BAR , EXROM Enable */
+	writeb(EN_BAR(0) | EN_BAR(1), base + BAR_EN_REG);
+	rc->inited = 1;
+}
+
+void f_pcie_host_clear_reset(struct dme_rc *rc)
+{
+	void __iomem *base = rc->rc_cfg_base;
+	int retry_power = 20, val;
+
+	val = readl(base + PCIE_PWR_CSR) & ~PCIE_PWR_CTL_PERST;
+	do {
+		writel(val, base + PCIE_PWR_CSR);
+		val = readl(base + PCIE_PWR_CSR) & PCIE_PWR_CTL_PERST;
+		if (!val)
+			return;
+
+		usleep_range(100, 500);
+		retry_power--;
+	} while (retry_power > 0);
+
+	dev_err(rc->dev, "can't clear PCIE_PWR_CTL_PERST bit\n");
+}
+
+void f_pcie_host_enable_cmd(void __iomem *base)
+{
+	u32 cmd;
+
+	cmd = readw(base + PCI_COMMAND);
+	cmd |= PCI_COMMAND_IO;
+	cmd |= PCI_COMMAND_MEMORY;
+	cmd |= PCI_COMMAND_MASTER;
+	writew(cmd, base + PCI_COMMAND);
+}
+
+int f_pcie_host_rd_conf(struct dme_rc *rc, struct pci_bus *bus,
+			u32 devfn, int where, int size, u32 *val)
+{
+	void __iomem *reg = NULL;
+	int stat, addr, times = 50;
+
+	/* root cfg */
+	reg = rc->rc_cfg_base;
+
+	/* device cfg */
+	if (bus->number > rc->root_bus_nr) {
+		reg = rc->ep_cfg_base;
+
+		addr = BUS_NUM(bus->number) | DEV_NUM(PCI_SLOT(devfn)) |
+				FUN_NUM(PCI_FUNC(devfn));
+
+		if (bus->number == rc->root_bus_nr + 1)
+			addr |= CFG_TLP_TYPE(TYPE0_ISSUE);
+		else
+			addr |= CFG_TLP_TYPE(TYPE1_ISSUE);
+
+		writel(addr, rc->rc_cfg_base + SLA_CONF_TAR_ID(0));
+	}
+
+	if (bus->number != rc->root_bus_nr) {
+		do {
+			stat = readb(rc->rc_cfg_base + SLA_IO_CFG_STA) & CFG_BZ;
+			if (!stat)
+				break;
+			times--;
+			usleep_range(100, 500);
+		} while (times >= 0);
+
+		if (times < 0) {
+			dev_err(rc->dev, "PCIBIOS_SET_FAILED:Timeout!\n");
+			return PCIBIOS_SET_FAILED;
+		}
+	}
+
+	switch (size) {
+	case 1:
+	*val = readb(reg + where);
+	break;
+
+	case 2:
+	*val = readw(reg + where);
+	break;
+
+	case 4:
+	*val = readl(reg + where);
+	break;
+
+	default:
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+	}
+
+	stat = readl(rc->rc_cfg_base + SLA_IO_CFG_INT_ST);
+	writel(IO_CFG_INT_ST_CLR, rc->rc_cfg_base + SLA_IO_CFG_INT_ST);
+
+	if (stat & CFG_RD_UNSUPPORTED_REQ && where == PCI_VENDOR_ID)
+		*val = 0;
+	else if (stat & 0xff)
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+
+	return PCIBIOS_SUCCESSFUL;
+}
+
+int f_pcie_host_wr_conf(struct dme_rc *rc, struct pci_bus *bus,
+			u32 devfn, int where, int size, u32 val)
+{
+	int stat, addr, ret = PCIBIOS_SUCCESSFUL, times = 20;
+	void __iomem *reg = NULL;
+
+	 /* root cfg */
+	reg = rc->rc_cfg_base;
+
+	 /* device cfg */
+	if (bus->number > rc->root_bus_nr) {
+		reg = rc->ep_cfg_base;
+
+		addr = BUS_NUM(bus->number) | DEV_NUM(PCI_SLOT(devfn)) |
+				FUN_NUM(PCI_FUNC(devfn));
+
+		if (bus->number == rc->root_bus_nr + 1)
+			addr |= CFG_TLP_TYPE(TYPE0_ISSUE);
+		else
+			addr |= CFG_TLP_TYPE(TYPE1_ISSUE);
+
+		writel(addr, rc->rc_cfg_base + SLA_CONF_TAR_ID(0));
+	}
+
+	if (bus->number != rc->root_bus_nr) {
+		do {
+			stat = readb(rc->rc_cfg_base + SLA_IO_CFG_STA) & CFG_BZ;
+			if (!stat)
+				break;
+			times--;
+			usleep_range(100, 500);
+		} while (times >= 0);
+
+		if (times < 0) {
+			dev_err(rc->dev, "PCIBIOS_SET_FAILED:Timeout!\n");
+			return PCIBIOS_SET_FAILED;
+		}
+	}
+
+	writel(IO_CFG_INT_ST_CLR, rc->rc_cfg_base + SLA_IO_CFG_INT_ST);
+
+	switch (size) {
+	case 1:
+	writeb(val, reg + where);
+	ret = PCIBIOS_SUCCESSFUL;
+	break;
+
+	case 2:
+	writew(val, reg + where);
+	ret = PCIBIOS_SUCCESSFUL;
+	break;
+
+	case 4:
+	writel(val, reg + where);
+	ret = PCIBIOS_SUCCESSFUL;
+	break;
+
+	default:
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+	}
+
+	stat = readl(rc->rc_cfg_base + SLA_IO_CFG_INT_ST);
+	writel(IO_CFG_INT_ST_CLR, rc->rc_cfg_base + SLA_IO_CFG_INT_ST);
+	if (stat)
+		return PCIBIOS_FUNC_NOT_SUPPORTED;
+
+	return ret;
+}
+
+static void f_pci_process_bridge_of_ranges(struct dme_rc *rc,
+			struct device_node *dev_node, int primary)
+{
+	struct resource res, *res_sav;
+	struct of_pci_range_parser parser;
+	struct of_pci_range range;
+	int memno = 0, iono = 0;
+
+	if (of_pci_range_parser_init(&parser, dev_node)) {
+		dev_err(rc->dev, "missing \"ranges\" property\n");
+		return;
+	}
+
+	for_each_of_pci_range(&parser, &range) {
+		of_pci_range_to_resource(&range, dev_node, &res);
+
+		switch (res.flags & IORESOURCE_TYPE_BITS) {
+		case IORESOURCE_IO:
+			if (iono >= IO_MEM_RCS_NUM)
+				continue;
+
+			rc->io_offset[iono] = range.cpu_addr;
+			res_sav = &rc->io_res[iono];
+
+			if (res_sav != NULL) {
+				rc->io_num = iono++;
+				memcpy(res_sav, &res, sizeof(res));
+			}
+			break;
+
+		case IORESOURCE_MEM:
+			if (memno >= IO_MEM_RCS_NUM)
+				continue;
+
+#ifndef CONFIG_ARM_LPAE
+			if ((range.cpu_addr >> 32))
+				continue;
+#endif
+			rc->mem_offset[memno] = range.pci_addr;
+			res_sav = &rc->mem_res[memno];
+
+			if (res_sav != NULL) {
+				rc->mem_num = memno++;
+				memcpy(res_sav, &res, sizeof(res));
+			}
+		break;
+
+		case DT_CONF_SPACE:
+			memcpy(&rc->cfg_res, &res, sizeof(res));
+
+		break;
+		default:
+		break;
+		}
+	}
+}
+
+static int f_pcie_host_setup(int nr, struct pci_sys_data *sys)
+{
+	struct f_pcie_port *dme_port = NULL;
+	struct dme_rc *rc = NULL;
+	int memno = 0, port_num;
+
+	for (port_num = 0; port_num < pcie_port.pcie_por_num; port_num++) {
+		dme_port = f_get_pcie_port(port_num);
+		if (!dme_port->rc) {
+			continue;
+		}
+		if (dme_port->rc->root_bus_nr < 0) {
+			pcie_port.nr_port_map[nr] = port_num;
+			break;
+		}
+	}
+
+	if (!dme_port->rc) {
+		pr_err("%s %d : dme_port->rc is NULL\n", __func__, __LINE__);
+		return -EINVAL;
+	}
+
+	sys->private_data = dme_port->rc;
+	rc = dme_port->rc;
+	rc->root_bus_nr = sys->busnr;
+
+	/* set_local_bus_nr */
+	writeb(sys->busnr, rc->rc_cfg_base + PCI_PRIMARY_BUS);
+
+	f_pcie_host_enable_cmd(rc->rc_cfg_base);
+
+	while (memno <= rc->mem_num) {
+		pci_add_resource_offset(&sys->resources,
+			&rc->mem_res[memno], sys->mem_offset);
+		memno++;
+	}
+
+	return 1;
+}
+
+static int f_pcie_valid_config(struct dme_rc *rc, struct pci_bus *bus, int dev)
+{
+	/*
+	 * Don't go out when trying to access nonexisting devices
+	 * on the local bus.
+	 */
+	if (bus->number == rc->root_bus_nr && dev > 0)
+		return 0;
+
+	if (bus->primary == rc->root_bus_nr && dev > 0)
+		return 0;
+
+	return 1;
+}
+
+static int f_pcie_rd_conf(struct pci_bus *bus,
+			u32 devfn, int where, int size, u32 *val)
+{
+	struct dme_rc *rc;
+	struct pci_sys_data *sys = bus->sysdata;
+	unsigned long flags;
+	int ret = 0;
+
+	rc = (struct dme_rc *)sys->private_data;
+	if (f_pcie_valid_config(rc, bus, PCI_SLOT(devfn)) == 0) {
+		*val = 0xffffffff;
+		return PCIBIOS_DEVICE_NOT_FOUND;
+	}
+
+	if (rc->link_up) {
+		spin_lock_irqsave(&rc->conf_lock, flags);
+		ret = f_pcie_host_rd_conf(rc, bus, devfn, where, size, val);
+		spin_unlock_irqrestore(&rc->conf_lock, flags);
+	}
+	return ret;
+}
+
+static int f_pcie_wr_conf(struct pci_bus *bus,
+			u32 devfn, int where, int size, u32 val)
+{
+	struct dme_rc *rc;
+	struct pci_sys_data *sys = bus->sysdata;
+	unsigned long flags;
+	int ret = 0;
+
+	rc = (struct dme_rc *)sys->private_data;
+	if (f_pcie_valid_config(rc, bus, PCI_SLOT(devfn)) == 0)
+		return PCIBIOS_DEVICE_NOT_FOUND;
+
+	if (rc->link_up) {
+		spin_lock_irqsave(&rc->conf_lock, flags);
+		ret = f_pcie_host_wr_conf(rc, bus, devfn, where, size, val);
+		spin_unlock_irqrestore(&rc->conf_lock, flags);
+	}
+	return ret;
+}
+
+static struct pci_ops pcie_ops = {
+	.read = f_pcie_rd_conf,
+	.write = f_pcie_wr_conf,
+};
+
+struct pci_bus *f_pcie_host_scan_root_bus(struct device *parent,
+			int bus, struct pci_ops *ops,
+			void *sysdata, struct list_head *resources)
+{
+	struct pci_bus *b;
+	int max, busnr = bus;
+
+	b = pci_create_root_bus(parent, bus, ops, sysdata, resources);
+	if (!b)
+		return NULL;
+
+	pci_bus_insert_busn_res(b, bus, 255);
+	max = pci_scan_child_bus(b);
+	if (max > (busnr + 1))
+		pci_rescan_bus(b);
+
+	pci_bus_update_busn_res_end(b, max);
+	pci_bus_add_devices(b);
+
+	return b;
+}
+
+static struct pci_bus *f_pcie_host_scan_bus(int nr,
+			struct pci_sys_data *sys)
+{
+	struct pci_bus *bus;
+	struct device *dev;
+	u32 nr_port;
+
+	nr_port = pcie_port.nr_port_map[nr];
+	dev = pcie_port.dme_pcie[nr_port].rc->dev;
+	if (nr_port >= pcie_port.pcie_por_num) {
+		dev_err(dev, "%s %d BUG().\n", __func__, __LINE__);
+		BUG();
+	}
+
+	bus = f_pcie_host_scan_root_bus(dev, sys->busnr, &pcie_ops,
+			sys, &sys->resources);
+
+	pcie_port.dme_pcie[nr_port].rc->root_pcibus = bus;
+	return bus;
+}
+
+static int f_pcie_host_map_irq(const struct pci_dev *dev,
+			u8 slot, u8 pin)
+{
+	struct pci_sys_data *sys = dev->bus->sysdata;
+	struct dme_rc *rc;
+	int irq;
+
+	if (dev->msi_enabled)
+		return dev->irq;
+
+	rc = (struct dme_rc *)sys->private_data;
+
+	if (pin < 1 || pin > 4) {
+		dev_warn(rc->dev, "pin number pin=%d\n", pin);
+		pin = 1;
+	}
+
+	irq = rc->msg_irq[pin - 1];
+	return irq;
+}
+
+static struct hw_pci f_pcie_host_dme_pci_ops = {
+	.setup		= f_pcie_host_setup,
+	.scan		= f_pcie_host_scan_bus,
+	.map_irq	= f_pcie_host_map_irq,
+};
+
+static irqreturn_t f_pcie_host_slave_mem_irq(int irqno, void *dev_id)
+{
+	struct dme_rc *rc = (struct dme_rc *)dev_id;
+	u32 status, err_flg;
+
+	status = readl(rc->rc_cfg_base + SLA_MEM_INT_ST);
+
+	/* clear interrupt status */
+	writel(status, rc->rc_cfg_base + SLA_MEM_INT_ST);
+
+	err_flg = MEM_RD_COMPLE_ABORT | MEM_RD_UNSUPPORT_REQ |
+		MEM_RD_COMPLE_TIMEOUT | MEM_RD_POISONED_TLP |
+		MEM_RD_TRANS_REQ;
+
+	if (status & err_flg)
+		dev_dbg(rc->dev, "0x%x\n", status);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t f_pcie_ep_intx_msi(int irq, void *dev_id)
+{
+	struct dme_ep *ep = dev_id;
+
+	pr_info("%s irq (%d) PCIE_MSI_DATA (0x%x)\n",
+		__func__, irq, readw(ep->regs + PCIE_MSI_DATA));
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t f_pcie_ep_interrupt(int irq, void *dev_id)
+{
+	struct dme_ep *ep = (struct dme_ep *)dev_id;
+	u32 dem_s, dma_s, err_flg, act_dma;
+
+	act_dma = ep->act_dma_number;
+
+	dma_s = readl(ep->regs + DMA_STATUS);
+	dma_s = ((dma_s >> (act_dma * 2)) & 0x03);
+
+	dem_s = readl(ep->regs + DEM_DMA_TRAN_SAT_CH(act_dma));
+	dem_s = dem_s & 0xFF;
+
+	err_flg = DMA_TRANS_ABORT_BY_USER | DMA_TRANS_ABORT_RESET_DET |
+			DMA_TRANS_ABORT_POISO_DET | DMA_TRANS_ABORT_BY_PCIE |
+			AXI_ERROR_DETECT | DMA_ABORT_ENDERR;
+
+	if (dem_s & err_flg)
+		dev_dbg(ep->dev, "0x%x\n", dem_s);
+
+	if (dma_s == DMA_NORMAL && dem_s == TRANS_COMPLETE) {
+		ep->dma_done = SAT_DMA_DONE;
+		wake_up_interruptible(&ep->dma_wait);
+	}
+
+	return IRQ_HANDLED;
+}
+
+int f_pcie_host_enable_msi(struct dme_rc *rc)
+{
+	int err = 0;
+#ifdef CONFIG_IOMMU_API
+	struct dma_iommu_mapping *mapping;
+	unsigned long iova;
+	int port = IOMMU_READ | IOMMU_WRITE | IOMMU_EXEC;
+	phys_addr_t paddr;
+#endif
+
+	if (!rc->msi_lig)
+		return -EINVAL;
+
+	writel((u32)rc->msi_data, rc->rc_cfg_base + PCIE_MSI_ADDR_LO);
+	writel(0, rc->rc_cfg_base + PCIE_MSI_ADDR_HI);
+
+#ifdef CONFIG_IOMMU_API
+	mapping = rc->dev->archdata.mapping;
+	if (mapping) {
+		paddr = (dma_addr_t)rc->msi_data & ~(SZ_4K - 1);
+		iova = (unsigned long)rc->msi_data & ~(SZ_4K - 1);
+		do {
+			err = iommu_iova_to_phys(mapping->domain, iova);
+			if (err <= 0) {
+				/* no one mapping this address */
+				break;
+			} else {
+				/* find next address */
+				iova += SZ_1M;
+				if (iova > SZ_1G) {
+					dev_err(rc->dev,
+						"can't mapping iova (0x%llx)of MSI data\n",
+						(u64)iova);
+					return -EINVAL;
+				}
+			}
+		} while (err > 0);
+
+		/* no one mapping this iova */
+		err = iommu_map(mapping->domain, iova, paddr, SZ_4K, port);
+		if (err < 0) {
+			dev_err(rc->dev, "LIG address mapping fail\n");
+			return err;
+		}
+
+		rc->msi_iova = iova;
+		rc->msi_iova_sz = SZ_4K;
+	}
+#endif
+
+	return err;
+}
+
+static void f_pcie_ep_dma_start(void __iomem *dma_reg, u32 dma_ch)
+{
+	u32 dma_set;
+
+	dma_set = DMA_START | DMA_NUM(dma_ch);
+	writeb(dma_set, dma_reg + DMAC_INSTR);
+}
+
+static void f_pcie_ep_dma_stop(void __iomem *dma_reg, u32 dma_ch)
+{
+	u32 dma_set;
+
+	dma_set = DMA_STOP | DMA_INT_CLR | DMA_NUM(dma_ch);
+	writel(dma_set, dma_reg + DMAC_INSTR);
+}
+
+static void f_pcie_ep_demand_dma_init(void __iomem *dma_reg,
+		u16 dma_ch, u32 dma_read_write,
+		dma_addr_t pcie_addr, dma_addr_t axi_addr, u32 dma_size)
+{
+	u32 dem_dma_set;
+
+	/* DMA channel 0~15 as demand */
+	writel(0, dma_reg + DMA_MODE_CONL);
+
+	/* Set PCIe Address */
+	writel(pcie_addr, dma_reg + DEM_DMA_PCIE_L_ADDR_CH(dma_ch));
+	writel(0, dma_reg + DEM_DMA_PCIE_U_ADDR_CH(dma_ch));
+
+	/* Set AXI Address */
+	writel(axi_addr, dma_reg + DEM_DMA_AXI_ADDR_CH(dma_ch));
+
+	/* Set TRANS Size */
+	writel(dma_size, dma_reg + DEM_DMA_TRANS_SZ_CH(dma_ch));
+
+	/* Demand DMA Transfer Setting */
+	dem_dma_set = MULTI_R_REQ_1_MULTI | dma_read_write |
+			DMA_TRAN_SPL_SZ(128);
+	writel(dem_dma_set, dma_reg + DEM_DMA_TRANS_SET_CH(dma_ch));
+}
+
+void f_pcie_ep_init(struct dme_ep *ep)
+{
+	u32 dl_sat, addr, res_mask, msi_multi;
+
+	/* device DL write enable */
+	dl_sat = readl(ep->regs + BRIDGE_MODE) | DL_WRITE_MODE;
+	writel(dl_sat, ep->regs + BRIDGE_MODE);
+
+	/* init pcie device class code */
+	writew(PCI_CLASS_MEMORY_RAM, ep->regs + PCI_CLASS_DEVICE);
+
+	/*
+	 * The numbers possessed by MSI Table are indicated
+	 * S73 supply (1-16)
+	 * 000h: 1 / 100: 16
+	 */
+	if (of_property_read_u32(ep->dev->of_node, "msi-multi",
+		&msi_multi)) {
+		dev_warn(ep->dev, "Missing msi-multi in dt\n");
+		msi_multi = 0;
+	}
+
+	res_mask = readb(ep->regs + PCIE_MSI_CTL);
+	res_mask &= ~0x0e;
+	res_mask |= (msi_multi << 1);
+	writeb(res_mask, ep->regs + PCIE_MSI_CTL);
+
+	/*
+	 * MSI-X table size
+	 * 000h: 1
+	 * 7FFh: 2048
+	 */
+	res_mask = (readw(ep->regs + PCIE_MSIX_CTL) & ~(0x7ff));
+	writeb(res_mask, ep->regs + PCIE_MSIX_CTL);
+
+	/*
+	 * MSI-X
+	 * setting  MSIX address table offset in 0x1000 of base 0
+	 * setting  MSIX Pending table offset in 0x1008 of base 0
+	 */
+	writel(0x1000, ep->regs + PCIE_MSIX_TABLE);
+	writel(0x1008, ep->regs + PCIE_MSIX_PBA);
+
+	/* device DL write disenable */
+	dl_sat = dl_sat & ~DL_WRITE_MODE;
+	writel(dl_sat, ep->regs + BRIDGE_MODE);
+
+	/* addr = 0xFFF80000; */ /* 512K DDR */
+	/* addr = 0xFFFE0000; */ /* 128K DDR */
+	/* addr = 0xFFFF0000; */ /* 64K DDR */
+	addr = 0xfff80000;
+	dl_sat = addr | BAR_SET_MEMORY_TYPE_32BIT | BAR_SET_MEM_SPACE;
+
+	/* BAR 0 enable  */
+	writel(dl_sat, ep->regs + BAR_RES_SET(0));
+
+	/* Only enabled BAR0 */
+	writel((EROM_EN | BAR_EN(0)), ep->regs + BAR_EN_REG);
+
+	/* AXI Master Error Interrupt Mask Register */
+	writel(0, ep->regs + AXI_MASTER_ERR_INT_MASK);
+	writel(0, ep->regs + DMA_INT_MASK);
+
+	/* AXI Slave Memory Interrupt Mask */
+	writel(0xff, ep->regs + SLA_MEM_INT_MASK);
+}
+
+static void f_pcie_ep_init_pcie_device(struct dme_ep *ep)
+{
+	/* HSIO DDR RAM BAR0 remap */
+	writel(ep->remap_addr_phys, ep->regs + AXI_MASTER_REMAP_ADDR(0));
+	writel(0x0, ep->regs + AXI_MASTER_REMAP_ADDR(1));
+	writel(0x0, ep->regs + AXI_MASTER_REMAP_ADDR(2));
+	writel(0x0, ep->regs + AXI_MASTER_REMAP_ADDR(3));
+	writel(0x0, ep->regs + AXI_MASTER_REMAP_ADDR(4));
+	writel(0x0, ep->regs + AXI_MASTER_REMAP_ADDR(5));
+}
+
+void f_desc_table_release(struct device *dev, struct desc_tab *tab,
+			u32 size)
+{
+	dma_free_coherent(dev, size, tab->pcie_buf, tab->pcie_lower_addr);
+	dma_free_coherent(NULL, size, tab->axi_buf, tab->axi_addr);
+	dma_free_coherent(NULL, size, tab->dma_tab, tab->dma_table_phys);
+}
+
+int f_desc_tab_init(struct dme_ep *ep, struct desc_tab *tab,
+			struct desc_tab *next_tab, u32 size)
+{
+	dma_addr_t buf_phys, buf2_phys, table_phys;
+	u32 *buf = NULL, *buf2 = NULL, *table = NULL;
+	u32 traf_set;
+
+	buf = dma_alloc_coherent(NULL, size, &buf_phys, GFP_KERNEL);
+	if (!buf) {
+		dev_err(ep->dev, "dma_alloc_coherent fail\n");
+		return -ENOMEM;
+	}
+
+	buf2 = dma_alloc_coherent(ep->dev, size, &buf2_phys, GFP_KERNEL);
+	if (!buf2) {
+		dev_err(ep->dev, "dma_alloc_coherent fail\n");
+		return -ENOMEM;
+	}
+
+	table = dma_alloc_coherent(NULL, size, &table_phys, GFP_KERNEL);
+	if (!table) {
+		dev_err(ep->dev, "dma_alloc_coherent fail\n");
+		return -ENOMEM;
+	}
+
+	tab->pcie_lower_addr = buf2_phys;
+	tab->pcie_upper_addr = 0;
+	tab->pcie_buf = buf2;
+	tab->axi_addr = buf_phys;
+	tab->axi_buf = buf;
+	tab->traf_size = size;
+
+	traf_set = MULTI_R_REQ_1_MULTI | DMA_TRAN_SPL_SZ(128);
+
+	tab->traf_set = traf_set;
+	tab->dma_tab = table;
+	tab->dma_table_phys = table_phys;
+	tab->next_tab_addr = 0;
+	tab->descr_cont = 0;
+
+	/* Set PCIe Address */
+	writel(tab->pcie_lower_addr, tab->dma_tab + DES_DMA_TB_L_ADDR);
+	writel(tab->pcie_upper_addr, tab->dma_tab + DES_DMA_TB_U_ADDR);
+
+	/* Set AXI Address */
+	writel(tab->axi_addr, tab->dma_tab + DES_DMA_TB_AXI_ADDR);
+
+	/* Set TRANS Size */
+	writel(tab->traf_size, tab->dma_tab + DES_DMA_TB_TRANS_SZ);
+
+	/* DMA Transfer Setting */
+	traf_set = 0;
+	writel(tab->traf_set, tab->dma_tab + DES_DMA_TB_TRANS_SET);
+
+	/* Descriptor DMA Transfer Table Address */
+	if (next_tab)
+		writel(next_tab->dma_tab, tab->dma_tab + DES_DMA_TB_ADDR_SET);
+
+	if (next_tab)
+		traf_set = 0 << DES_DMA_LAST_DESC;
+	else
+		traf_set = 1 << DES_DMA_LAST_DESC;
+
+	/* Last Descriptor & Descriptor ID */
+	traf_set |= tab->descr_cont << DES_DMA_DESC_ID;
+	writel(traf_set, tab->dma_tab + DES_DMA_TB_LDID_SET);
+
+	return 0;
+}
+
+static int f_pcie_ep_descr_dma_test(
+struct dme_ep *ep, u32 size)
+{
+	struct desc_tab descr_tab, descr_tab_next;
+	u32 i, j, k, err, dma_ch = 0, test_err = 0;
+	u32 axi_buf , pcie_buf, in_out_f;
+	u32 *buf1, *buf2, dma_type_rw = 2;
+
+	for (i = 0; i < dma_type_rw; i++) {
+		for (k = 0; k < TEST_NUM; k++) {
+			if (i == 0) {
+				axi_buf = 0;
+				in_out_f = DMA_TRANS_PCIE2AXI;
+				pcie_buf = test_patten[k];
+			} else {
+				pcie_buf = 0;
+				in_out_f = DMA_TRANS_AXI2PCIE;
+				axi_buf = test_patten[k];
+			}
+
+			f_desc_tab_init(ep, &descr_tab_next, NULL, size);
+			f_desc_tab_init(ep, &descr_tab, &descr_tab_next, size);
+
+			/* Set DMA descript mode */
+			writel(0xff, ep->regs + DMA_MODE_CONL);
+
+			/* Set first DMA descript table */
+			writel(descr_tab.dma_tab, ep->regs + TAB_ADDR(dma_ch));
+
+			/* Init AXI buffer */
+			memset(descr_tab.axi_buf, (u8)axi_buf, size);
+			memset(descr_tab_next.axi_buf, (u8)axi_buf, size);
+
+			/* Init PCIe buffer */
+			memset(descr_tab.pcie_buf, (u8)pcie_buf, size);
+			memset(descr_tab_next.pcie_buf, (u8)pcie_buf, size);
+
+			/* init wait flag */
+			ep->dma_done = SAT_DMA_WAIT;
+
+			/* DMA ch start */
+			f_pcie_ep_dma_start(ep->regs, dma_ch);
+
+			/* Wait DMA Interrupt */
+			err = wait_event_interruptible_timeout(ep->dma_wait,
+					(ep->dma_done == SAT_DMA_DONE),
+					ep->dma_wait_timeout);
+
+			if (!err)
+				dev_err(ep->dev,
+					"Desc DAM ch %d timeout\n", dma_ch);
+
+			/* DMA ch stop */
+			f_pcie_ep_dma_stop(ep->regs, dma_ch);
+
+			buf1 = descr_tab.pcie_buf;
+			buf2 = descr_tab.axi_buf;
+			for (j = 0; j < (size / 4); j++)
+				if (buf1[j] != buf2[j])
+					test_err++;
+
+			buf1 = descr_tab_next.pcie_buf;
+			buf2 = descr_tab_next.axi_buf;
+			for (j = 0; j < (size / 4); j++)
+				if (buf1[j] != buf2[j])
+					test_err++;
+
+			f_desc_table_release(ep->dev, &descr_tab, size);
+			f_desc_table_release(ep->dev, &descr_tab_next, size);
+		}
+	}
+
+	if (test_err > 0)
+		return  -EINVAL;
+
+	return 0;
+}
+
+static int
+f_pcie_ep_demand_dma_test(struct dme_ep *ep, u32 size)
+{
+	dma_addr_t buf_phys, buf2_phys;
+	u32 axi_buf , pcie_buf, in_out_f, dma_type_rw, type_data;
+	u32 *buf = NULL, *buf2 = NULL, j, err, dma_ch, test_err = 0;
+
+	buf = dma_alloc_coherent(NULL, size, &buf_phys, GFP_KERNEL);
+	if (!buf) {
+		dev_err(ep->dev, "dma_alloc fail\n");
+		return -ENOMEM;
+	}
+
+	buf2 = dma_alloc_coherent(ep->dev, size, &buf2_phys, GFP_KERNEL);
+	if (!buf2) {
+		dev_err(ep->dev, "dma_alloc fail\n");
+		return -ENOMEM;
+	}
+
+	for (dma_ch = 0; dma_ch < DMXH_DMA_CH; dma_ch++) {
+		ep->act_dma_number = dma_ch;
+
+		for (dma_type_rw = 0; dma_type_rw < 2; dma_type_rw++) {
+			ep->dma_type = DMA_TYPE_DEMAND;
+
+			if (dma_type_rw == 0)
+				in_out_f = DMA_TRANS_PCIE2AXI;
+			else
+				in_out_f = DMA_TRANS_AXI2PCIE;
+
+			f_pcie_ep_demand_dma_init(ep->regs, dma_ch, in_out_f,
+						buf2_phys, buf_phys, size);
+
+			for (type_data = 0; type_data < TEST_NUM; type_data++) {
+				if (in_out_f == DMA_TRANS_PCIE2AXI) {
+					pcie_buf = test_patten[type_data];
+					axi_buf = 0;
+				} else {
+					pcie_buf = 0;
+					axi_buf = test_patten[type_data];
+				}
+
+				/* init buf */
+				memset(buf2, pcie_buf, size);
+				memset(buf, axi_buf, size);
+				ep->dma_done = SAT_DMA_WAIT;
+
+				/* DMA ch start */
+				f_pcie_ep_dma_start(ep->regs, dma_ch);
+
+				err = wait_event_interruptible_timeout(ep->dma_wait,
+						(ep->dma_done == SAT_DMA_DONE),
+						ep->dma_wait_timeout);
+				if (!err)
+					dev_err(ep->dev,
+						"DAM ch %d timeout\n", dma_ch);
+
+				/* DMA ch stop */
+				f_pcie_ep_dma_stop(ep->regs, dma_ch);
+
+				/* data compare */
+				for (j = 0; j < size / 4; j++) {
+					pcie_buf = buf2[j];
+					axi_buf = test_patten[type_data];
+
+					if (axi_buf != pcie_buf)
+						test_err++;
+				}
+			}
+		}
+	}
+
+	/* DMA memory free */
+	 dma_free_coherent(NULL, size, buf, buf_phys);
+	 dma_free_coherent(ep->dev, size, buf2, buf2_phys);
+
+	if (test_err > 0)
+		return  -EINVAL;
+
+	return 0;
+}
+
+static int f_pcie_ep_mem_test(struct dme_ep *pdme_ep, u32 size)
+{
+	u32 i, j, test_w, test_r, err_cont = 0;
+
+	/* Memory read/write test */
+	for (j = 0; j < 4; j++) {
+		for (i = 0; i < size; i = i + 4) {
+			switch (j) {
+			case 0:
+				test_w = 0x00005555;
+			break;
+			case 1:
+				test_w = 0x55550000;
+			break;
+			case 2:
+				test_w = 0x0000FFFF;
+			break;
+			case 3:
+				test_w = 0xFFFF0000;
+			break;
+			default:
+			break;
+			}
+
+			writel(test_w, pdme_ep->pcie_addr + i);
+			schedule_timeout(WAIT_MEM_TIMEOUT);
+			test_r = readl(pdme_ep->pcie_addr + i);
+
+			if (test_w != test_r)
+				err_cont++;
+		}
+
+		if (err_cont > 0)
+			return  -EINVAL;
+
+		err_cont = 0;
+	}
+	return 0;
+}
+
+static int f_pcie_ep_init_one(struct pci_dev *pdev,
+			const struct pci_device_id *ent)
+{
+	struct dme_ep *ep;
+	struct f_pcie_port *dme_port;
+	void __iomem *msix_tb_io;
+	int test_err = 0, err, i;
+	resource_size_t phys_addr;
+	u32 size, msi_cont, table_offset, bir;
+
+	dme_port = f_get_pcie_ep_port(ent->device);
+	if (!dme_port) {
+		dev_err(&pdev->dev, "%s EP dme_port is null\n", __func__);
+		return -EINVAL;
+	}
+	ep = dme_port->ep;
+	ep->dev = &pdev->dev;
+
+	err = pci_enable_device(pdev);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot enable PCI device\n");
+		goto err_out_disable_pdev;
+	}
+
+	err = pci_request_regions(pdev, "ep");
+	if (err) {
+		dev_err(&pdev->dev, "Cannot obtain PCI resources\n");
+		goto err_out_iounmap;
+	}
+
+	init_waitqueue_head(&ep->dma_wait);
+	ep->dma_wait_timeout = WAIT_DMA_TIMEOUT;
+	ep->dma_done = SAT_DMA_WAIT;
+	size = pci_resource_len(pdev, 0);
+
+	ep->remap_addr = kmalloc(size, GFP_KERNEL);
+	if (!ep->remap_addr) {
+		dev_err(ep->dev, "EP remap_addr fail\n");
+		goto err_out_free_res;
+	}
+
+	ep->remap_addr_phys = __pa(ep->remap_addr);
+	ep->remap_addr_len = size;
+	f_pcie_ep_init_pcie_device(ep);
+
+	pci_set_master(pdev);
+
+	ep->pcie_addr = pci_ioremap_bar(pdev, 0);
+	if (!ep->pcie_addr) {
+		dev_err(&pdev->dev, "Cannot map device registers\n");
+		err = -ENOMEM;
+		goto err_out_free_res;
+	}
+
+	/* Memory test */
+	err = f_pcie_ep_mem_test(ep, pci_resource_len(pdev, 0));
+	if (err < 0)
+		test_err++;
+
+	if (ep->int_type == INT_MSI) {
+		/* MSI */
+		err = pci_enable_msi(pdev);
+		if (err)
+			dev_warn(ep->dev, "failed to allocate MSI entry\n");
+
+	} else if (ep->int_type == INT_MSIX) {
+		/* MSI-X */
+		msi_cont = readw(ep->regs + PCIE_MSIX_CTL);
+		msi_cont = (msi_cont & PCI_MSIX_FLAGS_QSIZE) + 1;
+		ep->msix_count = msi_cont;
+
+		ep->msix_entries =
+			kmalloc((sizeof(struct msix_entry)) * ep->msix_count,
+					GFP_KERNEL);
+		if (!ep->msix_entries) {
+			dev_err(ep->dev, "Failed to allocate MSI-X entries\n");
+			return -ENOMEM;
+		}
+
+		for (i = 0; i < ep->msix_count; i++) {
+			ep->msix_entries[i].entry = i;
+			ep->msix_entries[i].vector = 0;
+		}
+
+		err = pci_enable_msix(pdev, ep->msix_entries, ep->msix_count);
+		if (err) {
+			dev_err(ep->dev, "Failed to enable MSI-X\n");
+			return -EINVAL;
+		}
+
+		pci_read_config_dword(pdev, pdev->msix_cap + PCI_MSIX_TABLE,
+				&table_offset);
+
+		bir = table_offset & PCI_MSIX_TABLE_BIR;
+		table_offset &= PCI_MSIX_TABLE_OFFSET;
+		phys_addr = pci_resource_start(pdev, bir) + table_offset;
+
+		msix_tb_io = ioremap(phys_addr,
+			ep->msix_count * PCI_MSIX_ENTRY_SIZE);
+		if (!msix_tb_io) {
+			dev_err(ep->dev, "ioremap msix_tb_io fail");
+			return -EINVAL;
+		}
+
+		/* read content of MSI-X table */
+		writel(readl(msix_tb_io + PCI_MSIX_ENTRY_LOWER_ADDR),
+			ep->regs + MSIX_ADDR_L(0));
+		writel(readl(msix_tb_io + PCI_MSIX_ENTRY_UPPER_ADDR),
+			ep->regs + MSIX_ADDR_H(0));
+		writel(readl(msix_tb_io + PCI_MSIX_ENTRY_DATA),
+			ep->regs + MSIX_DATA(0));
+
+		iounmap(msix_tb_io);
+
+		for (i = 0; i < ep->msix_count; i++) {
+			err = request_irq(ep->msix_entries[i].vector,
+					f_pcie_ep_intx_msi, 0, "EP MSI-X", ep);
+			if (err) {
+				dev_err(ep->dev, "MSI-X (%d) request fail\n",
+					ep->msix_entries[i].vector);
+				pci_disable_msix(pdev);
+				return err;
+			}
+		}
+	}
+
+	if (ep->int_type == INT_MSI || ep->int_type == INT_INTX) {
+		err = request_irq(pdev->irq, f_pcie_ep_intx_msi, IRQF_SHARED,
+				"EP INT", ep);
+		if (err) {
+			dev_err(ep->dev, "request EP INT %d failed\n", pdev->irq);
+			return err;
+		}
+	}
+
+	/* DMA TEST */
+	err = f_pcie_ep_demand_dma_test(ep, pci_resource_len(pdev, 0));
+	if (err < 0)
+		test_err++;
+
+	/* Descript DAM chain test */
+	err = f_pcie_ep_descr_dma_test(ep, pci_resource_len(pdev, 0));
+	if (err < 0)
+		test_err++;
+
+	if (test_err > 0)
+		dev_info(ep->dev, "pcie EP TEST Err !\n");
+	else
+		dev_info(ep->dev, "pcie EP TEST PASS !\n");
+
+	ep->dev->platform_data = ep;
+	pci_set_drvdata(pdev, ep);
+
+	return err;
+
+err_out_iounmap:
+	if (ep->pcie_addr)
+		iounmap(ep->pcie_addr);
+
+	if (ep->regs)
+		iounmap(ep->regs);
+
+	ep->pcie_addr = NULL;
+	ep->regs = NULL;
+
+err_out_free_res:
+	pci_release_regions(pdev);
+
+err_out_disable_pdev:
+	if (pci_is_enabled(pdev))
+		pci_disable_device(pdev);
+
+	pci_set_drvdata(pdev, NULL);
+
+	return err;
+}
+
+static void f_pcie_ep_remove_one(struct pci_dev *pdev)
+{
+	struct dme_ep *pdme_ep;
+	int i;
+
+	pdme_ep = pci_get_drvdata(pdev);
+
+	if (pdme_ep->pcie_addr) {
+		iounmap(pdme_ep->pcie_addr);
+		pdme_ep->pcie_addr = NULL;
+	}
+
+	if (pdme_ep->int_type == INT_MSIX) {
+		for (i = 0; i < pdme_ep->msix_count; i++) {
+			disable_irq(pdme_ep->msix_entries[i].vector);
+			free_irq(pdme_ep->msix_entries[i].vector, pdme_ep);
+		}
+		pci_disable_msix(pdev);
+		kfree(pdme_ep->msix_entries);
+	} else {
+		disable_irq(pdev->irq);
+		free_irq(pdev->irq, pdme_ep);
+		if (pdme_ep->int_type == INT_MSI)
+			pci_disable_msi(pdev);
+	}
+
+	kfree(pdme_ep->remap_addr);
+	pci_set_drvdata(pdev, NULL);
+	pci_release_regions(pdev);
+	pci_disable_device(pdev);
+}
+
+#ifdef CONFIG_PM
+static int f_pcie_ep_suspend(struct device *dev)
+{
+	struct pci_dev *pdev = to_pci_dev(dev);
+	struct dme_ep *pdme_ep;
+	int i;
+
+	pdme_ep = dev_get_platdata(dev);
+
+	if (pdme_ep->int_type == INT_MSIX) {
+		for (i = 0; i < pdme_ep->msix_count; i++)
+			disable_irq(pdme_ep->msix_entries[i].vector);
+	} else {
+		disable_irq(pdev->irq);
+	}
+
+	return 0;
+}
+
+static int f_pcie_ep_resume(struct device *dev)
+{
+	struct dme_ep *pdme_ep;
+	struct pci_dev *pdev = to_pci_dev(dev);
+	int err, i;
+
+	pdme_ep = dev_get_platdata(dev);
+	f_pcie_ep_init_pcie_device(pdme_ep);
+
+	/* Memory test */
+	err = f_pcie_ep_mem_test(pdme_ep, pdme_ep->remap_addr_len);
+
+	if (err < 0)
+		dev_info(pdme_ep->dev, "pcie EP TEST Err\n");
+	else
+		dev_info(pdme_ep->dev, "pcie EP TEST PASS\n");
+
+	pdme_ep = dev_get_drvdata(dev);
+
+	if (pdme_ep->int_type == INT_MSIX) {
+		for (i = 0; i < pdme_ep->msix_count; i++)
+			enable_irq(pdme_ep->msix_entries[i].vector);
+	} else {
+		enable_irq(pdev->irq);
+	}
+
+	return 0;
+}
+
+static int f_pcie_ep_runtime_resume(struct device *dev)
+{
+	return f_pcie_ep_resume(dev);
+}
+
+static int f_pcie_ep_runtime_suspend(struct device *dev)
+{
+	return f_pcie_ep_suspend(dev);
+}
+
+static const struct dev_pm_ops f_pcie_ep_pm_ops = {
+	.suspend = f_pcie_ep_suspend,
+	.resume = f_pcie_ep_resume,
+	.runtime_suspend = f_pcie_ep_runtime_suspend,
+	.runtime_resume = f_pcie_ep_runtime_resume,
+};
+
+#define F_PCIE_EP_PM_OPS (&f_pcie_ep_pm_ops)
+
+#else
+#define F_PCIE_EP_PM_OPS NULL
+#endif
+
+DEFINE_PCI_DEVICE_TABLE(f_pcie_ep_pci_tbl) = {
+	{PCI_DEVICE(0x10cf, 0x2042)},
+	{PCI_DEVICE(0x10cf, 0x2046)},
+	{PCI_DEVICE(0x10cf, 0x204f)},
+	{PCI_DEVICE(0x10cf, 0x204c)},
+	{PCI_DEVICE(0x10cf, 0x2051)},
+	{}
+};
+MODULE_DEVICE_TABLE(pci, f_pcie_ep_pci_tbl);
+
+static struct pci_driver f_pcie_ep_driver = {
+	.id_table	= f_pcie_ep_pci_tbl,
+	.probe		= f_pcie_ep_init_one,
+	.remove		= f_pcie_ep_remove_one,
+	.driver.pm	= F_PCIE_EP_PM_OPS,
+};
+
+#ifdef CONFIG_PM
+static int f_pcie_pm_suspend(struct device *dev)
+{
+	struct f_pcie_port *dme_port;
+	struct dme_rc *rc;
+	struct dme_ep *ep;
+	u32 i = 0, clk_id, dma_irq_cnt;
+
+	dme_port = dev_get_platdata(dev);
+	if (!dme_port)
+		return 0;
+
+	if (!dme_port->rc && !dme_port->ep) {
+		dev_err(dev, "rc and ep are null\n");
+		return -EINVAL;
+	}
+
+	/* ep mode */
+	if (dme_port->ep && dme_port->ep->inited) {
+		ep = dme_port->ep;
+		dma_irq_cnt = sizeof(ep->dma_irq)/sizeof(int);
+		while (i < dma_irq_cnt) {
+			if (ep->dma_irq[i] > 0)
+				disable_irq(ep->dma_irq[i]);
+			i++;
+		};
+		for (clk_id = 0; clk_id < ep->clk_num; clk_id++)
+			if (ep->clk[clk_id])
+				clk_disable_unprepare(ep->clk[clk_id]);
+
+		ep->inited = 0;
+		return 0;
+	}
+
+	if (dme_port->rc && dme_port->rc->inited) {
+		/* rc mode */
+		rc = dme_port->rc;
+		for (clk_id = 0; clk_id < rc->clk_num; clk_id++)
+			clk_disable_unprepare(rc->clk[clk_id]);
+
+		disable_irq(rc->slav_mem_irq);
+		if (rc->lig_irq)
+			disable_irq(rc->lig_irq);
+		rc->inited = 0;
+	}
+	return 0;
+}
+
+static int f_pcie_pm_resume(struct device *dev)
+{
+	struct f_pcie_port *dme_port;
+	struct dme_rc *rc;
+	struct dme_ep *ep;
+	u32 i = 0, clk_id, dma_irq_cnt;
+
+	dme_port = dev_get_platdata(dev);
+	if (!dme_port)
+		return 0;
+
+	if (!dme_port->rc && !dme_port->ep) {
+		dev_err(dev, "rc and ep are null\n");
+		return -EINVAL;
+	}
+
+	/* ep mode */
+	if (dme_port->ep && !dme_port->ep->inited) {
+		ep = dme_port->ep;
+
+		if (dme_port->wrapper) {
+			writew(EN_DMA_INT, dme_port->wrapper + ST_DMA_INT_ENB);
+			usleep_range(10, 20);
+		}
+
+		for (clk_id = 0; clk_id < ep->clk_num; clk_id++)
+			if (ep->clk[clk_id])
+				clk_prepare_enable(ep->clk[clk_id]);
+
+		dma_irq_cnt = sizeof(ep->dma_irq)/sizeof(int);
+
+		while (i < dma_irq_cnt) {
+			if (ep->dma_irq[i] > 0)
+				enable_irq(ep->dma_irq[i]);
+			i++;
+		};
+
+		/* init pcie device */
+		if (ep->loop_coned > 0)
+			f_pcie_ep_init(dme_port->ep);
+
+		ep->inited = 1;
+
+		return 0;
+	}
+
+	/* rc mode */
+	rc = dme_port->rc;
+
+	if (dme_port->wrapper) {
+		writew(EN_DMA_INT, dme_port->wrapper + ST_DMA_INT_ENB);
+		usleep_range(10, 20);
+	}
+
+	if (!rc->inited) {
+		for (clk_id = 0; clk_id < rc->clk_num; clk_id++)
+			if (rc->clk[clk_id])
+				clk_prepare_enable(rc->clk[clk_id]);
+
+		f_pcie_host_clear_reset(rc);
+		f_pcie_host_init_rc(rc);
+
+#ifdef CONFIG_PCI_MSI
+		if (!f_pcie_host_enable_msi(rc))
+			writel(SET_LIG_ENABLE, rc->msi_lig + LIG_INT_EN);
+#endif
+		enable_irq(rc->slav_mem_irq);
+		if (rc->lig_irq)
+			enable_irq(rc->lig_irq);
+	}
+
+	return 0;
+}
+
+int f_pcie_pm_start(struct device *dev)
+{
+	return f_pcie_pm_resume(dev);
+}
+
+int f_pcie_pm_stop(struct device *dev)
+{
+	return f_pcie_pm_suspend(dev);
+}
+
+static struct gpd_dev_ops gpd_dev_ops = {
+	.start = f_pcie_pm_start,
+	.stop = f_pcie_pm_stop,
+};
+
+static int f_pcie_runtime_suspend(struct device *dev)
+{
+	return f_pcie_pm_suspend(dev);
+}
+
+static int f_pcie_runtime_resume(struct device *dev)
+{
+	return f_pcie_pm_resume(dev);
+}
+#endif
+
+static irqreturn_t msi_map_intr(struct dme_rc *rc)
+{
+	void __iomem *int_clr, *int_sts, *int_req;
+	u32 i, virq, irq_id;
+
+	int_req = rc->msi_lig + LIG_INT_REQ;
+	int_clr = rc->msi_lig + LIG_INT_CLR;
+	int_sts = rc->msi_lig + LIG_INT_STS;
+
+	/* S7x support 16 INT for MSI */
+	irq_id = readl(int_sts);
+	for (i = 0; i < MSI_HW_INT_N; i++)
+		if (irq_id & 1 << i)
+			break;
+
+	irq_id = i;
+	writel((readl(int_sts)), int_clr);
+	virq = irq_find_mapping(rc->irq_domain, irq_id);
+	generic_handle_irq(virq);
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t f_pcie_lig_int(int irq, void *dev_id)
+{
+	struct dme_rc *rc = dev_id;
+	irqreturn_t r;
+
+	r = msi_map_intr(rc);
+	return r;
+}
+
+#ifdef CONFIG_PCI_MSI
+struct irq_chip f_dme_msi_irq_chip = {
+	.name = "PCI-MSI",
+	.irq_enable = unmask_msi_irq,
+	.irq_disable = mask_msi_irq,
+	.irq_mask = mask_msi_irq,
+	.irq_unmask = unmask_msi_irq,
+};
+
+static int f_pcie_msi_map(struct irq_domain *domain, unsigned int irq,
+			irq_hw_number_t hwirq)
+{
+	irq_set_chip_and_handler(irq, &f_dme_msi_irq_chip, handle_simple_irq);
+	irq_set_chip_data(irq, domain->host_data);
+	set_irq_flags(irq, IRQF_VALID);
+	return 0;
+}
+
+static const struct irq_domain_ops msi_domain_ops = {
+	.map = f_pcie_msi_map,
+};
+#endif
+
+static int f_pcie_host_probe(struct platform_device *pdev,
+			struct device_node *host_dev_n, u32 id)
+{
+	struct resource res, *res_tmp;
+	struct f_pcie_port *dme_port;
+	struct dme_rc *rc;
+	struct device_node *h_dev_n = host_dev_n;
+	const char *clk_nam[20] = {0};
+	int err;
+	u32 clk_num , clk_id = 0;
+
+	dme_port = &pcie_port.dme_pcie[id];
+	if (!dme_port->rc) {
+		dev_err(&pdev->dev, "%s dme_port->rc is null\n", __func__);
+		return -EINVAL;
+	}
+
+	rc = dme_port->rc;
+	rc->dev = &pdev->dev;
+	rc->index = id;
+	platform_set_drvdata(pdev, rc);
+	spin_lock_init(&rc->conf_lock);
+	spin_lock_init(&rc->msi_lock);
+	memset(rc->virq, 0, sizeof(u32) * S7X_MSIX_VECT);
+
+	if (of_address_to_resource(h_dev_n, 0, &res)) {
+		dev_err(&pdev->dev, "no regs resource defined\n");
+		return -ENXIO;
+	}
+
+	rc->rc_cfg_base = ioremap(res.start, resource_size(&res));
+	for (id = MSG_INT_A_D(0); id <= MSG_INT_A_D(3); id++) {
+		rc->msg_irq[id] = of_irq_to_resource(h_dev_n, id, NULL);
+		if (rc->msg_irq[id] < 0) {
+			dev_err(&pdev->dev, "can not find irq_%d\n", id);
+			err = -ENODEV;
+			goto err_unmap_regs;
+		}
+	}
+
+	if (!of_address_to_resource(rc->dev->of_node, 2, &res)) {
+		rc->msi_lig = ioremap(res.start, resource_size(&res));
+		rc->msi_data = res.start;
+	} else {
+		dev_info(rc->dev, "MSI PCIE_LIG not supply\n");
+	}
+
+	rc->slav_mem_irq = of_irq_to_resource(h_dev_n, SLA_MEM_INT, NULL);
+	if (rc->slav_mem_irq < 0) {
+		dev_err(&pdev->dev, "no slav_mem_irq ID\n");
+		err = -ENODEV;
+		goto err_unmap_regs;
+	}
+
+	err = request_irq(rc->slav_mem_irq, f_pcie_host_slave_mem_irq,
+				IRQF_TRIGGER_RISING, "PCIe slave_memory", rc);
+	if (err != 0) {
+		dev_err(&pdev->dev, "slav_mem_irq:request_irq fail\n");
+		err = -ENODEV;
+		goto err_unmap_regs;
+	}
+
+	if (rc->msi_lig) {
+		rc->lig_irq = of_irq_to_resource(rc->dev->of_node, PCIE_WAP_LIG_INT,
+			NULL);
+		if (rc->lig_irq > 0) {
+			err = request_irq(rc->lig_irq, f_pcie_lig_int,
+				IRQF_TRIGGER_RISING, "PCIe LIG", rc);
+			if (err)
+				return -EINVAL;
+		}
+	}
+
+	if (of_property_read_u32(h_dev_n, "clock_num", &clk_num)) {
+		dev_err(&pdev->dev, "Missing \"clk_num\" in dt\n");
+		return -EINVAL;
+	}
+
+	rc->clk = kmalloc(clk_num * sizeof(struct clk *), GFP_KERNEL);
+	memset(rc->clk, 0, clk_num * sizeof(struct clk *));
+	rc->clk_num = clk_num;
+
+	while (1) {
+		of_property_read_string_index(h_dev_n,
+			"clock-names", clk_id, &clk_nam[clk_id]);
+
+		if (clk_nam[clk_id] == NULL)
+			break;
+
+		rc->clk[clk_id] = of_clk_get(h_dev_n, clk_id);
+		if (IS_ERR(rc->clk[clk_id])) {
+			dev_err(&pdev->dev, "clock%d not found.\n", clk_id);
+			err = PTR_ERR(rc->clk[clk_id]);
+			goto err_unmap_regs;
+		}
+
+		err = clk_prepare_enable(rc->clk[clk_id]);
+		if (err)
+			goto err_fail;
+
+		clk_id++;
+	}
+
+	rc->root_bus_nr = -1;
+	memset(rc->mem_res, 0, sizeof(rc->mem_res));
+	memset(rc->io_res, 0, sizeof(rc->io_res));
+
+	f_pci_process_bridge_of_ranges(rc, h_dev_n, 1);
+	res_tmp = &rc->cfg_res;
+	rc->ep_cfg_base = ioremap(res_tmp->start, resource_size(res_tmp));
+
+	/* PCIe will not link up until clears the PERSET# bit */
+	f_pcie_host_clear_reset(rc);
+	f_pcie_host_init_rc(rc);
+	f_pcie_dev_set_platdata(&pdev->dev, dme_port);
+	rc->nvect = 0;
+
+#ifdef CONFIG_PCI_MSI
+	err = f_pcie_host_enable_msi(rc);
+	rc->irq_domain = irq_domain_add_linear(pdev->dev.of_node,
+		S7X_MSIX_VECT, &msi_domain_ops, NULL);
+	if (!rc->irq_domain) {
+		dev_err(rc->dev, "Failed to get a MSI IRQ domain\n");
+		return PTR_ERR(rc->irq_domain);
+	}
+	/* enable local interrupt for MSI */
+	if (!err)
+		writel(SET_LIG_ENABLE, rc->msi_lig + LIG_INT_EN);
+#endif
+	return 0;
+
+err_fail:
+	for (clk_id = 0; clk_id < clk_num; clk_id++)
+		clk_put(rc->clk[clk_id]);
+
+err_unmap_regs:
+	iounmap(rc->rc_cfg_base);
+	iounmap(rc->ep_cfg_base);
+	kfree(rc->clk);
+	return err;
+}
+
+static int f_pcie_ep_probe(struct platform_device *pdev,
+			struct device_node *ep_dev_n, u32 index)
+{
+	struct resource res;
+	struct f_pcie_port *dme_port;
+	struct dme_ep *pdme_ep;
+	const char *clk_nam[20] = {0};
+	u32 i = 0, j = 0, err = 0, clk_num = 0, clk_id;
+	int irq = 0, int_type = 0, dma_int_off;
+
+	dme_port = &pcie_port.dme_pcie[index];
+	if (!dme_port->ep) {
+		dev_err(&pdev->dev, "dme_port->ep is null\n");
+		return -EINVAL;
+	}
+
+	pdme_ep = dme_port->ep;
+	pdme_ep->dev = &pdev->dev;
+	pdme_ep->index = index;
+
+	if (of_property_read_u32(ep_dev_n, "int_type", &int_type))
+		int_type = 0;
+
+	pdme_ep->int_type = int_type;
+
+	/* vendor ID */
+	if (of_property_read_u32_index(ep_dev_n, "ven_dev_id", 0,
+				&pdme_ep->vendor_id)) {
+		dev_err(&pdev->dev, "Missing vendor_id in dt\n");
+		return -EINVAL;
+	}
+
+	/* device ID */
+	if (of_property_read_u32_index(ep_dev_n, "ven_dev_id", 1,
+				&pdme_ep->device_id)) {
+		dev_err(&pdev->dev, "Missing device_id in dt\n");
+		return -EINVAL;
+	}
+
+	/* clock */
+	if (of_property_read_u32(ep_dev_n, "clock_num", &clk_num)) {
+		dev_err(&pdev->dev, "Missing clk_num in dt\n");
+		return -EINVAL;
+	}
+
+	pdme_ep->clk_num = clk_num;
+	pdme_ep->clk = devm_kzalloc(&pdev->dev,
+			clk_num * sizeof(struct clk *), GFP_KERNEL);
+	if (pdme_ep->clk == NULL) {
+		dev_err(&pdev->dev, "pdme_ep->clk alloc fail!\n");
+		return -EINVAL;
+	}
+
+	clk_id = 0;
+	while (1) {
+		of_property_read_string_index(ep_dev_n,
+			"clock-names", clk_id, &clk_nam[clk_id]);
+
+		if (clk_nam[clk_id]) {
+			pdme_ep->clk[clk_id] = of_clk_get(ep_dev_n, clk_id);
+
+			if (IS_ERR(pdme_ep->clk[clk_id])) {
+				dev_err(&pdev->dev,
+					"clock%d not found.\n", clk_id);
+				err = PTR_ERR(pdme_ep->clk[clk_id]);
+				goto err_unmap_regs;
+			}
+
+			err = clk_prepare_enable(pdme_ep->clk[clk_id]);
+			if (err)
+				goto err_fail;
+
+		} else {
+			break;
+		}
+
+		clk_id++;
+	}
+
+	if (of_address_to_resource(ep_dev_n, 0, &res)) {
+		dev_err(&pdev->dev, "no regs resource defined\n");
+		return -ENXIO;
+	}
+
+	if (of_property_read_u32_index(ep_dev_n, "dma_int_off", 0,
+				&dma_int_off)) {
+		dev_err(&pdev->dev, "Missing dma_int_off in dt\n");
+		return -EINVAL;
+	}
+
+	pdme_ep->regs = ioremap(res.start, resource_size(&res));
+	i = dma_int_off;
+	while (j < DMA_INT_NUM) {
+		irq = of_irq_to_resource(ep_dev_n, i, NULL);
+		if (irq) {
+			pdme_ep->dma_irq[j] = irq;
+			err = request_irq(pdme_ep->dma_irq[j],
+					f_pcie_ep_interrupt,
+					IRQF_TRIGGER_RISING,
+					"PCIe Endpoint DMA IRQ", (struct dme_ep *)pdme_ep);
+			if (err) {
+				dev_err(&pdev->dev, "request_irq err\n");
+				goto err_unmap_regs;
+			}
+		} else {
+			break;
+		}
+		i++;
+		j++;
+	}
+
+	/* init pcie device */
+	if (pdme_ep->loop_coned)
+		f_pcie_ep_init(dme_port->ep);
+
+	pdme_ep->inited = 1;
+	f_pcie_dev_set_platdata(&pdev->dev, dme_port);
+	return 0;
+
+err_unmap_regs:
+	if (pdme_ep->regs) {
+		iounmap(pdme_ep->regs);
+		pdme_ep->regs = NULL;
+	}
+
+err_fail:
+	if (pdme_ep->clk)
+		devm_kfree(&pdev->dev, pdme_ep->clk);
+
+	return err;
+}
+
+int pcie_init(void)
+{
+	struct dme_ep *pdme_ep;
+	struct f_pcie_port *dme_port;
+	int err = 0, port_num = 0;
+	char devname[EP_NAME_LEN];
+
+	f_pcie_host_dme_pci_ops.nr_controllers = pcie_port.rc_cnt;
+	pci_common_init(&f_pcie_host_dme_pci_ops);
+
+	while (port_num < pcie_port.pcie_por_num) {
+		dme_port = f_get_pcie_port(port_num);
+		if (!dme_port) {
+			port_num++;
+			continue;
+		}
+
+		if (dme_port->ep && dme_port->ep->loop_coned) {
+			pdme_ep = dme_port->ep;
+			snprintf(devname, EP_NAME_LEN, "%s-ep",
+				dev_name(dme_port->ep->dev));
+			f_pcie_ep_driver.name = devname;
+			err = pci_register_driver(&f_pcie_ep_driver);
+			if (err < 0)
+				dev_err(pdme_ep->dev, "pci_register_driver err!\n");
+		} else {
+			if (dme_port->ep)
+				dev_info(dme_port->ep->dev, "no loop cable!\n");
+		}
+		port_num++;
+	};
+
+	return 0;
+}
+
+void pcie_uninit(void)
+{
+	struct f_pcie_port *dme_port;
+	int port_num = 0;
+
+	while (port_num < pcie_port.pcie_por_num) {
+		dme_port = f_get_pcie_port(port_num);
+		if (!dme_port) {
+			port_num++;
+			continue;
+		}
+
+		if (dme_port->ep && dme_port->ep->loop_coned)
+			pci_unregister_driver(&f_pcie_ep_driver);
+
+		kfree(pcie_port.dme_pcie[port_num].ep);
+		kfree(pcie_port.dme_pcie[port_num].rc);
+		pcie_port.dme_pcie[port_num].ep = NULL;
+		pcie_port.dme_pcie[port_num].rc = NULL;
+		port_num++;
+	};
+
+	kfree(pcie_port.dme_pcie);
+	kfree(pcie_port.nr_port_map);
+	pcie_port.dme_pcie = NULL;
+	pcie_port.pcie_por_num = 0;
+	pcie_port.rc_cnt = 0;
+}
+
+static int f_pcie_probe(struct platform_device *pdev)
+{
+	struct resource wrap;
+	struct f_pcie_port *dme_port;
+	struct pcie_pro *sysoc_pcie;
+	struct device_node *np_pm, *dev_np;
+	struct device *dev = &pdev->dev;
+	struct device *parent_dev;
+	unsigned long flags;
+	int r = 0;
+	size_t size;
+	u32 index;
+
+	parent_dev = pdev->dev.parent;
+	sysoc_pcie = dev_get_drvdata(parent_dev);
+	if (!sysoc_pcie)
+		return -EINVAL;
+
+	pcie_port.pcie_por_num++;
+	dev_np = pdev->dev.of_node;
+	index = sysoc_pcie->id;
+	dme_port = pcie_port.dme_pcie;
+	if (!dme_port) {
+		size = sizeof(struct f_pcie_port) * (index + 1);
+		dme_port = kmalloc(size, GFP_KERNEL);
+		if (!dme_port)
+			goto fail2;
+
+		size = sizeof(u32) * (index + 1);
+		pcie_port.nr_port_map = kmalloc(size, GFP_KERNEL);
+		if (!pcie_port.nr_port_map)
+			goto fail2;
+
+		pcie_port.dme_pcie = dme_port;
+		INIT_LIST_HEAD(&pcie_port.list);
+		spin_lock_init(&pcie_port.lock);
+	}
+
+	dme_port->pcie_type = sysoc_pcie->pcie_type;
+	dme_port[index].wrapper = NULL;
+	dme_port[index].index = index;
+
+#ifdef CONFIG_PM_RUNTIME
+	pm_runtime_enable(dev);
+	pm_runtime_get_sync(dev);
+#endif
+
+	/* optional pcie wrapper -- used on S73 power */
+	r = of_address_to_resource(dev_np, 1, &wrap);
+	if (!r) {
+		void __iomem *wrap_io;
+
+		wrap_io = ioremap(wrap.start, resource_size(&wrap));
+		writew(EN_DMA_INT, wrap_io + ST_DMA_INT_ENB);
+		dme_port[index].wrapper = wrap_io;
+	}
+
+	spin_lock_irqsave(&pcie_port.lock, flags);
+	list_add_tail(&dme_port[index].ports, &pcie_port.list);
+	spin_unlock_irqrestore(&pcie_port.lock, flags);
+	if (dme_port->pcie_type) {
+		struct dme_rc *rc;
+
+		dev_info(dev, "Root Complex Mode (%d)\n", index);
+		rc = kzalloc(sizeof(struct dme_rc), GFP_KERNEL);
+		dme_port[index].rc = rc;
+		if (!dme_port[index].rc) {
+			dev_err(dev, "dme_port[%d].rc nill\n", index);
+			goto fail1;
+		}
+
+		dme_port[index].rc->dev = &pdev->dev;
+		dme_port[index].ep = NULL;
+		r = f_pcie_host_probe(pdev, pdev->dev.of_node, index);
+		if (r < 0) {
+			dev_err(dev, "f_pcie_host_probe fail\n");
+			goto fail1;
+		}
+		pcie_port.rc_cnt++;
+	} else {
+		struct dme_ep *ep;
+
+		dev_info(dev, "Endpoint Mode (%d)\n", index);
+		ep = kzalloc(sizeof(struct dme_ep), GFP_KERNEL);
+		dme_port[index].ep = ep;
+		if (!dme_port[index].ep) {
+			dev_err(dev, "dme_port[%d].ep nill\n", index);
+			goto fail1;
+		}
+
+		dme_port[index].ep->loop_coned = sysoc_pcie->loop_coned;
+		dme_port[index].ep->dev = &pdev->dev;
+		dme_port[index].rc = NULL;
+		r = f_pcie_ep_probe(pdev, pdev->dev.of_node, index);
+		if (r < 0) {
+			dev_err(dev, "f_pcie_ep_probe fail\n");
+			goto fail1;
+		}
+	}
+
+	pcie_port.dme_pcie[index].pm_cap = 0;
+	np_pm = of_parse_phandle(dev->of_node, "power-domains", 0);
+	if (np_pm) {
+		pcie_port.dme_pcie[index].pm_cap = 1;
+		r = pm_genpd_add_callbacks(dev, &gpd_dev_ops, NULL);
+		if (r)
+			dev_err(dev, "pm_genpd_add_callbacks fail\n");
+	}
+	if (dme_port[index].index == 0)
+		pcie_init();
+
+	return 0;
+fail1:
+
+	kfree(pcie_port.dme_pcie);
+
+	if (dme_port[index].wrapper)
+		iounmap(dme_port[index].wrapper);
+
+	kfree(pcie_port.dme_pcie[index].rc);
+	kfree(pcie_port.dme_pcie[index].ep);
+
+fail2:
+	return 0;
+}
+
+static int f_pcie_remove(struct platform_device *pdev)
+{
+	struct dme_rc *rc = NULL;
+	struct dme_ep *ep = NULL;
+	struct pci_bus *root_bus = NULL;
+	struct pci_dev *root_pci = NULL;
+	struct pci_sys_data *sys = NULL;
+	struct f_pcie_port *dme_port = NULL;
+	struct device *dev = &pdev->dev;
+	int r, i = 0, domain = 0;
+	u32 clk_id = 0, dma_irq_cnt = 0;
+
+	dme_port = dev_get_platdata(&pdev->dev);
+	if (!dme_port) {
+		dev_err(dev, "dev->platform_data is null\n");
+		return -EINVAL;
+	}
+
+	if (!dme_port->rc && !dme_port->ep) {
+		dev_err(dev, "f_pcie_port is fail\n");
+		return -EINVAL;
+	}
+
+	/* ep mode */
+	if (dme_port->ep) {
+		ep = dme_port->ep;
+		for (clk_id = 0; clk_id < ep->clk_num; clk_id++)
+			if (ep->clk[clk_id])
+				clk_put(ep->clk[clk_id]);
+
+		dma_irq_cnt = sizeof(ep->dma_irq)/sizeof(int);
+		while (i < dma_irq_cnt) {
+			if (ep->dma_irq[i])
+				free_irq(ep->dma_irq[i], ep);
+			i++;
+		};
+
+		if (ep->regs)
+			iounmap(ep->regs);
+
+		if (dme_port->wrapper)
+			iounmap(dme_port->wrapper);
+
+		if (dme_port->pm_cap) {
+			r = __pm_genpd_remove_callbacks(&pdev->dev, false);
+			if (r)
+				dev_err(dev, "pm_genpd_remove fail\n");
+		}
+		pdev->dev.platform_data = NULL;
+		pm_runtime_put_sync(&pdev->dev);
+		pm_runtime_disable(&pdev->dev);
+	}
+
+	/* rc mode */
+	if (dme_port->rc) {
+		rc = dme_port->rc;
+		root_bus = rc->root_pcibus;
+		if (!root_bus) {
+			dev_err(dev, "can't find root_bus\n");
+			return -EINVAL;
+		}
+		if (rc->irq_domain)
+			irq_domain_remove(rc->irq_domain);
+
+		disable_irq(rc->slav_mem_irq);
+		if (rc->lig_irq)
+			disable_irq(rc->lig_irq);
+		free_irq(rc->slav_mem_irq, rc);
+		if (rc->lig_irq)
+			free_irq(rc->lig_irq, rc);
+
+		domain = pci_domain_nr(root_bus);
+		root_pci = pci_get_domain_bus_and_slot(domain, 0, 0);
+		if (root_pci)
+			pci_stop_and_remove_bus_device(root_pci);
+
+		sys = (struct pci_sys_data *)root_bus->sysdata;
+		release_resource(&sys->io_res);
+		pci_stop_root_bus(root_bus);
+		pci_remove_root_bus(root_bus);
+
+		for (clk_id = 0; clk_id < rc->clk_num; clk_id++)
+			if (rc->clk[clk_id])
+				clk_put(rc->clk[clk_id]);
+
+		if (rc->rc_cfg_base)
+			iounmap(rc->rc_cfg_base);
+
+		if (rc->ep_cfg_base)
+			iounmap(rc->ep_cfg_base);
+
+		if (rc->msi_lig)
+			iounmap(rc->msi_lig);
+
+		kfree(rc->clk);
+		rc->inited = 0;
+		if (dme_port->pm_cap) {
+			r = __pm_genpd_remove_callbacks(dev, false);
+			if (r)
+				dev_err(dev, "pm_genpd_remove fail\n");
+		}
+		pdev->dev.platform_data = NULL;
+		pm_runtime_put_sync(&pdev->dev);
+		pm_runtime_disable(&pdev->dev);
+	}
+
+	if (dme_port->index == (pcie_port.pcie_por_num - 1))
+		pcie_uninit();
+
+	return 0;
+}
+
+static const struct of_device_id f_pcie_dt_ids[] = {
+	{.compatible = "fujitsu,mb86s7x-pcie_dme-integration" },
+	{/* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, f_pcie_dt_ids);
+
+#ifdef CONFIG_PM
+static const struct dev_pm_ops f_pcie_pm_ops = {
+	.suspend_noirq = f_pcie_pm_suspend,
+	.resume_noirq = f_pcie_pm_resume,
+
+#ifdef CONFIG_PM_RUNTIME
+	SET_RUNTIME_PM_OPS(f_pcie_runtime_suspend,
+		f_pcie_runtime_resume, NULL)
+#endif
+};
+#endif
+
+static struct platform_driver f_pcie_driver = {
+	.driver		= {
+		.name	= "f_pcie_dme",
+		.owner  = THIS_MODULE,
+		.of_match_table = f_pcie_dt_ids,
+
+#ifdef CONFIG_PM
+		.pm = &f_pcie_pm_ops,
+#endif
+
+	},
+	.probe		= f_pcie_probe,
+	.remove		= f_pcie_remove,
+};
+
+static int __init f_pcie_init(void)
+{
+	return platform_driver_register(&f_pcie_driver);
+}
+
+static void __exit f_pcie_exit(void)
+{
+	platform_driver_unregister(&f_pcie_driver);
+}
+
+module_init(f_pcie_init);
+module_exit(f_pcie_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("PCIE IP driver");
+MODULE_AUTHOR("slash.huang@tw.fujitsu.com");
diff --git a/drivers/pci/host/pcie_f_pcie2_dme.h b/drivers/pci/host/pcie_f_pcie2_dme.h
new file mode 100644
index 0000000..73a20e6
--- /dev/null
+++ b/drivers/pci/host/pcie_f_pcie2_dme.h
@@ -0,0 +1,66 @@
+#ifndef _LINUX_PCIE_F_PCIE2_DME_H
+#define _LINUX_PCIE_F_PCIE2_DME_H
+
+#define IO_MEM_RCS_NUM				10
+#define S7X_MSIX_VECT 				32
+#define S7X_MSI_VECT				16
+
+struct pcie_pro {
+	u32 pcie_type;
+	u32 loop_coned;
+	u32 id;
+};
+
+struct dme_rc {
+	struct pci_bus *root_pcibus;
+	struct resource mem_res[IO_MEM_RCS_NUM];
+	struct resource io_res[IO_MEM_RCS_NUM];
+	struct clk **clk;
+	struct resource cfg_res;
+	struct device *dev;
+	int slav_mem_irq;
+	int msg_irq[4];
+	int mem_num;
+	int io_num;
+	void __iomem *rc_cfg_base;
+	void __iomem *ep_cfg_base;
+	void __iomem *msi_lig;
+	spinlock_t conf_lock; /* for config r/w */
+	spinlock_t msi_lock; /* for msi */
+	u32 inited;
+	u32 clk_num;
+	u32 index;
+	u32 link_up;
+	resource_size_t mem_offset[IO_MEM_RCS_NUM];
+	resource_size_t io_offset[IO_MEM_RCS_NUM];
+	int lig_irq;
+	int root_bus_nr;
+	unsigned long msi_data;
+	unsigned long msi_iova;
+	size_t msi_iova_sz;
+	struct irq_domain *irq_domain;
+	DECLARE_BITMAP(msi_irq_in_use, S7X_MSIX_VECT);
+	u32 virq[S7X_MSIX_VECT];
+	u32 nvect;
+};
+
+struct f_pcie_port {
+	struct dme_rc *rc;
+	struct dme_ep *ep;
+	struct list_head ports;
+	void __iomem *wrapper;
+	int index;
+	int pm_cap;
+	u32 pcie_type;
+};
+
+struct pcie_port {
+	struct list_head list;
+	struct f_pcie_port *dme_pcie;
+	u32 *nr_port_map;
+	u32 pcie_por_num;
+	u32 rc_cnt;
+	spinlock_t lock; /* */
+};
+
+#endif
diff --git a/drivers/pci/host/pcie_f_pcie2_dme_msi.c b/drivers/pci/host/pcie_f_pcie2_dme_msi.c
new file mode 100644
index 0000000..c580a23
--- /dev/null
+++ b/drivers/pci/host/pcie_f_pcie2_dme_msi.c
@@ -0,0 +1,103 @@
+/*
+ * PCI MSI support for the F_PCIE2_DME
+ *
+ * Copyright (C) 2013-2014 Fujitsu Semiconductor Ltd
+ *
+ * F_PCIE2_DME functions for Fujitsu SoC
+ *
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ *
+ */
+
+#include <linux/irqdomain.h>
+#include <linux/interrupt.h>
+#include <linux/pci.h>
+#include <linux/msi.h>
+#include <asm/mach/irq.h>
+#include <asm/irq.h>
+#include "pcie_f_pcie2_dme.h"
+
+int plat_supply_msi;
+
+static struct dme_rc *sys_to_pcie(struct pci_sys_data *sys)
+{
+	return sys->private_data;
+}
+
+int get_virq_pos(struct dme_rc *rc, u32 virq)
+{
+	u32 pos;
+
+	for (pos = 0; pos < S7X_MSIX_VECT; pos++) {
+		if (rc->virq[pos] == virq)
+			break;
+	}
+	return pos;
+}
+
+void arch_teardown_msi_irq(unsigned int irq)
+{
+	struct msi_desc *desc = irq_get_msi_desc(irq);
+	struct dme_rc *rc = sys_to_pcie(desc->dev->bus->sysdata);
+	unsigned int pos;
+
+	pos = get_virq_pos(rc, irq);
+	irq_dispose_mapping(irq);
+	rc->virq[pos] = 0;
+	rc->nvect--;
+	clear_bit(pos, rc->msi_irq_in_use);
+}
+
+int arch_msi_check_device(struct pci_dev *dev, int nvec, int type)
+{
+	if (type == PCI_CAP_ID_MSI && nvec > S7X_MSI_VECT)
+		return -EINVAL;
+
+	if (type == PCI_CAP_ID_MSIX && nvec > S7X_MSIX_VECT)
+		return -EINVAL;
+
+	return 0;
+}
+
+int arch_setup_msi_irq(struct pci_dev *pdev, struct msi_desc *desc)
+{
+	struct msi_msg msg;
+	struct dme_rc *rc = sys_to_pcie(desc->dev->bus->sysdata);
+	unsigned long flags;
+	int pos, virq;
+
+	/*
+	 * S70 not supply "local interrupt", 
+	 * so S70 also not supply MSI/MSI-X
+	 */
+	if (rc->lig_irq <= 0)
+		return -EINVAL;
+
+	spin_lock_irqsave(&rc->msi_lock, flags);
+	pos = find_first_zero_bit(rc->msi_irq_in_use, S7X_MSIX_VECT);
+
+	if (pos < S7X_MSIX_VECT) {
+		set_bit(pos, rc->msi_irq_in_use);
+	} else {
+		dev_err(&pdev->dev,
+			"pos (%d)over VECT (%d)\n", pos, S7X_MSIX_VECT);
+		return -ENOSPC;
+	}
+
+	virq = irq_create_mapping(rc->irq_domain, pos);
+	irq_set_msi_desc(virq, desc);
+
+	msg.address_lo = virt_to_phys((void *)rc->msi_data);
+	msg.address_hi = 0;
+	msg.data = (1 << pos);
+	write_msi_msg(virq, &msg);
+	rc->nvect++;
+	rc->virq[pos] = virq;
+	spin_unlock_irqrestore(&rc->msi_lock, flags);
+
+	return 0;
+}
diff --git a/drivers/pci/host/sysoc_pcie.c b/drivers/pci/host/sysoc_pcie.c
new file mode 100644
index 0000000..c394a31
--- /dev/null
+++ b/drivers/pci/host/sysoc_pcie.c
@@ -0,0 +1,404 @@
+/*
+ * Copyright (C) 2013-2014 Fujitsu Semiconductor Ltd
+ *
+ * HSIO System control functions for Fujitsu SoC
+ *
+ * Author: Slash Huang <slash.huang@tw.fujitsu.com>
+ *
+ * This file is licensed under the terms of the GNU General Public
+ * License version 2. This program is licensed "as is" without any
+ * warranty of any kind, whether express or implied.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/pm_runtime.h>
+#include <linux/clk.h>
+#include <linux/of_address.h>
+#include <linux/pm_domain.h>
+#include <linux/delay.h>
+#include <asm/dma-iommu.h>
+#include "pcie_f_pcie2_dme.h"
+
+#define PCIE_PHY_RST			0x4
+#define PCIE_PHY_RST_DIS		1
+#define PCIE_PHY_RST_EN			0
+#define SYSOC_PCIE_TYPE		1
+#define SYSOC_GRGMX_SAT		(1 << 9)
+#define BIFURC_ENABLE			2
+#define PCIE0_RGS0				0X34
+#define PCIE1_RGS0				0X38
+#define PCIE1_RGS1				0X48
+
+struct sysoc {
+	struct list_head ports;
+	struct device *dev;
+	struct pcie_pro *port;
+	struct device_node node;
+	struct clk **clk;
+	void __iomem *sysoc_reg;
+	void __iomem *bifurc_reg;
+	void __iomem *gpv_reg;
+	u32 pm_cap;
+	u32 clk_num;
+	const char *phy_reset;
+};
+
+static LIST_HEAD(port_list);
+/* for link list */
+spinlock_t port_lock;
+struct sysoc *_sysoc;
+
+struct sysoc *find_sysoc(struct device *dev)
+{
+	unsigned long flags;
+	struct sysoc *tmp, *port;
+
+	spin_lock_irqsave(&port_lock, flags);
+	list_for_each_entry_safe(port, tmp, &port_list, ports) {
+		if (dev->of_node == port->dev->of_node) {
+			spin_unlock_irqrestore(&port_lock, flags);
+			return port;
+		}
+	}
+	spin_unlock_irqrestore(&port_lock, flags);
+	return NULL;
+}
+
+#ifdef CONFIG_PM
+static int f_sysoc_pcie_pm_suspend(struct device *dev)
+{
+	struct sysoc *port;
+	int clk_id;
+
+	port = find_sysoc(dev);
+
+	if (!port)
+		return 0;
+
+	for (clk_id = 0; clk_id < port->clk_num; clk_id++)
+		clk_disable_unprepare(port->clk[clk_id]);
+
+	return 0;
+}
+
+static int f_sysoc_pcie_pm_resume(struct device *dev)
+{
+	struct sysoc *port;
+	int clk_id;
+
+	port = find_sysoc(dev);
+
+	if (!port || !port->sysoc_reg || !port->sysoc_reg)
+		return 0;
+
+	/* assert phy reset */
+
+	for (clk_id = 0; clk_id < port->clk_num; clk_id++)
+		if (port->clk[clk_id])
+			clk_prepare_enable(port->clk[clk_id]);
+
+	if (port->phy_reset) {
+		writel(PCIE_PHY_RST_EN, port->sysoc_reg + PCIE_PHY_RST);
+		usleep_range(10, 20);
+	}
+
+	/* deassert phy reset */
+	if (port->phy_reset) {
+		writel(PCIE_PHY_RST_DIS, port->sysoc_reg + PCIE_PHY_RST);
+		usleep_range(10, 20);
+	}
+
+	return 0;
+}
+
+static int f_sysoc_pcie_runtime_suspend(struct device *dev)
+{
+	return f_sysoc_pcie_pm_suspend(dev);
+}
+
+static int f_sysoc_pcie_runtime_resume(struct device *dev)
+{
+	return f_sysoc_pcie_pm_resume(dev);
+}
+#endif
+
+int f_sysoc_pcie_pm_start(struct device *dev)
+{
+	return f_sysoc_pcie_pm_resume(dev);
+}
+
+int f_sysoc_pcie_pm_stop(struct device *dev)
+{
+	return f_sysoc_pcie_pm_suspend(dev);
+}
+
+static struct gpd_dev_ops gpd_dev_ops = {
+	.start = f_sysoc_pcie_pm_start,
+	.stop = f_sysoc_pcie_pm_stop,
+};
+
+static int sysoc_pcie_probe(struct platform_device *pdev)
+{
+	struct resource res;
+	struct device *dev = &pdev->dev;
+	struct device_node *node = dev->of_node, *np_pm;
+	const char *clk_nam[10] = {0};
+	struct pcie_pro *port;
+	unsigned long flags;
+	int ret = 0, phy_reset_sz, err;
+	u32 clk_num, clk_id = 0, gpv, id;
+
+	spin_lock_init(&port_lock);
+	_sysoc = kzalloc(sizeof(*_sysoc), GFP_KERNEL);
+	_sysoc->dev = &pdev->dev;
+	_sysoc->node = *dev->of_node;
+
+	ret = of_property_read_u32_index(node, "id", 0, &id);
+	if (ret) {
+		dev_err(dev, "Missing id in dt\n");
+		return -EINVAL;
+	}
+
+	if (!of_address_to_resource(node, 2, &res)) {
+		_sysoc->gpv_reg = ioremap(res.start, resource_size(&res));
+
+		switch (id) {
+		case 0:
+			gpv = 0x01 | readl(_sysoc->gpv_reg + PCIE0_RGS0);
+			writel(gpv, _sysoc->gpv_reg + PCIE0_RGS0);
+		break;
+
+		case 1:
+			gpv = 0x01 | readl(_sysoc->gpv_reg + PCIE1_RGS0);
+			writel(gpv, _sysoc->gpv_reg + PCIE1_RGS0);
+		break;
+
+		case 2:
+			gpv = 0x01 | readl(_sysoc->gpv_reg + PCIE1_RGS1);
+			writel(gpv, _sysoc->gpv_reg + PCIE1_RGS1);
+		break;
+
+		}
+	}
+
+	/* check Bifurcation enable or not */
+	if (!of_address_to_resource(node, 1, &res)) {
+		_sysoc->bifurc_reg = ioremap(res.start, resource_size(&res));
+
+		if (_sysoc->bifurc_reg) {
+			ret = readl(_sysoc->bifurc_reg) & 0x3;
+			dev_info(&pdev->dev, "bifurc status 0x%x\n", ret);
+			/* enable Bifurcation */
+			writeb(BIFURC_ENABLE, _sysoc->bifurc_reg);
+		}
+	}
+
+	if (of_property_read_u32(node, "clock_num", &clk_num))
+		clk_num = 0;
+
+	if (clk_num) {
+		_sysoc->clk = kmalloc(clk_num * sizeof(struct clk *), GFP_KERNEL);
+		memset(_sysoc->clk, 0, clk_num * sizeof(struct clk *));
+	}
+	_sysoc->clk_num = clk_num;
+
+	while (clk_num) {
+		of_property_read_string_index(node,
+			"clock-names", clk_id, &clk_nam[clk_id]);
+
+		if (clk_nam[clk_id] == NULL)
+			break;
+
+		_sysoc->clk[clk_id] = of_clk_get(node, clk_id);
+
+		if (IS_ERR(_sysoc->clk[clk_id])) {
+			dev_err(_sysoc->dev, "clock%d not found.\n", clk_id);
+			return -ENXIO;
+		}
+
+		err = clk_prepare_enable(_sysoc->clk[clk_id]);
+		if (err) {
+			dev_err(_sysoc->dev, "clk (%d) enable fail.\n", clk_id);
+			return -ENXIO;
+		}
+
+		clk_id++;
+	}
+
+	_sysoc->phy_reset = of_get_property(node, "phy_reset", &phy_reset_sz);
+
+	if (of_address_to_resource(node, 0, &res)) {
+		dev_err(&pdev->dev, "no regs resource defined\n");
+		return -ENXIO;
+	}
+
+	_sysoc->sysoc_reg = ioremap(res.start, resource_size(&res));
+	if (!_sysoc->sysoc_reg) {
+		dev_err(dev, "sysoc_reg fail\n");
+		return -ENXIO;
+	}
+
+#ifdef CONFIG_PM_RUNTIME
+	pm_runtime_enable(dev);
+	pm_runtime_get_sync(dev);
+#endif
+
+	_sysoc->port = kmalloc(sizeof(*_sysoc->port), GFP_KERNEL);
+	if (!_sysoc->port)
+		return -ENXIO;
+
+	port = _sysoc->port;
+
+	/* assert phy reset */
+	if (_sysoc->phy_reset) {
+		writel(PCIE_PHY_RST_EN, _sysoc->sysoc_reg + PCIE_PHY_RST);
+		usleep_range(10, 20);
+	}
+
+	/* deassert phy reset */
+	if (_sysoc->phy_reset) {
+		writel(PCIE_PHY_RST_DIS, _sysoc->sysoc_reg + PCIE_PHY_RST);
+		usleep_range(10, 20);
+	}
+
+	port->pcie_type = readl(_sysoc->sysoc_reg) & SYSOC_PCIE_TYPE;
+	port->loop_coned = readl(_sysoc->sysoc_reg) & SYSOC_GRGMX_SAT;
+	port->id = id;
+
+	dev_set_drvdata(dev, port);
+
+	_sysoc->pm_cap = 0;
+	np_pm = of_parse_phandle(pdev->dev.of_node, "power-domains", 0);
+	if (np_pm) {
+		_sysoc->pm_cap = 1;
+		ret = pm_genpd_add_callbacks(dev, &gpd_dev_ops, NULL);
+		if (ret)
+			dev_err(dev, "pm_genpd_add_callbacks fail\n");
+	}
+
+#ifdef CONFIG_ARM_DMA_USE_IOMMU
+	if (!port->pcie_type && port->loop_coned) {
+		if (dev->archdata.mapping) {
+			arm_iommu_detach_device(dev);
+			arm_iommu_release_mapping(dev->archdata.mapping);
+			dev->archdata.mapping = NULL;
+		}
+	}
+#endif
+
+	spin_lock_irqsave(&port_lock, flags);
+	list_add(&_sysoc->ports, &port_list);
+	spin_unlock_irqrestore(&port_lock, flags);
+
+	if (node) {
+		ret = of_platform_populate(node, NULL, NULL, dev);
+		if (ret)
+			dev_err(dev, "failed to add pcie core\n");
+	}
+
+	return 0;
+}
+
+static int sysoc_pcie_remove_child(struct device *dev, void *unused)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+
+	of_device_unregister(pdev);
+
+	return 0;
+}
+
+static int sysoc_pcie_remove(struct platform_device *pdev)
+{
+	int r = 0;
+	struct sysoc *pcie_sysoc;
+
+	pcie_sysoc = find_sysoc(&pdev->dev);
+	if (!pcie_sysoc)
+		return 0;
+
+	if (pcie_sysoc->pm_cap) {
+		r = __pm_genpd_remove_callbacks(&pdev->dev, false);
+		if (r)
+			dev_err(&pdev->dev, "pm_genpd_remove fail\n");
+	}
+
+	if (pcie_sysoc->sysoc_reg)
+		iounmap(pcie_sysoc->sysoc_reg);
+
+	if (pcie_sysoc->bifurc_reg)
+		iounmap(pcie_sysoc->bifurc_reg);
+
+	if (pcie_sysoc->gpv_reg)
+		iounmap(pcie_sysoc->gpv_reg);
+
+	pcie_sysoc->sysoc_reg = NULL;
+	pcie_sysoc->bifurc_reg = NULL;
+	list_del(&pcie_sysoc->ports);
+
+	kfree(pcie_sysoc->port);
+	kfree(pcie_sysoc);
+	device_for_each_child(&pdev->dev, NULL, sysoc_pcie_remove_child);
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	return 0;
+}
+
+static const struct of_device_id sysoc_pcie_match[] = {
+	{
+		.compatible = "fujitsu,sysoc-f_pcie_dmx"
+	},
+	{/* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, sysoc_pcie_match);
+
+
+#ifdef CONFIG_PM
+static const struct dev_pm_ops f_sysoc_pm_ops = {
+	.suspend_noirq = f_sysoc_pcie_pm_suspend,
+	.resume_noirq = f_sysoc_pcie_pm_resume,
+
+#ifdef CONFIG_PM_RUNTIME
+	SET_RUNTIME_PM_OPS(
+	f_sysoc_pcie_runtime_suspend,
+	f_sysoc_pcie_runtime_resume, NULL)
+#endif
+};
+#endif
+
+static struct platform_driver sysoc_pcie_driver = {
+	.probe		= sysoc_pcie_probe,
+	.remove		= sysoc_pcie_remove,
+	.driver		= {
+		.name	= "sysoc_pcie_driver",
+		.of_match_table	= of_match_ptr(sysoc_pcie_match),
+#ifdef CONFIG_PM
+		.pm = &f_sysoc_pm_ops,
+#endif
+	},
+};
+
+static int __init sysoc_pcie_init(void)
+{
+	return platform_driver_register(&sysoc_pcie_driver);
+}
+
+static void __exit sysoc_pcie_exit(void)
+{
+	platform_driver_unregister(&sysoc_pcie_driver);
+}
+
+module_init(sysoc_pcie_init);
+module_exit(sysoc_pcie_exit);
+
+MODULE_LICENSE("GPL v2");
+MODULE_DESCRIPTION("HSIO system control driver");
+MODULE_AUTHOR("slash.huang@tw.fujitsu.com");
diff --git a/drivers/pci/msi.c b/drivers/pci/msi.c
index 53bfdf3..b41b9b7 100644
--- a/drivers/pci/msi.c
+++ b/drivers/pci/msi.c
@@ -92,9 +92,10 @@ int __weak arch_setup_msi_irqs(struct pci_dev *dev, int nvec, int type)
 	 * If an architecture wants to support multiple MSI, it needs to
 	 * override arch_setup_msi_irqs()
 	 */
+	/*
 	if (type == PCI_CAP_ID_MSI && nvec > 1)
 		return 1;
-
+	*/
 	list_for_each_entry(entry, &dev->msi_list, list) {
 		ret = arch_setup_msi_irq(dev, entry);
 		if (ret < 0)
@@ -274,11 +275,13 @@ void mask_msi_irq(struct irq_data *data)
 {
 	msi_set_mask_bit(data, 1);
 }
+EXPORT_SYMBOL_GPL(mask_msi_irq);
 
 void unmask_msi_irq(struct irq_data *data)
 {
 	msi_set_mask_bit(data, 0);
 }
+EXPORT_SYMBOL_GPL(unmask_msi_irq);
 
 #endif /* CONFIG_GENERIC_HARDIRQS */
 
@@ -829,7 +832,7 @@ static int msix_capability_init(struct pci_dev *dev,
 
 	return 0;
 
-out_avail:
+error:
 	if (ret < 0) {
 		/*
 		 * If we had some success, report the number of irqs
diff --git a/drivers/pci/pci.c b/drivers/pci/pci.c
index 3a65301..ea85416 100644
--- a/drivers/pci/pci.c
+++ b/drivers/pci/pci.c
@@ -962,6 +962,7 @@ static void pci_restore_config_dword(struct pci_dev *pdev, int offset,
 	for (;;) {
 		dev_dbg(&pdev->dev, "restoring config space at offset "
 			"%#x (was %#x, writing %#x)\n", offset, val, saved_val);
+		msleep(100);
 		pci_write_config_dword(pdev, offset, saved_val);
 		if (retry-- <= 0)
 			return;
@@ -988,12 +989,12 @@ static void pci_restore_config_space_range(struct pci_dev *pdev,
 static void pci_restore_config_space(struct pci_dev *pdev)
 {
 	if (pdev->hdr_type == PCI_HEADER_TYPE_NORMAL) {
-		pci_restore_config_space_range(pdev, 10, 15, 0);
+		pci_restore_config_space_range(pdev, 10, 15, 5);
 		/* Restore BARs before the command register. */
 		pci_restore_config_space_range(pdev, 4, 9, 10);
-		pci_restore_config_space_range(pdev, 0, 3, 0);
+		pci_restore_config_space_range(pdev, 0, 3, 5);
 	} else {
-		pci_restore_config_space_range(pdev, 0, 15, 0);
+		pci_restore_config_space_range(pdev, 0, 15, 5);
 	}
 }
 
@@ -2340,6 +2341,9 @@ static int pci_std_enable_acs(struct pci_dev *dev)
 	u16 cap;
 	u16 ctrl;
 
+	if (!pci_acs_enable)
+		return;
+
 	pos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_ACS);
 	if (!pos)
 		return -ENODEV;
diff --git a/drivers/pci/pcie/pme.c b/drivers/pci/pcie/pme.c
index 795db1f..c863518 100644
--- a/drivers/pci/pcie/pme.c
+++ b/drivers/pci/pcie/pme.c
@@ -20,6 +20,8 @@
 #include <linux/device.h>
 #include <linux/pcieport_if.h>
 #include <linux/pm_runtime.h>
+#include <linux/of_irq.h>
+#include <linux/of_address.h>
 
 #include "../pci.h"
 #include "portdrv.h"
@@ -342,7 +344,7 @@ static int pcie_pme_probe(struct pcie_device *srv)
 {
 	struct pci_dev *port;
 	struct pcie_pme_service_data *data;
-	int ret;
+	int ret, pme_int_off = 0;
 
 	data = kzalloc(sizeof(*data), GFP_KERNEL);
 	if (!data)
@@ -354,6 +356,15 @@ static int pcie_pme_probe(struct pcie_device *srv)
 	set_service_data(srv, data);
 
 	port = srv->port;
+
+	if (of_property_read_u32(port->bus->dev.of_node,
+				"pme_int_off", &pme_int_off)) {
+	} else {
+		pme_int_off = 
+		srv->irq = of_irq_to_resource(port->bus->dev.of_node,
+				pme_int_off, NULL);
+	}
+
 	pcie_pme_interrupt_enable(port, false);
 	pcie_clear_root_pme_status(port);
 
diff --git a/drivers/pci/probe.c b/drivers/pci/probe.c
index 600eca6..bf1e7e4 100644
--- a/drivers/pci/probe.c
+++ b/drivers/pci/probe.c
@@ -1014,6 +1014,11 @@ int pci_setup_device(struct pci_dev *dev)
 	dev->revision = class & 0xff;
 	dev->class = class >> 8;		    /* upper 3 bytes */
 
+	if (dev->class == 0x60400) {
+		dev->hdr_type = 1;
+		hdr_type = 1;
+	}
+
 	dev_printk(KERN_DEBUG, &dev->dev, "[%04x:%04x] type %02x class %#08x\n",
 		   dev->vendor, dev->device, dev->hdr_type, dev->class);
 
@@ -1033,6 +1038,7 @@ int pci_setup_device(struct pci_dev *dev)
 		if (class == PCI_CLASS_BRIDGE_PCI)
 			goto bad;
 		pci_read_irq(dev);
+		dev->irq = 0;
 		pci_read_bases(dev, 6, PCI_ROM_ADDRESS);
 		pci_read_config_word(dev, PCI_SUBSYSTEM_VENDOR_ID, &dev->subsystem_vendor);
 		pci_read_config_word(dev, PCI_SUBSYSTEM_ID, &dev->subsystem_device);
@@ -1319,8 +1325,11 @@ void pci_device_add(struct pci_dev *dev, struct pci_bus *bus)
 	set_dev_node(&dev->dev, pcibus_to_node(bus));
 	dev->dev.dma_mask = &dev->dma_mask;
 	dev->dev.dma_parms = &dev->dma_parms;
+#ifdef CONFIG_PHYS_ADDR_T_64BIT
+	dev->dev.coherent_dma_mask = 0xffffffffffffffffull;
+#else
 	dev->dev.coherent_dma_mask = 0xffffffffull;
-
+#endif
 	pci_set_dma_max_seg_size(dev, 65536);
 	pci_set_dma_seg_boundary(dev, 0xffffffff);
 
@@ -1775,6 +1784,7 @@ err_out:
 	kfree(b);
 	return NULL;
 }
+EXPORT_SYMBOL(pci_create_root_bus);
 
 int pci_bus_insert_busn_res(struct pci_bus *b, int bus, int bus_max)
 {
@@ -1802,6 +1812,7 @@ int pci_bus_insert_busn_res(struct pci_bus *b, int bus, int bus_max)
 
 	return conflict == NULL;
 }
+EXPORT_SYMBOL(pci_bus_insert_busn_res);
 
 int pci_bus_update_busn_res_end(struct pci_bus *b, int bus_max)
 {
@@ -1824,6 +1835,7 @@ int pci_bus_update_busn_res_end(struct pci_bus *b, int bus_max)
 
 	return ret;
 }
+EXPORT_SYMBOL(pci_bus_update_busn_res_end);
 
 void pci_bus_release_busn_res(struct pci_bus *b)
 {
diff --git a/drivers/pci/probe.c.orig b/drivers/pci/probe.c.orig
new file mode 100644
index 0000000..600eca6
--- /dev/null
+++ b/drivers/pci/probe.c.orig
@@ -0,0 +1,1988 @@
+/*
+ * probe.c - PCI detection and setup code
+ */
+
+#include <linux/kernel.h>
+#include <linux/delay.h>
+#include <linux/init.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
+#include <linux/module.h>
+#include <linux/cpumask.h>
+#include <linux/pci-aspm.h>
+#include <asm-generic/pci-bridge.h>
+#include "pci.h"
+
+#define CARDBUS_LATENCY_TIMER	176	/* secondary latency timer */
+#define CARDBUS_RESERVE_BUSNR	3
+
+struct resource busn_resource = {
+	.name	= "PCI busn",
+	.start	= 0,
+	.end	= 255,
+	.flags	= IORESOURCE_BUS,
+};
+
+/* Ugh.  Need to stop exporting this to modules. */
+LIST_HEAD(pci_root_buses);
+EXPORT_SYMBOL(pci_root_buses);
+
+static LIST_HEAD(pci_domain_busn_res_list);
+
+struct pci_domain_busn_res {
+	struct list_head list;
+	struct resource res;
+	int domain_nr;
+};
+
+static struct resource *get_pci_domain_busn_res(int domain_nr)
+{
+	struct pci_domain_busn_res *r;
+
+	list_for_each_entry(r, &pci_domain_busn_res_list, list)
+		if (r->domain_nr == domain_nr)
+			return &r->res;
+
+	r = kzalloc(sizeof(*r), GFP_KERNEL);
+	if (!r)
+		return NULL;
+
+	r->domain_nr = domain_nr;
+	r->res.start = 0;
+	r->res.end = 0xff;
+	r->res.flags = IORESOURCE_BUS | IORESOURCE_PCI_FIXED;
+
+	list_add_tail(&r->list, &pci_domain_busn_res_list);
+
+	return &r->res;
+}
+
+static int find_anything(struct device *dev, void *data)
+{
+	return 1;
+}
+
+/*
+ * Some device drivers need know if pci is initiated.
+ * Basically, we think pci is not initiated when there
+ * is no device to be found on the pci_bus_type.
+ */
+int no_pci_devices(void)
+{
+	struct device *dev;
+	int no_devices;
+
+	dev = bus_find_device(&pci_bus_type, NULL, NULL, find_anything);
+	no_devices = (dev == NULL);
+	put_device(dev);
+	return no_devices;
+}
+EXPORT_SYMBOL(no_pci_devices);
+
+/*
+ * PCI Bus Class
+ */
+static void release_pcibus_dev(struct device *dev)
+{
+	struct pci_bus *pci_bus = to_pci_bus(dev);
+
+	if (pci_bus->bridge)
+		put_device(pci_bus->bridge);
+	pci_bus_remove_resources(pci_bus);
+	pci_release_bus_of_node(pci_bus);
+	kfree(pci_bus);
+}
+
+static struct class pcibus_class = {
+	.name		= "pci_bus",
+	.dev_release	= &release_pcibus_dev,
+	.dev_attrs	= pcibus_dev_attrs,
+};
+
+static int __init pcibus_class_init(void)
+{
+	return class_register(&pcibus_class);
+}
+postcore_initcall(pcibus_class_init);
+
+static u64 pci_size(u64 base, u64 maxbase, u64 mask)
+{
+	u64 size = mask & maxbase;	/* Find the significant bits */
+	if (!size)
+		return 0;
+
+	/* Get the lowest of them to find the decode size, and
+	   from that the extent.  */
+	size = (size & ~(size-1)) - 1;
+
+	/* base == maxbase can be valid only if the BAR has
+	   already been programmed with all 1s.  */
+	if (base == maxbase && ((base | size) & mask) != mask)
+		return 0;
+
+	return size;
+}
+
+static inline unsigned long decode_bar(struct pci_dev *dev, u32 bar)
+{
+	u32 mem_type;
+	unsigned long flags;
+
+	if ((bar & PCI_BASE_ADDRESS_SPACE) == PCI_BASE_ADDRESS_SPACE_IO) {
+		flags = bar & ~PCI_BASE_ADDRESS_IO_MASK;
+		flags |= IORESOURCE_IO;
+		return flags;
+	}
+
+	flags = bar & ~PCI_BASE_ADDRESS_MEM_MASK;
+	flags |= IORESOURCE_MEM;
+	if (flags & PCI_BASE_ADDRESS_MEM_PREFETCH)
+		flags |= IORESOURCE_PREFETCH;
+
+	mem_type = bar & PCI_BASE_ADDRESS_MEM_TYPE_MASK;
+	switch (mem_type) {
+	case PCI_BASE_ADDRESS_MEM_TYPE_32:
+		break;
+	case PCI_BASE_ADDRESS_MEM_TYPE_1M:
+		/* 1M mem BAR treated as 32-bit BAR */
+		break;
+	case PCI_BASE_ADDRESS_MEM_TYPE_64:
+		flags |= IORESOURCE_MEM_64;
+		break;
+	default:
+		/* mem unknown type treated as 32-bit BAR */
+		break;
+	}
+	return flags;
+}
+
+/**
+ * pci_read_base - read a PCI BAR
+ * @dev: the PCI device
+ * @type: type of the BAR
+ * @res: resource buffer to be filled in
+ * @pos: BAR position in the config space
+ *
+ * Returns 1 if the BAR is 64-bit, or 0 if 32-bit.
+ */
+int __pci_read_base(struct pci_dev *dev, enum pci_bar_type type,
+			struct resource *res, unsigned int pos)
+{
+	u32 l, sz, mask;
+	u16 orig_cmd;
+	struct pci_bus_region region;
+	bool bar_too_big = false, bar_disabled = false;
+
+	mask = type ? PCI_ROM_ADDRESS_MASK : ~0;
+
+	/* No printks while decoding is disabled! */
+	if (!dev->mmio_always_on) {
+		pci_read_config_word(dev, PCI_COMMAND, &orig_cmd);
+		pci_write_config_word(dev, PCI_COMMAND,
+			orig_cmd & ~(PCI_COMMAND_MEMORY | PCI_COMMAND_IO));
+	}
+
+	res->name = pci_name(dev);
+
+	pci_read_config_dword(dev, pos, &l);
+	pci_write_config_dword(dev, pos, l | mask);
+	pci_read_config_dword(dev, pos, &sz);
+	pci_write_config_dword(dev, pos, l);
+
+	/*
+	 * All bits set in sz means the device isn't working properly.
+	 * If the BAR isn't implemented, all bits must be 0.  If it's a
+	 * memory BAR or a ROM, bit 0 must be clear; if it's an io BAR, bit
+	 * 1 must be clear.
+	 */
+	if (!sz || sz == 0xffffffff)
+		goto fail;
+
+	/*
+	 * I don't know how l can have all bits set.  Copied from old code.
+	 * Maybe it fixes a bug on some ancient platform.
+	 */
+	if (l == 0xffffffff)
+		l = 0;
+
+	if (type == pci_bar_unknown) {
+		res->flags = decode_bar(dev, l);
+		res->flags |= IORESOURCE_SIZEALIGN;
+		if (res->flags & IORESOURCE_IO) {
+			l &= PCI_BASE_ADDRESS_IO_MASK;
+			mask = PCI_BASE_ADDRESS_IO_MASK & (u32) IO_SPACE_LIMIT;
+		} else {
+			l &= PCI_BASE_ADDRESS_MEM_MASK;
+			mask = (u32)PCI_BASE_ADDRESS_MEM_MASK;
+		}
+	} else {
+		res->flags |= (l & IORESOURCE_ROM_ENABLE);
+		l &= PCI_ROM_ADDRESS_MASK;
+		mask = (u32)PCI_ROM_ADDRESS_MASK;
+	}
+
+	if (res->flags & IORESOURCE_MEM_64) {
+		u64 l64 = l;
+		u64 sz64 = sz;
+		u64 mask64 = mask | (u64)~0 << 32;
+
+		pci_read_config_dword(dev, pos + 4, &l);
+		pci_write_config_dword(dev, pos + 4, ~0);
+		pci_read_config_dword(dev, pos + 4, &sz);
+		pci_write_config_dword(dev, pos + 4, l);
+
+		l64 |= ((u64)l << 32);
+		sz64 |= ((u64)sz << 32);
+
+		sz64 = pci_size(l64, sz64, mask64);
+
+		if (!sz64)
+			goto fail;
+
+		if ((sizeof(resource_size_t) < 8) && (sz64 > 0x100000000ULL)) {
+			bar_too_big = true;
+			goto fail;
+		}
+
+		if ((sizeof(resource_size_t) < 8) && l) {
+			/* Address above 32-bit boundary; disable the BAR */
+			pci_write_config_dword(dev, pos, 0);
+			pci_write_config_dword(dev, pos + 4, 0);
+			region.start = 0;
+			region.end = sz64;
+			pcibios_bus_to_resource(dev, res, &region);
+			bar_disabled = true;
+		} else {
+			region.start = l64;
+			region.end = l64 + sz64;
+			pcibios_bus_to_resource(dev, res, &region);
+		}
+	} else {
+		sz = pci_size(l, sz, mask);
+
+		if (!sz)
+			goto fail;
+
+		region.start = l;
+		region.end = l + sz;
+		pcibios_bus_to_resource(dev, res, &region);
+	}
+
+	goto out;
+
+
+fail:
+	res->flags = 0;
+out:
+	if (!dev->mmio_always_on)
+		pci_write_config_word(dev, PCI_COMMAND, orig_cmd);
+
+	if (bar_too_big)
+		dev_err(&dev->dev, "reg %x: can't handle 64-bit BAR\n", pos);
+	if (res->flags && !bar_disabled)
+		dev_printk(KERN_DEBUG, &dev->dev, "reg %x: %pR\n", pos, res);
+
+	return (res->flags & IORESOURCE_MEM_64) ? 1 : 0;
+}
+
+static void pci_read_bases(struct pci_dev *dev, unsigned int howmany, int rom)
+{
+	unsigned int pos, reg;
+
+	for (pos = 0; pos < howmany; pos++) {
+		struct resource *res = &dev->resource[pos];
+		reg = PCI_BASE_ADDRESS_0 + (pos << 2);
+		pos += __pci_read_base(dev, pci_bar_unknown, res, reg);
+	}
+
+	if (rom) {
+		struct resource *res = &dev->resource[PCI_ROM_RESOURCE];
+		dev->rom_base_reg = rom;
+		res->flags = IORESOURCE_MEM | IORESOURCE_PREFETCH |
+				IORESOURCE_READONLY | IORESOURCE_CACHEABLE |
+				IORESOURCE_SIZEALIGN;
+		__pci_read_base(dev, pci_bar_mem32, res, rom);
+	}
+}
+
+static void pci_read_bridge_io(struct pci_bus *child)
+{
+	struct pci_dev *dev = child->self;
+	u8 io_base_lo, io_limit_lo;
+	unsigned long io_mask, io_granularity, base, limit;
+	struct pci_bus_region region;
+	struct resource *res;
+
+	io_mask = PCI_IO_RANGE_MASK;
+	io_granularity = 0x1000;
+	if (dev->io_window_1k) {
+		/* Support 1K I/O space granularity */
+		io_mask = PCI_IO_1K_RANGE_MASK;
+		io_granularity = 0x400;
+	}
+
+	res = child->resource[0];
+	pci_read_config_byte(dev, PCI_IO_BASE, &io_base_lo);
+	pci_read_config_byte(dev, PCI_IO_LIMIT, &io_limit_lo);
+	base = (io_base_lo & io_mask) << 8;
+	limit = (io_limit_lo & io_mask) << 8;
+
+	if ((io_base_lo & PCI_IO_RANGE_TYPE_MASK) == PCI_IO_RANGE_TYPE_32) {
+		u16 io_base_hi, io_limit_hi;
+
+		pci_read_config_word(dev, PCI_IO_BASE_UPPER16, &io_base_hi);
+		pci_read_config_word(dev, PCI_IO_LIMIT_UPPER16, &io_limit_hi);
+		base |= ((unsigned long) io_base_hi << 16);
+		limit |= ((unsigned long) io_limit_hi << 16);
+	}
+
+	if (base <= limit) {
+		res->flags = (io_base_lo & PCI_IO_RANGE_TYPE_MASK) | IORESOURCE_IO;
+		region.start = base;
+		region.end = limit + io_granularity - 1;
+		pcibios_bus_to_resource(dev, res, &region);
+		dev_printk(KERN_DEBUG, &dev->dev, "  bridge window %pR\n", res);
+	}
+}
+
+static void pci_read_bridge_mmio(struct pci_bus *child)
+{
+	struct pci_dev *dev = child->self;
+	u16 mem_base_lo, mem_limit_lo;
+	unsigned long base, limit;
+	struct pci_bus_region region;
+	struct resource *res;
+
+	res = child->resource[1];
+	pci_read_config_word(dev, PCI_MEMORY_BASE, &mem_base_lo);
+	pci_read_config_word(dev, PCI_MEMORY_LIMIT, &mem_limit_lo);
+	base = ((unsigned long) mem_base_lo & PCI_MEMORY_RANGE_MASK) << 16;
+	limit = ((unsigned long) mem_limit_lo & PCI_MEMORY_RANGE_MASK) << 16;
+	if (base <= limit) {
+		res->flags = (mem_base_lo & PCI_MEMORY_RANGE_TYPE_MASK) | IORESOURCE_MEM;
+		region.start = base;
+		region.end = limit + 0xfffff;
+		pcibios_bus_to_resource(dev, res, &region);
+		dev_printk(KERN_DEBUG, &dev->dev, "  bridge window %pR\n", res);
+	}
+}
+
+static void pci_read_bridge_mmio_pref(struct pci_bus *child)
+{
+	struct pci_dev *dev = child->self;
+	u16 mem_base_lo, mem_limit_lo;
+	u64 base64, limit64;
+	dma_addr_t base, limit;
+	struct pci_bus_region region;
+	struct resource *res;
+
+	res = child->resource[2];
+	pci_read_config_word(dev, PCI_PREF_MEMORY_BASE, &mem_base_lo);
+	pci_read_config_word(dev, PCI_PREF_MEMORY_LIMIT, &mem_limit_lo);
+	base64 = ((unsigned long) mem_base_lo & PCI_PREF_RANGE_MASK) << 16;
+	limit64 = ((unsigned long) mem_limit_lo & PCI_PREF_RANGE_MASK) << 16;
+
+	if ((mem_base_lo & PCI_PREF_RANGE_TYPE_MASK) == PCI_PREF_RANGE_TYPE_64) {
+		u32 mem_base_hi, mem_limit_hi;
+
+		pci_read_config_dword(dev, PCI_PREF_BASE_UPPER32, &mem_base_hi);
+		pci_read_config_dword(dev, PCI_PREF_LIMIT_UPPER32, &mem_limit_hi);
+
+		/*
+		 * Some bridges set the base > limit by default, and some
+		 * (broken) BIOSes do not initialize them.  If we find
+		 * this, just assume they are not being used.
+		 */
+		if (mem_base_hi <= mem_limit_hi) {
+			base64 |= (u64) mem_base_hi << 32;
+			limit64 |= (u64) mem_limit_hi << 32;
+		}
+	}
+	base = (dma_addr_t) base64;
+	limit = (dma_addr_t) limit64;
+
+	if (base != base64) {
+		dev_err(&dev->dev, "can't handle bridge window above 4GB (bus address %#010llx)\n",
+			(unsigned long long) base64);
+		return;
+	}
+
+	if (base <= limit) {
+		res->flags = (mem_base_lo & PCI_PREF_RANGE_TYPE_MASK) |
+					 IORESOURCE_MEM | IORESOURCE_PREFETCH;
+		if (res->flags & PCI_PREF_RANGE_TYPE_64)
+			res->flags |= IORESOURCE_MEM_64;
+		region.start = base;
+		region.end = limit + 0xfffff;
+		pcibios_bus_to_resource(dev, res, &region);
+		dev_printk(KERN_DEBUG, &dev->dev, "  bridge window %pR\n", res);
+	}
+}
+
+void pci_read_bridge_bases(struct pci_bus *child)
+{
+	struct pci_dev *dev = child->self;
+	struct resource *res;
+	int i;
+
+	if (pci_is_root_bus(child))	/* It's a host bus, nothing to read */
+		return;
+
+	dev_info(&dev->dev, "PCI bridge to %pR%s\n",
+		 &child->busn_res,
+		 dev->transparent ? " (subtractive decode)" : "");
+
+	pci_bus_remove_resources(child);
+	for (i = 0; i < PCI_BRIDGE_RESOURCE_NUM; i++)
+		child->resource[i] = &dev->resource[PCI_BRIDGE_RESOURCES+i];
+
+	pci_read_bridge_io(child);
+	pci_read_bridge_mmio(child);
+	pci_read_bridge_mmio_pref(child);
+
+	if (dev->transparent) {
+		pci_bus_for_each_resource(child->parent, res, i) {
+			if (res) {
+				pci_bus_add_resource(child, res,
+						     PCI_SUBTRACTIVE_DECODE);
+				dev_printk(KERN_DEBUG, &dev->dev,
+					   "  bridge window %pR (subtractive decode)\n",
+					   res);
+			}
+		}
+	}
+}
+
+static struct pci_bus * pci_alloc_bus(void)
+{
+	struct pci_bus *b;
+
+	b = kzalloc(sizeof(*b), GFP_KERNEL);
+	if (b) {
+		INIT_LIST_HEAD(&b->node);
+		INIT_LIST_HEAD(&b->children);
+		INIT_LIST_HEAD(&b->devices);
+		INIT_LIST_HEAD(&b->slots);
+		INIT_LIST_HEAD(&b->resources);
+		b->max_bus_speed = PCI_SPEED_UNKNOWN;
+		b->cur_bus_speed = PCI_SPEED_UNKNOWN;
+	}
+	return b;
+}
+
+static struct pci_host_bridge *pci_alloc_host_bridge(struct pci_bus *b)
+{
+	struct pci_host_bridge *bridge;
+
+	bridge = kzalloc(sizeof(*bridge), GFP_KERNEL);
+	if (bridge) {
+		INIT_LIST_HEAD(&bridge->windows);
+		bridge->bus = b;
+	}
+
+	return bridge;
+}
+
+static unsigned char pcix_bus_speed[] = {
+	PCI_SPEED_UNKNOWN,		/* 0 */
+	PCI_SPEED_66MHz_PCIX,		/* 1 */
+	PCI_SPEED_100MHz_PCIX,		/* 2 */
+	PCI_SPEED_133MHz_PCIX,		/* 3 */
+	PCI_SPEED_UNKNOWN,		/* 4 */
+	PCI_SPEED_66MHz_PCIX_ECC,	/* 5 */
+	PCI_SPEED_100MHz_PCIX_ECC,	/* 6 */
+	PCI_SPEED_133MHz_PCIX_ECC,	/* 7 */
+	PCI_SPEED_UNKNOWN,		/* 8 */
+	PCI_SPEED_66MHz_PCIX_266,	/* 9 */
+	PCI_SPEED_100MHz_PCIX_266,	/* A */
+	PCI_SPEED_133MHz_PCIX_266,	/* B */
+	PCI_SPEED_UNKNOWN,		/* C */
+	PCI_SPEED_66MHz_PCIX_533,	/* D */
+	PCI_SPEED_100MHz_PCIX_533,	/* E */
+	PCI_SPEED_133MHz_PCIX_533	/* F */
+};
+
+static unsigned char pcie_link_speed[] = {
+	PCI_SPEED_UNKNOWN,		/* 0 */
+	PCIE_SPEED_2_5GT,		/* 1 */
+	PCIE_SPEED_5_0GT,		/* 2 */
+	PCIE_SPEED_8_0GT,		/* 3 */
+	PCI_SPEED_UNKNOWN,		/* 4 */
+	PCI_SPEED_UNKNOWN,		/* 5 */
+	PCI_SPEED_UNKNOWN,		/* 6 */
+	PCI_SPEED_UNKNOWN,		/* 7 */
+	PCI_SPEED_UNKNOWN,		/* 8 */
+	PCI_SPEED_UNKNOWN,		/* 9 */
+	PCI_SPEED_UNKNOWN,		/* A */
+	PCI_SPEED_UNKNOWN,		/* B */
+	PCI_SPEED_UNKNOWN,		/* C */
+	PCI_SPEED_UNKNOWN,		/* D */
+	PCI_SPEED_UNKNOWN,		/* E */
+	PCI_SPEED_UNKNOWN		/* F */
+};
+
+void pcie_update_link_speed(struct pci_bus *bus, u16 linksta)
+{
+	bus->cur_bus_speed = pcie_link_speed[linksta & PCI_EXP_LNKSTA_CLS];
+}
+EXPORT_SYMBOL_GPL(pcie_update_link_speed);
+
+static unsigned char agp_speeds[] = {
+	AGP_UNKNOWN,
+	AGP_1X,
+	AGP_2X,
+	AGP_4X,
+	AGP_8X
+};
+
+static enum pci_bus_speed agp_speed(int agp3, int agpstat)
+{
+	int index = 0;
+
+	if (agpstat & 4)
+		index = 3;
+	else if (agpstat & 2)
+		index = 2;
+	else if (agpstat & 1)
+		index = 1;
+	else
+		goto out;
+	
+	if (agp3) {
+		index += 2;
+		if (index == 5)
+			index = 0;
+	}
+
+ out:
+	return agp_speeds[index];
+}
+
+
+static void pci_set_bus_speed(struct pci_bus *bus)
+{
+	struct pci_dev *bridge = bus->self;
+	int pos;
+
+	pos = pci_find_capability(bridge, PCI_CAP_ID_AGP);
+	if (!pos)
+		pos = pci_find_capability(bridge, PCI_CAP_ID_AGP3);
+	if (pos) {
+		u32 agpstat, agpcmd;
+
+		pci_read_config_dword(bridge, pos + PCI_AGP_STATUS, &agpstat);
+		bus->max_bus_speed = agp_speed(agpstat & 8, agpstat & 7);
+
+		pci_read_config_dword(bridge, pos + PCI_AGP_COMMAND, &agpcmd);
+		bus->cur_bus_speed = agp_speed(agpstat & 8, agpcmd & 7);
+	}
+
+	pos = pci_find_capability(bridge, PCI_CAP_ID_PCIX);
+	if (pos) {
+		u16 status;
+		enum pci_bus_speed max;
+
+		pci_read_config_word(bridge, pos + PCI_X_BRIDGE_SSTATUS,
+				     &status);
+
+		if (status & PCI_X_SSTATUS_533MHZ) {
+			max = PCI_SPEED_133MHz_PCIX_533;
+		} else if (status & PCI_X_SSTATUS_266MHZ) {
+			max = PCI_SPEED_133MHz_PCIX_266;
+		} else if (status & PCI_X_SSTATUS_133MHZ) {
+			if ((status & PCI_X_SSTATUS_VERS) == PCI_X_SSTATUS_V2) {
+				max = PCI_SPEED_133MHz_PCIX_ECC;
+			} else {
+				max = PCI_SPEED_133MHz_PCIX;
+			}
+		} else {
+			max = PCI_SPEED_66MHz_PCIX;
+		}
+
+		bus->max_bus_speed = max;
+		bus->cur_bus_speed = pcix_bus_speed[
+			(status & PCI_X_SSTATUS_FREQ) >> 6];
+
+		return;
+	}
+
+	pos = pci_find_capability(bridge, PCI_CAP_ID_EXP);
+	if (pos) {
+		u32 linkcap;
+		u16 linksta;
+
+		pcie_capability_read_dword(bridge, PCI_EXP_LNKCAP, &linkcap);
+		bus->max_bus_speed = pcie_link_speed[linkcap & PCI_EXP_LNKCAP_SLS];
+
+		pcie_capability_read_word(bridge, PCI_EXP_LNKSTA, &linksta);
+		pcie_update_link_speed(bus, linksta);
+	}
+}
+
+
+static struct pci_bus *pci_alloc_child_bus(struct pci_bus *parent,
+					   struct pci_dev *bridge, int busnr)
+{
+	struct pci_bus *child;
+	int i;
+	int ret;
+
+	/*
+	 * Allocate a new bus, and inherit stuff from the parent..
+	 */
+	child = pci_alloc_bus();
+	if (!child)
+		return NULL;
+
+	child->parent = parent;
+	child->ops = parent->ops;
+	child->msi = parent->msi;
+	child->sysdata = parent->sysdata;
+	child->bus_flags = parent->bus_flags;
+
+	/* initialize some portions of the bus device, but don't register it
+	 * now as the parent is not properly set up yet.
+	 */
+	child->dev.class = &pcibus_class;
+	dev_set_name(&child->dev, "%04x:%02x", pci_domain_nr(child), busnr);
+
+	/*
+	 * Set up the primary, secondary and subordinate
+	 * bus numbers.
+	 */
+	child->number = child->busn_res.start = busnr;
+	child->primary = parent->busn_res.start;
+	child->busn_res.end = 0xff;
+
+	if (!bridge) {
+		child->dev.parent = parent->bridge;
+		goto add_dev;
+	}
+
+	child->self = bridge;
+	child->bridge = get_device(&bridge->dev);
+	child->dev.parent = child->bridge;
+	pci_set_bus_of_node(child);
+	pci_set_bus_speed(child);
+
+	/* Set up default resource pointers and names.. */
+	for (i = 0; i < PCI_BRIDGE_RESOURCE_NUM; i++) {
+		child->resource[i] = &bridge->resource[PCI_BRIDGE_RESOURCES+i];
+		child->resource[i]->name = child->name;
+	}
+	bridge->subordinate = child;
+
+add_dev:
+	ret = device_register(&child->dev);
+	WARN_ON(ret < 0);
+
+	pcibios_add_bus(child);
+
+	/* Create legacy_io and legacy_mem files for this bus */
+	pci_create_legacy_files(child);
+
+	return child;
+}
+
+struct pci_bus *__ref pci_add_new_bus(struct pci_bus *parent, struct pci_dev *dev, int busnr)
+{
+	struct pci_bus *child;
+
+	child = pci_alloc_child_bus(parent, dev, busnr);
+	if (child) {
+		down_write(&pci_bus_sem);
+		list_add_tail(&child->node, &parent->children);
+		up_write(&pci_bus_sem);
+	}
+	return child;
+}
+
+static void pci_fixup_parent_subordinate_busnr(struct pci_bus *child, int max)
+{
+	struct pci_bus *parent = child->parent;
+
+	/* Attempts to fix that up are really dangerous unless
+	   we're going to re-assign all bus numbers. */
+	if (!pcibios_assign_all_busses())
+		return;
+
+	while (parent->parent && parent->busn_res.end < max) {
+		parent->busn_res.end = max;
+		pci_write_config_byte(parent->self, PCI_SUBORDINATE_BUS, max);
+		parent = parent->parent;
+	}
+}
+
+/*
+ * If it's a bridge, configure it and scan the bus behind it.
+ * For CardBus bridges, we don't scan behind as the devices will
+ * be handled by the bridge driver itself.
+ *
+ * We need to process bridges in two passes -- first we scan those
+ * already configured by the BIOS and after we are done with all of
+ * them, we proceed to assigning numbers to the remaining buses in
+ * order to avoid overlaps between old and new bus numbers.
+ */
+int pci_scan_bridge(struct pci_bus *bus, struct pci_dev *dev, int max, int pass)
+{
+	struct pci_bus *child;
+	int is_cardbus = (dev->hdr_type == PCI_HEADER_TYPE_CARDBUS);
+	u32 buses, i, j = 0;
+	u16 bctl;
+	u8 primary, secondary, subordinate;
+	int broken = 0;
+
+	pci_read_config_dword(dev, PCI_PRIMARY_BUS, &buses);
+	primary = buses & 0xFF;
+	secondary = (buses >> 8) & 0xFF;
+	subordinate = (buses >> 16) & 0xFF;
+
+	dev_dbg(&dev->dev, "scanning [bus %02x-%02x] behind bridge, pass %d\n",
+		secondary, subordinate, pass);
+
+	if (!primary && (primary != bus->number) && secondary && subordinate) {
+		dev_warn(&dev->dev, "Primary bus is hard wired to 0\n");
+		primary = bus->number;
+	}
+
+	/* Check if setup is sensible at all */
+	if (!pass &&
+	    (primary != bus->number || secondary <= bus->number ||
+	     secondary > subordinate)) {
+		dev_info(&dev->dev, "bridge configuration invalid ([bus %02x-%02x]), reconfiguring\n",
+			 secondary, subordinate);
+		broken = 1;
+	}
+
+	/* Disable MasterAbortMode during probing to avoid reporting
+	   of bus errors (in some architectures) */ 
+	pci_read_config_word(dev, PCI_BRIDGE_CONTROL, &bctl);
+	pci_write_config_word(dev, PCI_BRIDGE_CONTROL,
+			      bctl & ~PCI_BRIDGE_CTL_MASTER_ABORT);
+
+	if ((secondary || subordinate) && !pcibios_assign_all_busses() &&
+	    !is_cardbus && !broken) {
+		unsigned int cmax;
+		/*
+		 * Bus already configured by firmware, process it in the first
+		 * pass and just note the configuration.
+		 */
+		if (pass)
+			goto out;
+
+		/*
+		 * If we already got to this bus through a different bridge,
+		 * don't re-add it. This can happen with the i450NX chipset.
+		 *
+		 * However, we continue to descend down the hierarchy and
+		 * scan remaining child buses.
+		 */
+		child = pci_find_bus(pci_domain_nr(bus), secondary);
+		if (!child) {
+			child = pci_add_new_bus(bus, dev, secondary);
+			if (!child)
+				goto out;
+			child->primary = primary;
+			pci_bus_insert_busn_res(child, secondary, subordinate);
+			child->bridge_ctl = bctl;
+		}
+
+		cmax = pci_scan_child_bus(child);
+		if (cmax > max)
+			max = cmax;
+		if (child->busn_res.end > max)
+			max = child->busn_res.end;
+	} else {
+		/*
+		 * We need to assign a number to this bus which we always
+		 * do in the second pass.
+		 */
+		if (!pass) {
+			if (pcibios_assign_all_busses() || broken)
+				/* Temporarily disable forwarding of the
+				   configuration cycles on all bridges in
+				   this bus segment to avoid possible
+				   conflicts in the second pass between two
+				   bridges programmed with overlapping
+				   bus ranges. */
+				pci_write_config_dword(dev, PCI_PRIMARY_BUS,
+						       buses & ~0xffffff);
+			goto out;
+		}
+
+		/* Clear errors */
+		pci_write_config_word(dev, PCI_STATUS, 0xffff);
+
+		/* Prevent assigning a bus number that already exists.
+		 * This can happen when a bridge is hot-plugged, so in
+		 * this case we only re-scan this bus. */
+		child = pci_find_bus(pci_domain_nr(bus), max+1);
+		if (!child) {
+			child = pci_add_new_bus(bus, dev, ++max);
+			if (!child)
+				goto out;
+			pci_bus_insert_busn_res(child, max, 0xff);
+		}
+		buses = (buses & 0xff000000)
+		      | ((unsigned int)(child->primary)     <<  0)
+		      | ((unsigned int)(child->busn_res.start)   <<  8)
+		      | ((unsigned int)(child->busn_res.end) << 16);
+
+		/*
+		 * yenta.c forces a secondary latency timer of 176.
+		 * Copy that behaviour here.
+		 */
+		if (is_cardbus) {
+			buses &= ~0xff000000;
+			buses |= CARDBUS_LATENCY_TIMER << 24;
+		}
+
+		/*
+		 * We need to blast all three values with a single write.
+		 */
+		pci_write_config_dword(dev, PCI_PRIMARY_BUS, buses);
+
+		if (!is_cardbus) {
+			child->bridge_ctl = bctl;
+			/*
+			 * Adjust subordinate busnr in parent buses.
+			 * We do this before scanning for children because
+			 * some devices may not be detected if the bios
+			 * was lazy.
+			 */
+			pci_fixup_parent_subordinate_busnr(child, max);
+			/* Now we can scan all subordinate buses... */
+			max = pci_scan_child_bus(child);
+			/*
+			 * now fix it up again since we have found
+			 * the real value of max.
+			 */
+			pci_fixup_parent_subordinate_busnr(child, max);
+		} else {
+			/*
+			 * For CardBus bridges, we leave 4 bus numbers
+			 * as cards with a PCI-to-PCI bridge can be
+			 * inserted later.
+			 */
+			for (i=0; i<CARDBUS_RESERVE_BUSNR; i++) {
+				struct pci_bus *parent = bus;
+				if (pci_find_bus(pci_domain_nr(bus),
+							max+i+1))
+					break;
+				while (parent->parent) {
+					if ((!pcibios_assign_all_busses()) &&
+					    (parent->busn_res.end > max) &&
+					    (parent->busn_res.end <= max+i)) {
+						j = 1;
+					}
+					parent = parent->parent;
+				}
+				if (j) {
+					/*
+					 * Often, there are two cardbus bridges
+					 * -- try to leave one valid bus number
+					 * for each one.
+					 */
+					i /= 2;
+					break;
+				}
+			}
+			max += i;
+			pci_fixup_parent_subordinate_busnr(child, max);
+		}
+		/*
+		 * Set the subordinate bus number to its real value.
+		 */
+		pci_bus_update_busn_res_end(child, max);
+		pci_write_config_byte(dev, PCI_SUBORDINATE_BUS, max);
+	}
+
+	sprintf(child->name,
+		(is_cardbus ? "PCI CardBus %04x:%02x" : "PCI Bus %04x:%02x"),
+		pci_domain_nr(bus), child->number);
+
+	/* Has only triggered on CardBus, fixup is in yenta_socket */
+	while (bus->parent) {
+		if ((child->busn_res.end > bus->busn_res.end) ||
+		    (child->number > bus->busn_res.end) ||
+		    (child->number < bus->number) ||
+		    (child->busn_res.end < bus->number)) {
+			dev_info(&child->dev, "%pR %s "
+				"hidden behind%s bridge %s %pR\n",
+				&child->busn_res,
+				(bus->number > child->busn_res.end &&
+				 bus->busn_res.end < child->number) ?
+					"wholly" : "partially",
+				bus->self->transparent ? " transparent" : "",
+				dev_name(&bus->dev),
+				&bus->busn_res);
+		}
+		bus = bus->parent;
+	}
+
+out:
+	pci_write_config_word(dev, PCI_BRIDGE_CONTROL, bctl);
+
+	return max;
+}
+
+/*
+ * Read interrupt line and base address registers.
+ * The architecture-dependent code can tweak these, of course.
+ */
+static void pci_read_irq(struct pci_dev *dev)
+{
+	unsigned char irq;
+
+	pci_read_config_byte(dev, PCI_INTERRUPT_PIN, &irq);
+	dev->pin = irq;
+	if (irq)
+		pci_read_config_byte(dev, PCI_INTERRUPT_LINE, &irq);
+	dev->irq = irq;
+}
+
+void set_pcie_port_type(struct pci_dev *pdev)
+{
+	int pos;
+	u16 reg16;
+
+	pos = pci_find_capability(pdev, PCI_CAP_ID_EXP);
+	if (!pos)
+		return;
+	pdev->is_pcie = 1;
+	pdev->pcie_cap = pos;
+	pci_read_config_word(pdev, pos + PCI_EXP_FLAGS, &reg16);
+	pdev->pcie_flags_reg = reg16;
+	pci_read_config_word(pdev, pos + PCI_EXP_DEVCAP, &reg16);
+	pdev->pcie_mpss = reg16 & PCI_EXP_DEVCAP_PAYLOAD;
+}
+
+void set_pcie_hotplug_bridge(struct pci_dev *pdev)
+{
+	u32 reg32;
+
+	pcie_capability_read_dword(pdev, PCI_EXP_SLTCAP, &reg32);
+	if (reg32 & PCI_EXP_SLTCAP_HPC)
+		pdev->is_hotplug_bridge = 1;
+}
+
+#define LEGACY_IO_RESOURCE	(IORESOURCE_IO | IORESOURCE_PCI_FIXED)
+
+/**
+ * pci_setup_device - fill in class and map information of a device
+ * @dev: the device structure to fill
+ *
+ * Initialize the device structure with information about the device's 
+ * vendor,class,memory and IO-space addresses,IRQ lines etc.
+ * Called at initialisation of the PCI subsystem and by CardBus services.
+ * Returns 0 on success and negative if unknown type of device (not normal,
+ * bridge or CardBus).
+ */
+int pci_setup_device(struct pci_dev *dev)
+{
+	u32 class;
+	u8 hdr_type;
+	struct pci_slot *slot;
+	int pos = 0;
+	struct pci_bus_region region;
+	struct resource *res;
+
+	if (pci_read_config_byte(dev, PCI_HEADER_TYPE, &hdr_type))
+		return -EIO;
+
+	dev->sysdata = dev->bus->sysdata;
+	dev->dev.parent = dev->bus->bridge;
+	dev->dev.bus = &pci_bus_type;
+	dev->hdr_type = hdr_type & 0x7f;
+	dev->multifunction = !!(hdr_type & 0x80);
+	dev->error_state = pci_channel_io_normal;
+	set_pcie_port_type(dev);
+
+	list_for_each_entry(slot, &dev->bus->slots, list)
+		if (PCI_SLOT(dev->devfn) == slot->number)
+			dev->slot = slot;
+
+	/* Assume 32-bit PCI; let 64-bit PCI cards (which are far rarer)
+	   set this higher, assuming the system even supports it.  */
+	dev->dma_mask = 0xffffffff;
+
+	dev_set_name(&dev->dev, "%04x:%02x:%02x.%d", pci_domain_nr(dev->bus),
+		     dev->bus->number, PCI_SLOT(dev->devfn),
+		     PCI_FUNC(dev->devfn));
+
+	pci_read_config_dword(dev, PCI_CLASS_REVISION, &class);
+	dev->revision = class & 0xff;
+	dev->class = class >> 8;		    /* upper 3 bytes */
+
+	dev_printk(KERN_DEBUG, &dev->dev, "[%04x:%04x] type %02x class %#08x\n",
+		   dev->vendor, dev->device, dev->hdr_type, dev->class);
+
+	/* need to have dev->class ready */
+	dev->cfg_size = pci_cfg_space_size(dev);
+
+	/* "Unknown power state" */
+	dev->current_state = PCI_UNKNOWN;
+
+	/* Early fixups, before probing the BARs */
+	pci_fixup_device(pci_fixup_early, dev);
+	/* device class may be changed after fixup */
+	class = dev->class >> 8;
+
+	switch (dev->hdr_type) {		    /* header type */
+	case PCI_HEADER_TYPE_NORMAL:		    /* standard header */
+		if (class == PCI_CLASS_BRIDGE_PCI)
+			goto bad;
+		pci_read_irq(dev);
+		pci_read_bases(dev, 6, PCI_ROM_ADDRESS);
+		pci_read_config_word(dev, PCI_SUBSYSTEM_VENDOR_ID, &dev->subsystem_vendor);
+		pci_read_config_word(dev, PCI_SUBSYSTEM_ID, &dev->subsystem_device);
+
+		/*
+		 *	Do the ugly legacy mode stuff here rather than broken chip
+		 *	quirk code. Legacy mode ATA controllers have fixed
+		 *	addresses. These are not always echoed in BAR0-3, and
+		 *	BAR0-3 in a few cases contain junk!
+		 */
+		if (class == PCI_CLASS_STORAGE_IDE) {
+			u8 progif;
+			pci_read_config_byte(dev, PCI_CLASS_PROG, &progif);
+			if ((progif & 1) == 0) {
+				region.start = 0x1F0;
+				region.end = 0x1F7;
+				res = &dev->resource[0];
+				res->flags = LEGACY_IO_RESOURCE;
+				pcibios_bus_to_resource(dev, res, &region);
+				region.start = 0x3F6;
+				region.end = 0x3F6;
+				res = &dev->resource[1];
+				res->flags = LEGACY_IO_RESOURCE;
+				pcibios_bus_to_resource(dev, res, &region);
+			}
+			if ((progif & 4) == 0) {
+				region.start = 0x170;
+				region.end = 0x177;
+				res = &dev->resource[2];
+				res->flags = LEGACY_IO_RESOURCE;
+				pcibios_bus_to_resource(dev, res, &region);
+				region.start = 0x376;
+				region.end = 0x376;
+				res = &dev->resource[3];
+				res->flags = LEGACY_IO_RESOURCE;
+				pcibios_bus_to_resource(dev, res, &region);
+			}
+		}
+		break;
+
+	case PCI_HEADER_TYPE_BRIDGE:		    /* bridge header */
+		if (class != PCI_CLASS_BRIDGE_PCI)
+			goto bad;
+		/* The PCI-to-PCI bridge spec requires that subtractive
+		   decoding (i.e. transparent) bridge must have programming
+		   interface code of 0x01. */ 
+		pci_read_irq(dev);
+		dev->transparent = ((dev->class & 0xff) == 1);
+		pci_read_bases(dev, 2, PCI_ROM_ADDRESS1);
+		set_pcie_hotplug_bridge(dev);
+		pos = pci_find_capability(dev, PCI_CAP_ID_SSVID);
+		if (pos) {
+			pci_read_config_word(dev, pos + PCI_SSVID_VENDOR_ID, &dev->subsystem_vendor);
+			pci_read_config_word(dev, pos + PCI_SSVID_DEVICE_ID, &dev->subsystem_device);
+		}
+		break;
+
+	case PCI_HEADER_TYPE_CARDBUS:		    /* CardBus bridge header */
+		if (class != PCI_CLASS_BRIDGE_CARDBUS)
+			goto bad;
+		pci_read_irq(dev);
+		pci_read_bases(dev, 1, 0);
+		pci_read_config_word(dev, PCI_CB_SUBSYSTEM_VENDOR_ID, &dev->subsystem_vendor);
+		pci_read_config_word(dev, PCI_CB_SUBSYSTEM_ID, &dev->subsystem_device);
+		break;
+
+	default:				    /* unknown header */
+		dev_err(&dev->dev, "unknown header type %02x, "
+			"ignoring device\n", dev->hdr_type);
+		return -EIO;
+
+	bad:
+		dev_err(&dev->dev, "ignoring class %#08x (doesn't match header "
+			"type %02x)\n", dev->class, dev->hdr_type);
+		dev->class = PCI_CLASS_NOT_DEFINED;
+	}
+
+	/* We found a fine healthy device, go go go... */
+	return 0;
+}
+
+static void pci_release_capabilities(struct pci_dev *dev)
+{
+	pci_vpd_release(dev);
+	pci_iov_release(dev);
+	pci_free_cap_save_buffers(dev);
+}
+
+/**
+ * pci_release_dev - free a pci device structure when all users of it are finished.
+ * @dev: device that's been disconnected
+ *
+ * Will be called only by the device core when all users of this pci device are
+ * done.
+ */
+static void pci_release_dev(struct device *dev)
+{
+	struct pci_dev *pci_dev;
+
+	pci_dev = to_pci_dev(dev);
+	pci_release_capabilities(pci_dev);
+	pci_release_of_node(pci_dev);
+	kfree(pci_dev);
+}
+
+/**
+ * pci_cfg_space_size - get the configuration space size of the PCI device.
+ * @dev: PCI device
+ *
+ * Regular PCI devices have 256 bytes, but PCI-X 2 and PCI Express devices
+ * have 4096 bytes.  Even if the device is capable, that doesn't mean we can
+ * access it.  Maybe we don't have a way to generate extended config space
+ * accesses, or the device is behind a reverse Express bridge.  So we try
+ * reading the dword at 0x100 which must either be 0 or a valid extended
+ * capability header.
+ */
+int pci_cfg_space_size_ext(struct pci_dev *dev)
+{
+	u32 status;
+	int pos = PCI_CFG_SPACE_SIZE;
+
+	if (pci_read_config_dword(dev, pos, &status) != PCIBIOS_SUCCESSFUL)
+		goto fail;
+	if (status == 0xffffffff)
+		goto fail;
+
+	return PCI_CFG_SPACE_EXP_SIZE;
+
+ fail:
+	return PCI_CFG_SPACE_SIZE;
+}
+
+int pci_cfg_space_size(struct pci_dev *dev)
+{
+	int pos;
+	u32 status;
+	u16 class;
+
+	class = dev->class >> 8;
+	if (class == PCI_CLASS_BRIDGE_HOST)
+		return pci_cfg_space_size_ext(dev);
+
+	if (!pci_is_pcie(dev)) {
+		pos = pci_find_capability(dev, PCI_CAP_ID_PCIX);
+		if (!pos)
+			goto fail;
+
+		pci_read_config_dword(dev, pos + PCI_X_STATUS, &status);
+		if (!(status & (PCI_X_STATUS_266MHZ | PCI_X_STATUS_533MHZ)))
+			goto fail;
+	}
+
+	return pci_cfg_space_size_ext(dev);
+
+ fail:
+	return PCI_CFG_SPACE_SIZE;
+}
+
+static void pci_release_bus_bridge_dev(struct device *dev)
+{
+	struct pci_host_bridge *bridge = to_pci_host_bridge(dev);
+
+	if (bridge->release_fn)
+		bridge->release_fn(bridge);
+
+	pci_free_resource_list(&bridge->windows);
+
+	kfree(bridge);
+}
+
+struct pci_dev *alloc_pci_dev(void)
+{
+	struct pci_dev *dev;
+
+	dev = kzalloc(sizeof(struct pci_dev), GFP_KERNEL);
+	if (!dev)
+		return NULL;
+
+	INIT_LIST_HEAD(&dev->bus_list);
+	dev->dev.type = &pci_dev_type;
+
+	return dev;
+}
+EXPORT_SYMBOL(alloc_pci_dev);
+
+bool pci_bus_read_dev_vendor_id(struct pci_bus *bus, int devfn, u32 *l,
+				 int crs_timeout)
+{
+	int delay = 1;
+
+	if (pci_bus_read_config_dword(bus, devfn, PCI_VENDOR_ID, l))
+		return false;
+
+	/* some broken boards return 0 or ~0 if a slot is empty: */
+	if (*l == 0xffffffff || *l == 0x00000000 ||
+	    *l == 0x0000ffff || *l == 0xffff0000)
+		return false;
+
+	/* Configuration request Retry Status */
+	while (*l == 0xffff0001) {
+		if (!crs_timeout)
+			return false;
+
+		msleep(delay);
+		delay *= 2;
+		if (pci_bus_read_config_dword(bus, devfn, PCI_VENDOR_ID, l))
+			return false;
+		/* Card hasn't responded in 60 seconds?  Must be stuck. */
+		if (delay > crs_timeout) {
+			printk(KERN_WARNING "pci %04x:%02x:%02x.%d: not "
+					"responding\n", pci_domain_nr(bus),
+					bus->number, PCI_SLOT(devfn),
+					PCI_FUNC(devfn));
+			return false;
+		}
+	}
+
+	return true;
+}
+EXPORT_SYMBOL(pci_bus_read_dev_vendor_id);
+
+/*
+ * Read the config data for a PCI device, sanity-check it
+ * and fill in the dev structure...
+ */
+static struct pci_dev *pci_scan_device(struct pci_bus *bus, int devfn)
+{
+	struct pci_dev *dev;
+	u32 l;
+
+	if (!pci_bus_read_dev_vendor_id(bus, devfn, &l, 60*1000))
+		return NULL;
+
+	dev = alloc_pci_dev();
+	if (!dev)
+		return NULL;
+
+	dev->bus = bus;
+	dev->devfn = devfn;
+	dev->vendor = l & 0xffff;
+	dev->device = (l >> 16) & 0xffff;
+
+	pci_set_of_node(dev);
+
+	if (pci_setup_device(dev)) {
+		kfree(dev);
+		return NULL;
+	}
+
+	return dev;
+}
+
+static void pci_init_capabilities(struct pci_dev *dev)
+{
+	/* MSI/MSI-X list */
+	pci_msi_init_pci_dev(dev);
+
+	/* Buffers for saving PCIe and PCI-X capabilities */
+	pci_allocate_cap_save_buffers(dev);
+
+	/* Power Management */
+	pci_pm_init(dev);
+
+	/* Vital Product Data */
+	pci_vpd_pci22_init(dev);
+
+	/* Alternative Routing-ID Forwarding */
+	pci_configure_ari(dev);
+
+	/* Single Root I/O Virtualization */
+	pci_iov_init(dev);
+
+	/* Enable ACS P2P upstream forwarding */
+	pci_enable_acs(dev);
+}
+
+void pci_device_add(struct pci_dev *dev, struct pci_bus *bus)
+{
+	int ret;
+
+	device_initialize(&dev->dev);
+	dev->dev.release = pci_release_dev;
+
+	set_dev_node(&dev->dev, pcibus_to_node(bus));
+	dev->dev.dma_mask = &dev->dma_mask;
+	dev->dev.dma_parms = &dev->dma_parms;
+	dev->dev.coherent_dma_mask = 0xffffffffull;
+
+	pci_set_dma_max_seg_size(dev, 65536);
+	pci_set_dma_seg_boundary(dev, 0xffffffff);
+
+	/* Fix up broken headers */
+	pci_fixup_device(pci_fixup_header, dev);
+
+	/* moved out from quirk header fixup code */
+	pci_reassigndev_resource_alignment(dev);
+
+	/* Clear the state_saved flag. */
+	dev->state_saved = false;
+
+	/* Initialize various capabilities */
+	pci_init_capabilities(dev);
+
+	/*
+	 * Add the device to our list of discovered devices
+	 * and the bus list for fixup functions, etc.
+	 */
+	down_write(&pci_bus_sem);
+	list_add_tail(&dev->bus_list, &bus->devices);
+	up_write(&pci_bus_sem);
+
+	ret = pcibios_add_device(dev);
+	WARN_ON(ret < 0);
+
+	/* Notifier could use PCI capabilities */
+	dev->match_driver = false;
+	ret = device_add(&dev->dev);
+	WARN_ON(ret < 0);
+
+	pci_proc_attach_device(dev);
+}
+
+struct pci_dev *__ref pci_scan_single_device(struct pci_bus *bus, int devfn)
+{
+	struct pci_dev *dev;
+
+	dev = pci_get_slot(bus, devfn);
+	if (dev) {
+		pci_dev_put(dev);
+		return dev;
+	}
+
+	dev = pci_scan_device(bus, devfn);
+	if (!dev)
+		return NULL;
+
+	pci_device_add(dev, bus);
+
+	return dev;
+}
+EXPORT_SYMBOL(pci_scan_single_device);
+
+static unsigned next_fn(struct pci_bus *bus, struct pci_dev *dev, unsigned fn)
+{
+	int pos;
+	u16 cap = 0;
+	unsigned next_fn;
+
+	if (pci_ari_enabled(bus)) {
+		if (!dev)
+			return 0;
+		pos = pci_find_ext_capability(dev, PCI_EXT_CAP_ID_ARI);
+		if (!pos)
+			return 0;
+
+		pci_read_config_word(dev, pos + PCI_ARI_CAP, &cap);
+		next_fn = PCI_ARI_CAP_NFN(cap);
+		if (next_fn <= fn)
+			return 0;	/* protect against malformed list */
+
+		return next_fn;
+	}
+
+	/* dev may be NULL for non-contiguous multifunction devices */
+	if (!dev || dev->multifunction)
+		return (fn + 1) % 8;
+
+	return 0;
+}
+
+static int only_one_child(struct pci_bus *bus)
+{
+	struct pci_dev *parent = bus->self;
+
+	if (!parent || !pci_is_pcie(parent))
+		return 0;
+	if (pci_pcie_type(parent) == PCI_EXP_TYPE_ROOT_PORT)
+		return 1;
+	if (pci_pcie_type(parent) == PCI_EXP_TYPE_DOWNSTREAM &&
+	    !pci_has_flag(PCI_SCAN_ALL_PCIE_DEVS))
+		return 1;
+	return 0;
+}
+
+/**
+ * pci_scan_slot - scan a PCI slot on a bus for devices.
+ * @bus: PCI bus to scan
+ * @devfn: slot number to scan (must have zero function.)
+ *
+ * Scan a PCI slot on the specified PCI bus for devices, adding
+ * discovered devices to the @bus->devices list.  New devices
+ * will not have is_added set.
+ *
+ * Returns the number of new devices found.
+ */
+int pci_scan_slot(struct pci_bus *bus, int devfn)
+{
+	unsigned fn, nr = 0;
+	struct pci_dev *dev;
+
+	if (only_one_child(bus) && (devfn > 0))
+		return 0; /* Already scanned the entire slot */
+
+	dev = pci_scan_single_device(bus, devfn);
+	if (!dev)
+		return 0;
+	if (!dev->is_added)
+		nr++;
+
+	for (fn = next_fn(bus, dev, 0); fn > 0; fn = next_fn(bus, dev, fn)) {
+		dev = pci_scan_single_device(bus, devfn + fn);
+		if (dev) {
+			if (!dev->is_added)
+				nr++;
+			dev->multifunction = 1;
+		}
+	}
+
+	/* only one slot has pcie device */
+	if (bus->self && nr)
+		pcie_aspm_init_link_state(bus->self);
+
+	return nr;
+}
+
+static int pcie_find_smpss(struct pci_dev *dev, void *data)
+{
+	u8 *smpss = data;
+
+	if (!pci_is_pcie(dev))
+		return 0;
+
+	/* For PCIE hotplug enabled slots not connected directly to a
+	 * PCI-E root port, there can be problems when hotplugging
+	 * devices.  This is due to the possibility of hotplugging a
+	 * device into the fabric with a smaller MPS that the devices
+	 * currently running have configured.  Modifying the MPS on the
+	 * running devices could cause a fatal bus error due to an
+	 * incoming frame being larger than the newly configured MPS.
+	 * To work around this, the MPS for the entire fabric must be
+	 * set to the minimum size.  Any devices hotplugged into this
+	 * fabric will have the minimum MPS set.  If the PCI hotplug
+	 * slot is directly connected to the root port and there are not
+	 * other devices on the fabric (which seems to be the most
+	 * common case), then this is not an issue and MPS discovery
+	 * will occur as normal.
+	 */
+	if (dev->is_hotplug_bridge && (!list_is_singular(&dev->bus->devices) ||
+	     (dev->bus->self &&
+	      pci_pcie_type(dev->bus->self) != PCI_EXP_TYPE_ROOT_PORT)))
+		*smpss = 0;
+
+	if (*smpss > dev->pcie_mpss)
+		*smpss = dev->pcie_mpss;
+
+	return 0;
+}
+
+static void pcie_write_mps(struct pci_dev *dev, int mps)
+{
+	int rc;
+
+	if (pcie_bus_config == PCIE_BUS_PERFORMANCE) {
+		mps = 128 << dev->pcie_mpss;
+
+		if (pci_pcie_type(dev) != PCI_EXP_TYPE_ROOT_PORT &&
+		    dev->bus->self)
+			/* For "Performance", the assumption is made that
+			 * downstream communication will never be larger than
+			 * the MRRS.  So, the MPS only needs to be configured
+			 * for the upstream communication.  This being the case,
+			 * walk from the top down and set the MPS of the child
+			 * to that of the parent bus.
+			 *
+			 * Configure the device MPS with the smaller of the
+			 * device MPSS or the bridge MPS (which is assumed to be
+			 * properly configured at this point to the largest
+			 * allowable MPS based on its parent bus).
+			 */
+			mps = min(mps, pcie_get_mps(dev->bus->self));
+	}
+
+	rc = pcie_set_mps(dev, mps);
+	if (rc)
+		dev_err(&dev->dev, "Failed attempting to set the MPS\n");
+}
+
+static void pcie_write_mrrs(struct pci_dev *dev)
+{
+	int rc, mrrs;
+
+	/* In the "safe" case, do not configure the MRRS.  There appear to be
+	 * issues with setting MRRS to 0 on a number of devices.
+	 */
+	if (pcie_bus_config != PCIE_BUS_PERFORMANCE)
+		return;
+
+	/* For Max performance, the MRRS must be set to the largest supported
+	 * value.  However, it cannot be configured larger than the MPS the
+	 * device or the bus can support.  This should already be properly
+	 * configured by a prior call to pcie_write_mps.
+	 */
+	mrrs = pcie_get_mps(dev);
+
+	/* MRRS is a R/W register.  Invalid values can be written, but a
+	 * subsequent read will verify if the value is acceptable or not.
+	 * If the MRRS value provided is not acceptable (e.g., too large),
+	 * shrink the value until it is acceptable to the HW.
+ 	 */
+	while (mrrs != pcie_get_readrq(dev) && mrrs >= 128) {
+		rc = pcie_set_readrq(dev, mrrs);
+		if (!rc)
+			break;
+
+		dev_warn(&dev->dev, "Failed attempting to set the MRRS\n");
+		mrrs /= 2;
+	}
+
+	if (mrrs < 128)
+		dev_err(&dev->dev, "MRRS was unable to be configured with a "
+			"safe value.  If problems are experienced, try running "
+			"with pci=pcie_bus_safe.\n");
+}
+
+static int pcie_bus_configure_set(struct pci_dev *dev, void *data)
+{
+	int mps, orig_mps;
+
+	if (!pci_is_pcie(dev))
+		return 0;
+
+	mps = 128 << *(u8 *)data;
+	orig_mps = pcie_get_mps(dev);
+
+	pcie_write_mps(dev, mps);
+	pcie_write_mrrs(dev);
+
+	dev_info(&dev->dev, "PCI-E Max Payload Size set to %4d/%4d (was %4d), "
+		 "Max Read Rq %4d\n", pcie_get_mps(dev), 128 << dev->pcie_mpss,
+		 orig_mps, pcie_get_readrq(dev));
+
+	return 0;
+}
+
+/* pcie_bus_configure_settings requires that pci_walk_bus work in a top-down,
+ * parents then children fashion.  If this changes, then this code will not
+ * work as designed.
+ */
+void pcie_bus_configure_settings(struct pci_bus *bus, u8 mpss)
+{
+	u8 smpss;
+
+	if (!pci_is_pcie(bus->self))
+		return;
+
+	if (pcie_bus_config == PCIE_BUS_TUNE_OFF)
+		return;
+
+	/* FIXME - Peer to peer DMA is possible, though the endpoint would need
+	 * to be aware to the MPS of the destination.  To work around this,
+	 * simply force the MPS of the entire system to the smallest possible.
+	 */
+	if (pcie_bus_config == PCIE_BUS_PEER2PEER)
+		smpss = 0;
+
+	if (pcie_bus_config == PCIE_BUS_SAFE) {
+		smpss = mpss;
+
+		pcie_find_smpss(bus->self, &smpss);
+		pci_walk_bus(bus, pcie_find_smpss, &smpss);
+	}
+
+	pcie_bus_configure_set(bus->self, &smpss);
+	pci_walk_bus(bus, pcie_bus_configure_set, &smpss);
+}
+EXPORT_SYMBOL_GPL(pcie_bus_configure_settings);
+
+unsigned int pci_scan_child_bus(struct pci_bus *bus)
+{
+	unsigned int devfn, pass, max = bus->busn_res.start;
+	struct pci_dev *dev;
+
+	dev_dbg(&bus->dev, "scanning bus\n");
+
+	/* Go find them, Rover! */
+	for (devfn = 0; devfn < 0x100; devfn += 8)
+		pci_scan_slot(bus, devfn);
+
+	/* Reserve buses for SR-IOV capability. */
+	max += pci_iov_bus_range(bus);
+
+	/*
+	 * After performing arch-dependent fixup of the bus, look behind
+	 * all PCI-to-PCI bridges on this bus.
+	 */
+	if (!bus->is_added) {
+		dev_dbg(&bus->dev, "fixups for bus\n");
+		pcibios_fixup_bus(bus);
+		bus->is_added = 1;
+	}
+
+	for (pass=0; pass < 2; pass++)
+		list_for_each_entry(dev, &bus->devices, bus_list) {
+			if (dev->hdr_type == PCI_HEADER_TYPE_BRIDGE ||
+			    dev->hdr_type == PCI_HEADER_TYPE_CARDBUS)
+				max = pci_scan_bridge(bus, dev, max, pass);
+		}
+
+	/*
+	 * We've scanned the bus and so we know all about what's on
+	 * the other side of any bridges that may be on this bus plus
+	 * any devices.
+	 *
+	 * Return how far we've got finding sub-buses.
+	 */
+	dev_dbg(&bus->dev, "bus scan returning with max=%02x\n", max);
+	return max;
+}
+
+/**
+ * pcibios_root_bridge_prepare - Platform-specific host bridge setup.
+ * @bridge: Host bridge to set up.
+ *
+ * Default empty implementation.  Replace with an architecture-specific setup
+ * routine, if necessary.
+ */
+int __weak pcibios_root_bridge_prepare(struct pci_host_bridge *bridge)
+{
+	return 0;
+}
+
+void __weak pcibios_add_bus(struct pci_bus *bus)
+{
+}
+
+void __weak pcibios_remove_bus(struct pci_bus *bus)
+{
+}
+
+struct pci_bus *pci_create_root_bus(struct device *parent, int bus,
+		struct pci_ops *ops, void *sysdata, struct list_head *resources)
+{
+	int error;
+	struct pci_host_bridge *bridge;
+	struct pci_bus *b, *b2;
+	struct pci_host_bridge_window *window, *n;
+	struct resource *res;
+	resource_size_t offset;
+	char bus_addr[64];
+	char *fmt;
+
+	b = pci_alloc_bus();
+	if (!b)
+		return NULL;
+
+	b->sysdata = sysdata;
+	b->ops = ops;
+	b->number = b->busn_res.start = bus;
+	b2 = pci_find_bus(pci_domain_nr(b), bus);
+	if (b2) {
+		/* If we already got to this bus through a different bridge, ignore it */
+		dev_dbg(&b2->dev, "bus already known\n");
+		goto err_out;
+	}
+
+	bridge = pci_alloc_host_bridge(b);
+	if (!bridge)
+		goto err_out;
+
+	bridge->dev.parent = parent;
+	bridge->dev.release = pci_release_bus_bridge_dev;
+	dev_set_name(&bridge->dev, "pci%04x:%02x", pci_domain_nr(b), bus);
+	error = pcibios_root_bridge_prepare(bridge);
+	if (error) {
+		kfree(bridge);
+		goto err_out;
+	}
+
+	error = device_register(&bridge->dev);
+	if (error) {
+		put_device(&bridge->dev);
+		goto err_out;
+	}
+	b->bridge = get_device(&bridge->dev);
+	device_enable_async_suspend(b->bridge);
+	pci_set_bus_of_node(b);
+
+	if (!parent)
+		set_dev_node(b->bridge, pcibus_to_node(b));
+
+	b->dev.class = &pcibus_class;
+	b->dev.parent = b->bridge;
+	dev_set_name(&b->dev, "%04x:%02x", pci_domain_nr(b), bus);
+	error = device_register(&b->dev);
+	if (error)
+		goto class_dev_reg_err;
+
+	pcibios_add_bus(b);
+
+	/* Create legacy_io and legacy_mem files for this bus */
+	pci_create_legacy_files(b);
+
+	if (parent)
+		dev_info(parent, "PCI host bridge to bus %s\n", dev_name(&b->dev));
+	else
+		printk(KERN_INFO "PCI host bridge to bus %s\n", dev_name(&b->dev));
+
+	/* Add initial resources to the bus */
+	list_for_each_entry_safe(window, n, resources, list) {
+		list_move_tail(&window->list, &bridge->windows);
+		res = window->res;
+		offset = window->offset;
+		if (res->flags & IORESOURCE_BUS)
+			pci_bus_insert_busn_res(b, bus, res->end);
+		else
+			pci_bus_add_resource(b, res, 0);
+		if (offset) {
+			if (resource_type(res) == IORESOURCE_IO)
+				fmt = " (bus address [%#06llx-%#06llx])";
+			else
+				fmt = " (bus address [%#010llx-%#010llx])";
+			snprintf(bus_addr, sizeof(bus_addr), fmt,
+				 (unsigned long long) (res->start - offset),
+				 (unsigned long long) (res->end - offset));
+		} else
+			bus_addr[0] = '\0';
+		dev_info(&b->dev, "root bus resource %pR%s\n", res, bus_addr);
+	}
+
+	down_write(&pci_bus_sem);
+	list_add_tail(&b->node, &pci_root_buses);
+	up_write(&pci_bus_sem);
+
+	return b;
+
+class_dev_reg_err:
+	put_device(&bridge->dev);
+	device_unregister(&bridge->dev);
+err_out:
+	kfree(b);
+	return NULL;
+}
+
+int pci_bus_insert_busn_res(struct pci_bus *b, int bus, int bus_max)
+{
+	struct resource *res = &b->busn_res;
+	struct resource *parent_res, *conflict;
+
+	res->start = bus;
+	res->end = bus_max;
+	res->flags = IORESOURCE_BUS;
+
+	if (!pci_is_root_bus(b))
+		parent_res = &b->parent->busn_res;
+	else {
+		parent_res = get_pci_domain_busn_res(pci_domain_nr(b));
+		res->flags |= IORESOURCE_PCI_FIXED;
+	}
+
+	conflict = insert_resource_conflict(parent_res, res);
+
+	if (conflict)
+		dev_printk(KERN_DEBUG, &b->dev,
+			   "busn_res: can not insert %pR under %s%pR (conflicts with %s %pR)\n",
+			    res, pci_is_root_bus(b) ? "domain " : "",
+			    parent_res, conflict->name, conflict);
+
+	return conflict == NULL;
+}
+
+int pci_bus_update_busn_res_end(struct pci_bus *b, int bus_max)
+{
+	struct resource *res = &b->busn_res;
+	struct resource old_res = *res;
+	resource_size_t size;
+	int ret;
+
+	if (res->start > bus_max)
+		return -EINVAL;
+
+	size = bus_max - res->start + 1;
+	ret = adjust_resource(res, res->start, size);
+	dev_printk(KERN_DEBUG, &b->dev,
+			"busn_res: %pR end %s updated to %02x\n",
+			&old_res, ret ? "can not be" : "is", bus_max);
+
+	if (!ret && !res->parent)
+		pci_bus_insert_busn_res(b, res->start, res->end);
+
+	return ret;
+}
+
+void pci_bus_release_busn_res(struct pci_bus *b)
+{
+	struct resource *res = &b->busn_res;
+	int ret;
+
+	if (!res->flags || !res->parent)
+		return;
+
+	ret = release_resource(res);
+	dev_printk(KERN_DEBUG, &b->dev,
+			"busn_res: %pR %s released\n",
+			res, ret ? "can not be" : "is");
+}
+
+struct pci_bus *pci_scan_root_bus(struct device *parent, int bus,
+		struct pci_ops *ops, void *sysdata, struct list_head *resources)
+{
+	struct pci_host_bridge_window *window;
+	bool found = false;
+	struct pci_bus *b;
+	int max;
+
+	list_for_each_entry(window, resources, list)
+		if (window->res->flags & IORESOURCE_BUS) {
+			found = true;
+			break;
+		}
+
+	b = pci_create_root_bus(parent, bus, ops, sysdata, resources);
+	if (!b)
+		return NULL;
+
+	if (!found) {
+		dev_info(&b->dev,
+		 "No busn resource found for root bus, will use [bus %02x-ff]\n",
+			bus);
+		pci_bus_insert_busn_res(b, bus, 255);
+	}
+
+	max = pci_scan_child_bus(b);
+
+	if (!found)
+		pci_bus_update_busn_res_end(b, max);
+
+	pci_bus_add_devices(b);
+	return b;
+}
+EXPORT_SYMBOL(pci_scan_root_bus);
+
+/* Deprecated; use pci_scan_root_bus() instead */
+struct pci_bus *pci_scan_bus_parented(struct device *parent,
+		int bus, struct pci_ops *ops, void *sysdata)
+{
+	LIST_HEAD(resources);
+	struct pci_bus *b;
+
+	pci_add_resource(&resources, &ioport_resource);
+	pci_add_resource(&resources, &iomem_resource);
+	pci_add_resource(&resources, &busn_resource);
+	b = pci_create_root_bus(parent, bus, ops, sysdata, &resources);
+	if (b)
+		pci_scan_child_bus(b);
+	else
+		pci_free_resource_list(&resources);
+	return b;
+}
+EXPORT_SYMBOL(pci_scan_bus_parented);
+
+struct pci_bus *pci_scan_bus(int bus, struct pci_ops *ops,
+					void *sysdata)
+{
+	LIST_HEAD(resources);
+	struct pci_bus *b;
+
+	pci_add_resource(&resources, &ioport_resource);
+	pci_add_resource(&resources, &iomem_resource);
+	pci_add_resource(&resources, &busn_resource);
+	b = pci_create_root_bus(NULL, bus, ops, sysdata, &resources);
+	if (b) {
+		pci_scan_child_bus(b);
+		pci_bus_add_devices(b);
+	} else {
+		pci_free_resource_list(&resources);
+	}
+	return b;
+}
+EXPORT_SYMBOL(pci_scan_bus);
+
+/**
+ * pci_rescan_bus_bridge_resize - scan a PCI bus for devices.
+ * @bridge: PCI bridge for the bus to scan
+ *
+ * Scan a PCI bus and child buses for new devices, add them,
+ * and enable them, resizing bridge mmio/io resource if necessary
+ * and possible.  The caller must ensure the child devices are already
+ * removed for resizing to occur.
+ *
+ * Returns the max number of subordinate bus discovered.
+ */
+unsigned int __ref pci_rescan_bus_bridge_resize(struct pci_dev *bridge)
+{
+	unsigned int max;
+	struct pci_bus *bus = bridge->subordinate;
+
+	max = pci_scan_child_bus(bus);
+
+	pci_assign_unassigned_bridge_resources(bridge);
+
+	pci_bus_add_devices(bus);
+
+	return max;
+}
+
+/**
+ * pci_rescan_bus - scan a PCI bus for devices.
+ * @bus: PCI bus to scan
+ *
+ * Scan a PCI bus and child buses for new devices, adds them,
+ * and enables them.
+ *
+ * Returns the max number of subordinate bus discovered.
+ */
+unsigned int __ref pci_rescan_bus(struct pci_bus *bus)
+{
+	unsigned int max;
+
+	max = pci_scan_child_bus(bus);
+	pci_assign_unassigned_bus_resources(bus);
+	pci_enable_bridges(bus);
+	pci_bus_add_devices(bus);
+
+	return max;
+}
+EXPORT_SYMBOL_GPL(pci_rescan_bus);
+
+EXPORT_SYMBOL(pci_add_new_bus);
+EXPORT_SYMBOL(pci_scan_slot);
+EXPORT_SYMBOL(pci_scan_bridge);
+EXPORT_SYMBOL_GPL(pci_scan_child_bus);
+
+static int __init pci_sort_bf_cmp(const struct device *d_a, const struct device *d_b)
+{
+	const struct pci_dev *a = to_pci_dev(d_a);
+	const struct pci_dev *b = to_pci_dev(d_b);
+
+	if      (pci_domain_nr(a->bus) < pci_domain_nr(b->bus)) return -1;
+	else if (pci_domain_nr(a->bus) > pci_domain_nr(b->bus)) return  1;
+
+	if      (a->bus->number < b->bus->number) return -1;
+	else if (a->bus->number > b->bus->number) return  1;
+
+	if      (a->devfn < b->devfn) return -1;
+	else if (a->devfn > b->devfn) return  1;
+
+	return 0;
+}
+
+void __init pci_sort_breadthfirst(void)
+{
+	bus_sort_breadthfirst(&pci_bus_type, &pci_sort_bf_cmp);
+}
diff --git a/drivers/pci/quirks.c b/drivers/pci/quirks.c
index f52f18d..19cb413 100644
--- a/drivers/pci/quirks.c
+++ b/drivers/pci/quirks.c
@@ -1519,6 +1519,25 @@ DECLARE_PCI_FIXUP_RESUME_EARLY(PCI_VENDOR_ID_JMICRON, PCI_DEVICE_ID_JMICRON_JMB3
 
 #endif
 
+static void quirk_fujitsu_pci_fix_res(struct pci_dev *dev)
+{
+	struct pci_sys_data *sys;
+
+	dev_dbg(&dev->dev, "%s\n", __func__);
+
+	if (dev->sysdata) {
+		sys = (struct pci_sys_data *)dev->sysdata;
+		if (sys->map_irq)
+			pci_fixup_irqs(pci_common_swizzle, sys->map_irq);
+	}
+
+	if (!pci_has_flag(PCI_PROBE_ONLY)) {
+		pci_bus_size_bridges(dev->bus);
+		pci_bus_assign_resources(dev->bus);
+	}
+}
+DECLARE_PCI_FIXUP_FINAL(PCI_VENDOR_ID_FUJITSU_ME, PCI_ANY_ID, quirk_fujitsu_pci_fix_res);
+
 #ifdef CONFIG_X86_IO_APIC
 static void quirk_alder_ioapic(struct pci_dev *pdev)
 {
diff --git a/drivers/pci/remove.c b/drivers/pci/remove.c
index b1e7372..17baf7f 100644
--- a/drivers/pci/remove.c
+++ b/drivers/pci/remove.c
@@ -130,6 +130,7 @@ void pci_stop_root_bus(struct pci_bus *bus)
 	/* stop the host bridge */
 	device_release_driver(&host_bridge->dev);
 }
+EXPORT_SYMBOL(pci_stop_root_bus);
 
 void pci_remove_root_bus(struct pci_bus *bus)
 {
@@ -149,3 +150,4 @@ void pci_remove_root_bus(struct pci_bus *bus)
 	/* remove the host bridge */
 	device_unregister(&host_bridge->dev);
 }
+EXPORT_SYMBOL(pci_remove_root_bus);
diff --git a/drivers/spi/Kconfig b/drivers/spi/Kconfig
index e749aca..a4b75f5 100644
--- a/drivers/spi/Kconfig
+++ b/drivers/spi/Kconfig
@@ -228,6 +228,24 @@ config SPI_MPC512x_PSC
 	  This enables using the Freescale MPC5121 Programmable Serial
 	  Controller in SPI master mode.
 
+config HS_SPI
+	tristate "Fujitsu's High speed SPI controller"
+	depends on ARCH_MB8AC0300 || ARCH_MB86S70
+	select SPI_BITBANG
+	help
+	  SPI driver for Fujitsu's High speed SPI controller which provides
+	  various operating modes for interfacing to serial peripheral devices
+	  that use the de-facto standard SPI protocol.
+
+	  It also supports the new dual-bit and quad-bit SPI protocol.
+
+config HS_SPI_DEBUG
+	bool "HS SPI Debugging"
+	depends on HS_SPI
+	help
+	  This turns on low-level debugging for the entire HS SPI driver.
+	  Normally, you should say 'N'.
+
 config SPI_FSL_LIB
 	tristate
 	depends on OF
diff --git a/drivers/spi/Makefile b/drivers/spi/Makefile
index bf7e8dc..b46f134 100644
--- a/drivers/spi/Makefile
+++ b/drivers/spi/Makefile
@@ -34,6 +34,8 @@ obj-$(CONFIG_SPI_FSL_LIB)		+= spi-fsl-lib.o
 obj-$(CONFIG_SPI_FSL_ESPI)		+= spi-fsl-espi.o
 obj-$(CONFIG_SPI_FSL_SPI)		+= spi-fsl-spi.o
 obj-$(CONFIG_SPI_GPIO)			+= spi-gpio.o
+obj-$(CONFIG_HS_SPI_OLD)		+= hs_spi.o
+obj-$(CONFIG_HS_SPI)			+= spi-mb86s7x.o
 obj-$(CONFIG_SPI_IMX)			+= spi-imx.o
 obj-$(CONFIG_SPI_LM70_LLP)		+= spi-lm70llp.o
 obj-$(CONFIG_SPI_MPC512x_PSC)		+= spi-mpc512x-psc.o
diff --git a/drivers/spi/hs_spi.c b/drivers/spi/hs_spi.c
new file mode 100644
index 0000000..9a26523
--- /dev/null
+++ b/drivers/spi/hs_spi.c
@@ -0,0 +1,1387 @@
+/*
+ * linux/drivers/spi/hs_spi.c - high speed SPI controller driver
+ *
+ * Copyright (C) 2010-2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#include <linux/init.h>
+#include <linux/spinlock.h>
+#include <linux/interrupt.h>
+#include <linux/errno.h>
+#include <linux/err.h>
+#include <linux/clk.h>
+#include <linux/platform_device.h>
+#include <linux/io.h>
+#include <linux/dma-mapping.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_data/dma-mb8ac0300-xdmac.h>
+#include <linux/delay.h>
+
+#include <linux/spi/spi.h>
+#include <linux/spi/spi_bitbang.h>
+
+#include <linux/platform_data/mb8ac0300-hs_spi.h>
+
+#include "hs_spi_reg.h"
+
+// #define DEBUG
+#define	DRV_NAME "hs-spi"
+
+/* HS_SPI all TX interrupts except Slave Select Released Interrupt */
+#define	HS_SPI_TXINT_EXCEPT_TSSRC	(TXC_TFMTC_MASK << TXC_TFMTC_OFFSET |\
+				TXC_TFLETC_MASK << TXC_TFLETC_OFFSET |\
+				TXC_TFUC_MASK << TXC_TFUC_OFFSET |\
+				TXC_TFOC_MASK << TXC_TFOC_OFFSET |\
+				TXC_TFEC_MASK << TXC_TFEC_OFFSET |\
+				TXC_TFFC_MASK << TXC_TFFC_OFFSET)
+/* HS_SPI all RX interrupts except Slave Select Released Interrupt */
+#define	HS_SPI_RXINT_EXCEPT_RSSRC	(RXC_RFMTC_MASK << RXC_RFMTC_OFFSET |\
+				RXC_RFLETC_MASK << RXC_RFLETC_OFFSET |\
+				RXC_RFUC_MASK << RXC_RFUC_OFFSET |\
+				RXC_RFOC_MASK << RXC_RFOC_OFFSET |\
+				RXC_RFEC_MASK << RXC_RFEC_OFFSET |\
+				RXC_RFFC_MASK << RXC_RFFC_OFFSET)
+/* HS_SPI all TX interrupts */
+#define	HS_SPI_TX_ALL_INT	(TXC_TSSRC_MASK << TXC_TSSRC_OFFSET |\
+				HS_SPI_TXINT_EXCEPT_TSSRC)
+/* HS_SPI all RX interrupts */
+#define	HS_SPI_RX_ALL_INT	(RXC_RSSRC_MASK << RXC_RSSRC_OFFSET |\
+				HS_SPI_RXINT_EXCEPT_RSSRC)
+/* HS_SPI all fault interrupts */
+#define	HS_SPI_ALL_FAULT	(FAULTC_DRCBSFC_MASK << FAULTC_DRCBSFC_OFFSET |\
+				FAULTC_DWCBSFC_MASK << FAULTC_DWCBSFC_OFFSET |\
+				FAULTC_PVFC_MASK << FAULTC_PVFC_OFFSET |\
+				FAULTC_WAFC_MASK << FAULTC_WAFC_OFFSET |\
+				FAULTC_UMAFC_MASK << FAULTC_UMAFC_OFFSET)
+/* HS_SPI mode bits mask value */
+#define	HS_SPI_MODE_MASK	(PCC_CPHA_MASK << PCC_CPHA_OFFSET |\
+				PCC_CPOL_MASK << PCC_CPOL_OFFSET |\
+				PCC_SSPOL_MASK << PCC_SSPOL_OFFSET |\
+				PCC_SDIR_MASK << PCC_SDIR_OFFSET)
+
+/*
+ * HS_SPI Controller state
+ */
+struct hs_spi_cs {
+	unsigned char	mode;
+	unsigned char	chip_select;
+	unsigned int	speed_hz;
+};
+
+/*
+ * Debugging macro and defines
+ */
+#ifdef CONFIG_HS_SPI_DEBUG
+#define HS_SPI_DEBUG(n, args...) pr_info(args);
+#else /* CONFIG_HS_SPI_DEBUG */
+#define HS_SPI_DEBUG(n, args...)
+#endif /* CONFIG_HS_SPI_DEBUG */
+
+/*
+ * hs_spi_write_tx_fifo - write datas into the transmit FIFO at direct mode
+ * @hs:		HS SPI device platform data.
+ *
+ * No more than 16 byte datas can be write once.
+ *
+ * Returns write data size
+ */
+static int hs_spi_write_tx_fifo(struct hs_spi *hs)
+{
+	unsigned int	txflevel = HSSPI_BITS_GET(l, DMSTATUS_TXFLEVEL, hs,
+			DMSTATUS);
+	unsigned int	txbytes = min(HS_SPI_FIFO_LEN - txflevel,
+			hs->len - hs->tx_cnt);
+	unsigned int	i;
+	unsigned char	txdata;
+
+	for (i = 0; i < txbytes; i++) {
+		txdata = hs->tx ? hs->tx[hs->tx_cnt + i] : 0xFF;
+		hs_spi_writeb(hs, TXFIFO0, txdata);
+	}
+
+	return txbytes;
+}
+
+/*
+ * hs_spi_read_rx_fifo - read datas from the receive FIFO direct mode
+ * @hs:		HS SPI device platform data.
+ *
+ * No more then 16 byte datas can be read once.
+ *
+ * Returns read data size
+ */
+static int hs_spi_read_rx_fifo(struct hs_spi *hs)
+{
+	unsigned int	rxflevel = HSSPI_BITS_GET(l, DMSTATUS_RXFLEVEL, hs,
+			DMSTATUS);
+	unsigned int	rxbytes = min(rxflevel, hs->len - hs->rx_cnt);
+	unsigned int	i;
+	unsigned char	rxdata;
+
+	for (i = 0; i < rxbytes; i++) {
+		rxdata = hs_spi_fiforead(hs, RXFIFO0);
+		hs->rx[hs->rx_cnt + i] = rxdata;
+	}
+	smp_rmb(); /* dunno why this is needed */
+
+	return rxbytes;
+}
+
+/*
+ * hs_spi_read_dummy - read dummy from the receive FIFO at direct mode
+ * @hs:		HS SPI device platform data.
+ *
+ * When transfer protocol is TX_RX,
+ * While TX-FIFO is transmitting data, RX-FIFO is also receiving dummy
+ * at the same time.
+ */
+static void hs_spi_read_dummy(struct hs_spi *hs)
+{
+	unsigned int	rxbytes = HSSPI_BITS_GET(l, DMSTATUS_RXFLEVEL, hs,
+			DMSTATUS);
+	unsigned int	i;
+	unsigned char	rxdata;
+
+	for (i = 0; i < rxbytes; i++)
+		rxdata = hs_spi_fiforead(hs, RXFIFO0);
+
+	smp_rmb(); /* why? */
+}
+
+/*
+ * hs_spi_set_speed - setup the clock frequency
+ * @spi:	SPI device data.
+ * @hz:		clock frequency to set.
+ *
+ * The clock frequency can be designated by spi devices or transfers.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_set_speed(struct spi_device *spi, unsigned int hz)
+{
+	struct hs_spi_cs	*cs = spi->controller_state;
+	struct hs_spi		*hs = spi_master_get_devdata(spi->master);
+	unsigned int		div;
+	unsigned long		rate;
+	unsigned char		safesync = 0;
+	u32 csval = 0x230508;
+
+	rate = clk_get_rate(hs->clk);
+
+	div = DIV_ROUND_UP(rate, hz * 2);
+	/*
+	 * If the resulting divider doesn't fit into the
+	 * register bitfield, we can't satisfy the constraint.
+	 */
+	if (div > 127) {
+		dev_err(&spi->dev,
+			"setup: %d Hz too slow, div %u; min %ld Hz\n",
+			hz, div, rate / (2 * 127));
+		return -EINVAL;
+	}
+
+	/* safesync bit */
+	if (hs->pdata->mode == HS_SPI_DIRECT_MODE) {
+		/* direct mode */
+		if (((spi->rx_bitwidth == 4) ||
+		     (spi->tx_bitwidth == 4)) &&
+		     (div < 3))
+			safesync = 1;
+	} else
+		/* cs mode */
+		if (hs->pdata->clock == HS_SPI_PCLK)
+			if (((spi->rx_bitwidth == 4) ||
+				 (spi->tx_bitwidth == 4)) &&
+			    (div < 3))
+				safesync = 1;
+
+	switch (cs->chip_select) {
+	case 0:
+		HSSPI_BITS_SET(l, PCC_CDRS, div, hs, PCC0);
+		HSSPI_BITS_SET(l, PCC_ESYNC, safesync, hs, PCC0);
+		HSSPI_BITS_SET(l, PCC_ACES, 0, hs, PCC0);
+		HSSPI_BITS_SET(l, PCC_CPHA, 0, hs, PCC0);
+		HS_SPI_DEBUG(&spi->dev, "Spi speed is set to %dHz, div:%u\n",
+			hz, HSSPI_BITS_GET(l, PCC_CDRS, hs, PCC0));
+
+		/* for now set them all to something good for mb86s70 */
+
+		writel(csval, hs->reg + 4);
+		writel(csval, hs->reg + 8);
+		writel(csval, hs->reg + 0xc);
+		writel(csval, hs->reg + 0x10);
+		break;
+	case 1:
+		HSSPI_BITS_SET(l, PCC_CDRS, div, hs, PCC1);
+		HSSPI_BITS_SET(l, PCC_ESYNC, safesync, hs, PCC1);
+		HS_SPI_DEBUG(&spi->dev, "Spi speed is set to %dHz, div:%u\n",
+			hz, HSSPI_BITS_GET(l, PCC_CDRS, hs, PCC1));
+		/* for now set them all to something good for mb86s70 */
+
+		writel(csval, hs->reg + 4);
+		writel(csval, hs->reg + 8);
+		writel(csval, hs->reg + 0xc);
+		writel(csval, hs->reg + 0x10);
+		break;
+	case 2:
+		HSSPI_BITS_SET(l, PCC_CDRS, div, hs, PCC2);
+		HSSPI_BITS_SET(l, PCC_ESYNC, safesync, hs, PCC2);
+		HS_SPI_DEBUG(&spi->dev, "Spi speed is set to %dHz, div:%u\n",
+			hz, HSSPI_BITS_GET(l, PCC_CDRS, hs, PCC2));
+		break;
+	case 3:
+		HSSPI_BITS_SET(l, PCC_CDRS, div, hs, PCC3);
+		HSSPI_BITS_SET(l, PCC_ESYNC, safesync, hs, PCC3);
+		HS_SPI_DEBUG(&spi->dev, "Spi speed is set to %dHz, div:%u\n",
+			hz, HSSPI_BITS_GET(l, PCC_CDRS, hs, PCC3));
+		break;
+	default:
+		dev_err(&spi->dev,
+			"setup: invalid chipselect %u (%u defined)\n",
+			cs->chip_select, spi->master->num_chipselect);
+		return -EINVAL;
+	}
+	cs->speed_hz = hz;
+
+	return 0;
+}
+
+/*
+ * hs_spi_quad_enable - enable quad mode for serial flash device
+ * @hs:		HS SPI device platform data.
+ * @spi:	SPI device data.
+ *
+ * The Quad bit of Configuration Register must be set to
+ * puts the device into Quad I/O mode.
+ */
+static void hs_spi_quad_enable(struct hs_spi *hs, struct spi_device *spi)
+{
+	unsigned char	cmd[3];
+
+	/* Set up the write data buffer. */
+	cmd[0] = HS_SPI_CMD_WRSR;
+	cmd[1] = 0x00;
+	cmd[2] = HS_SPI_CR_QUAD;
+
+	/* select chip */
+	HSSPI_BITS_SET(l, DMPSEL_PSEL, spi->chip_select, hs, DMPSEL);
+	/* Flush RX and TX FIFO  */
+	HSSPI_BITS_SET(l, FIFOCFG_TXFLSH, 1, hs, FIFOCFG);
+	HSSPI_BITS_SET(l, FIFOCFG_RXFLSH, 1, hs, FIFOCFG);
+	/* set transfer protocol */
+	HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_TX_ONLY, hs, DMTRP);
+	/* set transfer size */
+	HSSPI_BITS_SET(l, DMBCC_BCC, 3, hs, DMBCC);
+
+	/* transmit command byte at single line */
+	hs_spi_writeb(hs, TXFIFO0, cmd[0]);
+	hs_spi_writeb(hs, TXFIFO0, cmd[1]);
+	hs_spi_writeb(hs, TXFIFO0, cmd[2]);
+	/* start transfer */
+	HSSPI_BITS_SET(l, DMSTART_START, 1, hs, DMSTART);
+
+	init_completion(&hs->done);
+	hs_spi_writel(hs, TXE, TXE_TFEE_MASK << TXE_TFEE_OFFSET);
+	wait_for_completion(&hs->done);
+}
+
+/*
+ * hs_spi_read_sr - read the status register
+ * @hs:		HS SPI device platform data.
+ * @spi:	SPI device data.
+ *
+ * Returns the status register value.
+ */
+static unsigned char hs_spi_read_sr(struct hs_spi *hs, struct spi_device *spi)
+{
+	unsigned char	cmd;
+	unsigned char	status = 0;
+	int timeout = 100000;
+
+	/* Set up the write data buffer. */
+	cmd = HS_SPI_CMD_RDSR;
+
+	/* select chip */
+	HSSPI_BITS_SET(l, DMPSEL_PSEL, spi->chip_select, hs, DMPSEL);
+	/* Flush RX and TX FIFO  */
+	HSSPI_BITS_SET(l, FIFOCFG_TXFLSH, 1, hs, FIFOCFG);
+	HSSPI_BITS_SET(l, FIFOCFG_RXFLSH, 1, hs, FIFOCFG);
+	/* set transfer protocol */
+	HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_TX_ONLY, hs, DMTRP);
+	/* set transfer size */
+	HSSPI_BITS_SET(l, DMBCC_BCC, 2, hs, DMBCC);
+
+	/* start transfer */
+	HSSPI_BITS_SET(l, DMSTART_START, 1, hs, DMSTART);
+
+	/* transmit command byte at single line */
+	hs_spi_writeb(hs, TXFIFO0, cmd);
+
+	init_completion(&hs->done);
+	hs_spi_writel(hs, TXE, TXE_TFEE_MASK << TXE_TFEE_OFFSET);
+	wait_for_completion(&hs->done);
+
+	/* set transfer protocol */
+	HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_RX_ONLY, hs, DMTRP);
+
+	while (--timeout && HSSPI_BITS_GET(l, DMSTATUS_RXACTIVE, hs, DMSTATUS))
+		;
+
+	if (!timeout) {
+		dev_err(&spi->dev, "Timeout waiting for status\n");
+		return -ETIME;
+	}
+
+	status = hs_spi_readb(hs, RXFIFO0);
+
+	return status;
+}
+
+/*
+ * hs_spi_write_enable - enable the device to be write
+ * @hs:		HS SPI device platform data.
+ * @spi:	SPI device data.
+ *
+ * The Write Enable command must be sent to enables the device to accept a
+ * Write Status Register to enable Quad mode.
+ */
+static void hs_spi_write_enable(struct hs_spi *hs, struct spi_device *spi)
+{
+	unsigned char	cmd;
+	/* Set up the write data buffer. */
+	cmd = HS_SPI_CMD_WREN;
+
+	/* select chip */
+	HSSPI_BITS_SET(l, DMPSEL_PSEL, spi->chip_select, hs, DMPSEL);
+	/* Flush RX and TX FIFO  */
+	HSSPI_BITS_SET(l, FIFOCFG_TXFLSH, 1, hs, FIFOCFG);
+	HSSPI_BITS_SET(l, FIFOCFG_RXFLSH, 1, hs, FIFOCFG);
+	/* set transfer protocol */
+	HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_TX_ONLY, hs, DMTRP);
+	/* set transfer size */
+	HSSPI_BITS_SET(l, DMBCC_BCC, 1, hs, DMBCC);
+
+	/* transmit command byte at single line */
+	hs_spi_writeb(hs, TXFIFO0, cmd);
+	/* start transfer */
+	HSSPI_BITS_SET(l, DMSTART_START, 1, hs, DMSTART);
+
+	init_completion(&hs->done);
+	hs_spi_writel(hs, TXE, TXE_TFEE_MASK << TXE_TFEE_OFFSET);
+	wait_for_completion(&hs->done);
+}
+
+/*
+ * hs_spi_cs_initialize_device - initializ device for command sequencer mode
+ * @hs:		HS SPI device platform data.
+ * @spi:	SPI device data.
+ *
+ * In Command Sequencer mode, it is not possible to change between the
+ * legacy, dual-bit or quad-bit modes when a transfer has started.
+ * For this reason, for some of the newer Serial Flash devices - like
+ * the memory devices from Winbond, the Command Sequencer can be enabled
+ * only after the device has been initialized to work in the "Continuous
+ * Read Mode".
+ *
+ * The memory device can be programmed in the Continuous Read
+ * Mode, using the Direct Mode of Operation of HS_SPI.
+ */
+static int  hs_spi_cs_initialize_device(struct hs_spi *hs,
+	struct spi_device *spi)
+{
+	int	i = 0, offset = (int)(hs->reg + HS_SPI_REG_RDCSDC0);
+	int	*rdcsdc_p = NULL;
+	int timeout = 100000;
+	int	rdcsdc_int_data[3][8] = {
+	{
+#ifdef CONFIG_M25PXX_USE_FAST_READ
+		/* FAST_READ */
+		(HS_SPI_CMD_FAST_READ << 8) | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR2 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR1 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR0 | HS_SPI_CS_1BIT,
+		HS_SPI_HIGH_Z_BYTE | HS_SPI_CS_1BIT,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+#else
+		/* NORM_READ */
+		(HS_SPI_CMD_NORM_READ << 8) | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR2 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR1 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR0 | HS_SPI_CS_1BIT,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+#endif
+	},
+	{ /* DUAL_READ */
+		(HS_SPI_CMD_DOR<<8) | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR2 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR1 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR0 | HS_SPI_CS_1BIT,
+		HS_SPI_HIGH_Z_BYTE | HS_SPI_CS_1BIT,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+	},
+	{ /* QUAD_READ */
+		(HS_SPI_CMD_QOR<<8) | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR2 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR1 | HS_SPI_CS_1BIT,
+		HS_SPI_DECODE_ADDR0 | HS_SPI_CS_1BIT,
+		HS_SPI_HIGH_Z_BYTE | HS_SPI_CS_1BIT,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+		HS_SPI_LIST_END,
+	}
+	};
+
+#define SIZE_TO_FLAG(x)	({ \
+			int curr_bank, curr_flag; \
+			curr_bank = (x) / SZ_8K; \
+			for (curr_flag = 0;; curr_flag++) { \
+				if (curr_bank == 1) \
+					break; \
+				curr_bank >>= 1; \
+			} \
+			curr_flag; \
+			})
+
+	/* cs mode spi structure saved */
+	hs->spi[spi->chip_select] = spi;
+	if (spi->rx_bitwidth == 4) {
+		hs_spi_write_enable(hs, spi);
+		while (--timeout && hs_spi_read_sr(hs, spi)&HS_SPI_SR_WIP)
+			;
+
+		if (!timeout) {
+			dev_err(hs->dev, "Timeout in init\n");
+			return -ETIME;
+		}
+		hs_spi_quad_enable(hs, spi);
+
+		HS_SPI_DEBUG(hs->dev, "WRR quad read init command\n");
+		while (hs_spi_read_sr(hs, spi)&HS_SPI_SR_WIP)
+			;
+		while (!(HSSPI_BITS_GET(l, TXF_TSSRS, hs, TXF) |
+			HSSPI_BITS_GET(l, RXF_RSSRS, hs, RXF)))
+			;
+	}
+	/* disable module */
+	HSSPI_BITS_SET(l, MCTRL_MEN, 0, hs, MCTRL);
+
+	while (HSSPI_BITS_GET(l, MCTRL_MES, hs, MCTRL))
+		;
+
+	/* Disable BOOTEN */
+	HSSPI_BITS_SET(l, CSCFG_BOOTEN, 0, hs, CSCFG);
+	/* set operation mode to command sequencer */
+	HSSPI_BITS_SET(l, MCTRL_CSEN, 1, hs, MCTRL);
+	HSSPI_BITS_SET(l, CSCFG_MSEL, SIZE_TO_FLAG(hs->bank_size), hs, CSCFG);
+	/* reset the address extension field */
+	HSSPI_BITS_SET(l, CSAEXT_AEXT, 0, hs, CSAEXT);
+	/* set the Command Sequencer Idle Time */
+	HSSPI_BITS_SET(l, CSITIME_ITIME, HS_SPI_IDLE_TIME, hs, CSITIME);
+	HSSPI_BITS_SET(l, CSCFG_SSELEN, 1 << spi->chip_select, hs, CSCFG);
+	/* set Synchronizer ON bit */
+	if (hs->pdata->clock == HS_SPI_HCLK) {
+		HSSPI_BITS_SET(l, MCTRL_SYNCON, 1, hs, MCTRL);
+	} else {
+		if (hs->pdata->syncon)
+			HSSPI_BITS_SET(l, MCTRL_SYNCON, 1, hs, MCTRL);
+		else
+			HSSPI_BITS_SET(l, MCTRL_SYNCON, 0, hs, MCTRL);
+	}
+
+	if (spi->rx_bitwidth == 1) {
+		/* CS transfer mode */
+		HSSPI_BITS_SET(l, CSCFG_MBM, HS_SPI_LEGACY_BIT, hs, CSCFG);
+		rdcsdc_p = &rdcsdc_int_data[0][0];
+	} else if (spi->rx_bitwidth == 2) {
+		/* CS transfer mode */
+		HSSPI_BITS_SET(l, CSCFG_MBM, HS_SPI_DUAL_BIT, hs, CSCFG);
+		rdcsdc_p = &rdcsdc_int_data[1][0];
+	} else if (spi->rx_bitwidth == 4) {
+		/* CS transfer mode */
+		HSSPI_BITS_SET(l, CSCFG_MBM, HS_SPI_QUAD_BIT, hs, CSCFG);
+		rdcsdc_p = &rdcsdc_int_data[2][0];
+	}
+
+	for (i = 0; i < 8; i++) {
+		writew(rdcsdc_p[i], (void __iomem *)(offset + i * 2));
+		HS_SPI_DEBUG(hs->dev,
+			"address = %#x,Command Sequence is %#x\n",
+		offset + i * 2, readw((void __iomem *)(offset + i * 2)));
+	}
+
+	/* enable module */
+	HSSPI_BITS_SET(l, MCTRL_MEN, 1, hs, MCTRL);
+	while (!HSSPI_BITS_GET(l, MCTRL_MES, hs, MCTRL))
+		;
+	return 0;
+}
+
+/*
+ * hs_spi_init_hw - initializ hardware registers
+ * @hs:		HS SPI device platform data.
+ */
+static int  hs_spi_init_hw(struct hs_spi *hs)
+{
+	int timeout = 1000000;
+	/* change to dm mode */
+	HSSPI_BITS_SET(l, MCTRL_CSEN, 0, hs, MCTRL);
+
+	HSSPI_BITS_SET(l, MCTRL_MEN, 0, hs, MCTRL);
+	while (--timeout && HSSPI_BITS_GET(l, MCTRL_MES, hs, MCTRL))
+		;
+
+	if (!timeout)
+		return -ETIME;
+
+	/* disable interrupt */
+	hs_spi_writel(hs, TXE, 0x00);
+	hs_spi_writel(hs, RXE, 0x00);
+	/* clear interrupt flag */
+	hs_spi_writel(hs, TXC, HS_SPI_TXINT_EXCEPT_TSSRC);
+	hs_spi_writel(hs, RXC, HS_SPI_RXINT_EXCEPT_RSSRC);
+
+	/* read module ID */
+	dev_dbg(hs->dev, "HS SPI module ID:%#4x\n", hs_spi_readl(hs, MID));
+
+	/* Clock Division Source Select 0:AHBCLK 1:PCLK*/
+	if (hs->pdata->clock == HS_SPI_HCLK)
+		HSSPI_BITS_SET(l, MCTRL_CDSS, 0, hs, MCTRL);
+	else
+		HSSPI_BITS_SET(l, MCTRL_CDSS, 1, hs, MCTRL);
+
+	if (hs->pdata->mode != HS_SPI_COMMAND_SEQUENCER)
+		/* set to software flow control mode */
+		HSSPI_BITS_SET(l, DMCFG_SSDC, 0, hs, DMCFG);
+	else
+		/* set to hardware flow control mode */
+		HSSPI_BITS_SET(l, DMCFG_SSDC, 1, hs, DMCFG);
+
+	/* configure the FIFO threshold levels and the FIFO width */
+	hs_spi_writel(hs, FIFOCFG,
+		HSSPI_BITS(FIFOCFG_FWIDTH, HS_SPI_FIFO_WIDTH) |
+		HSSPI_BITS(FIFOCFG_TXFTH, HS_SPI_TX_FIFO_LEVEL) |
+		HSSPI_BITS(FIFOCFG_RXFTH, HS_SPI_RX_FIFO_LEVEL));
+	/* enable module */
+	HSSPI_BITS_SET(l, MCTRL_MEN, 1, hs, MCTRL);
+	timeout = 100000;
+	while (--timeout && !HSSPI_BITS_GET(l, MCTRL_MES, hs, MCTRL))
+		;
+
+	if (!timeout)
+		return -ETIME;
+
+	return 0;
+}
+
+/*
+ * hs_spi_tx_irq - deal with transmit interrupt request
+ * @irq:	Interrupt request number.
+ * @dev:	HS SPI device platform data.
+ *
+ * Returns IRQ_HANDLED on success
+ */
+static irqreturn_t hs_spi_tx_irq(int irq, void *dev)
+{
+	struct hs_spi		*hs = dev;
+	int			txf;
+	int			mode = hs->pdata->mode;
+
+	txf = hs_spi_readl(hs, TXF);
+	/* clear flags */
+	hs_spi_writel(hs, TXC, HS_SPI_TXINT_EXCEPT_TSSRC);
+
+	if (txf & HSSPI_BIT(TXF_TFLETS)) {
+		dev_dbg(hs->dev, "TX-FIFO Fill Level <= Threshold\n");
+		if ((mode == HS_SPI_DIRECT_MODE) && (hs->tx) &&
+			(hs->tx_cnt < hs->len))
+			hs->tx_cnt += hs_spi_write_tx_fifo(hs);
+	}
+
+	if (txf & HSSPI_BIT(TXF_TFES)) {
+		dev_dbg(hs->dev, "TX-FIFO and Shift Register is Empty\n");
+		if (mode == HS_SPI_DIRECT_MODE) {
+			if (hs->tx_cnt >= hs->len) {
+				hs_spi_writel(hs, TXE, 0x00);
+				complete(&hs->done);
+			}
+		} else {
+			/* for special cmd on cs mode (only) */
+			hs_spi_writel(hs, TXE, 0x00);
+			complete(&hs->done);
+		}
+	}
+
+#ifdef CONFIG_HS_SPI_DEBUG
+	if (txf & HSSPI_BIT(TXF_TFMTS))
+		dev_dbg(hs->dev, "TX-FIFO Fill Level is More Than Threshold\n");
+
+	if (txf & HSSPI_BIT(TXF_TFUS))
+		dev_dbg(hs->dev, "TX-FIFO Underrun\n");
+
+	if (txf & HSSPI_BIT(TXF_TFOS))
+		dev_dbg(hs->dev, "TX-FIFO Overrun\n");
+
+	if (txf & HSSPI_BIT(TXF_TFFS))
+		dev_dbg(hs->dev, "TX-FIFO Full\n");
+#endif /* CONFIG_HS_SPI_DEBUG */
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * hs_spi_rx_irq - deal with receive interrupt request
+ * @irq:	Interrupt request number.
+ * @dev:	HS SPI device platform data.
+ *
+ * Returns IRQ_HANDLED on success
+ */
+static irqreturn_t hs_spi_rx_irq(int irq, void *dev)
+{
+	struct hs_spi		*hs = dev;
+	int			rxf;
+
+	rxf = hs_spi_readl(hs, RXF);
+	/* clear flags */
+	hs_spi_writel(hs, RXC, HS_SPI_RXINT_EXCEPT_RSSRC);
+
+	if (rxf & HSSPI_BIT(RXF_RFMTS)) {
+		dev_dbg(hs->dev, "RX-FIFO Fill Level is More Than Threshold\n");
+		hs->rx_cnt += hs_spi_read_rx_fifo(hs);
+		if (hs->rx_cnt >= hs->len) {
+			hs_spi_writel(hs, RXE, 0x00);
+			complete(&hs->done);
+		}
+	}
+
+#ifdef CONFIG_HS_SPI_DEBUG
+	if (rxf & HSSPI_BIT(RXF_RFLETS))
+		dev_dbg(hs->dev, "RX-FIFO Fill Level is <= Threshold\n");
+
+	if (rxf & HSSPI_BIT(RXF_RFUS))
+		dev_dbg(hs->dev, "RX-FIFO Underrun\n");
+
+	if (rxf & HSSPI_BIT(RXF_RFOS))
+		dev_dbg(hs->dev, "RX-FIFO Overrun\n");
+
+	if (rxf & HSSPI_BIT(RXF_RFES))
+		dev_dbg(hs->dev, "RX-FIFO Empty\n");
+
+	if (rxf & HSSPI_BIT(RXF_RFFS))
+		dev_dbg(hs->dev, "RX-FIFO Full\n");
+#endif /* CONFIG_HS_SPI_DEBUG */
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * hs_spi_fault_irq - deal with fault interrupt request
+ * @irq:	Interrupt request number.
+ * @dev:	HS SPI device platform data.
+ *
+ * Returns IRQ_HANDLED on success
+ */
+static irqreturn_t hs_spi_fault_irq(int irq, void *dev)
+{
+	struct hs_spi		*hs = dev;
+	unsigned int		faultf;
+
+	faultf = hs_spi_readl(hs, FAULTF);
+	/* clear flags */
+	hs_spi_writel(hs, FAULTC, HS_SPI_ALL_FAULT);
+
+#ifdef CONFIG_HS_SPI_DEBUG
+	if (faultf & HSSPI_BIT(FAULTF_DRCBSFS))
+		dev_err(hs->dev, "DMA Read Channel Block Size Fault\n");
+
+	if (faultf & HSSPI_BIT(FAULTF_DWCBSFS))
+		dev_err(hs->dev, "DMA Write Channel Block Size Fault\n");
+
+	if (faultf & HSSPI_BIT(FAULTF_PVFS))
+		dev_err(hs->dev, "Protection Violation Fault\n");
+
+	if (faultf & HSSPI_BIT(FAULTF_WAFS))
+		dev_err(hs->dev, "Write Access Fault\n");
+
+	if (faultf & HSSPI_BIT(FAULTF_UMAFS))
+		dev_err(hs->dev, "Unmapped Memory Access Fault\n");
+#endif /* CONFIG_HS_SPI_DEBUG */
+
+	hs->fault_flag = 1;
+	hs_spi_writel(hs, TXE, 0x00);
+	hs_spi_writel(hs, RXE, 0x00);
+	complete(&hs->done);
+
+	return IRQ_HANDLED;
+}
+
+/*
+ * hs_spi_chipselect - select or release the chip
+ * @spi:	SPI device data.
+ * @value:	Active or inactive.
+ */
+static void hs_spi_chipselect(struct spi_device *spi, int value)
+{
+	struct hs_spi		*hs = spi_master_get_devdata(spi->master);
+
+	if (value == BITBANG_CS_INACTIVE) {
+		/* stop the transfer */
+		HSSPI_BITS_SET(l, DMSTOP_STOP, 1, hs, DMSTOP);
+		while (!HSSPI_BITS_GET(l, DMSTOP_STOP, hs, DMSTOP))
+			;
+		if (hs->rx)
+			hs_spi_read_dummy(hs);
+
+		while (!(HSSPI_BITS_GET(l, TXF_TSSRS, hs, TXF) |
+			HSSPI_BITS_GET(l, RXF_RSSRS, hs, RXF)))
+			;
+	} else {
+		/* select chip */
+		HSSPI_BITS_SET(l, DMPSEL_PSEL, spi->chip_select, hs, DMPSEL);
+		HSSPI_BITS_SET(l, DMSTOP_STOP, 0, hs, DMSTOP);
+	}
+}
+
+/*
+ * hs_spi_setup_transfer - setup the transfer attributes
+ * @spi:	SPI device data.
+ * @t:		Transfer data.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_setup_transfer(struct spi_device *spi,
+	struct spi_transfer *t)
+{
+	struct hs_spi_cs	*cs = spi->controller_state;
+	unsigned int		hz;
+	struct hs_spi		*hs = spi_master_get_devdata(spi->master);
+
+	if (hs->stop)
+		return -ESHUTDOWN;
+	hz  = t ? t->speed_hz : spi->max_speed_hz;
+
+	/*
+	 * Modify the configuration if the transfer overrides it.  Do not allow
+	 * the transfer to overwrite the generic configuration with zeros.
+	 */
+	if ((hz) && (cs->speed_hz != hz)) {
+		if (hz > spi->max_speed_hz) {
+			dev_err(&spi->dev, "%s: %dHz but max is %dHz\n",
+					__func__, hz, spi->max_speed_hz);
+			return -EINVAL;
+		}
+		/* Set the new speed */
+		return hs_spi_set_speed(spi, hz);
+	}
+
+	return 0;
+}
+
+/*
+ * hs_spi_setup - first setup for new devices
+ * @spi:	SPI device data.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_setup(struct spi_device *spi)
+{
+	struct hs_spi_cs	*cs = spi->controller_state;
+	struct hs_spi		*hs = spi_master_get_devdata(spi->master);
+	int			mode = hs->pdata->mode;
+	unsigned int		cfg = 0;
+	int			retval;
+
+	/* allocate settings on the first call */
+	if (!cs) {
+		cs = kzalloc(sizeof(struct hs_spi_cs), GFP_KERNEL);
+		if (!cs) {
+			dev_err(&spi->dev, "no memory for controller state\n");
+			return -ENOMEM;
+		}
+		spi->controller_state = cs;
+	}
+
+	cs->chip_select = spi->chip_select;
+
+	if (cs->chip_select >= spi->master->num_chipselect) {
+		dev_err(&spi->dev,
+			"setup: invalid chipselect %u (%u defined)\n",
+			cs->chip_select, spi->master->num_chipselect);
+		return -EINVAL;
+	}
+
+	cs->mode = spi->mode;
+
+	if (cs->mode & SPI_CPHA)
+		cfg |= HSSPI_BIT(PCC_CPHA);
+
+	if (cs->mode & SPI_CPOL)
+		cfg |= HSSPI_BIT(PCC_CPOL);
+
+	if (cs->mode & SPI_CS_HIGH)
+		cfg |= HSSPI_BIT(PCC_SSPOL);
+
+	if (cs->mode & SPI_LSB_FIRST)
+		cfg |= HSSPI_BIT(PCC_SDIR);
+
+	cfg |= HSSPI_BITS(PCC_SS2CD, HS_SPI_CS_DELAY);
+	cfg |= HSSPI_BITS(PCC_SENDIAN, 1);
+
+	switch (cs->chip_select) {
+	case 0:
+		hs_spi_writel(hs, PCC0, cfg);
+		HS_SPI_DEBUG(&spi->dev, "Spi mode is set to %#x\n",
+			hs_spi_readl(hs, PCC0) & HS_SPI_MODE_MASK);
+		break;
+	case 1:
+		hs_spi_writel(hs, PCC1, cfg);
+		HS_SPI_DEBUG(&spi->dev, "Spi mode is set to %#x\n",
+			hs_spi_readl(hs, PCC1) & HS_SPI_MODE_MASK);
+		break;
+	case 2:
+		hs_spi_writel(hs, PCC2, cfg);
+		HS_SPI_DEBUG(&spi->dev, "Spi mode is set to %#x\n",
+			hs_spi_readl(hs, PCC2) & HS_SPI_MODE_MASK);
+		break;
+	case 3:
+		hs_spi_writel(hs, PCC3, cfg);
+		HS_SPI_DEBUG(&spi->dev, "Spi mode is set to %#x\n",
+			hs_spi_readl(hs, PCC3) & HS_SPI_MODE_MASK);
+		break;
+	default:
+		dev_err(&spi->dev,
+			"setup: invalid chipselect %u (%u defined)\n",
+			cs->chip_select, spi->master->num_chipselect);
+		return -EINVAL;
+	}
+
+	retval = hs_spi_setup_transfer(spi, NULL);
+	if (retval < 0)
+		return retval;
+
+	if (mode == HS_SPI_COMMAND_SEQUENCER)
+		return hs_spi_cs_initialize_device(hs, spi);
+
+	return 0;
+}
+
+/*
+ * hs_spi_setup - clean driver specific data
+ * @spi:	SPI device data.
+ *
+ * callback for spi framework.
+ */
+static void hs_spi_cleanup(struct spi_device *spi)
+{
+	struct hs_spi	*hs = spi_master_get_devdata(spi->master);
+
+	HSSPI_BITS_SET(l, CSCFG_SSELEN, 0, hs, CSCFG);
+
+	kfree(spi->controller_state);
+}
+
+/*
+ * hs_spi_txrx - transmit or receive data
+ * @spi:	SPI device data.
+ * @t:		Transfer data.
+ *
+ * Called by spi bitbang.
+ *
+ * Returns transmitted or received data size
+ */
+static int hs_spi_txrx(struct spi_device *spi, struct spi_transfer *t)
+{
+	struct hs_spi	*hs = spi_master_get_devdata(spi->master);
+	int		mode = hs->pdata->mode;
+	if (mode == HS_SPI_COMMAND_SEQUENCER) {
+		dev_err(hs->dev, "invalid hs spi mode, need direct mode\n");
+		return -EINVAL;
+	}
+
+	hs->fault_flag	= 0;
+	hs->tx		= t->tx_buf;
+	hs->rx		= t->rx_buf;
+	hs->tx_cnt	= 0;
+	hs->rx_cnt	= 0;
+	hs->len		= t->len;
+	hs->bitwidth = t->bits_per_word;
+
+	init_completion(&hs->done);
+
+	/* Flush RX and TX FIFO  */
+	HSSPI_BITS_SET(l, FIFOCFG_TXFLSH, 1, hs, FIFOCFG);
+	HSSPI_BITS_SET(l, FIFOCFG_RXFLSH, 1, hs, FIFOCFG);
+
+	if (hs->tx) {
+		/* set tx transfer protocol */
+		if (hs->bitwidth == 2) {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_DUAL_TX_ONLY,
+				hs, DMTRP);
+		} else if (hs->bitwidth == 4) {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_QUAD_TX_ONLY,
+				hs, DMTRP);
+	} else {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_TX_ONLY,
+				hs, DMTRP);
+		}
+		hs->tx_cnt += hs_spi_write_tx_fifo(hs);
+	} else {
+		/* set rx transfer protocol */
+		if (hs->bitwidth == 2) {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_DUAL_RX_ONLY,
+				hs, DMTRP);
+		} else if (hs->bitwidth == 4) {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_QUAD_RX_ONLY,
+				hs, DMTRP);
+		} else {
+			HSSPI_BITS_SET(l, DMTRP_TRP, HS_SPI_LEGACY_RX_ONLY,
+				hs, DMTRP);
+		}
+	}
+	hs_spi_writel(hs, TXC, HS_SPI_TX_ALL_INT);
+	hs_spi_writel(hs, RXC, HS_SPI_RX_ALL_INT);
+	/* start transfer */
+	HSSPI_BITS_SET(l, DMSTART_START, 1, hs, DMSTART);
+	if (hs->tx)
+		hs_spi_writel(hs, TXE, TXE_TFLETE_MASK << TXE_TFLETE_OFFSET |
+					TXE_TFEE_MASK << TXE_TFEE_OFFSET);
+	else
+		hs_spi_writel(hs, RXE, RXE_RFMTE_MASK << RXE_RFMTE_OFFSET);
+
+	wait_for_completion(&hs->done);
+
+	if (hs->fault_flag)
+		return -EPERM;
+
+	return hs->rx ? hs->rx_cnt : hs->tx_cnt;
+}
+
+#ifdef CONFIG_PM
+static int hs_spi_suspend(struct device *dev)
+{
+	struct hs_spi	*hs = platform_get_drvdata(to_platform_device(dev));
+	unsigned long flags;
+	int ret = 0;
+
+	spin_lock_irqsave(&(hs->bitbang.lock), flags);
+
+	hs->stop = true;
+
+	hs->pcc[0] = hs_spi_readl(hs, PCC0);
+	hs->pcc[1] = hs_spi_readl(hs, PCC1);
+	hs->pcc[2] = hs_spi_readl(hs, PCC2);
+	hs->pcc[3] = hs_spi_readl(hs, PCC3);
+
+	spin_unlock_irqrestore(&hs->bitbang.lock, flags);
+
+	clk_disable_unprepare(hs->clk);
+
+	return ret;
+}
+
+static int hs_spi_resume(struct device *dev)
+{
+	struct hs_spi	*hs =
+		platform_get_drvdata(to_platform_device(dev));
+	int i;
+
+	clk_prepare_enable(hs->clk);
+	/* setup any hardware we can */
+	hs_spi_init_hw(hs);
+
+	/* PCC register restore */
+	hs_spi_writel(hs, PCC0, hs->pcc[0]);
+	hs_spi_writel(hs, PCC1, hs->pcc[1]);
+	hs_spi_writel(hs, PCC2, hs->pcc[2]);
+	hs_spi_writel(hs, PCC3, hs->pcc[3]);
+
+	/* CS mode init */
+	if (hs->pdata->mode == HS_SPI_COMMAND_SEQUENCER) {
+		for (i = 0; i < 4; i++) {
+			if (hs->spi[i])
+				hs_spi_cs_initialize_device(hs, hs->spi[i]);
+		}
+
+	}
+	hs->stop = false;
+
+	return 0;
+}
+
+static int hs_spi_freeze(struct device *dev)
+{
+	struct hs_spi	*hs =
+		platform_get_drvdata(to_platform_device(dev));
+	unsigned long flags;
+	int ret = 0;
+
+	spin_lock_irqsave(&(hs->bitbang.lock), flags);
+	hs->stop = true;
+	spin_unlock_irqrestore(&hs->bitbang.lock, flags);
+
+	return ret;
+}
+
+static int hs_spi_thaw(struct device *dev)
+{
+	struct hs_spi	*hs =
+		platform_get_drvdata(to_platform_device(dev));
+
+	hs->stop = false;
+
+	return 0;
+}
+
+static int hs_spi_restore(struct device *dev)
+{
+	struct hs_spi	*hs =
+		platform_get_drvdata(to_platform_device(dev));
+
+	hs->stop = false;
+
+	return 0;
+}
+
+static const struct dev_pm_ops hs_spi_pm_ops = {
+	.suspend = hs_spi_suspend,
+	.resume = hs_spi_resume,
+	.freeze = hs_spi_freeze,
+	.thaw = hs_spi_thaw,
+	.restore = hs_spi_restore,
+};
+
+#define HS_SPI_PM_OPS	(&hs_spi_pm_ops)
+#else
+#define HS_SPI_PM_OPS	NULL
+#endif
+
+/* needs to be adapted for any change to the platform data struct */
+
+static const char * const plat_data_names[] = {
+	"mode",
+	"num_chipselect",
+	"addr_width",
+	"bank_size",
+	"clock",
+	"sync_on",
+};
+
+/*
+ * hs_spi_probe - probe the HS SPI
+ * @pdev:	Platform device data.
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int hs_spi_probe(struct platform_device *pdev)
+{
+	struct hs_spi		*hs;
+	struct spi_master	*master;
+	struct resource		*reg_res, *csa_res;
+	int			i, err = 0;
+	u32 *m;
+	const u32 *p;
+	int n;
+
+	dev_dbg(&pdev->dev, "probing\n");
+
+	master = spi_alloc_master(&pdev->dev, sizeof(struct hs_spi));
+	if (master == NULL) {
+		dev_err(&pdev->dev, "No memory for spi_master\n");
+		err = -ENOMEM;
+		goto err_nomem;
+	}
+
+	hs = spi_master_get_devdata(master);
+	memset(hs, 0, sizeof(struct hs_spi));
+
+	hs->master = spi_master_get(master);
+
+	if (pdev->dev.of_node) {
+		hs->pdata = kzalloc(sizeof(*hs->pdata), GFP_KERNEL);
+		if (hs->pdata == NULL) {
+			dev_err(&pdev->dev, "Out of memory\n");
+			err = -ENOMEM;
+			goto err_no_pdata;
+		}
+
+		m = &hs->pdata->mode;
+		for (n = 0; n < ARRAY_SIZE(plat_data_names); n++) {
+			p = of_get_property(pdev->dev.of_node,
+					plat_data_names[n], NULL);
+			if (p)
+				*m++ = be32_to_cpu(*p);
+		}
+	} else {
+		hs->pdata = pdev->dev.platform_data;
+		if (hs->pdata == NULL) {
+			dev_err(&pdev->dev, "No platform data supplied\n");
+			err = -ENOENT;
+			goto err_no_pdata;
+		}
+	}
+	if ((hs->pdata->mode != HS_SPI_DIRECT_MODE) &&
+		(hs->pdata->mode != HS_SPI_COMMAND_SEQUENCER)) {
+		dev_err(&pdev->dev, "invalid hs spi mode\n");
+		err = -ENOENT;
+		goto err_pdata;
+	}
+
+	hs->dev		= &pdev->dev;
+	hs->bank_size	= hs->pdata->bank_size;
+	hs->stop = false;
+	for (i = 0; i < 4; i++)
+		hs->spi[i] = NULL;
+
+	platform_set_drvdata(pdev, hs);
+	init_completion(&hs->done);
+
+	/* setup the master state. */
+	master->mode_bits	= SPI_CPOL | SPI_CPHA |
+		SPI_CS_HIGH | SPI_LSB_FIRST;
+	master->num_chipselect	= hs->pdata->num_chipselect;
+	master->bus_num		= pdev->id == -1 ? 0 : pdev->id;
+
+	/* setup the state for the bitbang driver */
+
+	hs->bitbang.master		= hs->master;
+	hs->bitbang.setup_transfer	= hs_spi_setup_transfer;
+	hs->bitbang.chipselect		= hs_spi_chipselect;
+	hs->bitbang.txrx_bufs		= hs_spi_txrx;
+
+	hs->master->setup		= hs_spi_setup;
+	hs->master->cleanup		= hs_spi_cleanup;
+
+	master->dev.of_node = pdev->dev.of_node;
+
+	reg_res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (reg_res == NULL) {
+		dev_err(&pdev->dev, "Cannot get register IORESOURCE_MEM\n");
+		err = -ENOENT;
+		goto err_no_reg_iores;
+	}
+
+	hs->reg = ioremap(reg_res->start, resource_size(reg_res));
+	if (hs->reg == NULL) {
+		dev_err(&pdev->dev, "Cannot map register IO\n");
+		err = -ENXIO;
+		goto err_no_reg_iomap;
+	}
+
+	csa_res = platform_get_resource(pdev, IORESOURCE_MEM, 1);
+	if (csa_res == NULL) {
+		dev_err(&pdev->dev, "Cannot get cmd seq area resource\n");
+		err = -ENOENT;
+		goto err_no_csa_iores;
+	}
+	hs->csa_size = resource_size(csa_res);
+	hs->csa = ioremap(csa_res->start, resource_size(csa_res));
+	if (hs->csa == NULL) {
+		dev_err(&pdev->dev, "Cannot map command sequence area IO\n");
+		err = -ENXIO;
+		goto err_no_csa_iomap;
+	}
+
+	if (hs->pdata->clock == HS_SPI_HCLK)
+		hs->clk = clk_get(&pdev->dev, "iHCLK");
+	else
+		hs->clk = clk_get(&pdev->dev, "iPCLK");
+	if (IS_ERR(hs->clk)) {
+		dev_err(&pdev->dev, "No clock for device\n");
+		err = PTR_ERR(hs->clk);
+		goto err_no_clk;
+	}
+	/* for the moment, permanently enable the clock */
+	clk_prepare_enable(hs->clk);
+
+	dev_dbg(&pdev->dev, "pre init\n");
+	/* setup any hardware we can */
+	n = hs_spi_init_hw(hs);
+	if (n)
+		return n;
+	dev_dbg(&pdev->dev, "post init\n");
+
+	/* find and request our interrupt resources */
+	hs->tx_irq = platform_get_irq(pdev, 0);
+	if (hs->tx_irq < 0) {
+		dev_err(&pdev->dev, "No TX IRQ specified\n");
+		err = -ENOENT;
+		goto err_no_tx_irq;
+	}
+
+	err = request_irq(hs->tx_irq, hs_spi_tx_irq, 0, pdev->name, hs);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot claim TX IRQ\n");
+		goto err_no_tx_irq;
+	}
+
+	hs->rx_irq = platform_get_irq(pdev, 1);
+	if (hs->rx_irq < 0) {
+		dev_err(&pdev->dev, "No RX IRQ specified\n");
+		err = -ENOENT;
+		goto err_no_rx_irq;
+	}
+
+	err = request_irq(hs->rx_irq, hs_spi_rx_irq, 0, pdev->name, hs);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot claim RX IRQ\n");
+		goto err_no_rx_irq;
+	}
+
+	hs->fault_irq = platform_get_irq(pdev, 2);
+	if (hs->fault_irq < 0) {
+		dev_err(&pdev->dev, "No FAULT IRQ specified\n");
+		err = -ENOENT;
+		goto err_no_fault_irq;
+	}
+
+	err = request_irq(hs->fault_irq, hs_spi_fault_irq, 0, pdev->name, hs);
+	if (err) {
+		dev_err(&pdev->dev, "Cannot claim FAULT IRQ\n");
+		goto err_no_fault_irq;
+	}
+
+	/* register our spi controller */
+	dev_dbg(hs->dev, "bitbang at %p\n", &hs->bitbang);
+	err = spi_bitbang_start(&hs->bitbang);
+	if (err) {
+		dev_err(&pdev->dev, "Failed to register SPI master\n");
+		goto err_register;
+	}
+
+	return 0;
+
+ err_register:
+	clk_put(hs->clk);
+	free_irq(hs->fault_irq, hs);
+
+ err_no_fault_irq:
+	free_irq(hs->rx_irq, hs);
+
+ err_no_rx_irq:
+	free_irq(hs->tx_irq, hs);
+
+ err_no_tx_irq:
+	clk_disable_unprepare(hs->clk);
+	clk_put(hs->clk);
+
+err_no_clk:
+	iounmap(hs->csa);
+
+ err_no_csa_iomap:
+ err_no_csa_iores:
+	iounmap(hs->reg);
+
+ err_no_reg_iomap:
+ err_no_reg_iores:
+ err_pdata:
+	if (pdev->dev.of_node)
+		kfree(hs->pdata);
+ err_no_pdata:
+	spi_master_put(hs->master);
+
+ err_nomem:
+	return err;
+}
+
+/*
+ * hs_spi_remove - remove the HS SPI
+ * @pdev:	Platform device data.
+ *
+ * Stop hardware and remove the driver.
+ *
+ * Returns 0 on success
+ */
+static int __exit hs_spi_remove(struct platform_device *pdev)
+{
+	struct hs_spi		*hs = platform_get_drvdata(pdev);
+
+	if (!hs)
+		return 0;
+
+	/* Disconnect from the SPI framework */
+	spi_bitbang_stop(&hs->bitbang);
+
+	/* clear platform driver data */
+	platform_set_drvdata(pdev, NULL);
+
+	clk_disable_unprepare(hs->clk);
+	clk_put(hs->clk);
+
+	free_irq(hs->fault_irq, hs);
+	free_irq(hs->rx_irq, hs);
+	free_irq(hs->tx_irq, hs);
+	iounmap(hs->reg);
+	iounmap(hs->csa);
+
+	spi_master_put(hs->master);
+	if (pdev->dev.of_node)
+		kfree(hs->pdata);
+
+	return 0;
+}
+
+static const struct of_device_id f_hsspi_dt_ids[] = {
+	{ .compatible = "fujitsu,mb86s7x-hsspi" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, f_hsspi_dt_ids);
+
+/*
+ * Structure for a device driver
+ */
+static struct platform_driver hs_spi_driver = {
+	.probe		= hs_spi_probe,
+	.remove		= __exit_p(hs_spi_remove),
+	.driver		= {
+		.name	= DRV_NAME,
+		.owner	= THIS_MODULE,
+		.of_match_table = f_hsspi_dt_ids,
+		.pm = HS_SPI_PM_OPS,
+	},
+};
+
+/*
+ * hs_spi_init - initialize module
+ *
+ * Returns 0 on success; negative errno on failure
+ */
+static int __init hs_spi_init(void)
+{
+	return platform_driver_register(&hs_spi_driver);
+}
+
+/*
+ * hs_spi_exit - exit module
+ */
+static void __exit hs_spi_exit(void)
+{
+	platform_driver_unregister(&hs_spi_driver);
+}
+
+module_init(hs_spi_init);
+module_exit(hs_spi_exit);
+
+MODULE_DESCRIPTION("Fujitsu Semiconductor HS_SPI Driver");
+MODULE_AUTHOR("Fujitsu Semiconductor Limitd");
+MODULE_LICENSE("GPL");
+MODULE_ALIAS("platform:hs-spi");
diff --git a/drivers/spi/hs_spi_reg.h b/drivers/spi/hs_spi_reg.h
new file mode 100644
index 0000000..9edad96
--- /dev/null
+++ b/drivers/spi/hs_spi_reg.h
@@ -0,0 +1,485 @@
+/*
+ * linux/drivers/spi/hs_spi.h Register definitions for high speed SPI controller
+ *
+ * Copyright (C) 2010-2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This software is licensed under the terms of the GNU General Public
+ * License version 2, as published by the Free Software Foundation, and
+ * may be copied, distributed, and modified under those terms.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#ifndef __HS_SPI_REG_H__
+#define __HS_SPI_REG_H__
+
+/*
+ * HS_SPI register adress definitions
+ */
+#define	HS_SPI_REG_MCTRL	0x00
+#define	HS_SPI_REG_PCC0		0x04
+#define	HS_SPI_REG_PCC1		0x08
+#define	HS_SPI_REG_PCC2		0x0C
+#define	HS_SPI_REG_PCC3		0x10
+#define	HS_SPI_REG_TXF		0x14
+#define	HS_SPI_REG_TXE		0x18
+#define	HS_SPI_REG_TXC		0x1C
+#define	HS_SPI_REG_RXF		0x20
+#define	HS_SPI_REG_RXE		0x24
+#define	HS_SPI_REG_RXC		0x28
+#define	HS_SPI_REG_FAULTF	0x2C
+#define	HS_SPI_REG_FAULTC	0x30
+#define	HS_SPI_REG_DMCFG	0x34
+#define	HS_SPI_REG_DMDMAEN	0x34
+#define	HS_SPI_REG_DMSTART	0x38
+#define	HS_SPI_REG_DMSTOP	0x38
+#define	HS_SPI_REG_DMPSEL	0x38
+#define	HS_SPI_REG_DMTRP	0x38
+#define	HS_SPI_REG_DMBCC	0x3C
+#define	HS_SPI_REG_DMBCS	0x3C
+#define	HS_SPI_REG_DMSTATUS	0x40
+#define	HS_SPI_REG_TXBITCNT	0x44
+#define	HS_SPI_REG_FIFOCFG	0x4C
+#define	HS_SPI_REG_TXFIFO0	0x50
+#define	HS_SPI_REG_TXFIFO1	0x54
+#define	HS_SPI_REG_TXFIFO2	0x58
+#define	HS_SPI_REG_TXFIFO3	0x5C
+#define	HS_SPI_REG_TXFIFO4	0x60
+#define	HS_SPI_REG_TXFIFO5	0x64
+#define	HS_SPI_REG_TXFIFO6	0x68
+#define	HS_SPI_REG_TXFIFO7	0x6C
+#define	HS_SPI_REG_TXFIFO8	0x70
+#define	HS_SPI_REG_TXFIFO9	0x74
+#define	HS_SPI_REG_TXFIFO10	0x78
+#define	HS_SPI_REG_TXFIFO11	0x7C
+#define	HS_SPI_REG_TXFIFO12	0x80
+#define	HS_SPI_REG_TXFIFO13	0x84
+#define	HS_SPI_REG_TXFIFO14	0x88
+#define	HS_SPI_REG_TXFIFO15	0x8C
+#define	HS_SPI_REG_RXFIFO0	0x90
+#define	HS_SPI_REG_RXFIFO1	0x94
+#define	HS_SPI_REG_RXFIFO2	0x98
+#define	HS_SPI_REG_RXFIFO3	0x9C
+#define	HS_SPI_REG_RXFIFO4	0xA0
+#define	HS_SPI_REG_RXFIFO5	0xA4
+#define	HS_SPI_REG_RXFIFO6	0xA8
+#define	HS_SPI_REG_RXFIFO7	0xAC
+#define	HS_SPI_REG_RXFIFO8	0xB0
+#define	HS_SPI_REG_RXFIFO9	0xB4
+#define	HS_SPI_REG_RXFIFO10	0xB8
+#define	HS_SPI_REG_RXFIFO11	0xBC
+#define	HS_SPI_REG_RXFIFO12	0xC0
+#define	HS_SPI_REG_RXFIFO13	0xC4
+#define	HS_SPI_REG_RXFIFO14	0xC8
+#define	HS_SPI_REG_RXFIFO15	0xCC
+#define	HS_SPI_REG_CSCFG	0xD0
+#define	HS_SPI_REG_CSITIME	0xD4
+#define	HS_SPI_REG_CSAEXT	0xD8
+#define	HS_SPI_REG_RDCSDC0	0xDC
+#define	HS_SPI_REG_RDCSDC1	0xDE
+#define	HS_SPI_REG_RDCSDC2	0xE0
+#define	HS_SPI_REG_RDCSDC3	0xE2
+#define	HS_SPI_REG_RDCSDC4	0xE4
+#define	HS_SPI_REG_RDCSDC5	0xE6
+#define	HS_SPI_REG_RDCSDC6	0xE8
+#define	HS_SPI_REG_RDCSDC7	0xEA
+#define	HS_SPI_REG_WRCSDC0	0xEC
+#define	HS_SPI_REG_WRCSDC1	0xEE
+#define	HS_SPI_REG_WRCSDC2	0xF0
+#define	HS_SPI_REG_WRCSDC3	0xF2
+#define	HS_SPI_REG_WRCSDC4	0xF4
+#define	HS_SPI_REG_WRCSDC5	0xF6
+#define	HS_SPI_REG_WRCSDC6	0xF8
+#define	HS_SPI_REG_WRCSDC7	0xFA
+#define	HS_SPI_REG_MID		0xFC
+
+/*
+ * HS_SPI register bit definitions
+ */
+
+/* HS_SPI Module Control Register */
+#define	MCTRL_SYNCON_OFFSET	5
+#define	MCTRL_SYNCON_MASK	1
+
+#define	MCTRL_MES_OFFSET	4
+#define	MCTRL_MES_MASK		1
+
+#define	MCTRL_CDSS_OFFSET	3
+#define	MCTRL_CDSS_MASK		1
+
+#define	MCTRL_CSEN_OFFSET	1
+#define	MCTRL_CSEN_MASK		1
+
+#define	MCTRL_MEN_OFFSET	0
+#define	MCTRL_MEN_MASK		1
+
+/* HS_SPI Peripheral Communication Configuratio Register0-3 */
+#define	PCC_RDDSEL_OFFSET	21
+#define	PCC_RDDSEL_MASK		0x3
+
+#define	PCC_WRDSEL_OFFSET	17
+#define	PCC_WRDSEL_MASK		0xF
+
+#define	PCC_ESYNC_OFFSET	16
+#define	PCC_ESYNC_MASK		1
+
+#define	PCC_CDRS_OFFSET		9
+#define	PCC_CDRS_MASK		0x7F
+
+#define	PCC_SENDIAN_OFFSET	8
+#define	PCC_SENDIAN_MASK	1
+
+#define	PCC_SDIR_OFFSET		7
+#define	PCC_SDIR_MASK		1
+
+#define	PCC_SS2CD_OFFSET	5
+#define	PCC_SS2CD_MASK		3
+
+#define	PCC_SSPOL_OFFSET	4
+#define	PCC_SSPOL_MASK		1
+
+#define	PCC_ACES_OFFSET		2
+#define	PCC_ACES_MASK		1
+
+#define	PCC_CPOL_OFFSET		1
+#define	PCC_CPOL_MASK		1
+
+#define	PCC_CPHA_OFFSET		0
+#define	PCC_CPHA_MASK		1
+
+/* HS_SPI TX Interrupt Flag Register */
+#define	TXF_TSSRS_OFFSET	6
+#define	TXF_TSSRS_MASK		1
+
+#define	TXF_TFMTS_OFFSET	5
+#define	TXF_TFMTS_MASK		1
+
+#define	TXF_TFLETS_OFFSET	4
+#define	TXF_TFLETS_MASK		1
+
+#define	TXF_TFUS_OFFSET		3
+#define	TXF_TFUS_MASK		1
+
+#define	TXF_TFOS_OFFSET		2
+#define	TXF_TFOS_MASK		1
+
+#define	TXF_TFES_OFFSET		1
+#define	TXF_TFES_MASK		1
+
+#define	TXF_TFFS_OFFSET		0
+#define	TXF_TFFS_MASK		1
+
+/* HS_SPI TX Interrupt Enable Register */
+#define	TXE_TSSRE_OFFSET	6
+#define	TXE_TSSRE_MASK		1
+
+#define	TXE_TFMTE_OFFSET	5
+#define	TXE_TFMTE_MASK		1
+
+#define	TXE_TFLETE_OFFSET	4
+#define	TXE_TFLETE_MASK		1
+
+#define	TXE_TFUE_OFFSET		3
+#define	TXE_TFUE_MASK		1
+
+#define	TXE_TFOE_OFFSET		2
+#define	TXE_TFOE_MASK		1
+
+#define	TXE_TFEE_OFFSET		1
+#define	TXE_TFEE_MASK		1
+
+#define	TXE_TFFE_OFFSET		0
+#define	TXE_TFFE_MASK		1
+
+/* HS_SPI TX Interrupt Clear Register */
+#define	TXC_TSSRC_OFFSET	6
+#define	TXC_TSSRC_MASK		1
+
+#define	TXC_TFMTC_OFFSET	5
+#define	TXC_TFMTC_MASK		1
+
+#define	TXC_TFLETC_OFFSET	4
+#define	TXC_TFLETC_MASK		1
+
+#define	TXC_TFUC_OFFSET		3
+#define	TXC_TFUC_MASK		1
+
+#define	TXC_TFOC_OFFSET		2
+#define	TXC_TFOC_MASK		1
+
+#define	TXC_TFEC_OFFSET		1
+#define	TXC_TFEC_MASK		1
+
+#define	TXC_TFFC_OFFSET		0
+#define	TXC_TFFC_MASK		1
+
+/* HS_SPI RX Interrupt Flag Register */
+#define	RXF_RSSRS_OFFSET	6
+#define	RXF_RSSRS_MASK		1
+
+#define	RXF_RFMTS_OFFSET	5
+#define	RXF_RFMTS_MASK		1
+
+#define	RXF_RFLETS_OFFSET	4
+#define	RXF_RFLETS_MASK		1
+
+#define	RXF_RFUS_OFFSET		3
+#define	RXF_RFUS_MASK		1
+
+#define	RXF_RFOS_OFFSET		2
+#define	RXF_RFOS_MASK		1
+
+#define	RXF_RFES_OFFSET		1
+#define	RXF_RFES_MASK		1
+
+#define	RXF_RFFS_OFFSET		0
+#define	RXF_RFFS_MASK		1
+
+/* HS_SPI RX Interrupt Enable Register */
+#define	RXE_RSSRE_OFFSET	6
+#define	RXE_RSSRE_MASK		1
+
+#define	RXE_RFMTE_OFFSET	5
+#define	RXE_RFMTE_MASK		1
+
+#define	RXE_RFLETE_OFFSET	4
+#define	RXE_RFLETE_MASK		1
+
+#define	RXE_RFUE_OFFSET		3
+#define	RXE_RFUE_MASK		1
+
+#define	RXE_RFOE_OFFSET		2
+#define	RXE_RFOE_MASK		1
+
+#define	RXE_RFEE_OFFSET		1
+#define	RXE_RFEE_MASK		1
+
+#define	RXE_RFFE_OFFSET		0
+#define	RXE_RFFE_MASK		1
+
+/* HS_SPI RX Interrupt Clear Register */
+#define	RXC_RSSRC_OFFSET	6
+#define	RXC_RSSRC_MASK		1
+
+#define	RXC_RFMTC_OFFSET	5
+#define	RXC_RFMTC_MASK		1
+
+#define	RXC_RFLETC_OFFSET	4
+#define	RXC_RFLETC_MASK		1
+
+#define	RXC_RFUC_OFFSET		3
+#define	RXC_RFUC_MASK		1
+
+#define	RXC_RFOC_OFFSET		2
+#define	RXC_RFOC_MASK		1
+
+#define	RXC_RFEC_OFFSET		1
+#define	RXC_RFEC_MASK		1
+
+#define	RXC_RFFC_OFFSET		0
+#define	RXC_RFFC_MASK		1
+
+/* HS_SPI Fault Interrupt Flag Register */
+#define	FAULTF_DRCBSFS_OFFSET	4
+#define	FAULTF_DRCBSFS_MASK	1
+
+#define	FAULTF_DWCBSFS_OFFSET	3
+#define	FAULTF_DWCBSFS_MASK	1
+
+#define	FAULTF_PVFS_OFFSET	2
+#define	FAULTF_PVFS_MASK	1
+
+#define	FAULTF_WAFS_OFFSET	1
+#define	FAULTF_WAFS_MASK	1
+
+#define	FAULTF_UMAFS_OFFSET	0
+#define	FAULTF_UMAFS_MASK	1
+
+/* HS_SPI Fault Interrupt Flag Register */
+#define	FAULTC_DRCBSFC_OFFSET	4
+#define	FAULTC_DRCBSFC_MASK	1
+
+#define	FAULTC_DWCBSFC_OFFSET	3
+#define	FAULTC_DWCBSFC_MASK	1
+
+#define	FAULTC_PVFC_OFFSET	2
+#define	FAULTC_PVFC_MASK	1
+
+#define	FAULTC_WAFC_OFFSET	1
+#define	FAULTC_WAFC_MASK	1
+
+#define	FAULTC_UMAFC_OFFSET	0
+#define	FAULTC_UMAFC_MASK	1
+
+/* HS_SPI Direct Mode Configuration Register */
+#define	DMCFG_MSTARTEN_OFFSET	2
+#define	DMCFG_MSTARTEN_MASK	1
+
+#define	DMCFG_SSDC_OFFSET	1
+#define	DMCFG_SSDC_MASK		1
+
+/* HS_SPI Direct Mode DMA Enable Register */
+#define	DMDMAEN_TXDMAEN_OFFSET	9
+#define	DMDMAEN_TXDMAEN_MASK	1
+
+#define	DMDMAEN_RXDMAEN_OFFSET	8
+#define	DMDMAEN_RXDMAEN_MASK	1
+
+/* HS_SPI Direct Mode Start Register */
+#define	DMSTART_START_OFFSET	0
+#define	DMSTART_START_MASK	1
+
+/* HS_SPI Direct Mode Stop Register */
+#define	DMSTOP_STOP_OFFSET	8
+#define	DMSTOP_STOP_MASK	1
+
+/* HS_SPI Direct Mode Peripheral Select Register */
+#define	DMPSEL_PSEL_OFFSET	16
+#define	DMPSEL_PSEL_MASK	3
+
+/* HS_SPI Direct Mode Transfer Protocol Register */
+#define	DMTRP_TRP_OFFSET	24
+#define	DMTRP_TRP_MASK		0x0F
+
+/* HS_SPI Direct Mode Byte Count Control Register */
+#define	DMBCC_BCC_OFFSET	0
+#define	DMBCC_BCC_MASK		0xFFFF
+
+/* HS_SPI Direct Mode Byte Count Status Register */
+#define	DMBCS_BCS_OFFSET	16
+#define	DMBCS_BCS_MASK		0xFFFF
+
+/* HS_SPI Direct Mode Status Register */
+#define	DMSTATUS_TXFLEVEL_OFFSET	16
+#define	DMSTATUS_TXFLEVEL_MASK		0x1F
+
+#define	DMSTATUS_RXFLEVEL_OFFSET	8
+#define	DMSTATUS_RXFLEVEL_MASK		0x1F
+
+#define	DMSTATUS_TXACTIVE_OFFSET	1
+#define	DMSTATUS_TXACTIVE_MASK		1
+
+#define	DMSTATUS_RXACTIVE_OFFSET	0
+#define	DMSTATUS_RXACTIVE_MASK		1
+
+/* HS_SPI Transmit Bit Count Register */
+#define	TXBITCNT_TXBITCNT_OFFSET	0
+#define	TXBITCNT_TXBITCNT_MASK		0x3F
+
+/* HS_SPI FIFO Configuration Register */
+#define	FIFOCFG_TXFLSH_OFFSET	12
+#define	FIFOCFG_TXFLSH_MASK	1
+
+#define	FIFOCFG_RXFLSH_OFFSET	11
+#define	FIFOCFG_RXFLSH_MASK	1
+
+#define	FIFOCFG_TXCTRL_OFFSET	10
+#define	FIFOCFG_TXCTRL_MASK	1
+
+#define	FIFOCFG_FWIDTH_OFFSET	8
+#define	FIFOCFG_FWIDTH_MASK	3
+
+#define	FIFOCFG_TXFTH_OFFSET	4
+#define	FIFOCFG_TXFTH_MASK	0x0F
+
+#define	FIFOCFG_RXFTH_OFFSET	0
+#define	FIFOCFG_RXFTH_MASK	0x0F
+
+/* HS_SPI Command Sequencer Configuration Register */
+#define	CSCFG_MSEL_OFFSET	16
+#define	CSCFG_MSEL_MASK		0x0F
+
+#define	CSCFG_SSELEN_OFFSET	8
+#define	CSCFG_SSELEN_MASK	0xF
+
+#define	CSCFG_BSEL_OFFSET	5
+#define	CSCFG_BSEL_MASK	1
+
+#define	CSCFG_BOOTEN_OFFSET	4
+#define	CSCFG_BOOTEN_MASK	1
+
+#define	CSCFG_SPICHNG_OFFSET	3
+#define	CSCFG_SPICHNG_MASK	1
+
+#define	CSCFG_MBM_OFFSET	1
+#define	CSCFG_MBM_MASK		3
+
+#define	CSCFG_SRAM_OFFSET	0
+#define	CSCFG_SRAM_MASK		1
+
+/* HS_SPI Command Sequencer Idle Time Register */
+#define	CSITIME_ITIME_OFFSET	0
+#define	CSITIME_ITIME_MASK	0xFFFF
+
+/* HS_SPI Command Sequencer Address Extension Register */
+#define	CSAEXT_AEXT_OFFSET	16
+#define	CSAEXT_AEXT_MASK	0xFFFF
+
+/* HS_SPI Read Command Sequence Data/Control Register0-7 */
+#define	RDCSDC_RDCSDATA_OFFSET	8
+#define	RDCSDC_RDCSDATA_MASK	0x0F
+
+#define	RDCSDC_CONT_OFFSET	3
+#define	RDCSDC_CONT_MASK	1
+
+#define	RDCSDC_TRP_OFFSET	1
+#define	RDCSDC_TRP_MASK		0x3
+
+#define	RDCSDC_DEC_OFFSET	0
+#define	RDCSDC_DEC_MASK		1
+
+/* HS_SPI Write Command Sequence Data/Control Register0-7 */
+#define	WRCSDC_WRCSDATA_OFFSET	8
+#define	WRCSDC_WRCSDATA_MASK	0x0F
+
+#define	WRCSDC_CONT_OFFSET	3
+#define	WRCSDC_CONT_MASK	1
+
+#define	WRCSDC_TRP_OFFSET	1
+#define	WRCSDC_TRP_MASK		0x3
+
+#define	WRCSDC_DEC_OFFSET	0
+#define	WRCSDC_DEC_MASK		1
+
+/* Bit manipulation macros */
+#define HSSPI_BIT(bit) \
+	(1 << bit##_OFFSET)
+
+#define HSSPI_BITS(bits, val) \
+	(((val) & bits##_MASK) << bits##_OFFSET)
+
+#define HSSPI_BITS_GET(wid, bits, hs, reg) \
+	((hs_spi_read##wid(hs, reg) >> bits##_OFFSET) & bits##_MASK)
+
+#define HSSPI_BITS_SET(wid, bits, val, hs, reg) do { \
+		hs_spi_read##wid(hs, reg); \
+		hs_spi_write##wid(hs, reg, ((hs_spi_read##wid(hs, reg) & \
+		  ~(bits##_MASK << bits##_OFFSET)) | HSSPI_BITS(bits, val))); \
+	} while (0)
+
+/* Register access macros */
+#define hs_spi_fiforead(hs, regs) \
+	__raw_readb((hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_readb(hs, regs) \
+	readb((hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_readw(hs, regs) \
+	readw((hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_readl(hs, regs) \
+	readl((hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_writeb(hs, regs, val) \
+	writeb((val), (hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_writew(hs, regs, val) \
+	writew((val), (hs)->reg + HS_SPI_REG_##regs)
+
+#define hs_spi_writel(hs, regs, val) \
+	writel((val), (hs)->reg + HS_SPI_REG_##regs)
+
+#endif /* __HS_SPI_REG_H__ */
diff --git a/drivers/spi/spi-dw-mid.c b/drivers/spi/spi-dw-mid.c
index 0791c92..1389fef 100644
--- a/drivers/spi/spi-dw-mid.c
+++ b/drivers/spi/spi-dw-mid.c
@@ -222,7 +222,6 @@ int dw_spi_mid_init(struct dw_spi *dws)
 	iounmap(clk_reg);
 
 	dws->num_cs = 16;
-	dws->fifo_len = 40;	/* FIFO has 40 words buffer */
 
 #ifdef CONFIG_SPI_DW_MID_DMA
 	dws->dma_priv = kzalloc(sizeof(struct mid_dma), GFP_KERNEL);
diff --git a/drivers/spi/spi-mb86s7x.c b/drivers/spi/spi-mb86s7x.c
new file mode 100644
index 0000000..7eaacf0
--- /dev/null
+++ b/drivers/spi/spi-mb86s7x.c
@@ -0,0 +1,615 @@
+/*
+ * MB86S7x HSSPI controller driver
+ *
+ * Copyright (C) 2015 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ */
+
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/interrupt.h>
+#include <linux/io.h>
+#include <linux/irq.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/pm_runtime.h>
+#include <linux/scatterlist.h>
+#include <linux/slab.h>
+#include <linux/spi/spi.h>
+#include <linux/spinlock.h>
+
+#define MCTRL		0x0
+#define MEN	0
+#define CSEN	1
+#define IPCLK	3
+#define MES	4
+
+#define PCC0		0x4
+#define PCC(n)		(PCC0 + (n) * 4)
+#define RTM	3
+#define ACES	2
+#define SAFESYNC	16
+#define CPHA	0
+#define CPOL	1
+#define SSPOL	4
+#define SDIR	7
+#define SS2CD	5
+#define SENDIAN	8
+#define CDRS_SHIFT	9
+#define CDRS_MASK	0x7f
+
+#define TXF		0x14
+#define TXE		0x18
+#define TXC		0x1c
+#define RXF		0x20
+#define RXE		0x24
+#define RXC		0x28
+#define TFLETE	4
+#define RFMTE	5
+
+#define FAULTF		0x2c
+#define FAULTC		0x30
+
+#define DMCFG		0x34
+#define SSDC		1
+#define MSTARTEN	2
+
+#define	DMSTART		0x38
+#define TRIGGER		0
+#define DMSTOP		8
+#define CS_MASK		3
+#define CS_SHIFT	16
+#define DATA_TXRX	0
+#define DATA_RX		1
+#define DATA_TX		2
+#define DATA_MASK	3
+#define DATA_SHIFT	26
+#define BUS_WIDTH	24
+
+#define	DMBCC		0x3c
+#define DMSTATUS	0x40
+#define RX_DATA_MASK	0x1f
+#define RX_DATA_SHIFT	8
+#define TX_DATA_MASK	0x1f
+#define TX_DATA_SHIFT	16
+
+#define TXBITCNT	0x44
+
+#define FIFOCFG		0x4c
+#define BPW_MASK	0x3
+#define BPW_SHIFT	8
+#define RX_FLUSH	11
+#define TX_FLUSH	12
+#define RX_TRSHLD_MASK		0xf
+#define RX_TRSHLD_SHIFT		0
+#define TX_TRSHLD_MASK		0xf
+#define TX_TRSHLD_SHIFT		4
+
+#define TXFIFO		0x50
+#define RXFIFO		0x90
+#define MID		0xfc
+
+#define FIFO_DEPTH	16
+#define TX_TRSHLD	4
+#define RX_TRSHLD	(FIFO_DEPTH - TX_TRSHLD)
+
+#define TXBIT	1
+#define RXBIT	2
+
+struct s7x_hsspi {
+	spinlock_t lock;
+	struct device *dev;
+	struct spi_master *master;
+
+	unsigned cs;
+	unsigned bpw;
+	unsigned busy;
+	unsigned mode;
+	unsigned speed;
+	void *rx_buf;
+	const void *tx_buf;
+	struct clk *clk;
+	void __iomem *regs;
+	unsigned tx_words, rx_words;
+	unsigned bus_width;
+};
+
+static void read_fifo(struct s7x_hsspi *hsspi)
+{
+	u32 len = readl_relaxed(hsspi->regs + DMSTATUS);
+	int i;
+
+	len = (len >> RX_DATA_SHIFT) & RX_DATA_MASK;
+	len = min_t(unsigned, len, hsspi->rx_words);
+
+	switch (hsspi->bpw) {
+	case 8:
+		{
+		u8 *buf = hsspi->rx_buf;
+		for (i = 0; i < len; i++)
+			*buf++ = readb_relaxed(hsspi->regs + RXFIFO);
+		hsspi->rx_buf = buf;
+		break;
+		}
+	case 16:
+		{
+		u16 *buf = hsspi->rx_buf;
+		for (i = 0; i < len; i++)
+			*buf++ = readw_relaxed(hsspi->regs + RXFIFO);
+		hsspi->rx_buf = buf;
+		break;
+		}
+	default:
+		{
+		u32 *buf = hsspi->rx_buf;
+		for (i = 0; i < len; i++)
+			*buf++ = readl_relaxed(hsspi->regs + RXFIFO);
+		hsspi->rx_buf = buf;
+		break;
+		}
+	}
+
+	hsspi->rx_words -= len;
+}
+
+static void write_fifo(struct s7x_hsspi *hsspi)
+{
+	u32 len = readl_relaxed(hsspi->regs + DMSTATUS);
+	int i;
+
+	len = (len >> TX_DATA_SHIFT) & TX_DATA_MASK;
+	len = min_t(unsigned, FIFO_DEPTH - len, hsspi->tx_words);
+
+	switch (hsspi->bpw) {
+	case 8:
+		{
+		const u8 *buf = hsspi->tx_buf;
+		for (i = 0; i < len; i++)
+			writeb_relaxed(*buf++, hsspi->regs + TXFIFO);
+		hsspi->tx_buf = buf;
+		break;
+		}
+	case 16:
+		{
+		const u16 *buf = hsspi->tx_buf;
+		for (i = 0; i < len; i++)
+			writew_relaxed(*buf++, hsspi->regs + TXFIFO);
+		hsspi->tx_buf = buf;
+		break;
+		}
+	default:
+		{
+		const u32 *buf = hsspi->tx_buf;
+		for (i = 0; i < len; i++)
+			writel_relaxed(*buf++, hsspi->regs + TXFIFO);
+		hsspi->tx_buf = buf;
+		break;
+		}
+	}
+	hsspi->tx_words -= len;
+}
+
+static int s7x_hsspi_config(struct spi_master *master,
+				  struct spi_device *spi,
+				  struct spi_transfer *xfer)
+{
+	struct s7x_hsspi *hsspi = spi_master_get_devdata(master);
+	unsigned speed, mode, bpw, cs, bus_width;
+	unsigned long rate;
+	u32 val, div;
+
+	/* Full Duplex only on 1bit wide bus */
+	if (xfer->rx_buf && xfer->tx_buf &&
+			(xfer->rx_nbits != 1 || xfer->tx_nbits != 1)) {
+		dev_err(hsspi->dev, "RX and TX bus widths must match!\n");
+		return -EINVAL;
+	}
+
+	if (xfer->tx_buf)
+		bus_width = xfer->tx_nbits;
+	else
+		bus_width = xfer->rx_nbits;
+
+	mode = spi->mode;
+	cs = spi->chip_select;
+	speed = xfer->speed_hz ? : spi->max_speed_hz;
+	bpw = xfer->bits_per_word ? : spi->bits_per_word;
+
+	/* return if nothing to change */
+	if (speed == hsspi->speed &&
+			bus_width == hsspi->bus_width && bpw == hsspi->bpw &&
+			mode == hsspi->mode && cs == hsspi->cs) {
+		return 0;
+	}
+
+	rate = clk_get_rate(hsspi->clk);
+
+	div = DIV_ROUND_UP(rate, speed * 2);
+	if (div > 127) {
+		dev_err(hsspi->dev, "Requested rate too low (%u)\n",
+			hsspi->speed);
+		return -EINVAL;
+	}
+
+	val = readl_relaxed(hsspi->regs + PCC(cs));
+	val &= ~BIT(RTM);
+	val &= ~BIT(ACES);
+	val &= ~BIT(SAFESYNC);
+	if (bpw == 8 &&	(mode & (SPI_TX_DUAL | SPI_RX_DUAL)) && div < 3)
+		val |= BIT(SAFESYNC);
+	if (bpw == 8 &&	(mode & (SPI_TX_QUAD | SPI_RX_QUAD)) && div < 6)
+		val |= BIT(SAFESYNC);
+	if (bpw == 16 && (mode & (SPI_TX_QUAD | SPI_RX_QUAD)) && div < 3)
+		val |= BIT(SAFESYNC);
+
+	if (mode & SPI_CPHA)
+		val |= BIT(CPHA);
+	else
+		val &= ~BIT(CPHA);
+	if (mode & SPI_CPOL)
+		val |= BIT(CPOL);
+	else
+		val &= ~BIT(CPOL);
+	if (mode & SPI_CS_HIGH)
+		val |= BIT(SSPOL);
+	else
+		val &= ~BIT(SSPOL);
+	if (mode & SPI_LSB_FIRST)
+		val |= BIT(SDIR);
+	else
+		val &= ~BIT(SDIR);
+
+	val |= (3 << SS2CD);
+	val |= BIT(SENDIAN);
+
+	val &= ~(CDRS_MASK << CDRS_SHIFT);
+	val |= (div << CDRS_SHIFT);
+	writel_relaxed(val, hsspi->regs + PCC(cs));
+
+	val = readl_relaxed(hsspi->regs + FIFOCFG);
+	val &= ~(BPW_MASK << BPW_SHIFT);
+	val |= ((bpw / 8 - 1) << BPW_SHIFT);
+	writel_relaxed(val, hsspi->regs + FIFOCFG);
+
+	val = readl_relaxed(hsspi->regs + DMSTART);
+	val &= ~(DATA_MASK << DATA_SHIFT);
+	if (xfer->tx_buf && xfer->rx_buf)
+		val |= (DATA_TXRX << DATA_SHIFT);
+	else if (xfer->rx_buf)
+		val |= (DATA_RX << DATA_SHIFT);
+	else
+		val |= (DATA_TX << DATA_SHIFT);
+	val &= ~(3 << BUS_WIDTH);
+	val |= ((bus_width >> 1) << BUS_WIDTH);
+	writel_relaxed(val, hsspi->regs + DMSTART);
+
+	hsspi->bpw = bpw;
+	hsspi->mode = mode;
+	hsspi->speed = speed;
+	hsspi->cs = spi->chip_select;
+	hsspi->bus_width = bus_width;
+
+	return 0;
+}
+
+static int s7x_hsspi_transfer_one(struct spi_master *master,
+				  struct spi_device *spi,
+				  struct spi_transfer *xfer)
+{
+	struct s7x_hsspi *hsspi = spi_master_get_devdata(master);
+	unsigned long bpw, flags;
+	int ret, words;
+	u32 val;
+
+	val = readl_relaxed(hsspi->regs + FIFOCFG);
+	val |= (1 << RX_FLUSH);
+	val |= (1 << TX_FLUSH);
+	writel_relaxed(val, hsspi->regs + FIFOCFG);
+
+	/* See if we can tranfer 4-bytes as 1 word even if not asked */
+	bpw = xfer->bits_per_word ? : spi->bits_per_word;
+	if (bpw == 8 && !(xfer->len % 4) && !(spi->mode & SPI_LSB_FIRST)) {
+		bpw = xfer->bits_per_word;
+		xfer->bits_per_word = 32;
+	} else {
+		bpw = xfer->bits_per_word;
+	}
+
+	ret = s7x_hsspi_config(master, spi, xfer);
+	if (ret) {
+		xfer->bits_per_word = bpw;
+		return ret;
+	}
+
+	hsspi->tx_buf = xfer->tx_buf;
+	hsspi->rx_buf = xfer->rx_buf;
+
+	if (hsspi->bpw == 8)
+		words = xfer->len / 1;
+	else if (hsspi->bpw == 16)
+		words = xfer->len / 2;
+	else
+		words = xfer->len / 4;
+
+	spin_lock_irqsave(&hsspi->lock, flags);
+	if (xfer->tx_buf) {
+		hsspi->busy |= BIT(TXBIT);
+		hsspi->tx_words = words;
+	} else {
+		hsspi->busy &= ~BIT(TXBIT);
+		hsspi->tx_words = 0;
+	}
+
+	if (xfer->rx_buf) {
+		hsspi->busy |= BIT(RXBIT);
+		hsspi->rx_words = words;
+	} else {
+		hsspi->busy &= ~BIT(RXBIT);
+		hsspi->rx_words = 0;
+	}
+	spin_unlock_irqrestore(&hsspi->lock, flags);
+
+	if (xfer->tx_buf)
+		write_fifo(hsspi);
+
+	if (xfer->rx_buf) {
+		val = readl_relaxed(hsspi->regs + FIFOCFG);
+		val &= ~(RX_TRSHLD_MASK << RX_TRSHLD_SHIFT);
+		val |= ((hsspi->rx_words > FIFO_DEPTH ?
+			RX_TRSHLD : hsspi->rx_words) << RX_TRSHLD_SHIFT);
+		writel_relaxed(val, hsspi->regs + FIFOCFG);
+	}
+
+	val = readl_relaxed(hsspi->regs + DMSTART);
+	val &= ~(DATA_MASK << DATA_SHIFT);
+	if (xfer->tx_buf && xfer->rx_buf)
+		val |= (DATA_TXRX << DATA_SHIFT);
+	else if (xfer->rx_buf)
+		val |= (DATA_RX << DATA_SHIFT);
+	else
+		val |= (DATA_TX << DATA_SHIFT);
+	writel_relaxed(val, hsspi->regs + DMSTART);
+
+	writel_relaxed(~0, hsspi->regs + TXC);
+	writel_relaxed(~0, hsspi->regs + RXC);
+
+	/* Trigger */
+	val = readl_relaxed(hsspi->regs + DMSTART);
+	val |= BIT(TRIGGER);
+	writel_relaxed(val, hsspi->regs + DMSTART);
+
+	while (hsspi->busy & (BIT(RXBIT) | BIT(TXBIT))) {
+		if (hsspi->rx_words)
+			read_fifo(hsspi);
+		else
+			hsspi->busy &= ~BIT(RXBIT);
+
+		if (hsspi->tx_words)
+			write_fifo(hsspi);
+		else {
+			u32 len;
+			do { /* wait for shifter to empty out */
+				cpu_relax();
+				len = readl_relaxed(hsspi->regs + DMSTATUS);
+				len = (len >> TX_DATA_SHIFT) & TX_DATA_MASK;
+			} while (xfer->tx_buf && len);
+			hsspi->busy &= ~BIT(TXBIT);
+		}
+	}
+
+	/* restore */
+	xfer->bits_per_word = bpw;
+
+	return 0;
+}
+
+static void s7x_hsspi_set_cs(struct spi_device *spi, bool enable)
+{
+	struct s7x_hsspi *hsspi = spi_master_get_devdata(spi->master);
+	u32 val;
+
+	/*
+	 * drivers/spi/spi.c:
+	 * static void spi_set_cs(struct spi_device *spi, bool enable)
+	 * {
+	 *              if (spi->mode & SPI_CS_HIGH)
+	 *                      enable = !enable;
+	 *
+	 *              if (spi->cs_gpio >= 0)
+	 *                      gpio_set_value(spi->cs_gpio, !enable);
+	 *              else if (spi->master->set_cs)
+	 *              spi->master->set_cs(spi, !enable);
+	 * }
+	 *
+	 * Note: enable(s7x_hsspi_set_cs) = !enable(spi_set_cs)
+	 */
+	val = readl_relaxed(hsspi->regs + DMSTART);
+	val &= ~(CS_MASK << CS_SHIFT);
+	val |= spi->chip_select << CS_SHIFT;
+
+	if (!enable) {
+		writel_relaxed(val, hsspi->regs + DMSTART);
+
+		val = readl_relaxed(hsspi->regs + DMSTART);
+		val &= ~BIT(DMSTOP);
+		writel_relaxed(val, hsspi->regs + DMSTART);
+	} else {
+		val |= BIT(DMSTOP);
+		writel_relaxed(val, hsspi->regs + DMSTART);
+
+		if (hsspi->rx_buf) {
+			u32 buf[16];
+			hsspi->rx_buf = buf;
+			hsspi->rx_words = 16;
+			read_fifo(hsspi);
+		}
+	}
+}
+
+static int s7x_hsspi_probe(struct platform_device *pdev)
+{
+	struct spi_master *master;
+	struct s7x_hsspi *hsspi;
+	struct resource *res;
+	int ret, clkid;
+	u32 val;
+
+	master = spi_alloc_master(&pdev->dev, sizeof(*hsspi));
+	if (!master)
+		return -ENOMEM;
+	platform_set_drvdata(pdev, master);
+
+	hsspi = spi_master_get_devdata(master);
+	hsspi->dev = &pdev->dev;
+	hsspi->master = master;
+	spin_lock_init(&hsspi->lock);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	hsspi->regs = devm_ioremap_resource(hsspi->dev, res);
+	if (IS_ERR(hsspi->regs)) {
+		ret = PTR_ERR(hsspi->regs);
+		goto put_spi;
+	}
+
+	hsspi->clk = devm_clk_get(hsspi->dev, "iHCLK");
+	clkid = 0;
+	if (IS_ERR(hsspi->clk)) {
+		hsspi->clk = devm_clk_get(hsspi->dev, "iPCLK");
+		clkid = 1;
+		if (IS_ERR(hsspi->clk)) {
+			dev_err(&pdev->dev, "No source clock\n");
+			ret = PTR_ERR(hsspi->clk);
+			goto put_spi;
+		}
+	}
+
+	ret = clk_prepare_enable(hsspi->clk);
+	if (ret)
+		goto put_spi;
+
+	master->auto_runtime_pm = true;
+	master->bus_num = pdev->id;
+	master->mode_bits = SPI_CPOL | SPI_CPHA | SPI_TX_DUAL | SPI_RX_DUAL |
+				SPI_TX_QUAD | SPI_RX_QUAD;
+	master->num_chipselect = 4;
+	master->dev.of_node = pdev->dev.of_node;
+	master->bits_per_word_mask = SPI_BPW_MASK(32) | SPI_BPW_MASK(24)
+					 | SPI_BPW_MASK(16) | SPI_BPW_MASK(8);
+	master->max_speed_hz = clk_get_rate(hsspi->clk);
+	master->min_speed_hz = master->max_speed_hz / 254;
+
+	master->set_cs = s7x_hsspi_set_cs;
+	master->transfer_one = s7x_hsspi_transfer_one;
+
+	pm_runtime_set_active(hsspi->dev);
+	pm_runtime_enable(hsspi->dev);
+
+	/* Disable module */
+	writel_relaxed(0, hsspi->regs + MCTRL);
+	while (readl_relaxed(hsspi->regs + MCTRL) & BIT(MES))
+		cpu_relax();
+
+	writel_relaxed(0, hsspi->regs + TXE);
+	writel_relaxed(0, hsspi->regs + RXE);
+	val = readl_relaxed(hsspi->regs + TXF);
+	writel_relaxed(val, hsspi->regs + TXC);
+	val = readl_relaxed(hsspi->regs + RXF);
+	writel_relaxed(val, hsspi->regs + RXC);
+	val = readl_relaxed(hsspi->regs + FAULTF);
+	writel_relaxed(val, hsspi->regs + FAULTC);
+
+	val = readl_relaxed(hsspi->regs + DMCFG);
+	val &= ~BIT(SSDC);
+	val &= ~BIT(MSTARTEN);
+	writel_relaxed(val, hsspi->regs + DMCFG);
+
+	val = readl_relaxed(hsspi->regs + MCTRL);
+	if (clkid == 0)
+		val &= ~BIT(IPCLK);
+	else
+		val |= BIT(IPCLK);
+	val &= ~BIT(CSEN);
+	val |= BIT(MEN);
+	writel_relaxed(val, hsspi->regs + MCTRL);
+
+	ret = devm_spi_register_master(hsspi->dev, master);
+	if (ret)
+		goto disable_pm;
+
+	return 0;
+
+disable_pm:
+	pm_runtime_disable(hsspi->dev);
+	clk_disable_unprepare(hsspi->clk);
+put_spi:
+	spi_master_put(master);
+
+	return ret;
+}
+
+static int s7x_hsspi_remove(struct platform_device *pdev)
+{
+	struct spi_master *master = platform_get_drvdata(pdev);
+	struct s7x_hsspi *hsspi = spi_master_get_devdata(master);
+
+	pm_runtime_disable(hsspi->dev);
+	clk_disable_unprepare(hsspi->clk);
+	spi_master_put(master);
+
+	return 0;
+}
+
+#ifdef CONFIG_PM_SLEEP
+static int s7x_hsspi_suspend(struct device *dev)
+{
+	struct spi_master *master = dev_get_drvdata(dev);
+
+	return spi_master_suspend(master);
+}
+
+static int s7x_hsspi_resume(struct device *dev)
+{
+	struct spi_master *master = dev_get_drvdata(dev);
+	struct s7x_hsspi *hsspi = spi_master_get_devdata(master);
+	int ret;
+
+	/* Ensure reconfigure during next xfer */
+	hsspi->speed = 0;
+
+	ret = pm_runtime_get_sync(dev);
+	if (ret)
+		return ret;
+	pm_runtime_put(dev);
+
+	return spi_master_resume(master);
+}
+#endif /* CONFIG_PM_SLEEP */
+
+static const struct dev_pm_ops s7x_hsspi_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(s7x_hsspi_suspend, s7x_hsspi_resume)
+};
+
+static const struct of_device_id s7x_hsspi_of_match[] = {
+	{ .compatible = "fujitsu,mb86s7x-hsspi", },
+	{ },
+};
+MODULE_DEVICE_TABLE(of, s7x_hsspi_of_match);
+
+static struct platform_driver s7x_hsspi_driver = {
+	.driver = {
+		.name = "mb86s7x-hsspi",
+		.pm = &s7x_hsspi_pm_ops,
+		.of_match_table = of_match_ptr(s7x_hsspi_of_match),
+	},
+	.probe = s7x_hsspi_probe,
+	.remove = s7x_hsspi_remove,
+};
+module_platform_driver(s7x_hsspi_driver);
+
+MODULE_DESCRIPTION("Fujitsu MB86S70 HS-SPI controller driver");
+MODULE_AUTHOR("Jassi Brar <jassisinghbrar@gmail.com>");
+MODULE_LICENSE("GPL v2");
diff --git a/drivers/spi/spi.c b/drivers/spi/spi.c
index 917ca2b..46aa8a9 100644
--- a/drivers/spi/spi.c
+++ b/drivers/spi/spi.c
@@ -24,6 +24,8 @@
 #include <linux/device.h>
 #include <linux/init.h>
 #include <linux/cache.h>
+#include <linux/dma-mapping.h>
+#include <linux/dmaengine.h>
 #include <linux/mutex.h>
 #include <linux/of_device.h>
 #include <linux/of_irq.h>
@@ -37,7 +39,6 @@
 #include <linux/delay.h>
 #include <linux/kthread.h>
 #include <linux/ioport.h>
-#include <linux/acpi.h>
 
 static void spidev_release(struct device *dev)
 {
@@ -96,10 +97,6 @@ static int spi_match_device(struct device *dev, struct device_driver *drv)
 	if (of_driver_match_device(dev, drv))
 		return 1;
 
-	/* Then try ACPI */
-	if (acpi_driver_match_device(dev, drv))
-		return 1;
-
 	if (sdrv->id_table)
 		return !!spi_match_id(sdrv->id_table, spi);
 
@@ -323,7 +320,7 @@ struct spi_device *spi_alloc_device(struct spi_master *master)
 	if (!spi_master_get(master))
 		return NULL;
 
-	spi = kzalloc(sizeof *spi, GFP_KERNEL);
+	spi = kzalloc(sizeof(*spi), GFP_KERNEL);
 	if (!spi) {
 		dev_err(dev, "cannot alloc spi_device\n");
 		spi_master_put(master);
@@ -340,6 +337,23 @@ struct spi_device *spi_alloc_device(struct spi_master *master)
 }
 EXPORT_SYMBOL_GPL(spi_alloc_device);
 
+static void spi_dev_set_name(struct spi_device *spi)
+{
+	dev_set_name(&spi->dev, "%s.%u", dev_name(&spi->master->dev),
+		     spi->chip_select);
+}
+
+static int spi_dev_check(struct device *dev, void *data)
+{
+	struct spi_device *spi = to_spi_device(dev);
+	struct spi_device *new_spi = data;
+
+	if (spi->master == new_spi->master &&
+	    spi->chip_select == new_spi->chip_select)
+		return -EBUSY;
+	return 0;
+}
+
 /**
  * spi_add_device - Add spi_device allocated with spi_alloc_device
  * @spi: spi_device to register
@@ -354,7 +368,6 @@ int spi_add_device(struct spi_device *spi)
 	static DEFINE_MUTEX(spi_add_lock);
 	struct spi_master *master = spi->master;
 	struct device *dev = master->dev.parent;
-	struct device *d;
 	int status;
 
 	/* Chipselects are numbered 0..max; validate. */
@@ -366,9 +379,7 @@ int spi_add_device(struct spi_device *spi)
 	}
 
 	/* Set the bus ID string */
-	dev_set_name(&spi->dev, "%s.%u", dev_name(&spi->master->dev),
-			spi->chip_select);
-
+	spi_dev_set_name(spi);
 
 	/* We need to make sure there's no other device with this
 	 * chipselect **BEFORE** we call setup(), else we'll trash
@@ -376,12 +387,10 @@ int spi_add_device(struct spi_device *spi)
 	 */
 	mutex_lock(&spi_add_lock);
 
-	d = bus_find_device_by_name(&spi_bus_type, NULL, dev_name(&spi->dev));
-	if (d != NULL) {
+	status = bus_for_each_dev(&spi_bus_type, NULL, spi, spi_dev_check);
+	if (status) {
 		dev_err(dev, "chipselect %d already in use\n",
 				spi->chip_select);
-		put_device(d);
-		status = -EBUSY;
 		goto done;
 	}
 
@@ -503,6 +512,9 @@ int spi_register_board_info(struct spi_board_info const *info, unsigned n)
 	struct boardinfo *bi;
 	int i;
 
+	if (!n)
+		return -EINVAL;
+
 	bi = kzalloc(n * sizeof(*bi), GFP_KERNEL);
 	if (!bi)
 		return -ENOMEM;
@@ -523,6 +535,300 @@ int spi_register_board_info(struct spi_board_info const *info, unsigned n)
 
 /*-------------------------------------------------------------------------*/
 
+static void spi_set_cs(struct spi_device *spi, bool enable)
+{
+	if (spi->mode & SPI_CS_HIGH)
+		enable = !enable;
+
+	if (spi->cs_gpio >= 0)
+		gpio_set_value(spi->cs_gpio, !enable);
+	else if (spi->master->set_cs)
+		spi->master->set_cs(spi, !enable);
+}
+
+#ifdef CONFIG_HAS_DMA
+static int spi_map_buf(struct spi_master *master, struct device *dev,
+		       struct sg_table *sgt, void *buf, size_t len,
+		       enum dma_data_direction dir)
+{
+	const bool vmalloced_buf = is_vmalloc_addr(buf);
+	const int desc_len = vmalloced_buf ? PAGE_SIZE : master->max_dma_len;
+	const int sgs = DIV_ROUND_UP(len, desc_len);
+	struct page *vm_page;
+	void *sg_buf;
+	size_t min;
+	int i, ret;
+
+	ret = sg_alloc_table(sgt, sgs, GFP_KERNEL);
+	if (ret != 0)
+		return ret;
+
+	for (i = 0; i < sgs; i++) {
+		min = min_t(size_t, len, desc_len);
+
+		if (vmalloced_buf) {
+			vm_page = vmalloc_to_page(buf);
+			if (!vm_page) {
+				sg_free_table(sgt);
+				return -ENOMEM;
+			}
+			sg_set_page(&sgt->sgl[i], vm_page,
+				    min, offset_in_page(buf));
+		} else {
+			sg_buf = buf;
+			sg_set_buf(&sgt->sgl[i], sg_buf, min);
+		}
+
+
+		buf += min;
+		len -= min;
+	}
+
+	ret = dma_map_sg(dev, sgt->sgl, sgt->nents, dir);
+	if (!ret)
+		ret = -ENOMEM;
+	if (ret < 0) {
+		sg_free_table(sgt);
+		return ret;
+	}
+
+	sgt->nents = ret;
+
+	return 0;
+}
+
+static void spi_unmap_buf(struct spi_master *master, struct device *dev,
+			  struct sg_table *sgt, enum dma_data_direction dir)
+{
+	if (sgt->orig_nents) {
+		dma_unmap_sg(dev, sgt->sgl, sgt->orig_nents, dir);
+		sg_free_table(sgt);
+	}
+}
+
+static int __spi_map_msg(struct spi_master *master, struct spi_message *msg)
+{
+	struct device *tx_dev, *rx_dev;
+	struct spi_transfer *xfer;
+	int ret;
+
+	if (!master->can_dma)
+		return 0;
+
+	tx_dev = master->dma_tx->device->dev;
+	rx_dev = master->dma_rx->device->dev;
+
+	list_for_each_entry(xfer, &msg->transfers, transfer_list) {
+		if (!master->can_dma(master, msg->spi, xfer))
+			continue;
+
+		if (xfer->tx_buf != NULL) {
+			ret = spi_map_buf(master, tx_dev, &xfer->tx_sg,
+					  (void *)xfer->tx_buf, xfer->len,
+					  DMA_TO_DEVICE);
+			if (ret != 0)
+				return ret;
+		}
+
+		if (xfer->rx_buf != NULL) {
+			ret = spi_map_buf(master, rx_dev, &xfer->rx_sg,
+					  xfer->rx_buf, xfer->len,
+					  DMA_FROM_DEVICE);
+			if (ret != 0) {
+				spi_unmap_buf(master, tx_dev, &xfer->tx_sg,
+					      DMA_TO_DEVICE);
+				return ret;
+			}
+		}
+	}
+
+	master->cur_msg_mapped = true;
+
+	return 0;
+}
+
+static int spi_unmap_msg(struct spi_master *master, struct spi_message *msg)
+{
+	struct spi_transfer *xfer;
+	struct device *tx_dev, *rx_dev;
+
+	if (!master->cur_msg_mapped || !master->can_dma)
+		return 0;
+
+	tx_dev = master->dma_tx->device->dev;
+	rx_dev = master->dma_rx->device->dev;
+
+	list_for_each_entry(xfer, &msg->transfers, transfer_list) {
+		if (!master->can_dma(master, msg->spi, xfer))
+			continue;
+
+		spi_unmap_buf(master, rx_dev, &xfer->rx_sg, DMA_FROM_DEVICE);
+		spi_unmap_buf(master, tx_dev, &xfer->tx_sg, DMA_TO_DEVICE);
+	}
+
+	return 0;
+}
+#else /* !CONFIG_HAS_DMA */
+static inline int __spi_map_msg(struct spi_master *master,
+				struct spi_message *msg)
+{
+	return 0;
+}
+
+static inline int spi_unmap_msg(struct spi_master *master,
+				struct spi_message *msg)
+{
+	return 0;
+}
+#endif /* !CONFIG_HAS_DMA */
+
+static int spi_map_msg(struct spi_master *master, struct spi_message *msg)
+{
+	struct spi_transfer *xfer;
+	void *tmp;
+	unsigned int max_tx, max_rx;
+
+	if (master->flags & (SPI_MASTER_MUST_RX | SPI_MASTER_MUST_TX)) {
+		max_tx = 0;
+		max_rx = 0;
+
+		list_for_each_entry(xfer, &msg->transfers, transfer_list) {
+			if ((master->flags & SPI_MASTER_MUST_TX) &&
+			    !xfer->tx_buf)
+				max_tx = max(xfer->len, max_tx);
+			if ((master->flags & SPI_MASTER_MUST_RX) &&
+			    !xfer->rx_buf)
+				max_rx = max(xfer->len, max_rx);
+		}
+
+		if (max_tx) {
+			tmp = krealloc(master->dummy_tx, max_tx,
+				       GFP_KERNEL | GFP_DMA);
+			if (!tmp)
+				return -ENOMEM;
+			master->dummy_tx = tmp;
+			memset(tmp, 0, max_tx);
+		}
+
+		if (max_rx) {
+			tmp = krealloc(master->dummy_rx, max_rx,
+				       GFP_KERNEL | GFP_DMA);
+			if (!tmp)
+				return -ENOMEM;
+			master->dummy_rx = tmp;
+		}
+
+		if (max_tx || max_rx) {
+			list_for_each_entry(xfer, &msg->transfers,
+					    transfer_list) {
+				if (!xfer->tx_buf)
+					xfer->tx_buf = master->dummy_tx;
+				if (!xfer->rx_buf)
+					xfer->rx_buf = master->dummy_rx;
+			}
+		}
+	}
+
+	return __spi_map_msg(master, msg);
+}
+
+/*
+ * spi_transfer_one_message - Default implementation of transfer_one_message()
+ *
+ * This is a standard implementation of transfer_one_message() for
+ * drivers which impelment a transfer_one() operation.  It provides
+ * standard handling of delays and chip select management.
+ */
+static int spi_transfer_one_message(struct spi_master *master,
+				    struct spi_message *msg)
+{
+	struct spi_transfer *xfer;
+	bool keep_cs = false;
+	int ret = 0;
+	int ms = 1;
+
+	spi_set_cs(msg->spi, true);
+
+	list_for_each_entry(xfer, &msg->transfers, transfer_list) {
+
+		if (xfer->tx_buf || xfer->rx_buf) {
+			reinit_completion(&master->xfer_completion);
+
+			ret = master->transfer_one(master, msg->spi, xfer);
+			if (ret < 0) {
+				dev_err(&msg->spi->dev,
+					"SPI transfer failed: %d\n", ret);
+				goto out;
+			}
+
+			if (ret > 0) {
+				ret = 0;
+				ms = xfer->len * 8 * 1000 / xfer->speed_hz;
+				ms += ms + 100; /* some tolerance */
+
+				ms = wait_for_completion_timeout(&master->xfer_completion,
+								 msecs_to_jiffies(ms));
+			}
+
+			if (ms == 0) {
+				dev_err(&msg->spi->dev,
+					"SPI transfer timed out\n");
+				msg->status = -ETIMEDOUT;
+			}
+		} else {
+			if (xfer->len)
+				dev_err(&msg->spi->dev,
+					"Bufferless transfer has length %u\n",
+					xfer->len);
+		}
+
+
+		if (msg->status != -EINPROGRESS)
+			goto out;
+
+		if (xfer->delay_usecs)
+			udelay(xfer->delay_usecs);
+
+		if (xfer->cs_change) {
+			if (list_is_last(&xfer->transfer_list,
+					 &msg->transfers)) {
+				keep_cs = true;
+			} else {
+				spi_set_cs(msg->spi, false);
+				udelay(10);
+				spi_set_cs(msg->spi, true);
+			}
+		}
+
+		msg->actual_length += xfer->len;
+	}
+
+out:
+	if (ret != 0 || !keep_cs)
+		spi_set_cs(msg->spi, false);
+
+	if (msg->status == -EINPROGRESS)
+		msg->status = ret;
+
+	spi_finalize_current_message(master);
+
+	return ret;
+}
+
+/**
+ * spi_finalize_current_transfer - report completion of a transfer
+ * @master: the master reporting completion
+ *
+ * Called by SPI drivers using the core transfer_one_message()
+ * implementation to notify it that the current interrupt driven
+ * transfer has finished and the next one may be scheduled.
+ */
+void spi_finalize_current_transfer(struct spi_master *master)
+{
+	complete(&master->xfer_completion);
+}
+EXPORT_SYMBOL_GPL(spi_finalize_current_transfer);
+
 /**
  * spi_pump_messages - kthread work function which processes spi message queue
  * @work: pointer to kthread work struct contained in the master struct
@@ -549,10 +855,18 @@ static void spi_pump_messages(struct kthread_work *work)
 		}
 		master->busy = false;
 		spin_unlock_irqrestore(&master->queue_lock, flags);
+		kfree(master->dummy_rx);
+		master->dummy_rx = NULL;
+		kfree(master->dummy_tx);
+		master->dummy_tx = NULL;
 		if (master->unprepare_transfer_hardware &&
 		    master->unprepare_transfer_hardware(master))
 			dev_err(&master->dev,
 				"failed to unprepare transfer hardware\n");
+		if (master->auto_runtime_pm) {
+			pm_runtime_mark_last_busy(master->dev.parent);
+			pm_runtime_put_autosuspend(master->dev.parent);
+		}
 		return;
 	}
 
@@ -563,7 +877,7 @@ static void spi_pump_messages(struct kthread_work *work)
 	}
 	/* Extract head of queue */
 	master->cur_msg =
-	    list_entry(master->queue.next, struct spi_message, queue);
+		list_first_entry(&master->queue, struct spi_message, queue);
 
 	list_del_init(&master->cur_msg->queue);
 	if (master->busy)
@@ -572,13 +886,45 @@ static void spi_pump_messages(struct kthread_work *work)
 		master->busy = true;
 	spin_unlock_irqrestore(&master->queue_lock, flags);
 
+	if (!was_busy && master->auto_runtime_pm) {
+		ret = pm_runtime_get_sync(master->dev.parent);
+		if (ret < 0) {
+			dev_err(&master->dev, "Failed to power device: %d\n",
+				ret);
+			return;
+		}
+	}
+
 	if (!was_busy && master->prepare_transfer_hardware) {
 		ret = master->prepare_transfer_hardware(master);
 		if (ret) {
 			dev_err(&master->dev,
 				"failed to prepare transfer hardware\n");
+
+			if (master->auto_runtime_pm)
+				pm_runtime_put(master->dev.parent);
+			return;
+		}
+	}
+
+
+	if (master->prepare_message) {
+		ret = master->prepare_message(master, master->cur_msg);
+		if (ret) {
+			dev_err(&master->dev,
+				"failed to prepare message: %d\n", ret);
+			master->cur_msg->status = ret;
+			spi_finalize_current_message(master);
 			return;
 		}
+		master->cur_msg_prepared = true;
+	}
+
+	ret = spi_map_msg(master, master->cur_msg);
+	if (ret) {
+		master->cur_msg->status = ret;
+		spi_finalize_current_message(master);
+		return;
 	}
 
 	ret = master->transfer_one_message(master, master->cur_msg);
@@ -601,11 +947,11 @@ static int spi_init_queue(struct spi_master *master)
 
 	init_kthread_worker(&master->kworker);
 	master->kworker_task = kthread_run(kthread_worker_fn,
-					   &master->kworker,
+					   &master->kworker, "%s",
 					   dev_name(&master->dev));
 	if (IS_ERR(master->kworker_task)) {
 		dev_err(&master->dev, "failed to create message pump task\n");
-		return -ENOMEM;
+		return PTR_ERR(master->kworker_task);
 	}
 	init_kthread_work(&master->pump_messages, spi_pump_messages);
 
@@ -640,11 +986,8 @@ struct spi_message *spi_get_next_queued_message(struct spi_master *master)
 
 	/* get a pointer to the next message, if any */
 	spin_lock_irqsave(&master->queue_lock, flags);
-	if (list_empty(&master->queue))
-		next = NULL;
-	else
-		next = list_entry(master->queue.next,
-				  struct spi_message, queue);
+	next = list_first_entry_or_null(&master->queue, struct spi_message,
+					queue);
 	spin_unlock_irqrestore(&master->queue_lock, flags);
 
 	return next;
@@ -662,6 +1005,7 @@ void spi_finalize_current_message(struct spi_master *master)
 {
 	struct spi_message *mesg;
 	unsigned long flags;
+	int ret;
 
 	spin_lock_irqsave(&master->queue_lock, flags);
 	mesg = master->cur_msg;
@@ -670,6 +1014,17 @@ void spi_finalize_current_message(struct spi_master *master)
 	queue_kthread_work(&master->kworker, &master->pump_messages);
 	spin_unlock_irqrestore(&master->queue_lock, flags);
 
+	spi_unmap_msg(master, mesg);
+
+	if (master->cur_msg_prepared && master->unprepare_message) {
+		ret = master->unprepare_message(master, mesg);
+		if (ret) {
+			dev_err(&master->dev,
+				"failed to unprepare message: %d\n", ret);
+		}
+	}
+	master->cur_msg_prepared = false;
+
 	mesg->state = NULL;
 	if (mesg->complete)
 		mesg->complete(mesg->context);
@@ -712,7 +1067,7 @@ static int spi_stop_queue(struct spi_master *master)
 	 */
 	while ((!list_empty(&master->queue) || master->busy) && limit--) {
 		spin_unlock_irqrestore(&master->queue_lock, flags);
-		msleep(10);
+		usleep_range(10000, 11000);
 		spin_lock_irqsave(&master->queue_lock, flags);
 	}
 
@@ -774,7 +1129,7 @@ static int spi_queued_transfer(struct spi_device *spi, struct spi_message *msg)
 	msg->status = -EINPROGRESS;
 
 	list_add_tail(&msg->queue, &master->queue);
-	if (master->running && !master->busy)
+	if (!master->busy)
 		queue_kthread_work(&master->kworker, &master->pump_messages);
 
 	spin_unlock_irqrestore(&master->queue_lock, flags);
@@ -785,8 +1140,9 @@ static int spi_master_initialize_queue(struct spi_master *master)
 {
 	int ret;
 
-	master->queued = true;
 	master->transfer = spi_queued_transfer;
+	if (!master->transfer_one_message)
+		master->transfer_one_message = spi_transfer_one_message;
 
 	/* Initialize and start queue */
 	ret = spi_init_queue(master);
@@ -794,6 +1150,7 @@ static int spi_master_initialize_queue(struct spi_master *master)
 		dev_err(&master->dev, "problem initializing queue\n");
 		goto err_init_queue;
 	}
+	master->queued = true;
 	ret = spi_start_queue(master);
 	if (ret) {
 		dev_err(&master->dev, "problem starting queue\n");
@@ -803,61 +1160,47 @@ static int spi_master_initialize_queue(struct spi_master *master)
 	return 0;
 
 err_start_queue:
-err_init_queue:
 	spi_destroy_queue(master);
+err_init_queue:
 	return ret;
 }
 
 /*-------------------------------------------------------------------------*/
 
 #if defined(CONFIG_OF)
-/**
- * of_register_spi_devices() - Register child devices onto the SPI bus
- * @master:	Pointer to spi_master device
- *
- * Registers an spi_device for each child node of master node which has a 'reg'
- * property.
- */
-static void of_register_spi_devices(struct spi_master *master)
+static struct spi_device *
+of_register_spi_device(struct spi_master *master, struct device_node *nc)
 {
 	struct spi_device *spi;
-	struct device_node *nc;
-	const __be32 *prop;
-	char modalias[SPI_NAME_SIZE + 4];
 	int rc;
-	int len;
+	u32 value;
 
-	if (!master->dev.of_node)
-		return;
-
-	for_each_available_child_of_node(master->dev.of_node, nc) {
 		/* Alloc an spi_device */
 		spi = spi_alloc_device(master);
 		if (!spi) {
 			dev_err(&master->dev, "spi_device alloc error for %s\n",
 				nc->full_name);
-			spi_dev_put(spi);
-			continue;
+		rc = -ENOMEM;
+		goto err_out;
 		}
 
 		/* Select device driver */
-		if (of_modalias_node(nc, spi->modalias,
-				     sizeof(spi->modalias)) < 0) {
+	rc = of_modalias_node(nc, spi->modalias,
+				sizeof(spi->modalias));
+	if (rc < 0) {
 			dev_err(&master->dev, "cannot find modalias for %s\n",
 				nc->full_name);
-			spi_dev_put(spi);
-			continue;
+		goto err_out;
 		}
 
 		/* Device address */
-		prop = of_get_property(nc, "reg", &len);
-		if (!prop || len < sizeof(*prop)) {
-			dev_err(&master->dev, "%s has no 'reg' property\n",
-				nc->full_name);
-			spi_dev_put(spi);
-			continue;
+	rc = of_property_read_u32(nc, "reg", &value);
+	if (rc) {
+		dev_err(&master->dev, "%s has no valid 'reg' property (%d)\n",
+			nc->full_name, rc);
+		goto err_out;
 		}
-		spi->chip_select = be32_to_cpup(prop);
+	spi->chip_select = value;
 
 		/* Mode (clock phase/polarity/etc.) */
 		if (of_find_property(nc, "spi-cpha", NULL))
@@ -868,16 +1211,54 @@ static void of_register_spi_devices(struct spi_master *master)
 			spi->mode |= SPI_CS_HIGH;
 		if (of_find_property(nc, "spi-3wire", NULL))
 			spi->mode |= SPI_3WIRE;
+	if (of_find_property(nc, "spi-lsb-first", NULL))
+		spi->mode |= SPI_LSB_FIRST;
+
+	/* Device DUAL/QUAD mode */
+	if (!of_property_read_u32(nc, "spi-tx-bus-width", &value)) {
+		switch (value) {
+		case 1:
+			break;
+		case 2:
+			spi->mode |= SPI_TX_DUAL;
+			break;
+		case 4:
+			spi->mode |= SPI_TX_QUAD;
+			break;
+		default:
+			dev_warn(&master->dev,
+				"spi-tx-bus-width %d not supported\n",
+				value);
+			break;
+		}
+	}
+
+	if (!of_property_read_u32(nc, "spi-rx-bus-width", &value)) {
+		switch (value) {
+		case 1:
+			break;
+		case 2:
+			spi->mode |= SPI_RX_DUAL;
+			break;
+		case 4:
+			spi->mode |= SPI_RX_QUAD;
+			break;
+		default:
+			dev_warn(&master->dev,
+				"spi-rx-bus-width %d not supported\n",
+				value);
+			break;
+		}
+	}
 
 		/* Device speed */
-		prop = of_get_property(nc, "spi-max-frequency", &len);
-		if (!prop || len < sizeof(*prop)) {
-			dev_err(&master->dev, "%s has no 'spi-max-frequency' property\n",
-				nc->full_name);
-			spi_dev_put(spi);
-			continue;
+	rc = of_property_read_u32(nc, "spi-max-frequency", &value);
+	if (rc) {
+		dev_err(&master->dev, "%s has no valid 'spi-max-frequency' property (%d)\n",
+			nc->full_name, rc);
+		goto err_out;
 		}
-		spi->max_speed_hz = be32_to_cpup(prop);
+	spi->max_speed_hz = value;
 
 		/* IRQ */
 		spi->irq = irq_of_parse_and_map(nc, 0);
@@ -887,115 +1268,46 @@ static void of_register_spi_devices(struct spi_master *master)
 		spi->dev.of_node = nc;
 
 		/* Register the new device */
-		snprintf(modalias, sizeof(modalias), "%s%s", SPI_MODULE_PREFIX,
-			 spi->modalias);
-		request_module(modalias);
+	request_module("%s%s", SPI_MODULE_PREFIX, spi->modalias);
 		rc = spi_add_device(spi);
 		if (rc) {
 			dev_err(&master->dev, "spi_device register error %s\n",
 				nc->full_name);
-			spi_dev_put(spi);
-		}
-
-	}
-}
-#else
-static void of_register_spi_devices(struct spi_master *master) { }
-#endif
-
-#ifdef CONFIG_ACPI
-static int acpi_spi_add_resource(struct acpi_resource *ares, void *data)
-{
-	struct spi_device *spi = data;
-
-	if (ares->type == ACPI_RESOURCE_TYPE_SERIAL_BUS) {
-		struct acpi_resource_spi_serialbus *sb;
-
-		sb = &ares->data.spi_serial_bus;
-		if (sb->type == ACPI_RESOURCE_SERIAL_TYPE_SPI) {
-			spi->chip_select = sb->device_selection;
-			spi->max_speed_hz = sb->connection_speed;
-
-			if (sb->clock_phase == ACPI_SPI_SECOND_PHASE)
-				spi->mode |= SPI_CPHA;
-			if (sb->clock_polarity == ACPI_SPI_START_HIGH)
-				spi->mode |= SPI_CPOL;
-			if (sb->device_polarity == ACPI_SPI_ACTIVE_HIGH)
-				spi->mode |= SPI_CS_HIGH;
-		}
-	} else if (spi->irq < 0) {
-		struct resource r;
-
-		if (acpi_dev_resource_interrupt(ares, 0, &r))
-			spi->irq = r.start;
-	}
-
-	/* Always tell the ACPI core to skip this resource */
-	return 1;
-}
-
-static acpi_status acpi_spi_add_device(acpi_handle handle, u32 level,
-				       void *data, void **return_value)
-{
-	struct spi_master *master = data;
-	struct list_head resource_list;
-	struct acpi_device *adev;
-	struct spi_device *spi;
-	int ret;
-
-	if (acpi_bus_get_device(handle, &adev))
-		return AE_OK;
-	if (acpi_bus_get_status(adev) || !adev->status.present)
-		return AE_OK;
-
-	spi = spi_alloc_device(master);
-	if (!spi) {
-		dev_err(&master->dev, "failed to allocate SPI device for %s\n",
-			dev_name(&adev->dev));
-		return AE_NO_MEMORY;
+		goto err_out;
 	}
 
-	ACPI_COMPANION_SET(&spi->dev, adev);
-	spi->irq = -1;
-
-	INIT_LIST_HEAD(&resource_list);
-	ret = acpi_dev_get_resources(adev, &resource_list,
-				     acpi_spi_add_resource, spi);
-	acpi_dev_free_resource_list(&resource_list);
-
-	if (ret < 0 || !spi->max_speed_hz) {
-		spi_dev_put(spi);
-		return AE_OK;
-	}
+	return spi;
 
-	strlcpy(spi->modalias, dev_name(&adev->dev), sizeof(spi->modalias));
-	if (spi_add_device(spi)) {
-		dev_err(&master->dev, "failed to add SPI device %s from ACPI\n",
-			dev_name(&adev->dev));
+err_out:
 		spi_dev_put(spi);
+	return ERR_PTR(rc);
 	}
 
-	return AE_OK;
-}
-
-static void acpi_register_spi_devices(struct spi_master *master)
+/**
+ * of_register_spi_devices() - Register child devices onto the SPI bus
+ * @master:	Pointer to spi_master device
+ *
+ * Registers an spi_device for each child node of master node which has a 'reg'
+ * property.
+ */
+static void of_register_spi_devices(struct spi_master *master)
 {
-	acpi_status status;
-	acpi_handle handle;
+	struct spi_device *spi;
+	struct device_node *nc;
 
-	handle = ACPI_HANDLE(master->dev.parent);
-	if (!handle)
+	if (!master->dev.of_node)
 		return;
 
-	status = acpi_walk_namespace(ACPI_TYPE_DEVICE, handle, 1,
-				     acpi_spi_add_device, NULL,
-				     master, NULL);
-	if (ACPI_FAILURE(status))
-		dev_warn(&master->dev, "failed to enumerate SPI slaves\n");
+	for_each_available_child_of_node(master->dev.of_node, nc) {
+		spi = of_register_spi_device(master, nc);
+		if (IS_ERR(spi))
+			dev_warn(&master->dev, "Failed to create SPI device for %s\n",
+				nc->full_name);
+	}
 }
 #else
-static inline void acpi_register_spi_devices(struct spi_master *master) {}
-#endif /* CONFIG_ACPI */
+static void of_register_spi_devices(struct spi_master *master) { }
+#endif
 
 static void spi_master_release(struct device *dev)
 {
@@ -1040,7 +1352,7 @@ struct spi_master *spi_alloc_master(struct device *dev, unsigned size)
 	if (!dev)
 		return NULL;
 
-	master = kzalloc(size + sizeof *master, GFP_KERNEL);
+	master = kzalloc(size + sizeof(*master), GFP_KERNEL);
 	if (!master)
 		return NULL;
 
@@ -1065,7 +1377,7 @@ static int of_spi_register_master(struct spi_master *master)
 		return 0;
 
 	nb = of_gpio_named_count(np, "cs-gpios");
-	master->num_chipselect = max(nb, (int)master->num_chipselect);
+	master->num_chipselect = max_t(int, nb, master->num_chipselect);
 
 	/* Return error only for an incorrectly formed cs-gpios property */
 	if (nb == 0 || nb == -ENOENT)
@@ -1152,6 +1464,9 @@ int spi_register_master(struct spi_master *master)
 	spin_lock_init(&master->bus_lock_spinlock);
 	mutex_init(&master->bus_lock_mutex);
 	master->bus_lock_flag = 0;
+	init_completion(&master->xfer_completion);
+	if (!master->max_dma_len)
+		master->max_dma_len = INT_MAX;
 
 	/* register the device, then userspace will see it.
 	 * registration fails if the bus ID is in use.
@@ -1169,7 +1484,7 @@ int spi_register_master(struct spi_master *master)
 	else {
 		status = spi_master_initialize_queue(master);
 		if (status) {
-			device_unregister(&master->dev);
+			device_del(&master->dev);
 			goto done;
 		}
 	}
@@ -1180,14 +1495,48 @@ int spi_register_master(struct spi_master *master)
 		spi_match_master_to_boardinfo(master, &bi->board_info);
 	mutex_unlock(&board_lock);
 
-	/* Register devices from the device tree and ACPI */
+	/* Register devices from the device tree */
 	of_register_spi_devices(master);
-	acpi_register_spi_devices(master);
 done:
 	return status;
 }
 EXPORT_SYMBOL_GPL(spi_register_master);
 
+static void devm_spi_unregister(struct device *dev, void *res)
+{
+	spi_unregister_master(*(struct spi_master **)res);
+}
+
+/**
+ * dev_spi_register_master - register managed SPI master controller
+ * @dev:    device managing SPI master
+ * @master: initialized master, originally from spi_alloc_master()
+ * Context: can sleep
+ *
+ * Register a SPI device as with spi_register_master() which will
+ * automatically be unregister
+ */
+int devm_spi_register_master(struct device *dev, struct spi_master *master)
+{
+	struct spi_master **ptr;
+	int ret;
+
+	ptr = devres_alloc(devm_spi_unregister, sizeof(*ptr), GFP_KERNEL);
+	if (!ptr)
+		return -ENOMEM;
+
+	ret = spi_register_master(master);
+	if (!ret) {
+		*ptr = master;
+		devres_add(dev, ptr);
+	} else {
+		devres_free(ptr);
+	}
+
+	return ret;
+}
+EXPORT_SYMBOL_GPL(devm_spi_register_master);
+
 static int __unregister(struct device *dev, void *null)
 {
 	spi_unregister_device(to_spi_device(dev));
@@ -1313,13 +1662,35 @@ EXPORT_SYMBOL_GPL(spi_busnum_to_master);
  */
 int spi_setup(struct spi_device *spi)
 {
-	unsigned	bad_bits;
+	unsigned	bad_bits, ugly_bits;
 	int		status = 0;
 
+	/* check mode to prevent that DUAL and QUAD set at the same time
+	 */
+	if (((spi->mode & SPI_TX_DUAL) && (spi->mode & SPI_TX_QUAD)) ||
+		((spi->mode & SPI_RX_DUAL) && (spi->mode & SPI_RX_QUAD))) {
+		dev_err(&spi->dev,
+		"setup: can not select dual and quad at the same time\n");
+		return -EINVAL;
+	}
+	/* if it is SPI_3WIRE mode, DUAL and QUAD should be forbidden
+	 */
+	if ((spi->mode & SPI_3WIRE) && (spi->mode &
+		(SPI_TX_DUAL | SPI_TX_QUAD | SPI_RX_DUAL | SPI_RX_QUAD)))
+		return -EINVAL;
 	/* help drivers fail *cleanly* when they need options
 	 * that aren't supported with their current master
 	 */
 	bad_bits = spi->mode & ~spi->master->mode_bits;
+	ugly_bits = bad_bits &
+		    (SPI_TX_DUAL | SPI_TX_QUAD | SPI_RX_DUAL | SPI_RX_QUAD);
+	if (ugly_bits) {
+		dev_warn(&spi->dev,
+			 "setup: ignoring unsupported mode bits %x\n",
+			 ugly_bits);
+		spi->mode &= ~ugly_bits;
+		bad_bits &= ~ugly_bits;
+	}
 	if (bad_bits) {
 		dev_err(&spi->dev, "setup: unsupported mode bits %x\n",
 			bad_bits);
@@ -1329,11 +1700,13 @@ int spi_setup(struct spi_device *spi)
 	if (!spi->bits_per_word)
 		spi->bits_per_word = 8;
 
+	if (!spi->max_speed_hz)
+		spi->max_speed_hz = spi->master->max_speed_hz;
+
 	if (spi->master->setup)
 		status = spi->master->setup(spi);
 
-	dev_dbg(&spi->dev, "setup mode %d, %s%s%s%s"
-				"%u bits/w, %u Hz max --> %d\n",
+	dev_dbg(&spi->dev, "setup mode %d, %s%s%s%s%u bits/w, %u Hz max --> %d\n",
 			(int) (spi->mode & (SPI_CPOL | SPI_CPHA)),
 			(spi->mode & SPI_CS_HIGH) ? "cs_high, " : "",
 			(spi->mode & SPI_LSB_FIRST) ? "lsb, " : "",
@@ -1346,10 +1719,14 @@ int spi_setup(struct spi_device *spi)
 }
 EXPORT_SYMBOL_GPL(spi_setup);
 
-static int __spi_async(struct spi_device *spi, struct spi_message *message)
+static int __spi_validate(struct spi_device *spi, struct spi_message *message)
 {
 	struct spi_master *master = spi->master;
 	struct spi_transfer *xfer;
+	int w_size;
+
+	if (list_empty(&message->transfers))
+		return -EINVAL;
 
 	/* Half-duplex links include original MicroWire, and ones with
 	 * only one data pin like SPI_3WIRE (switches direction) or where
@@ -1373,12 +1750,21 @@ static int __spi_async(struct spi_device *spi, struct spi_message *message)
 	/**
 	 * Set transfer bits_per_word and max speed as spi device default if
 	 * it is not set for this transfer.
+	 * Set transfer tx_nbits and rx_nbits as single transfer default
+	 * (SPI_NBITS_SINGLE) if it is not set for this transfer.
 	 */
 	list_for_each_entry(xfer, &message->transfers, transfer_list) {
+		message->frame_length += xfer->len;
 		if (!xfer->bits_per_word)
 			xfer->bits_per_word = spi->bits_per_word;
+
 		if (!xfer->speed_hz)
 			xfer->speed_hz = spi->max_speed_hz;
+
+		if (master->max_speed_hz &&
+		    xfer->speed_hz > master->max_speed_hz)
+			xfer->speed_hz = master->max_speed_hz;
+
 		if (master->bits_per_word_mask) {
 			/* Only 32 bits fit in the mask */
 			if (xfer->bits_per_word > 32)
@@ -1387,10 +1773,72 @@ static int __spi_async(struct spi_device *spi, struct spi_message *message)
 					BIT(xfer->bits_per_word - 1)))
 				return -EINVAL;
 		}
+
+		/*
+		 * SPI transfer length should be multiple of SPI word size
+		 * where SPI word size should be power-of-two multiple
+		 */
+		if (xfer->bits_per_word <= 8)
+			w_size = 1;
+		else if (xfer->bits_per_word <= 16)
+			w_size = 2;
+		else
+			w_size = 4;
+
+		/* No partial transfers accepted */
+		if (xfer->len % w_size)
+			return -EINVAL;
+
+		if (xfer->speed_hz && master->min_speed_hz &&
+		    xfer->speed_hz < master->min_speed_hz)
+			return -EINVAL;
+
+		if (xfer->tx_buf && !xfer->tx_nbits)
+			xfer->tx_nbits = SPI_NBITS_SINGLE;
+		if (xfer->rx_buf && !xfer->rx_nbits)
+			xfer->rx_nbits = SPI_NBITS_SINGLE;
+		/* check transfer tx/rx_nbits:
+		 * 1. check the value matches one of single, dual and quad
+		 * 2. check tx/rx_nbits match the mode in spi_device
+		 */
+		if (xfer->tx_buf) {
+			if (xfer->tx_nbits != SPI_NBITS_SINGLE &&
+				xfer->tx_nbits != SPI_NBITS_DUAL &&
+				xfer->tx_nbits != SPI_NBITS_QUAD)
+				return -EINVAL;
+			if ((xfer->tx_nbits == SPI_NBITS_DUAL) &&
+				!(spi->mode & (SPI_TX_DUAL | SPI_TX_QUAD)))
+				return -EINVAL;
+			if ((xfer->tx_nbits == SPI_NBITS_QUAD) &&
+				!(spi->mode & SPI_TX_QUAD))
+				return -EINVAL;
+		}
+		/* check transfer rx_nbits */
+		if (xfer->rx_buf) {
+			if (xfer->rx_nbits != SPI_NBITS_SINGLE &&
+				xfer->rx_nbits != SPI_NBITS_DUAL &&
+				xfer->rx_nbits != SPI_NBITS_QUAD)
+				return -EINVAL;
+			if ((xfer->rx_nbits == SPI_NBITS_DUAL) &&
+				!(spi->mode & (SPI_RX_DUAL | SPI_RX_QUAD)))
+				return -EINVAL;
+			if ((xfer->rx_nbits == SPI_NBITS_QUAD) &&
+				!(spi->mode & SPI_RX_QUAD))
+				return -EINVAL;
+		}
 	}
 
-	message->spi = spi;
 	message->status = -EINPROGRESS;
+
+	return 0;
+}
+
+static int __spi_async(struct spi_device *spi, struct spi_message *message)
+{
+	struct spi_master *master = spi->master;
+
+	message->spi = spi;
+
 	return master->transfer(spi, message);
 }
 
@@ -1429,6 +1877,10 @@ int spi_async(struct spi_device *spi, struct spi_message *message)
 	int ret;
 	unsigned long flags;
 
+	ret = __spi_validate(spi, message);
+	if (ret != 0)
+		return ret;
+
 	spin_lock_irqsave(&master->bus_lock_spinlock, flags);
 
 	if (master->bus_lock_flag)
@@ -1477,6 +1929,10 @@ int spi_async_locked(struct spi_device *spi, struct spi_message *message)
 	int ret;
 	unsigned long flags;
 
+	ret = __spi_validate(spi, message);
+	if (ret != 0)
+		return ret;
+
 	spin_lock_irqsave(&master->bus_lock_spinlock, flags);
 
 	ret = __spi_async(spi, message);
@@ -1680,7 +2136,7 @@ int spi_write_then_read(struct spi_device *spi,
 	}
 
 	spi_message_init(&message);
-	memset(x, 0, sizeof x);
+	memset(x, 0, sizeof(x));
 	if (n_tx) {
 		x[0].len = n_tx;
 		spi_message_add_tail(&x[0], &message);
@@ -1710,6 +2166,86 @@ EXPORT_SYMBOL_GPL(spi_write_then_read);
 
 /*-------------------------------------------------------------------------*/
 
+#if IS_ENABLED(CONFIG_OF_DYNAMIC)
+static int __spi_of_device_match(struct device *dev, void *data)
+{
+	return dev->of_node == data;
+}
+
+/* must call put_device() when done with returned spi_device device */
+static struct spi_device *of_find_spi_device_by_node(struct device_node *node)
+{
+	struct device *dev = bus_find_device(&spi_bus_type, NULL, node,
+						__spi_of_device_match);
+	return dev ? to_spi_device(dev) : NULL;
+}
+
+static int __spi_of_master_match(struct device *dev, const void *data)
+{
+	return dev->of_node == data;
+}
+
+/* the spi masters are not using spi_bus, so we find it with another way */
+static struct spi_master *of_find_spi_master_by_node(struct device_node *node)
+{
+	struct device *dev;
+
+	dev = class_find_device(&spi_master_class, NULL, node,
+				__spi_of_master_match);
+	if (!dev)
+		return NULL;
+
+	/* reference got in class_find_device */
+	return container_of(dev, struct spi_master, dev);
+}
+
+static int of_spi_notify(struct notifier_block *nb, unsigned long action,
+			 void *arg)
+{
+	struct of_reconfig_data *rd = arg;
+	struct spi_master *master;
+	struct spi_device *spi;
+
+	switch (of_reconfig_get_state_change(action, arg)) {
+	case OF_RECONFIG_CHANGE_ADD:
+		master = of_find_spi_master_by_node(rd->dn->parent);
+		if (master == NULL)
+			return NOTIFY_OK;	/* not for us */
+
+		spi = of_register_spi_device(master, rd->dn);
+		put_device(&master->dev);
+
+		if (IS_ERR(spi)) {
+			pr_err("%s: failed to create for '%s'\n",
+					__func__, rd->dn->full_name);
+			return notifier_from_errno(PTR_ERR(spi));
+		}
+		break;
+
+	case OF_RECONFIG_CHANGE_REMOVE:
+		/* find our device by node */
+		spi = of_find_spi_device_by_node(rd->dn);
+		if (spi == NULL)
+			return NOTIFY_OK;	/* no? not meant for us */
+
+		/* unregister takes one ref away */
+		spi_unregister_device(spi);
+
+		/* and put the reference of the find */
+		put_device(&spi->dev);
+		break;
+	}
+
+	return NOTIFY_OK;
+}
+
+static struct notifier_block spi_of_notifier = {
+	.notifier_call = of_spi_notify,
+};
+#else /* IS_ENABLED(CONFIG_OF_DYNAMIC) */
+extern struct notifier_block spi_of_notifier;
+#endif /* IS_ENABLED(CONFIG_OF_DYNAMIC) */
+
 static int __init spi_init(void)
 {
 	int	status;
@@ -1727,6 +2263,10 @@ static int __init spi_init(void)
 	status = class_register(&spi_master_class);
 	if (status < 0)
 		goto err2;
+
+	if (IS_ENABLED(CONFIG_OF_DYNAMIC))
+		WARN_ON(of_reconfig_notifier_register(&spi_of_notifier));
+
 	return 0;
 
 err2:
diff --git a/drivers/thermal/Kconfig b/drivers/thermal/Kconfig
index 5e3c025..0777daf 100644
--- a/drivers/thermal/Kconfig
+++ b/drivers/thermal/Kconfig
@@ -17,8 +17,30 @@ if THERMAL
 
 config THERMAL_HWMON
 	bool
+	prompt "Expose thermal sensors as hwmon device"
 	depends on HWMON=y || HWMON=THERMAL
 	default y
+	help
+	  In case a sensor is registered with the thermal
+	  framework, this option will also register it
+	  as a hwmon. The sensor will then have the common
+	  hwmon sysfs interface.
+
+	  Say 'Y' here if you want all thermal sensors to
+	  have hwmon sysfs interface too.
+
+config THERMAL_OF
+	bool
+	prompt "APIs to parse thermal data out of device tree"
+	depends on OF
+	default y
+	help
+	  This options provides helpers to add the support to
+	  read and parse thermal data definitions out of the
+	  device tree blob.
+
+	  Say 'Y' here if you need to build thermal infrastructure
+	  based on device tree.
 
 choice
 	prompt "Default Thermal governor"
@@ -69,6 +91,7 @@ config THERMAL_GOV_USER_SPACE
 config CPU_THERMAL
 	bool "generic cpu cooling support"
 	depends on CPU_FREQ
+	depends on THERMAL_OF
 	select CPU_FREQ_TABLE
 	help
 	  This implements the generic cpu cooling mechanism through frequency
@@ -91,6 +114,16 @@ config THERMAL_EMULATION
 	  because userland can easily disable the thermal policy by simply
 	  flooding this sysfs node with low temperature values.
 
+config MB86S7X_THERMAL
+	tristate "Temperature sensor driver for Fujitsu S7x SoCs"
+	depends on ARCH_MB86S70
+	depends on CPU_THERMAL
+	help
+	  Support for Temperature Monitor (TEMPMON) found on Freescale i.MX SoCs.
+	  It supports one critical trip point and one passive trip point.  The
+	  cpufreq is used as the cooling device to throttle CPUs when the
+	  passive trip is crossed.
+
 config SPEAR_THERMAL
 	bool "SPEAr thermal sensor driver"
 	depends on PLAT_SPEAR
diff --git a/drivers/thermal/Makefile b/drivers/thermal/Makefile
index c054d41..b2803fd 100644
--- a/drivers/thermal/Makefile
+++ b/drivers/thermal/Makefile
@@ -5,6 +5,10 @@
 obj-$(CONFIG_THERMAL)		+= thermal_sys.o
 thermal_sys-y			+= thermal_core.o
 
+# interface to/from other layers providing sensors
+thermal_sys-$(CONFIG_THERMAL_HWMON)		+= thermal_hwmon.o
+thermal_sys-$(CONFIG_THERMAL_OF)		+= of-thermal.o
+
 # governors
 thermal_sys-$(CONFIG_THERMAL_GOV_FAIR_SHARE)	+= fair_share.o
 thermal_sys-$(CONFIG_THERMAL_GOV_STEP_WISE)	+= step_wise.o
@@ -23,4 +27,4 @@ obj-$(CONFIG_DB8500_THERMAL)	+= db8500_thermal.o
 obj-$(CONFIG_ARMADA_THERMAL)	+= armada_thermal.o
 obj-$(CONFIG_DB8500_CPUFREQ_COOLING)	+= db8500_cpufreq_cooling.o
 obj-$(CONFIG_INTEL_POWERCLAMP)	+= intel_powerclamp.o
-
+obj-$(CONFIG_MB86S7X_THERMAL)	+= mb86s7x_thermal.o
diff --git a/drivers/thermal/cpu_cooling.c b/drivers/thermal/cpu_cooling.c
index c94bf2e..d94a0ce 100644
--- a/drivers/thermal/cpu_cooling.c
+++ b/drivers/thermal/cpu_cooling.c
@@ -415,18 +415,21 @@ static struct notifier_block thermal_cpufreq_notifier_block = {
 };
 
 /**
- * cpufreq_cooling_register - function to create cpufreq cooling device.
+ * __cpufreq_cooling_register - helper function to create cpufreq cooling device
+ * @np: a valid struct device_node to the cooling device device tree node
  * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
  *
  * This interface function registers the cpufreq cooling device with the name
  * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
- * cooling devices.
+ * cooling devices. It also gives the opportunity to link the cooling device
+ * with a device tree node, in order to bind it via the thermal DT code.
  *
  * Return: a valid struct thermal_cooling_device pointer on success,
  * on failure, it returns a corresponding ERR_PTR().
  */
-struct thermal_cooling_device *
-cpufreq_cooling_register(const struct cpumask *clip_cpus)
+static struct thermal_cooling_device *
+__cpufreq_cooling_register(struct device_node *np,
+			   const struct cpumask *clip_cpus)
 {
 	struct thermal_cooling_device *cool_dev;
 	struct cpufreq_cooling_device *cpufreq_dev = NULL;
@@ -465,9 +468,9 @@ cpufreq_cooling_register(const struct cpumask *clip_cpus)
 	snprintf(dev_name, sizeof(dev_name), "thermal-cpufreq-%d",
 		 cpufreq_dev->id);
 
-	cool_dev = thermal_cooling_device_register(dev_name, cpufreq_dev,
+	cool_dev = thermal_of_cooling_device_register(np, dev_name, cpufreq_dev,
 						   &cpufreq_cooling_ops);
-	if (!cool_dev) {
+	if (IS_ERR(cool_dev)) {
 		release_idr(&cpufreq_idr, cpufreq_dev->id);
 		kfree(cpufreq_dev);
 		return ERR_PTR(-EINVAL);
@@ -486,9 +489,50 @@ cpufreq_cooling_register(const struct cpumask *clip_cpus)
 
 	return cool_dev;
 }
+
+/**
+ * cpufreq_cooling_register - function to create cpufreq cooling device.
+ * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
+ *
+ * This interface function registers the cpufreq cooling device with the name
+ * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
+ * cooling devices.
+ *
+ * Return: a valid struct thermal_cooling_device pointer on success,
+ * on failure, it returns a corresponding ERR_PTR().
+ */
+struct thermal_cooling_device *
+cpufreq_cooling_register(const struct cpumask *clip_cpus)
+{
+	return __cpufreq_cooling_register(NULL, clip_cpus);
+}
 EXPORT_SYMBOL_GPL(cpufreq_cooling_register);
 
 /**
+ * of_cpufreq_cooling_register - function to create cpufreq cooling device.
+ * @np: a valid struct device_node to the cooling device device tree node
+ * @clip_cpus: cpumask of cpus where the frequency constraints will happen.
+ *
+ * This interface function registers the cpufreq cooling device with the name
+ * "thermal-cpufreq-%x". This api can support multiple instances of cpufreq
+ * cooling devices. Using this API, the cpufreq cooling device will be
+ * linked to the device tree node provided.
+ *
+ * Return: a valid struct thermal_cooling_device pointer on success,
+ * on failure, it returns a corresponding ERR_PTR().
+ */
+struct thermal_cooling_device *
+of_cpufreq_cooling_register(struct device_node *np,
+			    const struct cpumask *clip_cpus)
+{
+	if (!np)
+		return ERR_PTR(-EINVAL);
+
+	return __cpufreq_cooling_register(np, clip_cpus);
+}
+EXPORT_SYMBOL_GPL(of_cpufreq_cooling_register);
+
+/**
  * cpufreq_cooling_unregister - function to remove cpufreq cooling device.
  * @cdev: thermal cooling device pointer.
  *
diff --git a/drivers/thermal/mb86s7x_thermal.c b/drivers/thermal/mb86s7x_thermal.c
new file mode 100644
index 0000000..6baf908
--- /dev/null
+++ b/drivers/thermal/mb86s7x_thermal.c
@@ -0,0 +1,142 @@
+/*
+ * Copyright (C) 2014-2015 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ */
+
+#include <linux/clkdev.h>
+#include <linux/err.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/cpu.h>
+#include <linux/clk-provider.h>
+#include <linux/spinlock.h>
+#include <linux/module.h>
+#include <linux/topology.h>
+#include <linux/cpufreq.h>
+#include <linux/thermal.h>
+#include <linux/cpu_cooling.h>
+#include <linux/mailbox_client.h>
+#include <linux/platform_device.h>
+#include <linux/scb_mhu_api.h>
+
+static DEFINE_PER_CPU(struct thermal_cooling_device *, cdev);
+static DEFINE_PER_CPU(struct thermal_zone_device *, tzd);
+
+static int scb_get_temp(void *devdata, long *temp)
+{
+	struct mb86s7x_thermal_info cmd;
+	struct completion got_rsp;
+	int ret, id = (int)devdata;
+
+	cmd.payload_size = sizeof(cmd);
+	init_completion(&got_rsp);
+	/*
+	 * We could read temperatures N times seperated by a few ms.
+	 * and then average them out before reporting 'final' value.
+	 * But for now we assume SCB provides averaged/calibrated readings.
+	 */
+	ret = mhu_send_packet(CMD_THERMAL_INFO_REQ,
+				&cmd, sizeof(cmd), &got_rsp);
+	if (ret < 0) {
+		pr_err("%s:%d failed!\n", __func__, __LINE__);
+		return ret;
+	}
+	if (ret)
+		wait_for_completion(&got_rsp);
+
+	*temp = cmd.temp[id];
+	pr_debug("%s:%d CPU-%d Tmp=%ld\n", __func__, __LINE__, id, *temp);
+
+	return 0;
+}
+
+static int scb_thermal_probe(struct platform_device *pdev)
+{
+	u32 ca15_0_id, ca7_0_id, ca15_1_id, ca7_1_id;
+	struct cpumask clip_cpus;
+	char cpu[12] = "/cpus/cpu@0";
+	int i;
+
+	of_property_read_u32(pdev->dev.of_node, "ca15-cpu0-id", &ca15_0_id);
+	of_property_read_u32(pdev->dev.of_node, "ca15-cpu1-id", &ca15_1_id);
+	of_property_read_u32(pdev->dev.of_node, "ca7-cpu0-id", &ca7_0_id);
+	of_property_read_u32(pdev->dev.of_node, "ca7-cpu1-id", &ca7_1_id);
+
+	per_cpu(tzd, 0) = thermal_zone_of_sensor_register(&pdev->dev, 0x0,
+				(void *)ca15_0_id, scb_get_temp, NULL);
+	per_cpu(tzd, 1) = thermal_zone_of_sensor_register(&pdev->dev, 0x1,
+				(void *)ca15_1_id, scb_get_temp, NULL);
+	per_cpu(tzd, 2) = thermal_zone_of_sensor_register(&pdev->dev, 0x100,
+				(void *)ca7_0_id, scb_get_temp, NULL);
+	per_cpu(tzd, 3) = thermal_zone_of_sensor_register(&pdev->dev, 0x101,
+				(void *)ca7_1_id, scb_get_temp, NULL);
+
+	for_each_possible_cpu(i) {
+		struct device *cpu_dev = get_cpu_device(i);
+
+		cpu[10] = '0' + i;
+		cpu_dev->of_node = of_find_node_by_path(cpu);
+		pr_err("%s:%d cpu=%u dev=%p np=%p %s\n",
+			__func__, __LINE__, i, cpu_dev, cpu_dev->of_node,
+			cpu_dev->of_node ? cpu_dev->of_node->full_name : "NULL");
+		cpumask_clear(&clip_cpus);
+		cpumask_set_cpu(i, &clip_cpus);
+		per_cpu(cdev, i) = of_cpufreq_cooling_register(cpu_dev->of_node, &clip_cpus);
+	}
+
+	return 0;
+}
+
+static int scb_thermal_remove(struct platform_device *pdev)
+{
+	int i;
+
+	for_each_possible_cpu(i) {
+		if (!IS_ERR(per_cpu(cdev, i)))
+			cpufreq_cooling_unregister(per_cpu(cdev, i));
+		if (!IS_ERR(per_cpu(tzd, i)))
+			thermal_zone_of_sensor_unregister(&pdev->dev, per_cpu(tzd, i));
+	}
+
+	return 0;
+}
+
+static const struct of_device_id of_scb_thermal_match[] = {
+	{.compatible = "fujitsu,scb-thermal-s70" },
+	{ }, /* Sentinel */
+};
+MODULE_DEVICE_TABLE(of, of_scb_thermal_match);
+
+static struct platform_driver scb_thermal_driver = {
+	.probe = scb_thermal_probe,
+	.remove = scb_thermal_remove,
+	.driver = {
+			.name = "fujitsu-scb-thermal-s70",
+			.of_match_table	= of_scb_thermal_match,
+	},
+};
+
+static int __init scb_thermal_cooling_init(void)
+{
+	return platform_driver_register(&scb_thermal_driver);
+}
+late_initcall(scb_thermal_cooling_init);
+
+static void __exit scb_thermal_cooling_exit(void)
+{
+	platform_driver_unregister(&scb_thermal_driver);
+}
+module_exit(scb_thermal_cooling_exit);
+
+MODULE_DESCRIPTION("Fujitsu SCB-Thermal driver for S70");
+MODULE_LICENSE("GPL v2");
+MODULE_ALIAS("platform:fujitsu-scb-thermal");
+MODULE_AUTHOR("Jassi Brar <jaswinder.singh@linaro.org>");
diff --git a/drivers/thermal/of-thermal.c b/drivers/thermal/of-thermal.c
new file mode 100644
index 0000000..62143ba
--- /dev/null
+++ b/drivers/thermal/of-thermal.c
@@ -0,0 +1,886 @@
+/*
+ *  of-thermal.c - Generic Thermal Management device tree support.
+ *
+ *  Copyright (C) 2013 Texas Instruments
+ *  Copyright (C) 2013 Eduardo Valentin <eduardo.valentin@ti.com>
+ *
+ *
+ *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+#include <linux/thermal.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/of_device.h>
+#include <linux/of_platform.h>
+#include <linux/err.h>
+#include <linux/export.h>
+#include <linux/string.h>
+
+#include "thermal_core.h"
+
+/***   Private data structures to represent thermal device tree data ***/
+
+/**
+ * struct __thermal_trip - representation of a point in temperature domain
+ * @np: pointer to struct device_node that this trip point was created from
+ * @temperature: temperature value in miliCelsius
+ * @hysteresis: relative hysteresis in miliCelsius
+ * @type: trip point type
+ */
+
+struct __thermal_trip {
+	struct device_node *np;
+	unsigned long int temperature;
+	unsigned long int hysteresis;
+	enum thermal_trip_type type;
+};
+
+/**
+ * struct __thermal_bind_param - a match between trip and cooling device
+ * @cooling_device: a pointer to identify the referred cooling device
+ * @trip_id: the trip point index
+ * @usage: the percentage (from 0 to 100) of cooling contribution
+ * @min: minimum cooling state used at this trip point
+ * @max: maximum cooling state used at this trip point
+ */
+
+struct __thermal_bind_params {
+	struct device_node *cooling_device;
+	unsigned int trip_id;
+	unsigned int usage;
+	unsigned long min;
+	unsigned long max;
+};
+
+/**
+ * struct __thermal_zone - internal representation of a thermal zone
+ * @mode: current thermal zone device mode (enabled/disabled)
+ * @passive_delay: polling interval while passive cooling is activated
+ * @polling_delay: zone polling interval
+ * @ntrips: number of trip points
+ * @trips: an array of trip points (0..ntrips - 1)
+ * @num_tbps: number of thermal bind params
+ * @tbps: an array of thermal bind params (0..num_tbps - 1)
+ * @sensor_data: sensor private data used while reading temperature and trend
+ * @get_temp: sensor callback to read temperature
+ * @get_trend: sensor callback to read temperature trend
+ */
+
+struct __thermal_zone {
+	enum thermal_device_mode mode;
+	int passive_delay;
+	int polling_delay;
+
+	/* trip data */
+	int ntrips;
+	struct __thermal_trip *trips;
+
+	/* cooling binding data */
+	int num_tbps;
+	struct __thermal_bind_params *tbps;
+
+	/* sensor interface */
+	void *sensor_data;
+	int (*get_temp)(void *, long *);
+	int (*get_trend)(void *, long *);
+};
+
+/***   DT thermal zone device callbacks   ***/
+
+static int of_thermal_get_temp(struct thermal_zone_device *tz,
+			       unsigned long *temp)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (!data->get_temp)
+		return -EINVAL;
+
+	return data->get_temp(data->sensor_data, temp);
+}
+
+static int of_thermal_get_trend(struct thermal_zone_device *tz, int trip,
+				enum thermal_trend *trend)
+{
+	struct __thermal_zone *data = tz->devdata;
+	long dev_trend;
+	int r;
+
+	if (!data->get_trend)
+		return -EINVAL;
+
+	r = data->get_trend(data->sensor_data, &dev_trend);
+	if (r)
+		return r;
+
+	/* TODO: These intervals might have some thresholds, but in core code */
+	if (dev_trend > 0)
+		*trend = THERMAL_TREND_RAISING;
+	else if (dev_trend < 0)
+		*trend = THERMAL_TREND_DROPPING;
+	else
+		*trend = THERMAL_TREND_STABLE;
+
+	return 0;
+}
+
+static int of_thermal_bind(struct thermal_zone_device *thermal,
+			   struct thermal_cooling_device *cdev)
+{
+	struct __thermal_zone *data = thermal->devdata;
+	int i;
+
+	if (!data || IS_ERR(data))
+		return -ENODEV;
+
+	/* find where to bind */
+	for (i = 0; i < data->num_tbps; i++) {
+		struct __thermal_bind_params *tbp = data->tbps + i;
+
+		if (tbp->cooling_device == cdev->np) {
+			int ret;
+
+			ret = thermal_zone_bind_cooling_device(thermal,
+						tbp->trip_id, cdev,
+						tbp->max,
+						tbp->min);
+			if (ret)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int of_thermal_unbind(struct thermal_zone_device *thermal,
+			     struct thermal_cooling_device *cdev)
+{
+	struct __thermal_zone *data = thermal->devdata;
+	int i;
+
+	if (!data || IS_ERR(data))
+		return -ENODEV;
+
+	/* find where to unbind */
+	for (i = 0; i < data->num_tbps; i++) {
+		struct __thermal_bind_params *tbp = data->tbps + i;
+
+		if (tbp->cooling_device == cdev->np) {
+			int ret;
+
+			ret = thermal_zone_unbind_cooling_device(thermal,
+						tbp->trip_id, cdev);
+			if (ret)
+				return ret;
+		}
+	}
+
+	return 0;
+}
+
+static int of_thermal_get_mode(struct thermal_zone_device *tz,
+			       enum thermal_device_mode *mode)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	*mode = data->mode;
+
+	return 0;
+}
+
+static int of_thermal_set_mode(struct thermal_zone_device *tz,
+			       enum thermal_device_mode mode)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	mutex_lock(&tz->lock);
+
+	if (mode == THERMAL_DEVICE_ENABLED)
+		tz->polling_delay = data->polling_delay;
+	else
+		tz->polling_delay = 0;
+
+	mutex_unlock(&tz->lock);
+
+	data->mode = mode;
+	thermal_zone_device_update(tz);
+
+	return 0;
+}
+
+static int of_thermal_get_trip_type(struct thermal_zone_device *tz, int trip,
+				    enum thermal_trip_type *type)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (trip >= data->ntrips || trip < 0)
+		return -EDOM;
+
+	*type = data->trips[trip].type;
+
+	return 0;
+}
+
+static int of_thermal_get_trip_temp(struct thermal_zone_device *tz, int trip,
+				    unsigned long *temp)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (trip >= data->ntrips || trip < 0)
+		return -EDOM;
+
+	*temp = data->trips[trip].temperature;
+
+	return 0;
+}
+
+static int of_thermal_set_trip_temp(struct thermal_zone_device *tz, int trip,
+				    unsigned long temp)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (trip >= data->ntrips || trip < 0)
+		return -EDOM;
+
+	/* thermal framework should take care of data->mask & (1 << trip) */
+	data->trips[trip].temperature = temp;
+
+	return 0;
+}
+
+static int of_thermal_get_trip_hyst(struct thermal_zone_device *tz, int trip,
+				    unsigned long *hyst)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (trip >= data->ntrips || trip < 0)
+		return -EDOM;
+
+	*hyst = data->trips[trip].hysteresis;
+
+	return 0;
+}
+
+static int of_thermal_set_trip_hyst(struct thermal_zone_device *tz, int trip,
+				    unsigned long hyst)
+{
+	struct __thermal_zone *data = tz->devdata;
+
+	if (trip >= data->ntrips || trip < 0)
+		return -EDOM;
+
+	/* thermal framework should take care of data->mask & (1 << trip) */
+	data->trips[trip].hysteresis = hyst;
+
+	return 0;
+}
+
+static int of_thermal_get_crit_temp(struct thermal_zone_device *tz,
+				    unsigned long *temp)
+{
+	struct __thermal_zone *data = tz->devdata;
+	int i;
+
+	for (i = 0; i < data->ntrips; i++)
+		if (data->trips[i].type == THERMAL_TRIP_CRITICAL) {
+			*temp = data->trips[i].temperature;
+			return 0;
+		}
+
+	return -EINVAL;
+}
+
+static struct thermal_zone_device_ops of_thermal_ops = {
+	.get_mode = of_thermal_get_mode,
+	.set_mode = of_thermal_set_mode,
+
+	.get_trip_type = of_thermal_get_trip_type,
+	.get_trip_temp = of_thermal_get_trip_temp,
+	.set_trip_temp = of_thermal_set_trip_temp,
+	.get_trip_hyst = of_thermal_get_trip_hyst,
+	.set_trip_hyst = of_thermal_set_trip_hyst,
+	.get_crit_temp = of_thermal_get_crit_temp,
+
+	.bind = of_thermal_bind,
+	.unbind = of_thermal_unbind,
+};
+
+/***   sensor API   ***/
+
+static struct thermal_zone_device *
+thermal_zone_of_add_sensor(struct device_node *zone,
+			   struct device_node *sensor, void *data,
+			   int (*get_temp)(void *, long *),
+			   int (*get_trend)(void *, long *))
+{
+	struct thermal_zone_device *tzd;
+	struct __thermal_zone *tz;
+
+	tzd = thermal_zone_get_zone_by_name(zone->name);
+	if (IS_ERR(tzd))
+		return ERR_PTR(-EPROBE_DEFER);
+
+	tz = tzd->devdata;
+
+	mutex_lock(&tzd->lock);
+	tz->get_temp = get_temp;
+	tz->get_trend = get_trend;
+	tz->sensor_data = data;
+
+	tzd->ops->get_temp = of_thermal_get_temp;
+	tzd->ops->get_trend = of_thermal_get_trend;
+	mutex_unlock(&tzd->lock);
+
+	return tzd;
+}
+
+/**
+ * thermal_zone_of_sensor_register - registers a sensor to a DT thermal zone
+ * @dev: a valid struct device pointer of a sensor device. Must contain
+ *       a valid .of_node, for the sensor node.
+ * @sensor_id: a sensor identifier, in case the sensor IP has more
+ *             than one sensors
+ * @data: a private pointer (owned by the caller) that will be passed
+ *        back, when a temperature reading is needed.
+ * @get_temp: a pointer to a function that reads the sensor temperature.
+ * @get_trend: a pointer to a function that reads the sensor temperature trend.
+ *
+ * This function will search the list of thermal zones described in device
+ * tree and look for the zone that refer to the sensor device pointed by
+ * @dev->of_node as temperature providers. For the zone pointing to the
+ * sensor node, the sensor will be added to the DT thermal zone device.
+ *
+ * The thermal zone temperature is provided by the @get_temp function
+ * pointer. When called, it will have the private pointer @data back.
+ *
+ * The thermal zone temperature trend is provided by the @get_trend function
+ * pointer. When called, it will have the private pointer @data back.
+ *
+ * TODO:
+ * 01 - This function must enqueue the new sensor instead of using
+ * it as the only source of temperature values.
+ *
+ * 02 - There must be a way to match the sensor with all thermal zones
+ * that refer to it.
+ *
+ * Return: On success returns a valid struct thermal_zone_device,
+ * otherwise, it returns a corresponding ERR_PTR(). Caller must
+ * check the return value with help of IS_ERR() helper.
+ */
+struct thermal_zone_device *
+thermal_zone_of_sensor_register(struct device *dev, int sensor_id,
+				void *data, int (*get_temp)(void *, long *),
+				int (*get_trend)(void *, long *))
+{
+	struct device_node *np, *child, *sensor_np;
+	struct thermal_zone_device *tzd = ERR_PTR(-ENODEV);
+
+	np = of_find_node_by_name(NULL, "thermal-zones");
+	if (!np)
+		return ERR_PTR(-ENODEV);
+
+	if (!dev || !dev->of_node) {
+		of_node_put(np);
+		return ERR_PTR(-EINVAL);
+	}
+
+	sensor_np = of_node_get(dev->of_node);
+
+	for_each_child_of_node(np, child) {
+		struct of_phandle_args sensor_specs;
+		int ret, id;
+
+		/* Check whether child is enabled or not */
+		if (!of_device_is_available(child))
+			continue;
+
+		/* For now, thermal framework supports only 1 sensor per zone */
+		ret = of_parse_phandle_with_args(child, "thermal-sensors",
+						 "#thermal-sensor-cells",
+						 0, &sensor_specs);
+		if (ret)
+			continue;
+
+		if (sensor_specs.args_count >= 1) {
+			id = sensor_specs.args[0];
+			WARN(sensor_specs.args_count > 1,
+			     "%s: too many cells in sensor specifier %d\n",
+			     sensor_specs.np->name, sensor_specs.args_count);
+		} else {
+			id = 0;
+		}
+
+		if (sensor_specs.np == sensor_np && id == sensor_id) {
+			tzd = thermal_zone_of_add_sensor(child, sensor_np,
+							 data,
+							 get_temp,
+							 get_trend);
+			of_node_put(sensor_specs.np);
+			of_node_put(child);
+			goto exit;
+		}
+		of_node_put(sensor_specs.np);
+	}
+exit:
+	of_node_put(sensor_np);
+	of_node_put(np);
+
+	return tzd;
+}
+EXPORT_SYMBOL_GPL(thermal_zone_of_sensor_register);
+
+/**
+ * thermal_zone_of_sensor_unregister - unregisters a sensor from a DT thermal zone
+ * @dev: a valid struct device pointer of a sensor device. Must contain
+ *       a valid .of_node, for the sensor node.
+ * @tzd: a pointer to struct thermal_zone_device where the sensor is registered.
+ *
+ * This function removes the sensor callbacks and private data from the
+ * thermal zone device registered with thermal_zone_of_sensor_register()
+ * API. It will also silent the zone by remove the .get_temp() and .get_trend()
+ * thermal zone device callbacks.
+ *
+ * TODO: When the support to several sensors per zone is added, this
+ * function must search the sensor list based on @dev parameter.
+ *
+ */
+void thermal_zone_of_sensor_unregister(struct device *dev,
+				       struct thermal_zone_device *tzd)
+{
+	struct __thermal_zone *tz;
+
+	if (!dev || !tzd || !tzd->devdata)
+		return;
+
+	tz = tzd->devdata;
+
+	/* no __thermal_zone, nothing to be done */
+	if (!tz)
+		return;
+
+	mutex_lock(&tzd->lock);
+	tzd->ops->get_temp = NULL;
+	tzd->ops->get_trend = NULL;
+
+	tz->get_temp = NULL;
+	tz->get_trend = NULL;
+	tz->sensor_data = NULL;
+	mutex_unlock(&tzd->lock);
+}
+EXPORT_SYMBOL_GPL(thermal_zone_of_sensor_unregister);
+
+/***   functions parsing device tree nodes   ***/
+
+/**
+ * thermal_of_populate_bind_params - parse and fill cooling map data
+ * @np: DT node containing a cooling-map node
+ * @__tbp: data structure to be filled with cooling map info
+ * @trips: array of thermal zone trip points
+ * @ntrips: number of trip points inside trips.
+ *
+ * This function parses a cooling-map type of node represented by
+ * @np parameter and fills the read data into @__tbp data structure.
+ * It needs the already parsed array of trip points of the thermal zone
+ * in consideration.
+ *
+ * Return: 0 on success, proper error code otherwise
+ */
+static int thermal_of_populate_bind_params(struct device_node *np,
+					   struct __thermal_bind_params *__tbp,
+					   struct __thermal_trip *trips,
+					   int ntrips)
+{
+	struct of_phandle_args cooling_spec;
+	struct device_node *trip;
+	int ret, i;
+	u32 prop;
+
+	/* Default weight. Usage is optional */
+	__tbp->usage = 0;
+	ret = of_property_read_u32(np, "contribution", &prop);
+	if (ret == 0)
+		__tbp->usage = prop;
+
+	trip = of_parse_phandle(np, "trip", 0);
+	if (!trip) {
+		pr_err("missing trip property\n");
+		return -ENODEV;
+	}
+
+	/* match using device_node */
+	for (i = 0; i < ntrips; i++)
+		if (trip == trips[i].np) {
+			__tbp->trip_id = i;
+			break;
+		}
+
+	if (i == ntrips) {
+		ret = -ENODEV;
+		goto end;
+	}
+
+	ret = of_parse_phandle_with_args(np, "cooling-device", "#cooling-cells",
+					 0, &cooling_spec);
+	if (ret < 0) {
+		pr_err("missing cooling_device property\n");
+		goto end;
+	}
+	__tbp->cooling_device = cooling_spec.np;
+	if (cooling_spec.args_count >= 2) { /* at least min and max */
+		__tbp->min = cooling_spec.args[0];
+		__tbp->max = cooling_spec.args[1];
+	} else {
+		pr_err("wrong reference to cooling device, missing limits\n");
+	}
+
+end:
+	of_node_put(trip);
+
+	return ret;
+}
+
+/**
+ * It maps 'enum thermal_trip_type' found in include/linux/thermal.h
+ * into the device tree binding of 'trip', property type.
+ */
+static const char * const trip_types[] = {
+	[THERMAL_TRIP_ACTIVE]	= "active",
+	[THERMAL_TRIP_PASSIVE]	= "passive",
+	[THERMAL_TRIP_HOT]	= "hot",
+	[THERMAL_TRIP_CRITICAL]	= "critical",
+};
+
+/**
+ * thermal_of_get_trip_type - Get phy mode for given device_node
+ * @np:	Pointer to the given device_node
+ * @type: Pointer to resulting trip type
+ *
+ * The function gets trip type string from property 'type',
+ * and store its index in trip_types table in @type,
+ *
+ * Return: 0 on success, or errno in error case.
+ */
+static int thermal_of_get_trip_type(struct device_node *np,
+				    enum thermal_trip_type *type)
+{
+	const char *t;
+	int err, i;
+
+	err = of_property_read_string(np, "type", &t);
+	if (err < 0)
+		return err;
+
+	for (i = 0; i < ARRAY_SIZE(trip_types); i++)
+		if (!strcasecmp(t, trip_types[i])) {
+			*type = i;
+			return 0;
+		}
+
+	return -ENODEV;
+}
+
+/**
+ * thermal_of_populate_trip - parse and fill one trip point data
+ * @np: DT node containing a trip point node
+ * @trip: trip point data structure to be filled up
+ *
+ * This function parses a trip point type of node represented by
+ * @np parameter and fills the read data into @trip data structure.
+ *
+ * Return: 0 on success, proper error code otherwise
+ */
+static int thermal_of_populate_trip(struct device_node *np,
+				    struct __thermal_trip *trip)
+{
+	int prop;
+	int ret;
+
+	ret = of_property_read_u32(np, "temperature", &prop);
+	if (ret < 0) {
+		pr_err("missing temperature property\n");
+		return ret;
+	}
+	trip->temperature = prop;
+
+	ret = of_property_read_u32(np, "hysteresis", &prop);
+	if (ret < 0) {
+		pr_err("missing hysteresis property\n");
+		return ret;
+	}
+	trip->hysteresis = prop;
+
+	ret = thermal_of_get_trip_type(np, &trip->type);
+	if (ret < 0) {
+		pr_err("wrong trip type property\n");
+		return ret;
+	}
+
+	/* Required for cooling map matching */
+	trip->np = np;
+	of_node_get(np);
+
+	return 0;
+}
+
+/**
+ * thermal_of_build_thermal_zone - parse and fill one thermal zone data
+ * @np: DT node containing a thermal zone node
+ *
+ * This function parses a thermal zone type of node represented by
+ * @np parameter and fills the read data into a __thermal_zone data structure
+ * and return this pointer.
+ *
+ * TODO: Missing properties to parse: thermal-sensor-names and coefficients
+ *
+ * Return: On success returns a valid struct __thermal_zone,
+ * otherwise, it returns a corresponding ERR_PTR(). Caller must
+ * check the return value with help of IS_ERR() helper.
+ */
+static struct __thermal_zone *
+thermal_of_build_thermal_zone(struct device_node *np)
+{
+	struct device_node *child = NULL, *gchild;
+	struct __thermal_zone *tz;
+	int ret, i;
+	u32 prop;
+
+	if (!np) {
+		pr_err("no thermal zone np\n");
+		return ERR_PTR(-EINVAL);
+	}
+
+	tz = kzalloc(sizeof(*tz), GFP_KERNEL);
+	if (!tz)
+		return ERR_PTR(-ENOMEM);
+
+	ret = of_property_read_u32(np, "polling-delay-passive", &prop);
+	if (ret < 0) {
+		pr_err("missing polling-delay-passive property\n");
+		goto free_tz;
+	}
+	tz->passive_delay = prop;
+
+	ret = of_property_read_u32(np, "polling-delay", &prop);
+	if (ret < 0) {
+		pr_err("missing polling-delay property\n");
+		goto free_tz;
+	}
+	tz->polling_delay = prop;
+
+	/* trips */
+	child = of_get_child_by_name(np, "trips");
+
+	/* No trips provided */
+	if (!child)
+		goto finish;
+
+	tz->ntrips = of_get_child_count(child);
+	if (tz->ntrips == 0) /* must have at least one child */
+		goto finish;
+
+	tz->trips = kzalloc(tz->ntrips * sizeof(*tz->trips), GFP_KERNEL);
+	if (!tz->trips) {
+		ret = -ENOMEM;
+		goto free_tz;
+	}
+
+	i = 0;
+	for_each_child_of_node(child, gchild) {
+		ret = thermal_of_populate_trip(gchild, &tz->trips[i++]);
+		if (ret)
+			goto free_trips;
+	}
+
+	of_node_put(child);
+
+	/* cooling-maps */
+	child = of_get_child_by_name(np, "cooling-maps");
+
+	/* cooling-maps not provided */
+	if (!child)
+		goto finish;
+
+	tz->num_tbps = of_get_child_count(child);
+	if (tz->num_tbps == 0)
+		goto finish;
+
+	tz->tbps = kzalloc(tz->num_tbps * sizeof(*tz->tbps), GFP_KERNEL);
+	if (!tz->tbps) {
+		ret = -ENOMEM;
+		goto free_trips;
+	}
+
+	i = 0;
+	for_each_child_of_node(child, gchild) {
+		ret = thermal_of_populate_bind_params(gchild, &tz->tbps[i++],
+						      tz->trips, tz->ntrips);
+		if (ret)
+			goto free_tbps;
+	}
+
+finish:
+	of_node_put(child);
+	tz->mode = THERMAL_DEVICE_DISABLED;
+
+	return tz;
+
+free_tbps:
+	for (i = 0; i < tz->num_tbps; i++)
+		of_node_put(tz->tbps[i].cooling_device);
+	kfree(tz->tbps);
+free_trips:
+	for (i = 0; i < tz->ntrips; i++)
+		of_node_put(tz->trips[i].np);
+	kfree(tz->trips);
+	of_node_put(gchild);
+free_tz:
+	kfree(tz);
+	of_node_put(child);
+
+	return ERR_PTR(ret);
+}
+
+static inline void of_thermal_free_zone(struct __thermal_zone *tz)
+{
+	int i;
+
+	for (i = 0; i < tz->num_tbps; i++)
+		of_node_put(tz->tbps[i].cooling_device);
+	kfree(tz->tbps);
+	for (i = 0; i < tz->ntrips; i++)
+		of_node_put(tz->trips[i].np);
+	kfree(tz->trips);
+	kfree(tz);
+}
+
+/**
+ * of_parse_thermal_zones - parse device tree thermal data
+ *
+ * Initialization function that can be called by machine initialization
+ * code to parse thermal data and populate the thermal framework
+ * with hardware thermal zones info. This function only parses thermal zones.
+ * Cooling devices and sensor devices nodes are supposed to be parsed
+ * by their respective drivers.
+ *
+ * Return: 0 on success, proper error code otherwise
+ *
+ */
+int __init of_parse_thermal_zones(void)
+{
+	struct device_node *np, *child;
+	struct __thermal_zone *tz;
+	struct thermal_zone_device_ops *ops;
+
+	np = of_find_node_by_name(NULL, "thermal-zones");
+	if (!np) {
+		pr_debug("unable to find thermal zones\n");
+		return 0; /* Run successfully on systems without thermal DT */
+	}
+
+	for_each_child_of_node(np, child) {
+		struct thermal_zone_device *zone;
+		struct thermal_zone_params *tzp;
+
+		/* Check whether child is enabled or not */
+		if (!of_device_is_available(child))
+			continue;
+
+		tz = thermal_of_build_thermal_zone(child);
+		if (IS_ERR(tz)) {
+			pr_err("failed to build thermal zone %s: %ld\n",
+			       child->name,
+			       PTR_ERR(tz));
+			continue;
+		}
+
+		ops = kmemdup(&of_thermal_ops, sizeof(*ops), GFP_KERNEL);
+		if (!ops)
+			goto exit_free;
+
+		tzp = kzalloc(sizeof(*tzp), GFP_KERNEL);
+		if (!tzp) {
+			kfree(ops);
+			goto exit_free;
+		}
+
+		/* No hwmon because there might be hwmon drivers registering */
+		tzp->no_hwmon = true;
+
+		zone = thermal_zone_device_register(child->name, tz->ntrips,
+						    0, tz,
+						    ops, tzp,
+						    tz->passive_delay,
+						    tz->polling_delay);
+		if (IS_ERR(zone)) {
+			pr_err("Failed to build %s zone %ld\n", child->name,
+			       PTR_ERR(zone));
+			kfree(tzp);
+			kfree(ops);
+			of_thermal_free_zone(tz);
+			/* attempting to build remaining zones still */
+		}
+	}
+	of_node_put(np);
+
+	return 0;
+
+exit_free:
+	of_node_put(child);
+	of_node_put(np);
+	of_thermal_free_zone(tz);
+
+	/* no memory available, so free what we have built */
+	of_thermal_destroy_zones();
+
+	return -ENOMEM;
+}
+
+/**
+ * of_thermal_destroy_zones - remove all zones parsed and allocated resources
+ *
+ * Finds all zones parsed and added to the thermal framework and remove them
+ * from the system, together with their resources.
+ *
+ */
+void of_thermal_destroy_zones(void)
+{
+	struct device_node *np, *child;
+
+	np = of_find_node_by_name(NULL, "thermal-zones");
+	if (!np) {
+		pr_err("unable to find thermal zones\n");
+		return;
+	}
+
+	for_each_child_of_node(np, child) {
+		struct thermal_zone_device *zone;
+
+		/* Check whether child is enabled or not */
+		if (!of_device_is_available(child))
+			continue;
+
+		zone = thermal_zone_get_zone_by_name(child->name);
+		if (IS_ERR(zone))
+			continue;
+
+		thermal_zone_device_unregister(zone);
+		kfree(zone->tzp);
+		kfree(zone->ops);
+		of_thermal_free_zone(zone->devdata);
+	}
+	of_node_put(np);
+}
diff --git a/drivers/thermal/thermal_core.c b/drivers/thermal/thermal_core.c
index d755440..f17baa4 100644
--- a/drivers/thermal/thermal_core.c
+++ b/drivers/thermal/thermal_core.c
@@ -33,10 +33,13 @@
 #include <linux/idr.h>
 #include <linux/thermal.h>
 #include <linux/reboot.h>
+#include <linux/string.h>
+#include <linux/of.h>
 #include <net/netlink.h>
 #include <net/genetlink.h>
 
 #include "thermal_core.h"
+#include "thermal_hwmon.h"
 
 MODULE_AUTHOR("Zhang Rui");
 MODULE_DESCRIPTION("Generic thermal management sysfs support");
@@ -53,12 +56,17 @@ static LIST_HEAD(thermal_governor_list);
 static DEFINE_MUTEX(thermal_list_lock);
 static DEFINE_MUTEX(thermal_governor_lock);
 
+static struct thermal_governor *def_governor;
+
 static struct thermal_governor *__find_governor(const char *name)
 {
 	struct thermal_governor *pos;
 
+	if (!name || !name[0])
+		return def_governor;
+
 	list_for_each_entry(pos, &thermal_governor_list, governor_list)
-		if (!strnicmp(name, pos->name, THERMAL_NAME_LENGTH))
+		if (!strncasecmp(name, pos->name, THERMAL_NAME_LENGTH))
 			return pos;
 
 	return NULL;
@@ -79,18 +87,24 @@ int thermal_register_governor(struct thermal_governor *governor)
 	if (__find_governor(governor->name) == NULL) {
 		err = 0;
 		list_add(&governor->governor_list, &thermal_governor_list);
+		if (!def_governor && !strncmp(governor->name,
+			DEFAULT_THERMAL_GOVERNOR, THERMAL_NAME_LENGTH))
+			def_governor = governor;
 	}
 
 	mutex_lock(&thermal_list_lock);
 
 	list_for_each_entry(pos, &thermal_tz_list, node) {
+		/*
+		 * only thermal zones with specified tz->tzp->governor_name
+		 * may run with tz->govenor unset
+		 */
 		if (pos->governor)
 			continue;
-		if (pos->tzp)
+
 			name = pos->tzp->governor_name;
-		else
-			name = DEFAULT_THERMAL_GOVERNOR;
-		if (!strnicmp(name, governor->name, THERMAL_NAME_LENGTH))
+
+		if (!strncasecmp(name, governor->name, THERMAL_NAME_LENGTH))
 			pos->governor = governor;
 	}
 
@@ -115,7 +129,7 @@ void thermal_unregister_governor(struct thermal_governor *governor)
 	mutex_lock(&thermal_list_lock);
 
 	list_for_each_entry(pos, &thermal_tz_list, node) {
-		if (!strnicmp(pos->governor->name, governor->name,
+		if (!strncasecmp(pos->governor->name, governor->name,
 						THERMAL_NAME_LENGTH))
 			pos->governor = NULL;
 	}
@@ -155,7 +169,8 @@ int get_tz_trend(struct thermal_zone_device *tz, int trip)
 {
 	enum thermal_trend trend;
 
-	if (!tz->ops->get_trend || tz->ops->get_trend(tz, trip, &trend)) {
+	if (tz->emul_temperature || !tz->ops->get_trend ||
+	    tz->ops->get_trend(tz, trip, &trend)) {
 		if (tz->temperature > tz->last_temperature)
 			trend = THERMAL_TREND_RAISING;
 		else if (tz->temperature < tz->last_temperature)
@@ -199,14 +214,23 @@ static void print_bind_err_msg(struct thermal_zone_device *tz,
 }
 
 static void __bind(struct thermal_zone_device *tz, int mask,
-			struct thermal_cooling_device *cdev)
+			struct thermal_cooling_device *cdev,
+			unsigned long *limits)
 {
 	int i, ret;
 
 	for (i = 0; i < tz->trips; i++) {
 		if (mask & (1 << i)) {
+			unsigned long upper, lower;
+
+			upper = THERMAL_NO_LIMIT;
+			lower = THERMAL_NO_LIMIT;
+			if (limits) {
+				lower = limits[i * 2];
+				upper = limits[i * 2 + 1];
+			}
 			ret = thermal_zone_bind_cooling_device(tz, i, cdev,
-					THERMAL_NO_LIMIT, THERMAL_NO_LIMIT);
+							       upper, lower);
 			if (ret)
 				print_bind_err_msg(tz, cdev, ret);
 		}
@@ -235,10 +259,11 @@ static void bind_cdev(struct thermal_cooling_device *cdev)
 		if (!pos->tzp && !pos->ops->bind)
 			continue;
 
-		if (!pos->tzp && pos->ops->bind) {
+		if (pos->ops->bind) {
 			ret = pos->ops->bind(pos, cdev);
 			if (ret)
 				print_bind_err_msg(pos, cdev, ret);
+			continue;
 		}
 
 		tzp = pos->tzp;
@@ -251,7 +276,8 @@ static void bind_cdev(struct thermal_cooling_device *cdev)
 			if (tzp->tbp[i].match(pos, cdev))
 				continue;
 			tzp->tbp[i].cdev = cdev;
-			__bind(pos, tzp->tbp[i].trip_mask, cdev);
+			__bind(pos, tzp->tbp[i].trip_mask, cdev,
+			       tzp->tbp[i].binding_limits);
 		}
 	}
 
@@ -269,8 +295,8 @@ static void bind_tz(struct thermal_zone_device *tz)
 
 	mutex_lock(&thermal_list_lock);
 
-	/* If there is no platform data, try to use ops->bind */
-	if (!tzp && tz->ops->bind) {
+	/* If there is ops->bind, try to use ops->bind */
+	if (tz->ops->bind) {
 		list_for_each_entry(pos, &thermal_cdev_list, node) {
 			ret = tz->ops->bind(tz, pos);
 			if (ret)
@@ -289,7 +315,8 @@ static void bind_tz(struct thermal_zone_device *tz)
 			if (tzp->tbp[i].match(tz, pos))
 				continue;
 			tzp->tbp[i].cdev = pos;
-			__bind(tz, tzp->tbp[i].trip_mask, pos);
+			__bind(tz, tzp->tbp[i].trip_mask, pos,
+			       tzp->tbp[i].binding_limits);
 		}
 	}
 exit:
@@ -326,8 +353,8 @@ static void monitor_thermal_zone(struct thermal_zone_device *tz)
 static void handle_non_critical_trips(struct thermal_zone_device *tz,
 			int trip, enum thermal_trip_type trip_type)
 {
-	if (tz->governor)
-		tz->governor->throttle(tz, trip);
+	tz->governor ? tz->governor->throttle(tz, trip) :
+		       def_governor->throttle(tz, trip);
 }
 
 static void handle_critical_trips(struct thermal_zone_device *tz,
@@ -388,7 +415,7 @@ int thermal_zone_get_temp(struct thermal_zone_device *tz, unsigned long *temp)
 	enum thermal_trip_type type;
 #endif
 
-	if (!tz || IS_ERR(tz))
+	if (!tz || IS_ERR(tz) || !tz->ops->get_temp)
 		goto exit;
 
 	mutex_lock(&tz->lock);
@@ -435,12 +462,18 @@ static void update_temperature(struct thermal_zone_device *tz)
 	tz->last_temperature = tz->temperature;
 	tz->temperature = temp;
 	mutex_unlock(&tz->lock);
+
+	dev_dbg(&tz->device, "last_temperature=%d, current_temperature=%d\n",
+				tz->last_temperature, tz->temperature);
 }
 
 void thermal_zone_device_update(struct thermal_zone_device *tz)
 {
 	int count;
 
+	if (!tz->ops->get_temp)
+		return;
+
 	update_temperature(tz);
 
 	for (count = 0; count < tz->trips; count++)
@@ -713,10 +746,13 @@ policy_store(struct device *dev, struct device_attribute *attr,
 	int ret = -EINVAL;
 	struct thermal_zone_device *tz = to_thermal_zone(dev);
 	struct thermal_governor *gov;
+	char name[THERMAL_NAME_LENGTH];
+
+	snprintf(name, sizeof(name), "%s", buf);
 
 	mutex_lock(&thermal_governor_lock);
 
-	gov = __find_governor(buf);
+	gov = __find_governor(strim(name));
 	if (!gov)
 		goto exit;
 
@@ -756,6 +792,9 @@ emul_temp_store(struct device *dev, struct device_attribute *attr,
 		ret = tz->ops->set_emul_temp(tz, temperature);
 	}
 
+	if (!ret)
+		thermal_zone_device_update(tz);
+
 	return ret ? ret : count;
 }
 static DEVICE_ATTR(emul_temp, S_IWUSR, NULL, emul_temp_store);
@@ -854,260 +893,6 @@ thermal_cooling_device_trip_point_show(struct device *dev,
 
 /* Device management */
 
-#if defined(CONFIG_THERMAL_HWMON)
-
-/* hwmon sys I/F */
-#include <linux/hwmon.h>
-
-/* thermal zone devices with the same type share one hwmon device */
-struct thermal_hwmon_device {
-	char type[THERMAL_NAME_LENGTH];
-	struct device *device;
-	int count;
-	struct list_head tz_list;
-	struct list_head node;
-};
-
-struct thermal_hwmon_attr {
-	struct device_attribute attr;
-	char name[16];
-};
-
-/* one temperature input for each thermal zone */
-struct thermal_hwmon_temp {
-	struct list_head hwmon_node;
-	struct thermal_zone_device *tz;
-	struct thermal_hwmon_attr temp_input;	/* hwmon sys attr */
-	struct thermal_hwmon_attr temp_crit;	/* hwmon sys attr */
-};
-
-static LIST_HEAD(thermal_hwmon_list);
-
-static ssize_t
-name_show(struct device *dev, struct device_attribute *attr, char *buf)
-{
-	struct thermal_hwmon_device *hwmon = dev_get_drvdata(dev);
-	return sprintf(buf, "%s\n", hwmon->type);
-}
-static DEVICE_ATTR(name, 0444, name_show, NULL);
-
-static ssize_t
-temp_input_show(struct device *dev, struct device_attribute *attr, char *buf)
-{
-	long temperature;
-	int ret;
-	struct thermal_hwmon_attr *hwmon_attr
-			= container_of(attr, struct thermal_hwmon_attr, attr);
-	struct thermal_hwmon_temp *temp
-			= container_of(hwmon_attr, struct thermal_hwmon_temp,
-				       temp_input);
-	struct thermal_zone_device *tz = temp->tz;
-
-	ret = thermal_zone_get_temp(tz, &temperature);
-
-	if (ret)
-		return ret;
-
-	return sprintf(buf, "%ld\n", temperature);
-}
-
-static ssize_t
-temp_crit_show(struct device *dev, struct device_attribute *attr,
-		char *buf)
-{
-	struct thermal_hwmon_attr *hwmon_attr
-			= container_of(attr, struct thermal_hwmon_attr, attr);
-	struct thermal_hwmon_temp *temp
-			= container_of(hwmon_attr, struct thermal_hwmon_temp,
-				       temp_crit);
-	struct thermal_zone_device *tz = temp->tz;
-	long temperature;
-	int ret;
-
-	ret = tz->ops->get_trip_temp(tz, 0, &temperature);
-	if (ret)
-		return ret;
-
-	return sprintf(buf, "%ld\n", temperature);
-}
-
-
-static struct thermal_hwmon_device *
-thermal_hwmon_lookup_by_type(const struct thermal_zone_device *tz)
-{
-	struct thermal_hwmon_device *hwmon;
-
-	mutex_lock(&thermal_list_lock);
-	list_for_each_entry(hwmon, &thermal_hwmon_list, node)
-		if (!strcmp(hwmon->type, tz->type)) {
-			mutex_unlock(&thermal_list_lock);
-			return hwmon;
-		}
-	mutex_unlock(&thermal_list_lock);
-
-	return NULL;
-}
-
-/* Find the temperature input matching a given thermal zone */
-static struct thermal_hwmon_temp *
-thermal_hwmon_lookup_temp(const struct thermal_hwmon_device *hwmon,
-			  const struct thermal_zone_device *tz)
-{
-	struct thermal_hwmon_temp *temp;
-
-	mutex_lock(&thermal_list_lock);
-	list_for_each_entry(temp, &hwmon->tz_list, hwmon_node)
-		if (temp->tz == tz) {
-			mutex_unlock(&thermal_list_lock);
-			return temp;
-		}
-	mutex_unlock(&thermal_list_lock);
-
-	return NULL;
-}
-
-static int
-thermal_add_hwmon_sysfs(struct thermal_zone_device *tz)
-{
-	struct thermal_hwmon_device *hwmon;
-	struct thermal_hwmon_temp *temp;
-	int new_hwmon_device = 1;
-	int result;
-
-	hwmon = thermal_hwmon_lookup_by_type(tz);
-	if (hwmon) {
-		new_hwmon_device = 0;
-		goto register_sys_interface;
-	}
-
-	hwmon = kzalloc(sizeof(struct thermal_hwmon_device), GFP_KERNEL);
-	if (!hwmon)
-		return -ENOMEM;
-
-	INIT_LIST_HEAD(&hwmon->tz_list);
-	strlcpy(hwmon->type, tz->type, THERMAL_NAME_LENGTH);
-	hwmon->device = hwmon_device_register(NULL);
-	if (IS_ERR(hwmon->device)) {
-		result = PTR_ERR(hwmon->device);
-		goto free_mem;
-	}
-	dev_set_drvdata(hwmon->device, hwmon);
-	result = device_create_file(hwmon->device, &dev_attr_name);
-	if (result)
-		goto free_mem;
-
- register_sys_interface:
-	temp = kzalloc(sizeof(struct thermal_hwmon_temp), GFP_KERNEL);
-	if (!temp) {
-		result = -ENOMEM;
-		goto unregister_name;
-	}
-
-	temp->tz = tz;
-	hwmon->count++;
-
-	snprintf(temp->temp_input.name, sizeof(temp->temp_input.name),
-		 "temp%d_input", hwmon->count);
-	temp->temp_input.attr.attr.name = temp->temp_input.name;
-	temp->temp_input.attr.attr.mode = 0444;
-	temp->temp_input.attr.show = temp_input_show;
-	sysfs_attr_init(&temp->temp_input.attr.attr);
-	result = device_create_file(hwmon->device, &temp->temp_input.attr);
-	if (result)
-		goto free_temp_mem;
-
-	if (tz->ops->get_crit_temp) {
-		unsigned long temperature;
-		if (!tz->ops->get_crit_temp(tz, &temperature)) {
-			snprintf(temp->temp_crit.name,
-				 sizeof(temp->temp_crit.name),
-				"temp%d_crit", hwmon->count);
-			temp->temp_crit.attr.attr.name = temp->temp_crit.name;
-			temp->temp_crit.attr.attr.mode = 0444;
-			temp->temp_crit.attr.show = temp_crit_show;
-			sysfs_attr_init(&temp->temp_crit.attr.attr);
-			result = device_create_file(hwmon->device,
-						    &temp->temp_crit.attr);
-			if (result)
-				goto unregister_input;
-		}
-	}
-
-	mutex_lock(&thermal_list_lock);
-	if (new_hwmon_device)
-		list_add_tail(&hwmon->node, &thermal_hwmon_list);
-	list_add_tail(&temp->hwmon_node, &hwmon->tz_list);
-	mutex_unlock(&thermal_list_lock);
-
-	return 0;
-
- unregister_input:
-	device_remove_file(hwmon->device, &temp->temp_input.attr);
- free_temp_mem:
-	kfree(temp);
- unregister_name:
-	if (new_hwmon_device) {
-		device_remove_file(hwmon->device, &dev_attr_name);
-		hwmon_device_unregister(hwmon->device);
-	}
- free_mem:
-	if (new_hwmon_device)
-		kfree(hwmon);
-
-	return result;
-}
-
-static void
-thermal_remove_hwmon_sysfs(struct thermal_zone_device *tz)
-{
-	struct thermal_hwmon_device *hwmon;
-	struct thermal_hwmon_temp *temp;
-
-	hwmon = thermal_hwmon_lookup_by_type(tz);
-	if (unlikely(!hwmon)) {
-		/* Should never happen... */
-		dev_dbg(&tz->device, "hwmon device lookup failed!\n");
-		return;
-	}
-
-	temp = thermal_hwmon_lookup_temp(hwmon, tz);
-	if (unlikely(!temp)) {
-		/* Should never happen... */
-		dev_dbg(&tz->device, "temperature input lookup failed!\n");
-		return;
-	}
-
-	device_remove_file(hwmon->device, &temp->temp_input.attr);
-	if (tz->ops->get_crit_temp)
-		device_remove_file(hwmon->device, &temp->temp_crit.attr);
-
-	mutex_lock(&thermal_list_lock);
-	list_del(&temp->hwmon_node);
-	kfree(temp);
-	if (!list_empty(&hwmon->tz_list)) {
-		mutex_unlock(&thermal_list_lock);
-		return;
-	}
-	list_del(&hwmon->node);
-	mutex_unlock(&thermal_list_lock);
-
-	device_remove_file(hwmon->device, &dev_attr_name);
-	hwmon_device_unregister(hwmon->device);
-	kfree(hwmon);
-}
-#else
-static int
-thermal_add_hwmon_sysfs(struct thermal_zone_device *tz)
-{
-	return 0;
-}
-
-static void
-thermal_remove_hwmon_sysfs(struct thermal_zone_device *tz)
-{
-}
-#endif
-
 /**
  * thermal_zone_bind_cooling_device() - bind a cooling device to a thermal zone
  * @tz:		pointer to struct thermal_zone_device
@@ -1275,7 +1060,8 @@ static void thermal_release(struct device *dev)
 		     sizeof("thermal_zone") - 1)) {
 		tz = to_thermal_zone(dev);
 		kfree(tz);
-	} else {
+	} else if(!strncmp(dev_name(dev), "cooling_device",
+			sizeof("cooling_device") - 1)){
 		cdev = to_cooling_device(dev);
 		kfree(cdev);
 	}
@@ -1287,7 +1073,8 @@ static struct class thermal_class = {
 };
 
 /**
- * thermal_cooling_device_register() - register a new thermal cooling device
+ * __thermal_cooling_device_register() - register a new thermal cooling device
+ * @np:		a pointer to a device tree node.
  * @type:	the thermal cooling device type.
  * @devdata:	device private data.
  * @ops:		standard thermal cooling devices callbacks.
@@ -1295,12 +1082,15 @@ static struct class thermal_class = {
  * This interface function adds a new thermal cooling device (fan/processor/...)
  * to /sys/class/thermal/ folder as cooling_device[0-*]. It tries to bind itself
  * to all the thermal zone devices registered at the same time.
+ * It also gives the opportunity to link the cooling device to a device tree
+ * node, so that it can be bound to a thermal zone created out of device tree.
  *
  * Return: a pointer to the created struct thermal_cooling_device or an
  * ERR_PTR. Caller must check return value with IS_ERR*() helpers.
  */
-struct thermal_cooling_device *
-thermal_cooling_device_register(char *type, void *devdata,
+static struct thermal_cooling_device *
+__thermal_cooling_device_register(struct device_node *np,
+				  char *type, void *devdata,
 				const struct thermal_cooling_device_ops *ops)
 {
 	struct thermal_cooling_device *cdev;
@@ -1326,8 +1116,9 @@ thermal_cooling_device_register(char *type, void *devdata,
 	strlcpy(cdev->type, type ? : "", sizeof(cdev->type));
 	mutex_init(&cdev->lock);
 	INIT_LIST_HEAD(&cdev->thermal_instances);
+	cdev->np = np;
 	cdev->ops = ops;
-	cdev->updated = true;
+	cdev->updated = false;
 	cdev->device.class = &thermal_class;
 	cdev->devdata = devdata;
 	dev_set_name(&cdev->device, "cooling_device%d", cdev->id);
@@ -1368,9 +1159,53 @@ unregister:
 	device_unregister(&cdev->device);
 	return ERR_PTR(result);
 }
+
+/**
+ * thermal_cooling_device_register() - register a new thermal cooling device
+ * @type:	the thermal cooling device type.
+ * @devdata:	device private data.
+ * @ops:		standard thermal cooling devices callbacks.
+ *
+ * This interface function adds a new thermal cooling device (fan/processor/...)
+ * to /sys/class/thermal/ folder as cooling_device[0-*]. It tries to bind itself
+ * to all the thermal zone devices registered at the same time.
+ *
+ * Return: a pointer to the created struct thermal_cooling_device or an
+ * ERR_PTR. Caller must check return value with IS_ERR*() helpers.
+ */
+struct thermal_cooling_device *
+thermal_cooling_device_register(char *type, void *devdata,
+				const struct thermal_cooling_device_ops *ops)
+{
+	return __thermal_cooling_device_register(NULL, type, devdata, ops);
+}
 EXPORT_SYMBOL_GPL(thermal_cooling_device_register);
 
 /**
+ * thermal_of_cooling_device_register() - register an OF thermal cooling device
+ * @np:		a pointer to a device tree node.
+ * @type:	the thermal cooling device type.
+ * @devdata:	device private data.
+ * @ops:		standard thermal cooling devices callbacks.
+ *
+ * This function will register a cooling device with device tree node reference.
+ * This interface function adds a new thermal cooling device (fan/processor/...)
+ * to /sys/class/thermal/ folder as cooling_device[0-*]. It tries to bind itself
+ * to all the thermal zone devices registered at the same time.
+ *
+ * Return: a pointer to the created struct thermal_cooling_device or an
+ * ERR_PTR. Caller must check return value with IS_ERR*() helpers.
+ */
+struct thermal_cooling_device *
+thermal_of_cooling_device_register(struct device_node *np,
+				   char *type, void *devdata,
+				   const struct thermal_cooling_device_ops *ops)
+{
+	return __thermal_cooling_device_register(np, type, devdata, ops);
+}
+EXPORT_SYMBOL_GPL(thermal_of_cooling_device_register);
+
+/**
  * thermal_cooling_device_unregister - removes the registered thermal cooling device
  * @cdev:	the thermal cooling device to remove.
  *
@@ -1442,6 +1277,8 @@ void thermal_cdev_update(struct thermal_cooling_device *cdev)
 	mutex_lock(&cdev->lock);
 	/* Make sure cdev enters the deepest cooling state */
 	list_for_each_entry(instance, &cdev->thermal_instances, cdev_node) {
+		dev_dbg(&cdev->device, "zone%d->target=%lu\n",
+				instance->tz->id, instance->target);
 		if (instance->target == THERMAL_NO_TARGET)
 			continue;
 		if (instance->target > target)
@@ -1450,6 +1287,7 @@ void thermal_cdev_update(struct thermal_cooling_device *cdev)
 	mutex_unlock(&cdev->lock);
 	cdev->ops->set_cur_state(cdev, target);
 	cdev->updated = true;
+	dev_dbg(&cdev->device, "set to state %lu\n", target);
 }
 EXPORT_SYMBOL(thermal_cdev_update);
 
@@ -1605,7 +1443,7 @@ static void remove_trip_attrs(struct thermal_zone_device *tz)
  */
 struct thermal_zone_device *thermal_zone_device_register(const char *type,
 	int trips, int mask, void *devdata,
-	const struct thermal_zone_device_ops *ops,
+	struct thermal_zone_device_ops *ops,
 	const struct thermal_zone_params *tzp,
 	int passive_delay, int polling_delay)
 {
@@ -1621,10 +1459,10 @@ struct thermal_zone_device *thermal_zone_device_register(const char *type,
 	if (trips > THERMAL_MAX_TRIPS || trips < 0 || mask >> trips)
 		return ERR_PTR(-EINVAL);
 
-	if (!ops || !ops->get_temp)
+	if (!ops)
 		return ERR_PTR(-EINVAL);
 
-	if (trips > 0 && !ops->get_trip_type)
+	if (trips > 0 && (!ops->get_trip_type || !ops->get_trip_temp))
 		return ERR_PTR(-EINVAL);
 
 	tz = kzalloc(sizeof(struct thermal_zone_device), GFP_KERNEL);
@@ -1706,13 +1544,15 @@ struct thermal_zone_device *thermal_zone_device_register(const char *type,
 	if (tz->tzp)
 		tz->governor = __find_governor(tz->tzp->governor_name);
 	else
-		tz->governor = __find_governor(DEFAULT_THERMAL_GOVERNOR);
+		tz->governor = def_governor;
 
 	mutex_unlock(&thermal_governor_lock);
 
+	if (!tz->tzp || !tz->tzp->no_hwmon) {
 	result = thermal_add_hwmon_sysfs(tz);
 	if (result)
 		goto unregister;
+	}
 
 	mutex_lock(&thermal_list_lock);
 	list_add_tail(&tz->node, &thermal_tz_list);
@@ -1723,9 +1563,11 @@ struct thermal_zone_device *thermal_zone_device_register(const char *type,
 
 	INIT_DELAYED_WORK(&(tz->poll_queue), thermal_zone_device_check);
 
+	if (!tz->ops->get_temp)
+		thermal_zone_device_set_polling(tz, 0);
+
 	thermal_zone_device_update(tz);
 
-	if (!result)
 		return tz;
 
 unregister:
@@ -1822,7 +1664,7 @@ struct thermal_zone_device *thermal_zone_get_zone_by_name(const char *name)
 
 	mutex_lock(&thermal_list_lock);
 	list_for_each_entry(pos, &thermal_tz_list, node)
-		if (!strnicmp(name, pos->type, THERMAL_NAME_LENGTH)) {
+		if (!strncasecmp(name, pos->type, THERMAL_NAME_LENGTH)) {
 			found++;
 			ref = pos;
 		}
@@ -1980,8 +1822,14 @@ static int __init thermal_init(void)
 	if (result)
 		goto unregister_class;
 
+	result = of_parse_thermal_zones();
+	if (result)
+		goto exit_netlink;
+
 	return 0;
 
+exit_netlink:
+	genetlink_exit();
 unregister_governors:
 	thermal_unregister_governors();
 unregister_class:
@@ -1997,6 +1845,7 @@ error:
 
 static void __exit thermal_exit(void)
 {
+	of_thermal_destroy_zones();
 	genetlink_exit();
 	class_unregister(&thermal_class);
 	thermal_unregister_governors();
diff --git a/drivers/thermal/thermal_core.h b/drivers/thermal/thermal_core.h
index 7cf2f66..3db339f 100644
--- a/drivers/thermal/thermal_core.h
+++ b/drivers/thermal/thermal_core.h
@@ -77,4 +77,13 @@ static inline int thermal_gov_user_space_register(void) { return 0; }
 static inline void thermal_gov_user_space_unregister(void) {}
 #endif /* CONFIG_THERMAL_GOV_USER_SPACE */
 
+/* device tree support */
+#ifdef CONFIG_THERMAL_OF
+int of_parse_thermal_zones(void);
+void of_thermal_destroy_zones(void);
+#else
+static inline int of_parse_thermal_zones(void) { return 0; }
+static inline void of_thermal_destroy_zones(void) { }
+#endif
+
 #endif /* __THERMAL_CORE_H__ */
diff --git a/drivers/thermal/thermal_hwmon.h b/drivers/thermal/thermal_hwmon.h
new file mode 100644
index 0000000..c798fdb
--- /dev/null
+++ b/drivers/thermal/thermal_hwmon.h
@@ -0,0 +1,49 @@
+/*
+ *  thermal_hwmon.h - Generic Thermal Management hwmon support.
+ *
+ *  Code based on Intel thermal_core.c. Copyrights of the original code:
+ *  Copyright (C) 2008 Intel Corp
+ *  Copyright (C) 2008 Zhang Rui <rui.zhang@intel.com>
+ *  Copyright (C) 2008 Sujith Thomas <sujith.thomas@intel.com>
+ *
+ *  Copyright (C) 2013 Texas Instruments
+ *  Copyright (C) 2013 Eduardo Valentin <eduardo.valentin@ti.com>
+ *  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ *
+ *  This program is free software; you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation; version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful, but
+ *  WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ *  General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License along
+ *  with this program; if not, write to the Free Software Foundation, Inc.,
+ *  59 Temple Place, Suite 330, Boston, MA 02111-1307 USA.
+ *
+ * ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
+ */
+#ifndef __THERMAL_HWMON_H__
+#define __THERMAL_HWMON_H__
+
+#include <linux/thermal.h>
+
+#ifdef CONFIG_THERMAL_HWMON
+int thermal_add_hwmon_sysfs(struct thermal_zone_device *tz);
+void thermal_remove_hwmon_sysfs(struct thermal_zone_device *tz);
+#else
+static int
+thermal_add_hwmon_sysfs(struct thermal_zone_device *tz)
+{
+	return 0;
+}
+
+static void
+thermal_remove_hwmon_sysfs(struct thermal_zone_device *tz)
+{
+}
+#endif
+
+#endif /* __THERMAL_HWMON_H__ */
diff --git a/drivers/tty/serial/8250/8250_dw.c b/drivers/tty/serial/8250/8250_dw.c
index 5caf10e..3ab6b9f 100644
--- a/drivers/tty/serial/8250/8250_dw.c
+++ b/drivers/tty/serial/8250/8250_dw.c
@@ -28,6 +28,7 @@
 #include <linux/acpi.h>
 #include <linux/clk.h>
 #include <linux/pm_runtime.h>
+#include <linux/pinctrl/consumer.h>
 
 #include <asm/byteorder.h>
 
@@ -217,14 +218,14 @@ static unsigned int dw8250_serial_in32(struct uart_port *p, int offset)
 
 static int dw8250_handle_irq(struct uart_port *p)
 {
-	struct dw8250_data *d = p->private_data;
 	unsigned int iir = p->serial_in(p, UART_IIR);
 
 	if (serial8250_handle_irq(p, iir)) {
 		return 1;
 	} else if ((iir & UART_IIR_BUSY) == UART_IIR_BUSY) {
 		/* Clear the USR */
-		(void)p->serial_in(p, d->usr_reg);
+		(void)p->serial_in(p, DW_UART_USR);
+
 		return 1;
 	}
 
@@ -385,6 +386,7 @@ static int dw8250_probe(struct platform_device *pdev)
 	struct resource *regs = platform_get_resource(pdev, IORESOURCE_MEM, 0);
 	struct resource *irq = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
 	struct dw8250_data *data;
+	struct pinctrl *pinctrl;
 	int err;
 
 	if (!regs || !irq) {
@@ -392,6 +394,10 @@ static int dw8250_probe(struct platform_device *pdev)
 		return -EINVAL;
 	}
 
+	pinctrl = devm_pinctrl_get_select_default(&pdev->dev);
+	if (IS_ERR(pinctrl))
+		dev_warn(&pdev->dev, "no pins associated\n");
+
 	spin_lock_init(&uart.port.lock);
 	uart.port.mapbase = regs->start;
 	uart.port.irq = irq->start;
diff --git a/drivers/tty/serial/8250/8250_early.c b/drivers/tty/serial/8250/8250_early.c
index 721904f..3ac5cc9 100644
--- a/drivers/tty/serial/8250/8250_early.c
+++ b/drivers/tty/serial/8250/8250_early.c
@@ -35,18 +35,8 @@
 #include <linux/serial_8250.h>
 #include <asm/io.h>
 #include <asm/serial.h>
-#ifdef CONFIG_FIX_EARLYCON_MEM
-#include <asm/pgtable.h>
-#include <asm/fixmap.h>
-#endif
 
-struct early_serial8250_device {
-	struct uart_port port;
-	char options[16];		/* e.g., 115200n8 */
-	unsigned int baud;
-};
-
-static struct early_serial8250_device early_device;
+static struct earlycon_device *early_device;
 
 unsigned int __weak __init serial8250_early_in(struct uart_port *port, int offset)
 {
@@ -100,7 +90,7 @@ static void __init serial_putc(struct uart_port *port, int c)
 static void __init early_serial8250_write(struct console *console,
 					const char *s, unsigned int count)
 {
-	struct uart_port *port = &early_device.port;
+	struct uart_port *port = &early_device->port;
 	unsigned int ier;
 
 	/* Save the IER and disable interrupts */
@@ -129,7 +119,7 @@ static unsigned int __init probe_baud(struct uart_port *port)
 	return (port->uartclk / 16) / quot;
 }
 
-static void __init init_port(struct early_serial8250_device *device)
+static void __init init_port(struct earlycon_device *device)
 {
 	struct uart_port *port = &device->port;
 	unsigned int divisor;
@@ -148,127 +138,32 @@ static void __init init_port(struct early_serial8250_device *device)
 	serial8250_early_out(port, UART_LCR, c & ~UART_LCR_DLAB);
 }
 
-static int __init parse_options(struct early_serial8250_device *device,
-								char *options)
+static int __init early_serial8250_setup(struct earlycon_device *device,
+					 const char *options)
 {
-	struct uart_port *port = &device->port;
-	int mmio, mmio32, length;
-
-	if (!options)
-		return -ENODEV;
-
-	port->uartclk = BASE_BAUD * 16;
-
-	mmio = !strncmp(options, "mmio,", 5);
-	mmio32 = !strncmp(options, "mmio32,", 7);
-	if (mmio || mmio32) {
-		port->iotype = (mmio ? UPIO_MEM : UPIO_MEM32);
-		port->mapbase = simple_strtoul(options + (mmio ? 5 : 7),
-					       &options, 0);
-		if (mmio32)
-			port->regshift = 2;
-#ifdef CONFIG_FIX_EARLYCON_MEM
-		set_fixmap_nocache(FIX_EARLYCON_MEM_BASE,
-					port->mapbase & PAGE_MASK);
-		port->membase =
-			(void __iomem *)__fix_to_virt(FIX_EARLYCON_MEM_BASE);
-		port->membase += port->mapbase & ~PAGE_MASK;
-#else
-		port->membase = ioremap_nocache(port->mapbase, 64);
-		if (!port->membase) {
-			printk(KERN_ERR "%s: Couldn't ioremap 0x%llx\n",
-				__func__,
-			       (unsigned long long) port->mapbase);
-			return -ENOMEM;
-		}
-#endif
-	} else if (!strncmp(options, "io,", 3)) {
-		port->iotype = UPIO_PORT;
-		port->iobase = simple_strtoul(options + 3, &options, 0);
-		mmio = 0;
-	} else
-		return -EINVAL;
-
-	options = strchr(options, ',');
-	if (options) {
-		options++;
-		device->baud = simple_strtoul(options, NULL, 0);
-		length = min(strcspn(options, " "), sizeof(device->options));
-		strlcpy(device->options, options, length);
-	} else {
-		device->baud = probe_baud(port);
-		snprintf(device->options, sizeof(device->options), "%u",
-			device->baud);
-	}
-
-	if (mmio || mmio32)
-		printk(KERN_INFO
-		       "Early serial console at MMIO%s 0x%llx (options '%s')\n",
-			mmio32 ? "32" : "",
-			(unsigned long long)port->mapbase,
-			device->options);
-	else
-		printk(KERN_INFO
-		      "Early serial console at I/O port 0x%lx (options '%s')\n",
-			port->iobase,
-			device->options);
-
+	if (!(device->port.membase || device->port.iobase))
 	return 0;
-}
-
-static struct console early_serial8250_console __initdata = {
-	.name	= "uart",
-	.write	= early_serial8250_write,
-	.flags	= CON_PRINTBUFFER | CON_BOOT,
-	.index	= -1,
-};
-
-static int __init early_serial8250_setup(char *options)
-{
-	struct early_serial8250_device *device = &early_device;
-	int err;
-
-	if (device->port.membase || device->port.iobase)
-		return 0;
 
-	err = parse_options(device, options);
-	if (err < 0)
-		return err;
+	if (!device->baud)
+		device->baud = probe_baud(&device->port);
 
 	init_port(device);
-	return 0;
-}
-
-int __init setup_early_serial8250_console(char *cmdline)
-{
-	char *options;
-	int err;
-
-	options = strstr(cmdline, "uart8250,");
-	if (!options) {
-		options = strstr(cmdline, "uart,");
-		if (!options)
-			return 0;
-	}
-
-	options = strchr(cmdline, ',') + 1;
-	err = early_serial8250_setup(options);
-	if (err < 0)
-		return err;
-
-	register_console(&early_serial8250_console);
 
+	early_device = device;
+	device->con->write = early_serial8250_write;
 	return 0;
 }
+EARLYCON_DECLARE(uart8250, early_serial8250_setup);
+EARLYCON_DECLARE(uart, early_serial8250_setup);
 
 int serial8250_find_port_for_earlycon(void)
 {
-	struct early_serial8250_device *device = &early_device;
-	struct uart_port *port = &device->port;
+	struct earlycon_device *device = early_device;
+	struct uart_port *port = device ? &device->port : NULL;
 	int line;
 	int ret;
 
-	if (!device->port.membase && !device->port.iobase)
+	if (!port || (!port->membase && !port->iobase))
 		return -ENODEV;
 
 	line = serial8250_find_port(port);
@@ -283,5 +178,3 @@ int serial8250_find_port_for_earlycon(void)
 
 	return ret;
 }
-
-early_param("earlycon", setup_early_serial8250_console);
diff --git a/drivers/tty/serial/8250/Kconfig b/drivers/tty/serial/8250/Kconfig
index 80fe91e..902c4bf 100644
--- a/drivers/tty/serial/8250/Kconfig
+++ b/drivers/tty/serial/8250/Kconfig
@@ -62,6 +62,7 @@ config SERIAL_8250_CONSOLE
 	bool "Console on 8250/16550 and compatible serial port"
 	depends on SERIAL_8250=y
 	select SERIAL_CORE_CONSOLE
+	select SERIAL_EARLYCON
 	---help---
 	  If you say Y here, it will be possible to use a serial port as the
 	  system console (the system console is the device which receives all
diff --git a/drivers/tty/serial/Kconfig b/drivers/tty/serial/Kconfig
index 7e7006f..1ad9aad 100644
--- a/drivers/tty/serial/Kconfig
+++ b/drivers/tty/serial/Kconfig
@@ -7,6 +7,13 @@ if TTY
 menu "Serial drivers"
 	depends on HAS_IOMEM && GENERIC_HARDIRQS
 
+config SERIAL_EARLYCON
+	bool
+	help
+	  Support for early consoles with the earlycon parameter. This enables
+	  the console before standard serial driver is probed. The console is
+	  enabled when early_param is processed.
+
 source "drivers/tty/serial/8250/Kconfig"
 
 comment "Non-8250 serial port support"
diff --git a/drivers/tty/serial/Makefile b/drivers/tty/serial/Makefile
index eedfec4..e51c680 100644
--- a/drivers/tty/serial/Makefile
+++ b/drivers/tty/serial/Makefile
@@ -5,6 +5,8 @@
 obj-$(CONFIG_SERIAL_CORE) += serial_core.o
 obj-$(CONFIG_SERIAL_21285) += 21285.o
 
+obj-$(CONFIG_SERIAL_EARLYCON) += earlycon.o
+
 # These Sparc drivers have to appear before others such as 8250
 # which share ttySx minor node space.  Otherwise console device
 # names change and other unplesantries.
diff --git a/drivers/tty/serial/earlycon.c b/drivers/tty/serial/earlycon.c
new file mode 100644
index 0000000..73bf1e2
--- /dev/null
+++ b/drivers/tty/serial/earlycon.c
@@ -0,0 +1,152 @@
+/*
+ * Copyright (C) 2014 Linaro Ltd.
+ * Author: Rob Herring <robh@kernel.org>
+ *
+ * Based on 8250 earlycon:
+ * (c) Copyright 2004 Hewlett-Packard Development Company, L.P.
+ *	Bjorn Helgaas <bjorn.helgaas@hp.com>
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+#include <linux/console.h>
+#include <linux/kernel.h>
+#include <linux/init.h>
+#include <linux/io.h>
+#include <linux/serial_core.h>
+
+#ifdef CONFIG_FIX_EARLYCON_MEM
+#include <asm/fixmap.h>
+#endif
+
+#include <asm/serial.h>
+
+static struct console early_con = {
+	.name =		"earlycon",
+	.flags =	CON_PRINTBUFFER | CON_BOOT,
+	.index =	-1,
+};
+
+static struct earlycon_device early_console_dev = {
+	.con = &early_con,
+};
+
+static void __iomem * __init earlycon_map(unsigned long paddr, size_t size)
+{
+	void __iomem *base;
+#ifdef CONFIG_FIX_EARLYCON_MEM
+	set_fixmap_io(FIX_EARLYCON_MEM_BASE, paddr & PAGE_MASK);
+	base = (void __iomem *)__fix_to_virt(FIX_EARLYCON_MEM_BASE);
+	base += paddr & ~PAGE_MASK;
+#else
+	base = ioremap(paddr, size);
+#endif
+	if (!base)
+		pr_err("%s: Couldn't map 0x%llx\n", __func__,
+		       (unsigned long long)paddr);
+
+	return base;
+}
+
+static int __init parse_options(struct earlycon_device *device,
+				char *options)
+{
+	struct uart_port *port = &device->port;
+	int mmio, mmio32, length, ret;
+	unsigned long addr;
+
+	if (!options)
+		return -ENODEV;
+
+	mmio = !strncmp(options, "mmio,", 5);
+	mmio32 = !strncmp(options, "mmio32,", 7);
+	if (mmio || mmio32) {
+		port->iotype = (mmio ? UPIO_MEM : UPIO_MEM32);
+		options += mmio ? 5 : 7;
+		ret = kstrtoul(options, 0, &addr);
+		if (ret)
+			return ret;
+		port->mapbase = addr;
+		if (mmio32)
+			port->regshift = 2;
+	} else if (!strncmp(options, "io,", 3)) {
+		port->iotype = UPIO_PORT;
+		options += 3;
+		ret = kstrtoul(options, 0, &addr);
+		if (ret)
+			return ret;
+		port->iobase = addr;
+		mmio = 0;
+	} else if (!strncmp(options, "0x", 2)) {
+		port->iotype = UPIO_MEM;
+		ret = kstrtoul(options, 0, &addr);
+		if (ret)
+			return ret;
+		port->mapbase = addr;
+	} else {
+		return -EINVAL;
+	}
+
+	port->uartclk = BASE_BAUD * 16;
+
+	options = strchr(options, ',');
+	if (options) {
+		options++;
+		ret = kstrtouint(options, 0, &device->baud);
+		if (ret)
+			return ret;
+		length = min(strcspn(options, " ") + 1,
+			     (size_t)(sizeof(device->options)));
+		strlcpy(device->options, options, length);
+	}
+
+	if (mmio || mmio32)
+		pr_info("Early serial console at MMIO%s 0x%llx (options '%s')\n",
+			mmio32 ? "32" : "",
+			(unsigned long long)port->mapbase,
+			device->options);
+	else
+		pr_info("Early serial console at I/O port 0x%lx (options '%s')\n",
+			port->iobase,
+			device->options);
+
+	return 0;
+}
+
+int __init setup_earlycon(char *buf, const char *match,
+			  int (*setup)(struct earlycon_device *, const char *))
+{
+	int err;
+	size_t len;
+	struct uart_port *port = &early_console_dev.port;
+
+	if (!buf || !match || !setup)
+		return 0;
+
+	len = strlen(match);
+	if (strncmp(buf, match, len))
+		return 0;
+	if (buf[len] && (buf[len] != ','))
+		return 0;
+
+	buf += len + 1;
+
+	err = parse_options(&early_console_dev, buf);
+	/* On parsing error, pass the options buf to the setup function */
+	if (!err)
+		buf = NULL;
+
+	if (port->mapbase)
+		port->membase = earlycon_map(port->mapbase, 64);
+
+	early_console_dev.con->data = &early_console_dev;
+	err = setup(&early_console_dev, buf);
+	if (err < 0)
+		return err;
+	if (!early_console_dev.con->write)
+		return -ENODEV;
+
+	register_console(early_console_dev.con);
+	return 0;
+}
diff --git a/drivers/tty/serial/serial_core.c b/drivers/tty/serial/serial_core.c
index 1d73109..7da388f 100644
--- a/drivers/tty/serial/serial_core.c
+++ b/drivers/tty/serial/serial_core.c
@@ -89,6 +89,9 @@ static void __uart_start(struct tty_struct *tty)
 	struct uart_state *state = tty->driver_data;
 	struct uart_port *port = state->uart_port;
 
+	if (port->ops->wake_peer)
+		port->ops->wake_peer(port);
+
 	if (!uart_circ_empty(&state->xmit) && state->xmit.buf &&
 	    !tty->stopped && !tty->hw_stopped)
 		port->ops->start_tx(port);
diff --git a/drivers/usb/Kconfig b/drivers/usb/Kconfig
index d9c48f3..418fce3 100644
--- a/drivers/usb/Kconfig
+++ b/drivers/usb/Kconfig
@@ -2,60 +2,16 @@
 # USB device configuration
 #
 
-# many non-PCI SOC chips embed OHCI
+# These are unused now, remove them once they are no longer selected
 config USB_ARCH_HAS_OHCI
-	boolean
-	# ARM:
-	default y if SA1111
-	default y if ARCH_OMAP
-	default y if ARCH_S3C24XX
-	default y if PXA27x
-	default y if PXA3xx
-	default y if ARCH_EP93XX
-	default y if ARCH_AT91
-	default y if MFD_TC6393XB
-	default y if ARCH_W90X900
-	default y if ARCH_DAVINCI_DA8XX
-	default y if ARCH_CNS3XXX
-	default y if PLAT_SPEAR
-	default y if ARCH_EXYNOS
-	# PPC:
-	default y if STB03xxx
-	default y if PPC_MPC52xx
-	# MIPS:
-	default y if MIPS_ALCHEMY
-	default y if MACH_JZ4740
-	# more:
-	default PCI
-
-# some non-PCI hcds implement EHCI
+	bool
+
 config USB_ARCH_HAS_EHCI
-	boolean
-	default y if FSL_SOC
-	default y if PPC_MPC512x
-	default y if ARCH_IXP4XX
-	default y if ARCH_W90X900
-	default y if ARCH_AT91
-	default y if ARCH_MXC
-	default y if ARCH_MXS
-	default y if ARCH_OMAP3
-	default y if ARCH_CNS3XXX
-	default y if ARCH_VT8500
-	default y if PLAT_SPEAR
-	default y if PLAT_S5P
-	default y if ARCH_MSM
-	default y if MICROBLAZE
-	default y if ARCH_ZYNQ
-	default y if SPARC_LEON
-	default y if ARCH_MMP
-	default y if MACH_LOONGSON1
-	default y if PLAT_ORION
-	default PCI
-
-# some non-PCI HCDs implement xHCI
+	bool
+
 config USB_ARCH_HAS_XHCI
-	boolean
-	default PCI
+	bool
+	default y if ARCH_MB86S70
 
 menuconfig USB_SUPPORT
 	bool "USB support"
@@ -72,19 +28,8 @@ config USB_COMMON
 	default y
 	depends on USB || USB_GADGET
 
-# Host-side USB depends on having a host controller
-# NOTE:  dummy_hcd is always an option, but it's ignored here ...
-# NOTE:  SL-811 option should be board-specific ...
 config USB_ARCH_HAS_HCD
-	boolean
-	default y if USB_ARCH_HAS_OHCI
-	default y if USB_ARCH_HAS_EHCI
-	default y if USB_ARCH_HAS_XHCI
-	default y if PCMCIA && !M32R			# sl811_cs
-	default y if ARM				# SL-811
-	default y if BLACKFIN				# SL-811
-	default y if SUPERH				# r8a66597-hcd
-	default PCI
+	def_bool y
 
 # ARM SA1111 chips have a non-PCI based "OHCI-compatible" USB host interface.
 config USB
diff --git a/drivers/usb/Makefile b/drivers/usb/Makefile
index c41feba..5a3781b 100644
--- a/drivers/usb/Makefile
+++ b/drivers/usb/Makefile
@@ -25,6 +25,8 @@ obj-$(CONFIG_USB_HWA_HCD)	+= host/
 obj-$(CONFIG_USB_ISP1760_HCD)	+= host/
 obj-$(CONFIG_USB_IMX21_HCD)	+= host/
 obj-$(CONFIG_USB_FSL_MPH_DR_OF)	+= host/
+obj-$(CONFIG_USB_FUSBH200_HCD)	+= host/
+obj-$(CONFIG_USB_F_USB20HDC_HCD)+= host/
 
 obj-$(CONFIG_USB_C67X00_HCD)	+= c67x00/
 
diff --git a/drivers/usb/core/hcd-pci.c b/drivers/usb/core/hcd-pci.c
index 4676917..c06352a 100644
--- a/drivers/usb/core/hcd-pci.c
+++ b/drivers/usb/core/hcd-pci.c
@@ -22,7 +22,7 @@
 #include <linux/usb.h>
 #include <linux/usb/hcd.h>
 
-#include <asm/io.h>
+#include <linux/io.h>
 #include <asm/irq.h>
 
 #ifdef CONFIG_PPC_PMAC
@@ -188,6 +188,15 @@ int usb_hcd_pci_probe(struct pci_dev *dev, const struct pci_device_id *id)
 	if (!driver)
 		return -EINVAL;
 
+	dev_dbg(&dev->dev, "%s->0x%llx\n",
+	    __func__, (unsigned long long)pci_resource_start(dev, 0));
+	dev_dbg(&dev->dev, "%s dev->irq %d\n",
+	    __func__, dev->irq);
+
+	if ((unsigned long long)pci_resource_start(dev, 0) == 0 ||
+	    dev->irq == 0)
+		return -EPROBE_DEFER;
+
 	if (pci_enable_device(dev) < 0)
 		return -ENODEV;
 	dev->current_state = PCI_D0;
diff --git a/drivers/usb/core/hub.c b/drivers/usb/core/hub.c
index 6cdac9d..28317d9 100644
--- a/drivers/usb/core/hub.c
+++ b/drivers/usb/core/hub.c
@@ -1745,7 +1745,11 @@ static int hub_probe(struct usb_interface *intf, const struct usb_device_id *id)
 		const struct hc_driver *drv = bus_to_hcd(hdev->bus)->driver;
 
 		if (drv->bus_suspend && drv->bus_resume)
+#ifdef CONFIG_USB_ZYNQ_PHY
+			usb_disable_autosuspend(hdev);
+#else
 			usb_enable_autosuspend(hdev);
+#endif
 	}
 
 	if (hdev->level == MAX_TOPO_LEVEL) {
@@ -2039,6 +2043,20 @@ static void choose_devnum(struct usb_device *udev)
 	} else {
 		/* Try to allocate the next devnum beginning at
 		 * bus->devnum_next. */
+#if defined(CONFIG_USB_F_USB20HDC_HCD) || \
+			defined(CONFIG_USB_F_USB20HDC_HCD_MODULE)
+		devnum = find_next_zero_bit(bus->devmap.devicemap, 16,
+					    bus->devnum_next);
+		if (devnum >= 16)
+			devnum = find_next_zero_bit(bus->devmap.devicemap,
+						    16, 1);
+		bus->devnum_next = (devnum >= 15 ? 1 : devnum + 1);
+	}
+	if (devnum < 16) {
+		set_bit(devnum, bus->devmap.devicemap);
+		udev->devnum = devnum;
+	}
+#else
 		devnum = find_next_zero_bit(bus->devmap.devicemap, 128,
 					    bus->devnum_next);
 		if (devnum >= 128)
@@ -2050,6 +2068,7 @@ static void choose_devnum(struct usb_device *udev)
 		set_bit(devnum, bus->devmap.devicemap);
 		udev->devnum = devnum;
 	}
+#endif
 }
 
 static void release_devnum(struct usb_device *udev)
diff --git a/drivers/usb/dwc3/Makefile b/drivers/usb/dwc3/Makefile
index 982661a..7aa227b 100644
--- a/drivers/usb/dwc3/Makefile
+++ b/drivers/usb/dwc3/Makefile
@@ -35,6 +35,7 @@ endif
 
 obj-$(CONFIG_USB_DWC3)		+= dwc3-omap.o
 obj-$(CONFIG_USB_DWC3)		+= dwc3-exynos.o
+obj-$(CONFIG_USB_DWC3)		+= dwc3-mb86s70.o
 
 ifneq ($(CONFIG_PCI),)
 	obj-$(CONFIG_USB_DWC3)		+= dwc3-pci.o
diff --git a/drivers/usb/dwc3/core.c b/drivers/usb/dwc3/core.c
index eb78a3e..7cac3b2 100644
--- a/drivers/usb/dwc3/core.c
+++ b/drivers/usb/dwc3/core.c
@@ -59,11 +59,11 @@
 #include "io.h"
 
 #include "debug.h"
+#include "../host/xhci.h"
 
 static char *maximum_speed = "super";
 module_param(maximum_speed, charp, 0);
 MODULE_PARM_DESC(maximum_speed, "Maximum supported speed.");
-
 /* -------------------------------------------------------------------------- */
 
 void dwc3_set_mode(struct dwc3 *dwc, u32 mode)
@@ -101,6 +101,7 @@ int dwc3_core_soft_reset(struct dwc3 *dwc)
 
 	usb_phy_init(dwc->usb2_phy);
 	usb_phy_init(dwc->usb3_phy);
+
 	mdelay(100);
 
 	/* Clear USB3 PHY reset */
@@ -388,6 +389,12 @@ static int dwc3_probe(struct platform_device *pdev)
 	dwc = PTR_ALIGN(mem, DWC3_ALIGN_MASK + 1);
 	dwc->mem = mem;
 
+	/* quirks initialization */
+	dwc->quirks = 0;
+	if (of_property_read_bool(node, "disconnection-quirk"))
+		dwc->quirks |= XHCI_DISCONNECT_QUIRK;
+
+
 	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
 	if (!res) {
 		dev_err(dev, "missing IRQ\n");
@@ -437,34 +444,27 @@ static int dwc3_probe(struct platform_device *pdev)
 
 	if (IS_ERR(dwc->usb2_phy)) {
 		ret = PTR_ERR(dwc->usb2_phy);
-
-		/*
-		 * if -ENXIO is returned, it means PHY layer wasn't
-		 * enabled, so it makes no sense to return -EPROBE_DEFER
-		 * in that case, since no PHY driver will ever probe.
-		 */
-		if (ret == -ENXIO)
+		if (ret == -ENXIO || ret == -ENODEV) {
+			dwc->usb2_phy = NULL;
+		} else if (ret == -EPROBE_DEFER) {
 			return ret;
-
+		} else {
 		dev_err(dev, "no usb2 phy configured\n");
-		return -EPROBE_DEFER;
+			return ret;
+		}
 	}
 
 	if (IS_ERR(dwc->usb3_phy)) {
 		ret = PTR_ERR(dwc->usb3_phy);
-
-		/*
-		 * if -ENXIO is returned, it means PHY layer wasn't
-		 * enabled, so it makes no sense to return -EPROBE_DEFER
-		 * in that case, since no PHY driver will ever probe.
-		 */
-		if (ret == -ENXIO)
+		if (ret == -ENXIO || ret == -ENODEV) {
+			dwc->usb3_phy = NULL;
+		} else if (ret == -EPROBE_DEFER) {
 			return ret;
-
+		} else {
 		dev_err(dev, "no usb3 phy configured\n");
-		return -EPROBE_DEFER;
+			return ret;
+		}
 	}
-
 	usb_phy_set_suspend(dwc->usb2_phy, 0);
 	usb_phy_set_suspend(dwc->usb3_phy, 0);
 
@@ -715,6 +715,7 @@ static int dwc3_resume(struct device *dev)
 
 	usb_phy_init(dwc->usb3_phy);
 	usb_phy_init(dwc->usb2_phy);
+
 	msleep(100);
 
 	spin_lock_irqsave(&dwc->lock, flags);
@@ -748,7 +749,7 @@ static const struct dev_pm_ops dwc3_dev_pm_ops = {
 	SET_SYSTEM_SLEEP_PM_OPS(dwc3_suspend, dwc3_resume)
 };
 
-#define DWC3_PM_OPS	&(dwc3_dev_pm_ops)
+#define DWC3_PM_OPS	(&dwc3_dev_pm_ops)
 #else
 #define DWC3_PM_OPS	NULL
 #endif
diff --git a/drivers/usb/dwc3/core.h b/drivers/usb/dwc3/core.h
index e145006..7d2bb43 100644
--- a/drivers/usb/dwc3/core.h
+++ b/drivers/usb/dwc3/core.h
@@ -755,6 +755,7 @@ struct dwc3 {
 
 	u8			test_mode;
 	u8			test_mode_nr;
+	u32			quirks;
 };
 
 /* -------------------------------------------------------------------------- */
diff --git a/drivers/usb/dwc3/dwc3-mb86s70.c b/drivers/usb/dwc3/dwc3-mb86s70.c
new file mode 100644
index 0000000..1b1b861
--- /dev/null
+++ b/drivers/usb/dwc3/dwc3-mb86s70.c
@@ -0,0 +1,216 @@
+/**
+ * dwc3-mb86s70.c - Fujitsu mb86s70 DWC3 Specific Glue layer
+ *
+ * Copyright (c) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *		http://jp.fujitsu.com/group/fsl
+ *
+ * based on dwc3-exynos.c
+ *
+ * Author: FUJITSU SEMICONDUCTOR
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+/*#define DEBUG */
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/platform_device.h>
+#include <linux/dma-mapping.h>
+#include <linux/interrupt.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/pm_runtime.h>
+#include <linux/clk.h>
+
+struct dwc3_mb86s70 {
+	struct device		*dev;
+	struct clk **clk_table;
+	int irq;
+};
+
+/* return 0 means successful */
+static int dwc3_mb86s70_clk_control(struct device *dev, bool on)
+{
+	int ret, i;
+	struct clk *clk;
+
+	dev_dbg(dev, "%s() is started (on:%d).\n", __func__, on);
+
+	if (!on)
+		goto clock_off;
+
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		ret = clk_prepare_enable(clk);
+		if (ret) {
+			dev_err(dev, "failed to enable clock[%d]\n", i);
+			goto clock_off;
+		}
+		dev_info(dev, "enabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+
+clock_off:
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		clk_disable_unprepare(clk);
+		dev_info(dev, "disabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return on;
+}
+
+static int dwc3_mb86s70_remove_child(struct device *dev, void *unused)
+{
+	struct platform_device *pdev = to_platform_device(dev);
+
+	of_device_unregister(pdev);
+
+	return 0;
+}
+
+static u64 dwc3_mb86s70_dma_mask = DMA_BIT_MASK(32);
+
+static int dwc3_mb86s70_probe(struct platform_device *pdev)
+{
+	struct dwc3_mb86s70	*mb86s70;
+	struct device		*dev = &pdev->dev;
+	struct device_node	*node = dev->of_node;
+
+	int			ret;
+
+	mb86s70 = devm_kzalloc(dev, sizeof(*mb86s70), GFP_KERNEL);
+	if (!mb86s70) {
+		dev_err(dev, "not enough memory\n");
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	/*
+	 * Right now device-tree probed devices don't get dma_mask set.
+	 * Since shared usb code relies on it, set it here for now.
+	 * Once we move to full device tree support this will vanish off.
+	 */
+	dev->dma_mask = &dwc3_mb86s70_dma_mask;
+
+	platform_set_drvdata(pdev, mb86s70);
+
+	mb86s70->dev = dev;
+
+	/* resume driver for clock, power */
+	pm_runtime_enable(&pdev->dev);
+	ret = pm_runtime_get_sync(&pdev->dev);
+	if (ret < 0) {
+		dev_err(&pdev->dev, "get_sync failed with err %d\n", ret);
+		goto irq_err;
+	}
+
+	if (node) {
+		ret = of_platform_populate(node, NULL, NULL, dev);
+		if (!ret)
+			return 0;
+		dev_err(dev, "failed to add dwc3 core\n");
+	}
+	dev_err(dev, "no device node, failed to add dwc3 core\n");
+	ret = -ENODEV;
+irq_err:
+	kfree(mb86s70);
+err:
+	return ret;
+}
+
+static int dwc3_mb86s70_remove(struct platform_device *pdev)
+{
+	device_for_each_child(&pdev->dev, NULL, dwc3_mb86s70_remove_child);
+
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	return 0;
+}
+
+
+static const struct of_device_id mb86s70_dwc3_match[] = {
+	{ .compatible = "fujitsu,mb86s70-dwc3" },
+	{},
+};
+MODULE_DEVICE_TABLE(of, mb86s70_dwc3_match);
+
+#ifdef CONFIG_PM
+#ifdef CONFIG_PM_RUNTIME
+static int dwc3_mb86s70_runtime_suspend(struct device *dev)
+{
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	dwc3_mb86s70_clk_control(dev, false);
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+
+static int  dwc3_mb86s70_runtime_resume(struct device *dev)
+{
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	dwc3_mb86s70_clk_control(dev, true);
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+#endif /* CONFIG_PM_RUNTIME */
+static int dwc3_mb86s70_suspend(struct device *dev)
+{
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	return dwc3_mb86s70_runtime_suspend(dev);
+}
+
+static int dwc3_mb86s70_resume(struct device *dev)
+{
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	return dwc3_mb86s70_runtime_resume(dev);
+}
+
+static const struct dev_pm_ops dwc3_mb86s70_dev_pm_ops = {
+	.suspend = dwc3_mb86s70_suspend,
+	.resume = dwc3_mb86s70_resume,
+	SET_RUNTIME_PM_OPS(
+		dwc3_mb86s70_runtime_suspend,
+		dwc3_mb86s70_runtime_resume, NULL)
+};
+
+#define DEV_PM_OPS	(&dwc3_mb86s70_dev_pm_ops)
+#else
+#define DEV_PM_OPS	NULL
+#endif /* CONFIG_PM_SLEEP */
+
+static struct platform_driver dwc3_mb86s70_driver = {
+	.probe		= dwc3_mb86s70_probe,
+	.remove		= dwc3_mb86s70_remove,
+	.driver		= {
+		.name	= "mb86s70-dwc3",
+		.of_match_table = of_match_ptr(mb86s70_dwc3_match),
+		.pm	= DEV_PM_OPS,
+	},
+};
+
+module_platform_driver(dwc3_mb86s70_driver);
+
+MODULE_ALIAS("platform:mb86s70-dwc3");
+MODULE_AUTHOR("FUJITSU SEMICONDUCTOR");
+MODULE_LICENSE("GPL");
+MODULE_DESCRIPTION("DesignWare USB3 mb86s70 Glue Layer");
diff --git a/drivers/usb/dwc3/dwc3-pci.c b/drivers/usb/dwc3/dwc3-pci.c
index 2357c4e..9db6bec 100644
--- a/drivers/usb/dwc3/dwc3-pci.c
+++ b/drivers/usb/dwc3/dwc3-pci.c
@@ -216,7 +216,7 @@ static DEFINE_PCI_DEVICE_TABLE(dwc3_pci_id_table) = {
 };
 MODULE_DEVICE_TABLE(pci, dwc3_pci_id_table);
 
-#ifdef CONFIG_PM
+#ifdef CONFIG_SUSPEND
 static int dwc3_pci_suspend(struct device *dev)
 {
 	struct pci_dev	*pci = to_pci_dev(dev);
diff --git a/drivers/usb/dwc3/gadget.c b/drivers/usb/dwc3/gadget.c
index 051b613..299292c 100644
--- a/drivers/usb/dwc3/gadget.c
+++ b/drivers/usb/dwc3/gadget.c
@@ -907,8 +907,7 @@ static void dwc3_prepare_trbs(struct dwc3_ep *dep, bool starting)
 
 				if (i == (request->num_mapped_sgs - 1) ||
 						sg_is_last(s)) {
-					if (list_is_last(&req->list,
-							&dep->request_list))
+					if (list_empty(&dep->request_list))
 						last_one = true;
 					chain = false;
 				}
@@ -926,6 +925,9 @@ static void dwc3_prepare_trbs(struct dwc3_ep *dep, bool starting)
 				if (last_one)
 					break;
 			}
+
+			if (last_one)
+				break;
 		} else {
 			dma = req->request.dma;
 			length = req->request.length;
diff --git a/drivers/usb/dwc3/host.c b/drivers/usb/dwc3/host.c
index 3b0c9f8..b06d34d 100644
--- a/drivers/usb/dwc3/host.c
+++ b/drivers/usb/dwc3/host.c
@@ -38,11 +38,13 @@
 #include <linux/platform_device.h>
 
 #include "core.h"
+#include "../host/xhci.h"
 
 int dwc3_host_init(struct dwc3 *dwc)
 {
 	struct platform_device	*xhci;
 	int			ret;
+	struct xhci_platform_data platform_data;
 
 	xhci = platform_device_alloc("xhci-hcd", PLATFORM_DEVID_AUTO);
 	if (!xhci) {
diff --git a/drivers/usb/gadget/Kconfig b/drivers/usb/gadget/Kconfig
index 11fc47b..0972fa2 100644
--- a/drivers/usb/gadget/Kconfig
+++ b/drivers/usb/gadget/Kconfig
@@ -391,6 +391,83 @@ config USB_M66592
 	   dynamically linked module called "m66592_udc" and force all
 	   gadget drivers to also be dynamically linked.
 
+config USB_GADGET_F_USB30
+	boolean "F_USB30 USB3.0 Function Controller"
+	depends on ARCH_MB8AC0300 && ((USB_F_USB20HDC_OTG=n)||(USB_F_USB20HDC_OTG=m))
+	select USB_GADGET_DUALSPEED
+	select USB_GADGET_SUPERSPEED
+	help
+	  F_USB30 is an integrated USB3.0 peripheral controller
+	  that supports super, high and full speed data transfers.
+
+	  It has six configurable endpoints, as well as endpoint zero
+	  (for control transfers) and several endpoints with dedicated
+	  functions.
+
+	  Say "y" to link the driver statically, or "m" to build a
+	  dynamically linked module called "f_usb30_udc" and force all
+	  gadget drivers to also be dynamically linked.
+
+config USB_F_USB30
+	tristate
+	depends on USB_GADGET_F_USB30
+	default USB_GADGET
+	select USB_GADGET_SELECTED
+
+config USB_GADGET_F_USB30_USED_BUS_RESET_DUMMY_DISCONNECT_NOTIFY
+	bool "Report a disconnection on bus reset"
+	depends on USB_GADGET_F_USB30
+	default n
+	help
+	  A dummy disconnect event will occur when USB3.0 controller receive
+	  a bus reset.
+	  If you want to be notified and do something, say Y here.
+	  If unsure, say N.
+
+config USB_GADGET_F_USB30_BULK_IN_END_NOTIFY_TIMING_TO_HOST
+	bool "Notify a gadget when bulk IN transfer is acutually completed"
+	depends on USB_GADGET_F_USB30
+	default n
+	help
+	  If you want to be notified when the last Bulk IN data are
+	  transfered to host from endpoint FIFO, say Y here.
+	  otherwise you are notified when the last Bulk IN data are
+	  written into enpoint FIFO.
+	  If unsure, say N.
+
+config USB_GADGET_F_USB30_USED_DMA_TRANSFER
+	bool "Use DMA transfer"
+	depends on USB_GADGET_F_USB30
+	select USB_F_USB30_HF if ARCH_MB8AC0300 || ARCH_MB86S70
+	default y
+	help
+	  If you want to use DMA transfer,
+	  say Y here.
+	  Things will be slow with ths off.
+	  If unsure, say Y.
+
+config USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF
+	bool "Use bounce buffers"
+	depends on USB_GADGET_F_USB30_USED_DMA_TRANSFER
+	default n
+	help
+	  By default(say n here), F_USB30 driver uses cache-able buffers as
+	  transmit and receive buffers, which directly come from or go to
+	  protocol stack. So no copy is needed, but cache coherency must be
+	  maintained. This configuration gives a choice to use DMA-coherent
+	  memory instead of cache-able buffers (because data must be copied
+	  between them, the former is called a bounce buffer) and eliminate
+	  cache maintenance operations (e.g. invalidate, clean). This would
+	  improve the IO thoughput if such operations are really heavy.
+	  Make sure that system has enough DMA-coherent memory before enabling
+	  this feature. The size of DMA-coherent memory is specified by
+	  CONSISTENT_DMA_SIZE defined in <include/mach/memory.h>. The value
+	  should be between 2M and 14M, in multiples of 2M.
+	  F_USB30 driver requires, at least, 16KB * 2 memory.
+
+	  Say 'y' here if you want to use bounce buffers as transmit and
+	  receive buffers. If unsure, say 'n' here.
+
 #
 # Controllers available only in discrete form (and all PCI controllers)
 #
diff --git a/drivers/usb/gadget/Makefile b/drivers/usb/gadget/Makefile
index ecd27d3..d93c70d 100644
--- a/drivers/usb/gadget/Makefile
+++ b/drivers/usb/gadget/Makefile
@@ -36,6 +36,7 @@ mv_udc-y			:= mv_udc_core.o
 obj-$(CONFIG_USB_XUSBPS)	+= xilinx_usbps_udc.o
 obj-$(CONFIG_USB_FUSB300)	+= fusb300_udc.o
 obj-$(CONFIG_USB_MV_U3D)	+= mv_u3d_core.o
+obj-$(CONFIG_USB_F_USB30)	+= f_usb30_udc.o
 
 # USB Functions
 usb_f_acm-y			:= f_acm.o
diff --git a/drivers/usb/gadget/composite.c b/drivers/usb/gadget/composite.c
index 44a292b..1ffc85e 100644
--- a/drivers/usb/gadget/composite.c
+++ b/drivers/usb/gadget/composite.c
@@ -812,7 +812,7 @@ done:
 }
 EXPORT_SYMBOL_GPL(usb_add_config);
 
-static void remove_config(struct usb_composite_dev *cdev,
+static void unbind_config(struct usb_composite_dev *cdev,
 			      struct usb_configuration *config)
 {
 	while (!list_empty(&config->functions)) {
@@ -827,7 +827,6 @@ static void remove_config(struct usb_composite_dev *cdev,
 			/* may free memory for "f" */
 		}
 	}
-	list_del(&config->list);
 	if (config->unbind) {
 		DBG(cdev, "unbind config '%s'/%p\n", config->label, config);
 		config->unbind(config);
@@ -849,14 +848,21 @@ void usb_remove_config(struct usb_composite_dev *cdev,
 {
 	unsigned long flags;
 
+	if (!config->cdev) {
+		pr_err("%s() config doesn't belong to device\n", __func__);
+		return;
+	}
+
 	spin_lock_irqsave(&cdev->lock, flags);
 
 	if (cdev->config == config)
 		reset_config(cdev);
 
+	list_del(&config->list);
+
 	spin_unlock_irqrestore(&cdev->lock, flags);
 
-	remove_config(cdev, config);
+	unbind_config(cdev, config);
 }
 
 /*-------------------------------------------------------------------------*/
@@ -1525,7 +1531,8 @@ static void __composite_unbind(struct usb_gadget *gadget, bool unbind_driver)
 		struct usb_configuration	*c;
 		c = list_first_entry(&cdev->configs,
 				struct usb_configuration, list);
-		remove_config(cdev, c);
+		list_del(&c->list);
+		unbind_config(cdev, c);
 	}
 	if (cdev->driver->unbind && unbind_driver)
 		cdev->driver->unbind(cdev);
diff --git a/drivers/usb/gadget/f_usb30_udc.c b/drivers/usb/gadget/f_usb30_udc.c
new file mode 100644
index 0000000..6da55fb
--- /dev/null
+++ b/drivers/usb/gadget/f_usb30_udc.c
@@ -0,0 +1,6071 @@
+/*
+ * linux/drivers/usb/gadget/f_usb30_udc.c - F_USB30 USB3.0 Function
+ * Controller Driver
+ *
+ * based on F_USB20LP USB2.0 Function Controller Driver
+ *
+ * Copyright (C) 2002 Intrinsyc, Inc. (Frank Becker)
+ * Copyright (C) 2003 Robert Schwebel, Pengutronix
+ * Copyright (C) 2003 Benedikt Spranger, Pengutronix
+ * Copyright (C) 2003 David Brownell
+ * Copyright (C) 2003 Joshua Wise
+ * Copyright (C) 2006 - 2007 Lineo Solutions, Inc.
+ * Copyright (C) FUJITSU ELECTRONICS INC. 2011. All rights reserved.
+ * Copyright (C) 2011 - 2012 FUJITSU SEMICONDUCTOR LIMITED
+ */
+
+/* #define DEBUG */
+#include <linux/clk.h>
+#include <linux/device.h>
+#include <linux/dma-mapping.h>
+#include <linux/errno.h>
+#include <linux/init.h>
+#include <linux/interrupt.h>
+#include <linux/irq.h>
+#include <linux/io.h>
+#include <linux/of.h>
+#include <linux/ioport.h>
+#include <linux/kernel.h>
+#include <linux/list.h>
+#include <linux/module.h>
+#include <linux/platform_device.h>
+#include <linux/spinlock.h>
+#include <linux/timer.h>
+#include <linux/types.h>
+#include <linux/usb/ch9.h>
+#include <linux/usb/gadget.h>
+
+#include <linux/dma-mapping.h>
+
+#include <linux/irqchip/irq-mb8ac0300.h>
+
+#include "f_usb30_udc.h"
+
+/* F_USB30 UDC device driver request structure */
+struct f_usb30_request {
+	struct usb_request req;			/* USB request structure */
+	struct list_head queue;			/* request queue head */
+	unsigned char dma_buffer_map;		/* DMA trans buffer map flag */
+};
+
+/* F_USB30 UDC device driver endpoint structure */
+struct f_usb30_ep {
+	struct usb_ep ep;			/* endpoint structure */
+	struct f_usb30_udc *udc;		/*
+						 * F_USB30 UDC device
+						 * driver structure
+						 */
+	unsigned char attributes;		/* endpoint attributes */
+	signed char interface_channel;		/* interface channle */
+	unsigned char alternate_channels;	/* alternate channle count */
+	signed char dma_channel;		/* DMA channel */
+	struct f_usb30_request *req;		/* current request structure */
+	struct list_head queue;			/* endpoint queue head */
+	unsigned char enabled;			/* endpoint enabled flag */
+	unsigned char halt;			/* transfer halt flag */
+	unsigned char force_halt;		/* transfer force halt flag */
+	unsigned char dma_transfer;		/* DMA transfer flag */
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	void __iomem *ss_dma_fifo_addr;		/*
+						 * SS DMA transfer endpoint's
+						 * FIFO pyhsical address
+						 */
+	void __iomem *hs_dma_fifo_addr;		/*
+						 * HS/FS DMA transfer endpoint's
+						 * FIFO pyhsical address
+						 */
+#endif
+};
+
+/* F_USB30 UDC device driver structure */
+struct f_usb30_udc {
+	struct clk *clk;			/* clock */
+	struct usb_gadget gadget;		/* gadget structure */
+	struct usb_gadget_driver *gadget_driver;/* gadget driver structure */
+	spinlock_t lock;			/* device lock */
+	struct resource *device_resource;	/* F_USB30 device resource */
+	void __iomem *register_base_address;	/* F_USB30 reg base addres */
+	int hs_irq;				/* F_USB30 HF IRQ number */
+	int ss_irq;				/* F_USB30 SS IRQ number */
+#if defined(CONFIG_ARCH_MB8AC0300)
+	int vbus_on_irq;			/* vbus on detect extint num */
+	int vbus_off_irq;			/* vbus off detect extint num */
+	int vbus_on_hwirq, vbus_off_hwirq;
+	unsigned char vbus_active_level;	/* vbus detect active level */
+#else
+/* for other soc */
+#endif
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	int dmac_in_irq;			/* DMAC in IRQ number */
+	int dmac_out_irq;			/* DMAC out IRQ number */
+	unsigned char dma_ep[F_USB30_MAX_DMAC];
+						/* DMA endpoint channel array */
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	void *buffer[F_USB30_MAX_DMAC];		/*
+						 * DMA transfer noncachable
+						 * buffer's virtual address
+						 */
+	dma_addr_t dma_buffer[F_USB30_MAX_DMAC];/*
+						 * DMA transfer noncachable
+						 * buffer's physical address
+						 */
+#endif
+#endif
+	struct f_usb30_ep udc_endpoint[F_USB30_MAX_EP];
+						/*
+						 * F_USB30 UDC device driver
+						 * endpoint structure array
+						 */
+	unsigned char vbus;			/* bus connect status flag */
+	enum usb_device_state device_state;	/* USB device state */
+	enum usb_device_state device_state_last;/* last USB device state */
+	unsigned char configure_value_last;	/* last configure value */
+	enum f_usb30_ss_link_state link_state;/* ss link training state */
+	int ss_discnt;				/* link training fail counter */
+	enum f_usb30_ctrl_stage ctrl_stage;	/* control transfer stage */
+	unsigned char ctrl_pri_dir;		/*
+						 * control transfer
+						 * priority-processing
+						 * direction flag
+						 */
+};
+
+static struct f_usb30_udc *f_usb30_data;
+
+static void initialize_dma_controller(struct f_usb30_udc *f_usb30)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	unsigned long counter;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* initialize F_USB30 DMA controller register */
+	for (counter = 0; counter < F_USB30_MAX_DMAC; counter++) {
+		set_dmainit(base_addr, counter, 1);
+		set_mskintuexdreq(base_addr, counter, 1);
+		set_mskintstarterr(base_addr, counter, 1);
+		set_mskintslverr(base_addr, counter, 1);
+		set_mskintdecerr(base_addr, counter, 1);
+		set_mskintaxifin(base_addr, counter, 1);
+		set_mskintusbunder(base_addr, counter, 1);
+		set_mskintusbover(base_addr, counter, 1);
+		set_mskintexokay(base_addr, counter, 1);
+		set_mskintnull(base_addr, counter, 1);
+		set_mskintusbfin(base_addr, counter, 1);
+		set_mskintremainfin(base_addr, counter, 1);
+		clear_intuexdreq(base_addr, counter);
+		clear_intstarterr(base_addr, counter);
+		clear_intslverr(base_addr, counter);
+		clear_intdecerr(base_addr, counter);
+		clear_intaxifin(base_addr, counter);
+		clear_intusbunder(base_addr, counter);
+		clear_intusbover(base_addr, counter);
+		clear_intexokay(base_addr, counter);
+		clear_intnull(base_addr, counter);
+		clear_intusbfin(base_addr, counter);
+		clear_intremainfin(base_addr, counter);
+	}
+#endif
+	return;
+}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+static void set_dma_controller(struct f_usb30_ep *endpoint,
+	 unsigned long source, unsigned long destination, unsigned long bytes)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check DMA channel */
+	if (unlikely(endpoint->dma_channel == -1))
+		return;
+
+	/* check DMA transfer and DMA abort active */
+	if (unlikely(get_dmaactive(base_addr, endpoint->dma_channel) ||
+			 get_abortactive(base_addr, endpoint->dma_channel)))
+		return;
+
+	/* setup F_USB30 DMA controller register */
+	f_usb30->dma_ep[endpoint->dma_channel] = ep_channel;
+	set_issue(base_addr, endpoint->dma_channel, 1);
+	set_bursttype(base_addr, endpoint->dma_channel, DMAC_BURST_INCREMENT);
+	endpoint->ep.address & USB_DIR_IN ?
+		 set_maxlen(base_addr, endpoint->dma_channel, source & 0x7 ?
+			 DMAC_MAX_BURST_8 : DMAC_MAX_BURST_16) :
+		 set_maxlen(base_addr, endpoint->dma_channel, destination & 0x7
+			 ? DMAC_MAX_BURST_8 : DMAC_MAX_BURST_16);
+	set_nullcntl(base_addr, endpoint->dma_channel, 0);
+	set_abortcntl(base_addr, endpoint->dma_channel, 1);
+	set_axiaddress(base_addr, endpoint->dma_channel, endpoint->ep.address &
+			 USB_DIR_IN ? source : destination);
+	set_dmadatasize(base_addr, endpoint->dma_channel, bytes);
+	if (!(endpoint->ep.address & USB_DIR_IN))
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_tcnt(base_addr, ep_channel, bytes) :
+			 set_hs_tcnt(base_addr, ep_channel, bytes);
+
+	return;
+}
+#endif
+
+static void enable_dma_transfer(struct f_usb30_ep *endpoint,
+				 unsigned char enable)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check DMA channel */
+	if (unlikely(endpoint->dma_channel == -1))
+		return;
+
+	if (enable) {
+		/* check DMA transfer and DMA abort active */
+		if (unlikely(get_dmaactive(base_addr, endpoint->dma_channel) ||
+			 get_abortactive(base_addr, endpoint->dma_channel)))
+			return;
+
+		/* enable DMA transfer */
+		endpoint->dma_transfer = 1;
+		if (endpoint->ep.address & USB_DIR_IN) {
+			clear_intuexdreq(base_addr, endpoint->dma_channel);
+			clear_intstarterr(base_addr, endpoint->dma_channel);
+			clear_intslverr(base_addr, endpoint->dma_channel);
+			clear_intdecerr(base_addr, endpoint->dma_channel);
+			clear_intexokay(base_addr, endpoint->dma_channel);
+			clear_intusbfin(base_addr, endpoint->dma_channel);
+			clear_intaxifin(base_addr, endpoint->dma_channel);
+			clear_intnull(base_addr, endpoint->dma_channel);
+			clear_intremainfin(base_addr, endpoint->dma_channel);
+			set_mskintuexdreq(base_addr, endpoint->dma_channel, 0);
+			set_mskintstarterr(base_addr, endpoint->dma_channel, 0);
+			set_mskintslverr(base_addr, endpoint->dma_channel, 0);
+			set_mskintdecerr(base_addr, endpoint->dma_channel, 0);
+			set_mskintexokay(base_addr, endpoint->dma_channel, 0);
+			set_mskintusbfin(base_addr, endpoint->dma_channel, 0);
+			set_mskintremainfin(base_addr, endpoint->dma_channel,
+						0);
+			set_mskintnull(base_addr, endpoint->dma_channel, 0);
+			set_mskintaxifin(base_addr, endpoint->dma_channel, 0);
+		} else {
+			clear_intuexdreq(base_addr, endpoint->dma_channel);
+			clear_intstarterr(base_addr, endpoint->dma_channel);
+			clear_intslverr(base_addr, endpoint->dma_channel);
+			clear_intdecerr(base_addr, endpoint->dma_channel);
+			clear_intaxifin(base_addr, endpoint->dma_channel);
+			clear_intusbfin(base_addr, endpoint->dma_channel);
+			clear_intusbunder(base_addr, endpoint->dma_channel);
+			clear_intusbover(base_addr, endpoint->dma_channel);
+			clear_intexokay(base_addr, endpoint->dma_channel);
+			clear_intnull(base_addr, endpoint->dma_channel);
+			clear_intremainfin(base_addr, endpoint->dma_channel);
+			set_mskintuexdreq(base_addr, endpoint->dma_channel, 0);
+			set_mskintstarterr(base_addr, endpoint->dma_channel, 0);
+			set_mskintslverr(base_addr, endpoint->dma_channel, 0);
+			set_mskintdecerr(base_addr, endpoint->dma_channel, 0);
+			set_mskintaxifin(base_addr, endpoint->dma_channel, 0);
+			set_mskintusbfin(base_addr, endpoint->dma_channel, 0);
+			set_mskintusbunder(base_addr, endpoint->dma_channel, 0);
+			set_mskintusbover(base_addr, endpoint->dma_channel, 0);
+			set_mskintexokay(base_addr, endpoint->dma_channel, 0);
+			set_mskintnull(base_addr, endpoint->dma_channel, 0);
+			set_mskintremainfin(base_addr, endpoint->dma_channel,
+						0);
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskdend(base_addr, ep_channel, 0) :
+				 set_hs_mskdend(base_addr, ep_channel, 0);
+			if (endpoint->attributes != USB_ENDPOINT_XFER_ISOC) {
+				/* enable SPDD mode */
+				if (f_usb30->gadget.speed ==
+							 USB_SPEED_SUPER) {
+					clear_ss_intspd(base_addr, ep_channel);
+					set_ss_mskspd(base_addr, ep_channel, 0);
+					set_ss_enspd(base_addr, ep_channel, 0);
+				} else {
+					clear_hs_intspdd(base_addr, ep_channel);
+					set_hs_mskspdd(base_addr, ep_channel,
+							1);
+					set_hs_enspdd(base_addr, ep_channel, 0);
+				}
+			}
+		}
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_dmamode(base_addr, ep_channel, 1) :
+			 set_hs_dmamode(base_addr, ep_channel, 1);
+		set_dmastart(base_addr, endpoint->dma_channel, 1);
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskdmareq(base_addr, ep_channel, 0) :
+			 set_hs_mskdmareq(base_addr, ep_channel, 0);
+	} else {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskdmareq(base_addr, ep_channel, 1) :
+			 set_hs_mskdmareq(base_addr, ep_channel, 1);
+		set_mskintuexdreq(base_addr, endpoint->dma_channel, 1);
+		set_mskintstarterr(base_addr, endpoint->dma_channel, 1);
+		set_mskintslverr(base_addr, endpoint->dma_channel, 1);
+		set_mskintdecerr(base_addr, endpoint->dma_channel, 1);
+		set_mskintaxifin(base_addr, endpoint->dma_channel, 1);
+		set_mskintusbunder(base_addr, endpoint->dma_channel, 1);
+		set_mskintusbover(base_addr, endpoint->dma_channel, 1);
+		set_mskintexokay(base_addr, endpoint->dma_channel, 1);
+		set_mskintnull(base_addr, endpoint->dma_channel, 1);
+		set_mskintusbfin(base_addr, endpoint->dma_channel, 1);
+		set_mskintremainfin(base_addr, endpoint->dma_channel, 1);
+		clear_intuexdreq(base_addr, endpoint->dma_channel);
+		clear_intstarterr(base_addr, endpoint->dma_channel);
+		clear_intslverr(base_addr, endpoint->dma_channel);
+		clear_intdecerr(base_addr, endpoint->dma_channel);
+		clear_intaxifin(base_addr, endpoint->dma_channel);
+		clear_intusbunder(base_addr, endpoint->dma_channel);
+		clear_intusbover(base_addr, endpoint->dma_channel);
+		clear_intexokay(base_addr, endpoint->dma_channel);
+		clear_intnull(base_addr, endpoint->dma_channel);
+		clear_intusbfin(base_addr, endpoint->dma_channel);
+		clear_intremainfin(base_addr, endpoint->dma_channel);
+		if (!(endpoint->ep.address & USB_DIR_IN)) {
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskdend(base_addr, ep_channel, 1) :
+				 set_hs_mskdend(base_addr, ep_channel, 1);
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 clear_ss_intdend(base_addr, ep_channel) :
+				 clear_hs_intdend(base_addr, ep_channel);
+			if (endpoint->attributes != USB_ENDPOINT_XFER_ISOC) {
+				/* disable SPDD mode */
+				if (f_usb30->gadget.speed ==
+						 USB_SPEED_SUPER) {
+					set_ss_mskspd(base_addr, ep_channel, 1);
+					clear_ss_intspd(base_addr, ep_channel);
+					set_ss_enspd(base_addr, ep_channel, 0);
+
+				} else {
+					set_hs_mskspdd(base_addr, ep_channel,
+							1);
+					clear_hs_intspdd(base_addr, ep_channel);
+					set_hs_enspdd(base_addr, ep_channel, 0);
+				}
+			}
+		}
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_dmamode(base_addr, ep_channel, 0) :
+			 set_hs_dmamode(base_addr, ep_channel, 0);
+		endpoint->dma_transfer = 0;
+	}
+#endif
+	return;
+}
+
+static unsigned char set_in_transfer_dma(struct f_usb30_ep *endpoint)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	dma_addr_t dma_addr;
+	unsigned long bytes;
+
+	/* check argument */
+	if (unlikely(!endpoint->enabled))
+		return 0;
+
+	/* calculate this time transfer byte */
+	bytes = request->req.length < F_USB30_DMAC_TRANS_MAX_BYTES ?
+			 request->req.length : F_USB30_DMAC_TRANS_MAX_BYTES;
+
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* copy IN transfer data to noncachable buffer */
+	memcpy(f_usb30->buffer[endpoint->dma_channel], request->req.buf, bytes);
+#else
+	/* check DMA transfer buffer mapping */
+	if (request->req.dma == F_USB30_DMA_ADDR_INVALID) {
+		/* map DMA transfer buffer and sync DMA transfer buffer */
+		request->req.dma = dma_map_single(f_usb30->gadget.dev.parent,
+						 request->req.buf,
+						 request->req.length,
+						 DMA_TO_DEVICE);
+		request->dma_buffer_map = 1;
+	} else {
+		dma_sync_single_for_device(f_usb30->gadget.dev.parent,
+						 request->req.dma,
+						 request->req.length,
+						 DMA_TO_DEVICE);
+		request->dma_buffer_map = 0;
+	}
+#endif
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u DMA is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+	/* update actual byte */
+	request->req.actual = bytes;
+
+	/* set StreamID if stream transfer enabled */
+	if ((f_usb30->gadget.speed == USB_SPEED_SUPER) &&
+		 endpoint->ep.comp_desc->bmAttributes &&
+		 !get_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel))
+		set_ss_streamid(f_usb30->register_base_address,
+				 ep_channel, request->req.stream_id);
+
+	/* set dma transfer source address */
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	dma_addr = f_usb30->dma_buffer[endpoint->dma_channel];
+#else
+	dma_addr = request->req.dma;
+#endif
+
+	/* setup DMA transfer */
+	set_dma_controller(endpoint, (unsigned long)dma_addr,
+			 f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (unsigned long) endpoint->ss_dma_fifo_addr :
+			 (unsigned long) endpoint->hs_dma_fifo_addr, bytes);
+
+	/* enable DMA transfer */
+	enable_dma_transfer(endpoint, 1);
+
+	return 1;
+#else
+	return 0;
+#endif
+}
+
+static unsigned char set_out_transfer_dma(struct f_usb30_ep *endpoint)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	dma_addr_t dma_addr;
+	unsigned long bytes;
+
+	/* check argument */
+	if (unlikely(!endpoint->enabled))
+		return 0;
+
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* check DMA transfer buffer mapping */
+	if (request->req.dma == F_USB30_DMA_ADDR_INVALID) {
+		/* map DMA transfer buffer and sync DMA transfer buffer */
+		request->req.dma = dma_map_single(f_usb30->gadget.dev.parent,
+						 request->req.buf,
+						 request->req.length,
+						 DMA_FROM_DEVICE);
+		request->dma_buffer_map = 1;
+	} else {
+		dma_sync_single_for_device(f_usb30->gadget.dev.parent,
+						 request->req.dma,
+						 request->req.length,
+						 DMA_FROM_DEVICE);
+		request->dma_buffer_map = 0;
+	}
+#endif
+	/* calculate this time transfer byte */
+	bytes = request->req.length < F_USB30_DMAC_TRANS_MAX_BYTES ?
+		 request->req.length : F_USB30_DMAC_TRANS_MAX_BYTES;
+
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u DMA is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+	/* set total data size and StreamID if stream transfer enabled */
+	if ((f_usb30->gadget.speed == USB_SPEED_SUPER) &&
+		 endpoint->ep.comp_desc->bmAttributes &&
+		 !get_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel)) {
+		set_ss_stcnt(f_usb30->register_base_address,
+				 ep_channel, bytes);
+		set_ss_streamid(f_usb30->register_base_address,
+				 ep_channel, request->req.stream_id);
+	}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	dma_addr = f_usb30->dma_buffer[endpoint->dma_channel];
+#else
+	dma_addr = request->req.dma;
+#endif
+
+	/* setup DMA transfer */
+	set_dma_controller(endpoint, f_usb30->gadget.speed ==
+			 USB_SPEED_SUPER ?
+			 (unsigned long) endpoint->ss_dma_fifo_addr :
+			 (unsigned long) endpoint->hs_dma_fifo_addr,
+			 (unsigned long) dma_addr, bytes);
+
+	/* enable DMA transfer */
+	enable_dma_transfer(endpoint, 1);
+
+	return 1;
+#else
+	return 0;
+#endif
+}
+
+static void initialize_endpoint_hw(struct f_usb30_ep *,
+	 unsigned char, unsigned char);
+static void abort_in_transfer_dma(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	struct f_usb30_request *request = endpoint->req;
+#endif
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+
+	/* disable DMA transfer */
+	enable_dma_transfer(endpoint, 0);
+
+	/* stop stream transfer */
+	if ((f_usb30->gadget.speed == USB_SPEED_SUPER) &&
+		 endpoint->ep.comp_desc->bmAttributes &&
+		 get_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel))
+		set_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel, 0);
+
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* check DMA transfer buffer unmap */
+	if ((endpoint->dma_transfer) &&
+		 (request->req.dma != F_USB30_DMA_ADDR_INVALID)) {
+		if (request->dma_buffer_map) {
+			/* unmap DMA trans buffer and sync DMA trans buffer */
+			dma_unmap_single(f_usb30->gadget.dev.parent,
+					 request->req.dma,
+					 request->req.length,
+					 DMA_TO_DEVICE);
+			request->req.dma = F_USB30_DMA_ADDR_INVALID;
+			request->dma_buffer_map = 0;
+		} else {
+			dma_sync_single_for_cpu(f_usb30->gadget.dev.parent,
+						 request->req.dma,
+						 request->req.length,
+						 DMA_TO_DEVICE);
+		}
+	}
+#endif
+	/* disable IntEmpty interrupt */
+	f_usb30->gadget.speed == USB_SPEED_SUPER ?
+		 set_ss_mskempty(f_usb30->register_base_address,
+				 ep_channel, 1) :
+		 set_hs_mskempty(f_usb30->register_base_address,
+				 ep_channel, 1);
+
+	if (!initialize)
+		return;
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 1, 0);
+#endif
+	return;
+}
+
+static void abort_out_transfer_dma(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	struct f_usb30_request *request = endpoint->req;
+#endif
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+
+	/* disable DMA transfer */
+	enable_dma_transfer(endpoint, 0);
+
+	/* stop stream transfer */
+	if ((f_usb30->gadget.speed == USB_SPEED_SUPER) &&
+		 endpoint->ep.comp_desc->bmAttributes &&
+		 get_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel))
+		set_ss_streamactive(f_usb30->register_base_address,
+					 ep_channel, 0);
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* check DMA transfer buffer unmap */
+	if ((endpoint->dma_transfer) &&
+		 (request->req.dma != F_USB30_DMA_ADDR_INVALID)) {
+		if (request->dma_buffer_map) {
+			/* unmap DMA trans buffer and sync DMA trans buffer */
+			dma_unmap_single(f_usb30->gadget.dev.parent,
+					 request->req.dma,
+					 request->req.length,
+					 DMA_FROM_DEVICE);
+			request->req.dma = F_USB30_DMA_ADDR_INVALID;
+			request->dma_buffer_map = 0;
+		} else {
+			dma_sync_single_for_cpu(f_usb30->gadget.dev.parent,
+						 request->req.dma,
+						 request->req.length,
+						 DMA_FROM_DEVICE);
+		}
+	}
+#endif
+	if (!initialize)
+		return;
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 0, 1);
+#endif
+	return;
+}
+
+static unsigned char end_in_transfer_dma(struct f_usb30_ep *endpoint)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+	dma_addr_t dma_addr;
+	unsigned long bytes;
+
+	/* check empty wait */
+	if ((f_usb30->gadget.speed == USB_SPEED_SUPER ?
+		 (!get_ss_mskempty(base_addr, ep_channel)) :
+		 (!get_hs_mskempty(base_addr, ep_channel)))) {
+		/* complete request */
+		abort_in_transfer_dma(endpoint, 0);
+		return 1;
+	}
+
+	/* check transfer remain byte */
+	if (request->req.length == request->req.actual) {
+#if defined(CONFIG_USB_GADGET_F_USB30_BULK_IN_END_NOTIFY_TIMING_TO_HOST)
+		/* check bulk transfer type */
+		if (endpoint->attributes == USB_ENDPOINT_XFER_BULK) {
+			/* clear & enable IntEmpty interrupt */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 clear_ss_intempty(base_addr, ep_channel) :
+				 clear_hs_intempty(base_addr, ep_channel);
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskempty(base_addr, ep_channel, 0) :
+				 set_hs_mskempty(base_addr, ep_channel, 0);
+			return 0;
+		}
+#endif
+		/* complete request */
+		abort_in_transfer_dma(endpoint, 0);
+		return 1;
+	}
+
+	/* calculate this time transfer byte */
+	bytes = (request->req.length - request->req.actual) <
+		 F_USB30_DMAC_TRANS_MAX_BYTES ?
+		 request->req.length - request->req.actual :
+		 F_USB30_DMAC_TRANS_MAX_BYTES;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u DMA is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* copy IN transfer data to noncachable buffer */
+	memcpy(f_usb30->buffer[endpoint->dma_channel],
+				 request->req.buf + request->req.actual, bytes);
+
+	dma_addr = f_usb30->dma_buffer[endpoint->dma_channel];
+#else
+	dma_addr = request->req.dma + request->req.actual;
+#endif
+
+	/* update actual byte */
+	request->req.actual += bytes;
+
+	/* setup DMA transfer */
+	set_dma_controller(endpoint, (unsigned long) dma_addr,
+			 f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (unsigned long) endpoint->ss_dma_fifo_addr :
+			 (unsigned long) endpoint->hs_dma_fifo_addr, bytes);
+
+	/* enable DMA transfer */
+	enable_dma_transfer(endpoint, 1);
+#endif
+	return 0;
+}
+
+static unsigned char end_out_transfer_dma(struct f_usb30_ep *endpoint)
+{
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+	dma_addr_t dma_addr;
+	unsigned long bytes;
+
+	/* get this time transfer byte */
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		bytes = get_ss_tcnt(base_addr, ep_channel);
+	} else {
+		bytes = get_hs_tcnt(base_addr, ep_channel);
+		if (bytes == (unsigned long) -1)
+			bytes = (request->req.length - request->req.actual) <
+				F_USB30_DMAC_TRANS_MAX_BYTES ?
+				(request->req.length - request->req.actual) :
+				F_USB30_DMAC_TRANS_MAX_BYTES;
+		else
+			bytes = request->req.length - bytes;
+	}
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* copy OUT transfer data from noncachable buffer */
+	memcpy(request->req.buf + request->req.actual,
+				 f_usb30->buffer[endpoint->dma_channel], bytes);
+#endif
+	/* update actual bytes */
+	request->req.actual += bytes;
+
+	/* check transfer request complete */
+	if ((request->req.length <= request->req.actual) ||
+		 (bytes % endpoint->ep.maxpacket) || (!bytes)) {
+		/* complete request */
+		abort_out_transfer_dma(endpoint, 0);
+		return 1;
+	}
+
+	/* calculate this time transfer byte */
+	bytes = (request->req.length - request->req.actual) <
+		 F_USB30_DMAC_TRANS_MAX_BYTES ?
+		 (request->req.length - request->req.actual) :
+		 F_USB30_DMAC_TRANS_MAX_BYTES;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u DMA is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	dma_addr = f_usb30->dma_buffer[endpoint->dma_channel];
+#else
+	dma_addr = request->req.dma + request->req.actual;
+#endif
+
+	/* setup DMA transfer */
+	set_dma_controller(endpoint,
+			 f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (unsigned long) endpoint->ss_dma_fifo_addr :
+			 (unsigned long) endpoint->hs_dma_fifo_addr,
+			 (unsigned long) dma_addr, bytes);
+
+	/* enable DMA transfer */
+	enable_dma_transfer(endpoint, 1);
+#endif
+	return 0;
+}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+static void on_end_in_transfer(struct f_usb30_ep *);
+static void on_end_out_transfer(struct f_usb30_ep *);
+static irqreturn_t on_dma_transfer_contoller(int irq, void *dev_id)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = dev_id;
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned char *channel = &f_usb30->dma_ep[0];
+
+	/* check argument */
+	if (unlikely(!dev_id))
+		return IRQ_NONE;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+							 __func__);
+
+	/* DMA controller interrupt request assert check */
+	if (unlikely((irq != f_usb30->dmac_in_irq) &&
+			 (irq != f_usb30->dmac_out_irq))) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"%s() is ended at non process.\n", __func__);
+		return IRQ_NONE;
+	}
+
+	/* get spin lock */
+	spin_lock(&f_usb30->lock);
+
+	for (counter = 0; counter < F_USB30_MAX_DMAC; counter++) {
+		if (endpoint[channel[counter]].ep.address & USB_DIR_IN) {
+			/* DMA IN transfer end interrupt factor */
+			if ((get_intusbfin(base_addr, counter)) &&
+				 (!get_mskintusbfin(base_addr, counter))) {
+				/* clear UsbFin interrupt factor */
+				clear_intusbfin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "UsbFin%u interrrupt occurred.\n",
+						 (unsigned int) counter);
+
+				/* process IN transfer end */
+				on_end_in_transfer(&endpoint[channel[counter]]);
+			} else if ((get_intstarterr(base_addr, counter)) &&
+				 (!get_mskintstarterr(base_addr, counter))) {
+				/* clear StartErr interrupt factor */
+				clear_intstarterr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "StartErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort IN transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intslverr(base_addr, counter)) &&
+				 (!get_mskintslverr(base_addr, counter))) {
+				/* clear SlvErr interrupt factor */
+				clear_intslverr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "SlvErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort IN transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intdecerr(base_addr, counter)) &&
+				 (!get_mskintdecerr(base_addr, counter))) {
+				/* clear DecErr interrupt factor */
+				clear_intdecerr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "DecErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort IN transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intuexdreq(base_addr, counter)) &&
+				 (!get_mskintuexdreq(base_addr, counter))) {
+				/* clear UexDreq interrupt factor */
+				clear_intuexdreq(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "UexDreq%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort IN transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intaxifin(base_addr, counter)) &&
+				 (!get_mskintaxifin(base_addr, counter))) {
+				/* clear AXIFin interrupt factor */
+				clear_intaxifin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "AXIFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else if ((get_intexokay(base_addr, counter)) &&
+				 (!get_mskintexokay(base_addr, counter))) {
+				/* clear ExOkay interrupt factor */
+				clear_intexokay(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "ExOkay%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else if ((get_intremainfin(base_addr, counter)) &&
+				 (!get_mskintremainfin(base_addr, counter))) {
+				/* clear RemainFin interrupt factor */
+				clear_intremainfin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "RemainFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else if ((get_intnull(base_addr, counter)) &&
+				 (!get_mskintnull(base_addr, counter))) {
+				/* clear RemainFin interrupt factor */
+				clear_intnull(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "RemainFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else {
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "Other interrrupt occurred.\n");
+			}
+		} else {
+			/* DMA OUT transfer end interrupt factor */
+			if ((get_intaxifin(base_addr, counter)) &&
+				 (!get_mskintaxifin(base_addr, counter))) {
+				/* clear AXIFin interrupt factor */
+				clear_intaxifin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "AXIFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* process OUT transfer end */
+				on_end_out_transfer(
+						&endpoint[channel[counter]]);
+			} else if ((get_intusbover(base_addr, counter)) &&
+				 (!get_mskintusbover(base_addr, counter))) {
+				/* clear UsbOver interrupt factor */
+				clear_intusbover(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "UsbOver%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* process OUT transfer end */
+				on_end_out_transfer(
+						&endpoint[channel[counter]]);
+			} else if ((get_intusbunder(base_addr, counter)) &&
+				 (!get_mskintusbunder(base_addr, counter))) {
+				/* clear UsbUnder interrupt factor */
+				clear_intusbunder(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "UsbUnder%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort OUT transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intnull(base_addr, counter)) &&
+				 (!get_mskintnull(base_addr, counter))) {
+				/* clear Null interrupt factor */
+				clear_intnull(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "Null%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* process OUT transfer end */
+				on_end_out_transfer(
+						&endpoint[channel[counter]]);
+			} else if ((get_intstarterr(base_addr, counter)) &&
+				 (!get_mskintstarterr(base_addr, counter))) {
+				/* clear StartErr interrupt factor */
+				clear_intstarterr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "StartErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort OUT transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intslverr(base_addr, counter)) &&
+				 (!get_mskintslverr(base_addr, counter))) {
+				/* clear SlvErr interrupt factor */
+				clear_intslverr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "SlvErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort OUT transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intdecerr(base_addr, counter)) &&
+				 (!get_mskintdecerr(base_addr, counter))) {
+				/* clear DecErr interrupt factor */
+				clear_intdecerr(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "DecErr%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort OUT transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intuexdreq(base_addr, counter)) &&
+				 (!get_mskintuexdreq(base_addr, counter))) {
+				/* clear UexDreq interrupt factor */
+				clear_intuexdreq(base_addr, counter);
+
+				dev_err(f_usb30->gadget.dev.parent,
+					 "UexDreq%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+
+				/* abort OUT transfer */
+				set_dmaabort(base_addr, counter, 1);
+			} else if ((get_intusbfin(base_addr, counter)) &&
+				 (!get_mskintusbfin(base_addr, counter))) {
+				/* clear UsbFin interrupt factor */
+				clear_intusbfin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "UsbFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else if ((get_intexokay(base_addr, counter)) &&
+				 (!get_mskintexokay(base_addr, counter))) {
+				/* clear ExOkay interrupt factor */
+				clear_intexokay(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "ExOkay%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else if ((get_intremainfin(base_addr, counter)) &&
+				 (!get_mskintremainfin(base_addr, counter))) {
+				/* clear RemainFin interrupt factor */
+				clear_intremainfin(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "RemainFin%u interrrupt occurred.\n",
+					 (unsigned int) counter);
+			} else {
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "Other interrrupt occurred.\n");
+			}
+		}
+	}
+
+	/* release spin lock */
+	spin_unlock(&f_usb30->lock);
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n", __func__);
+
+	return IRQ_HANDLED;
+}
+#endif
+
+static void initialize_controller(struct f_usb30_udc *f_usb30, int ss)
+{
+	unsigned long counter;
+	unsigned long alt_counter;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* initialize F_USB30 DMA controller register */
+	initialize_dma_controller(f_usb30);
+
+	/* initialize F_USB30 USB3.0 controller register */
+	if (ss) {
+		/* initialize F_USB30 SS device control/status register */
+		set_ss_selfpw(base_addr, 1);
+		set_ss_mskfncsusp(base_addr, 1);
+		set_ss_msku2inactto(base_addr, 1);
+		set_ss_msksetup(base_addr, 1);
+		set_ss_msksetconf(base_addr, 1);
+		set_ss_msksuspendb(base_addr, 1);
+		set_ss_msksuspende(base_addr, 1);
+		set_ss_mskpolltou0(base_addr, 1);
+		set_ss_mskenterpoll(base_addr, 1);
+		set_ss_mskssdisable(base_addr, 1);
+		set_ss_mskvdtest(base_addr, 1);
+		set_ss_msksshrstb(base_addr, 1);
+		set_ss_msksshrste(base_addr, 1);
+		set_ss_msksswrstb(base_addr, 1);
+		set_ss_msksswrste(base_addr, 1);
+		clear_ss_intfncsusp(base_addr);
+		clear_ss_intu2inactto(base_addr);
+		clear_ss_intsetup(base_addr);
+		clear_ss_intsetconf(base_addr);
+		clear_ss_intsuspendb(base_addr);
+		clear_ss_intsuspende(base_addr);
+		clear_ss_intpolltou0(base_addr);
+		clear_ss_intenterpoll(base_addr);
+		clear_ss_intssdisable(base_addr);
+		clear_ss_intvdtest(base_addr);
+		clear_ss_intsshrstb(base_addr);
+		clear_ss_intsshrste(base_addr);
+		clear_ss_intsswrstb(base_addr);
+		clear_ss_intsswrste(base_addr);
+
+		/* disable F_USB30 SS U1/U2 status auto entry */
+		set_ss_cnt_u1inactto(base_addr, 0xFF);
+		set_ss_cnt_u2inactto(base_addr, 0xFF);
+		set_ss_blki_u1inactto(base_addr, 0xFF);
+		set_ss_blki_u2inactto(base_addr, 0xFF);
+		set_ss_blko_u1inactto(base_addr, 0xFF);
+		set_ss_blko_u2inactto(base_addr, 0xFF);
+
+		/* set to reject U1/U2 status entry request from HOST  */
+		set_ss_ssovrden(base_addr, 1);
+		set_ss_rjct_rcvu1(base_addr, 1);
+		set_ss_rjct_rcvu2(base_addr, 1);
+		set_ss_ssovrden(base_addr, 0);
+
+		/* initialize F_USB30 SS ep0 control/status controller reg */
+		set_ss_reqstall0(base_addr, 0);
+		set_ss_nrdyresp0(base_addr, 0);
+		set_ss_rewdifo0(base_addr, 0);
+		set_ss_mskep(base_addr, ENDPOINT0, 1);
+		set_ss_mskready0i(base_addr, 1);
+		set_ss_mskready0o(base_addr, 1);
+		set_ss_mskpktpnd0(base_addr, 1);
+		set_ss_mskstalled0(base_addr, 1);
+		set_ss_msknrdy0(base_addr, 1);
+		set_ss_mskclstall0(base_addr, 1);
+		clear_ss_intready0i(base_addr);
+		clear_ss_intready0o(base_addr);
+		clear_ss_intpktpnd0(base_addr);
+		clear_ss_intstalled0(base_addr);
+		clear_ss_intnrdy0(base_addr);
+		clear_ss_intclstall0(base_addr);
+
+		/* initialize F_USB30 SS epx control/status controller reg */
+		for (counter = ENDPOINT1;
+			 counter < F_USB30_MAX_EP; counter++) {
+			set_ss_reqstall(base_addr, counter, 0);
+			set_ss_rewdifo(base_addr, counter, 0);
+			set_ss_stalldis(base_addr, counter, 0);
+			set_ss_inistall(base_addr, counter, 0);
+			set_ss_nrdyresp(base_addr, counter, 0);
+			set_ss_enspr(base_addr, counter, 0);
+			set_ss_enspd(base_addr, counter, 0);
+			set_ss_dmamode(base_addr, counter, 0);
+			set_ss_mskdmareq(base_addr, counter, 1);
+			set_ss_mskep(base_addr, counter, 1);
+			set_ss_mskspr(base_addr, counter, 1);
+			set_ss_mskspd(base_addr, counter, 1);
+			set_ss_mskclstream(base_addr, counter, 1);
+			set_ss_mskready(base_addr, counter, 1);
+			set_ss_mskpktpnd(base_addr, counter, 1);
+			set_ss_msksdend(base_addr, counter, 1);
+			set_ss_mskdend(base_addr, counter, 1);
+			set_ss_mskempty(base_addr, counter, 1);
+			set_ss_mskstalled(base_addr, counter, 1);
+			set_ss_msknrdy(base_addr, counter, 1);
+			set_ss_mskclstall(base_addr, counter, 1);
+			clear_ss_intspr(base_addr, counter);
+			clear_ss_intspd(base_addr, counter);
+			clear_ss_intclstream(base_addr, counter);
+			clear_ss_intready(base_addr, counter);
+			clear_ss_intpktpnd(base_addr, counter);
+			clear_ss_intsdend(base_addr, counter);
+			clear_ss_intdend(base_addr, counter);
+			clear_ss_intempty(base_addr, counter);
+			clear_ss_intstalled(base_addr, counter);
+			clear_ss_intnrdy(base_addr, counter);
+			clear_ss_intclstall(base_addr, counter);
+		}
+
+		/* initialize F_USB30 SS intf/alt controller register */
+		set_ss_numintf(base_addr, F_USB30_MAX_INTF);
+		set_ss_firstintf(base_addr, INTERFACE0, 1);
+		set_ss_numaintf(base_addr, INTERFACE0, 1);
+		clear_ss_achg(base_addr, INTERFACE0);
+		for (counter = INTERFACE1;
+				 counter < F_USB30_MAX_INTF; counter++) {
+			set_ss_firstintf(base_addr, counter, 0);
+			set_ss_numaintf(base_addr, counter, 1);
+			clear_ss_achg(base_addr, counter);
+		}
+
+		/* initialize F_USB30 SS endpoint config controller reg */
+		set_ss_configwren(base_addr, 1);
+		for (counter = F_USB30_MAX_EP - 1;
+				 counter > ENDPOINT0; counter--) {
+			set_ss_epnum(base_addr, counter, counter);
+			set_ss_epconf(base_addr, counter, 1);
+			set_ss_intf(base_addr, counter, 1);
+			set_ss_altmap(base_addr, counter, 1);
+			set_ss_numpmode(base_addr, counter, 0);
+			set_ss_diseob(base_addr, counter, 0);
+			set_ss_maxburst(base_addr, counter, 7);
+		}
+		set_ss_configwren(base_addr, 0);
+
+	} else {
+		/* initialize F_USB30 HS cpu access controller register */
+		set_hs_softreset(base_addr);
+
+		/* select High Speed or Full Speed by Gadget Driver */
+		set_hs_reqspeed(base_addr,
+				 gadget_is_dualspeed(&f_usb30->gadget) ?
+				 REQ_SPEED_HIGH_SPEED :
+				 REQ_SPEED_FULL_SPEED);
+
+		/* initialize HS/FS device control/status register */
+		set_hs_reqresume(base_addr, 0);
+		set_hs_enrmtwkup(base_addr, 1);
+		set_hs_selfpwr(base_addr, 1);
+		set_hs_physusp(base_addr, 0);
+		set_hs_pmode(base_addr, 0);
+		set_hs_lmode(base_addr, 0);
+		set_hs_sofsel(base_addr, 0);
+		set_hs_lpbkphy(base_addr, 0);
+		set_hs_fscalib(base_addr, 3);
+		set_hs_hscalib(base_addr, 2);
+		set_hs_mskerraticerr(base_addr, 1);
+		set_hs_msksof(base_addr, 1);
+		set_hs_mskusbrstb(base_addr, 1);
+		set_hs_mskusbrste(base_addr, 1);
+		set_hs_msksuspendb(base_addr, 1);
+		set_hs_msksuspende(base_addr, 1);
+		set_hs_msksetup(base_addr, 1);
+		set_hs_msksetconf(base_addr, 1);
+		clear_hs_interraticerr(base_addr);
+		clear_hs_intsof(base_addr);
+		clear_hs_intusbrstb(base_addr);
+		clear_hs_intusbrste(base_addr);
+		clear_hs_intsuspendb(base_addr);
+		clear_hs_intsuspende(base_addr);
+		clear_hs_intsetup(base_addr);
+		clear_hs_intsetconf(base_addr);
+
+		/* initialize HS/FS ep0 control/status controller reg */
+		set_hs_testmode0(base_addr, 0);
+		set_hs_reqstall0(base_addr, 0);
+		set_hs_seltx0i(base_addr, 0);
+		set_hs_seltx0o(base_addr, 0);
+		set_hs_mskep(base_addr, ENDPOINT0, 1);
+		set_hs_mskready0i(base_addr, 1);
+		set_hs_mskready0o(base_addr, 1);
+		set_hs_mskping0o(base_addr, 1);
+		set_hs_mskstalled0(base_addr, 1);
+		set_hs_msknack0(base_addr, 1);
+		set_hs_mskclstall0(base_addr, 1);
+		clear_hs_intready0i(base_addr);
+		clear_hs_intready0o(base_addr);
+		clear_hs_intping0o(base_addr);
+		clear_hs_intstalled0(base_addr);
+		clear_hs_intnack0(base_addr);
+		clear_hs_intclstall0(base_addr);
+
+		/* initialize HS/FS epx control/status controller reg */
+		for (counter = ENDPOINT1; counter < F_USB30_MAX_EP;
+			 counter++) {
+			set_hs_reqstall(base_addr, counter, 0);
+			set_hs_nackresp(base_addr, counter, 1);
+			set_hs_nullresp(base_addr, counter, 0);
+			set_hs_toggledis(base_addr, counter, 0);
+			set_hs_stalldis(base_addr, counter, 0);
+			set_hs_seltx(base_addr, counter, 0);
+			set_hs_enspr(base_addr, counter, 0);
+			set_hs_enspdd(base_addr, counter, 0);
+			set_hs_mskdmareq(base_addr, counter, 1);
+			set_hs_dmamode(base_addr, counter, 0);
+			set_hs_mskep(base_addr, counter, 1);
+			set_hs_mskspr(base_addr, counter, 1);
+			set_hs_mskspdd(base_addr, counter, 1);
+			set_hs_mskready(base_addr, counter, 1);
+			set_hs_mskping(base_addr, counter, 1);
+			set_hs_mskachgif(base_addr, counter, 1);
+			set_hs_mskdend(base_addr, counter, 1);
+			set_hs_mskempty(base_addr, counter, 1);
+			set_hs_mskstalled(base_addr, counter, 1);
+			set_hs_msknack(base_addr, counter, 1);
+			set_hs_mskclstall(base_addr, counter, 1);
+			clear_hs_intspr(base_addr, counter);
+			clear_hs_intspdd(base_addr, counter);
+			clear_hs_intready(base_addr, counter);
+			clear_hs_intping(base_addr, counter);
+			clear_hs_intachgif(base_addr, counter);
+			clear_hs_intdend(base_addr, counter);
+			clear_hs_intempty(base_addr, counter);
+			clear_hs_intstalled(base_addr, counter);
+			clear_hs_intnack(base_addr, counter);
+			clear_hs_intclstall(base_addr, counter);
+		}
+
+		/* initialize HS/FS interface/alternate controller register */
+		set_hs_numintf(base_addr, 0);
+		set_hs_testmodeif(base_addr, 0);
+		for (counter = INTERFACE0;
+			 counter < F_USB30_MAX_INTF; counter++) {
+			set_hs_numaltintf(base_addr, counter, 0);
+			set_hs_mskachgif(base_addr, counter, 1);
+			clear_hs_intachgif(base_addr, counter);
+		}
+
+		/* initialize HS/FS endpoint config controller reg */
+		set_hs_configwren(base_addr, 1);
+		set_hs_makeupdata(base_addr);
+		for (counter = F_USB30_MAX_EP - 1;
+			 counter > ENDPOINT0; counter--) {
+			for (alt_counter = 0; alt_counter < F_USB30_MAX_ALT;
+				 alt_counter++) {
+				set_hs_epnum(base_addr, counter,
+						alt_counter, 0);
+				set_hs_io(base_addr, counter, alt_counter, 0);
+				set_hs_type(base_addr, counter, alt_counter,
+						 TYPE_UNUSED);
+				set_hs_conf(base_addr, counter, alt_counter, 0);
+				set_hs_intf(base_addr, counter, alt_counter, 0);
+				set_hs_alt(base_addr, counter, alt_counter, 0);
+				set_hs_size(base_addr, counter, alt_counter, 0);
+				set_hs_numtr(base_addr, counter, alt_counter);
+			}
+		}
+		set_hs_epnum(base_addr, ENDPOINT0, 0, 0);
+		set_hs_io(base_addr, ENDPOINT0, 0, 0);
+		set_hs_type(base_addr, ENDPOINT0, 0, TYPE_UNUSED);
+		set_hs_conf(base_addr, ENDPOINT0, 0, 0);
+		set_hs_intf(base_addr, ENDPOINT0, 0, 0);
+		set_hs_alt(base_addr, ENDPOINT0, 0, 0);
+		set_hs_size(base_addr, ENDPOINT0, 0, 0);
+		set_hs_numtr(base_addr, ENDPOINT0, 0);
+		set_hs_configwren(base_addr, 0);
+
+		/* wait PHY reset release */
+		for (counter = 0xffff; ((counter) &&
+				 (get_hs_phyreset(base_addr))); counter--)
+			;
+	}
+	return;
+}
+
+static void initialize_endpoint_hw(struct f_usb30_ep *endpoint,
+	 unsigned char in_fifo, unsigned char out_fifo)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		if (ep_channel == ENDPOINT0) {
+			if (in_fifo) {
+				/* check new SETUP transfer or reset */
+				if ((get_ss_intsetup(base_addr)) ||
+					 (get_ss_intsshrstb(base_addr)) ||
+					 (get_ss_intsswrstb(base_addr)))
+					in_fifo = 0;
+				else
+					/* initialize IN FIFO */
+					set_ss_init0i(base_addr);
+			}
+
+			if (out_fifo) {
+				/* check new SETUP transfer or reset */
+				if ((get_ss_intsetup(base_addr)) ||
+					 (get_ss_intsshrstb(base_addr)) ||
+					 (get_ss_intsswrstb(base_addr)))
+					out_fifo = 0;
+				else
+					/* initialize OUT FIFO */
+					set_ss_init0o(base_addr);
+			}
+
+			/* initialize endpoint control / status register */
+			set_ss_nrdyresp0(base_addr, 0);
+			set_ss_reqstall0(base_addr, 0);
+			if (in_fifo) {
+				clear_ss_intready0i(base_addr);
+				set_ss_mskready0i(base_addr, 1);
+			}
+			if (out_fifo)
+				set_ss_mskready0o(base_addr, 1);
+			set_ss_mskpktpnd0(base_addr, 1);
+			set_ss_mskstalled0(base_addr, 0);
+			set_ss_msknrdy0(base_addr, 1);
+			set_ss_mskclstall0(base_addr, 0);
+			set_ss_mskep(base_addr, ep_channel, 0);
+		} else {
+			/* check endpoint transfer direction */
+			if (((endpoint->ep.address & USB_DIR_IN) &&
+				 (in_fifo)) || ((!(endpoint->ep.address &
+				 USB_DIR_IN)) && (out_fifo)))
+				/* initialize FIFO */
+				set_ss_init(base_addr, ep_channel);
+
+			/* initialize endpoint control / status register */
+			set_ss_rewdifo(base_addr, ep_channel, 0);
+			set_ss_stalldis(base_addr, ep_channel, 0);
+			set_ss_inistall(base_addr, ep_channel, 0);
+			set_ss_nrdyresp(base_addr, ep_channel, 0);
+			set_ss_enspr(base_addr, ep_channel, 0);
+			set_ss_enspd(base_addr, ep_channel, 0);
+			set_ss_mskdmareq(base_addr, ep_channel, 1);
+			set_ss_dmamode(base_addr, ep_channel, 0);
+			clear_ss_intready(base_addr, ep_channel);
+			clear_ss_intempty(base_addr, ep_channel);
+			set_ss_mskep(base_addr, ep_channel, 0);
+			set_ss_mskspr(base_addr, ep_channel, 1);
+			set_ss_mskspd(base_addr, ep_channel, 1);
+			set_ss_mskclstream(base_addr, ep_channel, 1);
+			set_ss_mskready(base_addr, ep_channel, 1);
+			set_ss_mskpktpnd(base_addr, ep_channel, 1);
+			set_ss_msksdend(base_addr, ep_channel, 1);
+			set_ss_mskdend(base_addr, ep_channel, 1);
+			set_ss_mskempty(base_addr, ep_channel, 1);
+			set_ss_mskstalled(base_addr, ep_channel, 0);
+			set_ss_msknrdy(base_addr, ep_channel, 1);
+			set_ss_mskclstall(base_addr, ep_channel, 0);
+		}
+	} else {
+		if (ep_channel == ENDPOINT0) {
+			if (in_fifo) {
+				/* check new SETUP transfer or bus reset */
+				if ((get_hs_intsetup(base_addr)) ||
+					 (get_hs_intusbrstb(base_addr)))
+					in_fifo = 0;
+				else
+					/* initialize IN FIFO */
+					set_hs_init0i(base_addr);
+			}
+
+			if (out_fifo) {
+				/* check new SETUP transfer or bus reset */
+				if ((get_hs_intsetup(base_addr)) ||
+					 (get_hs_intusbrstb(base_addr)))
+					out_fifo = 0;
+				else
+					/* initialize OUT FIFO */
+					set_hs_init0o(base_addr);
+			}
+
+			/* initialize endpoint control / status register */
+			set_hs_testmode0(base_addr, 0);
+			set_hs_reqstall0(base_addr, 0);
+			set_hs_seltx0i(base_addr, 0);
+			set_hs_seltx0o(base_addr, 0);
+			if (in_fifo) {
+				clear_hs_intready0i(base_addr);
+				set_hs_mskready0i(base_addr, 1);
+			}
+			if (out_fifo)
+				set_hs_mskready0o(base_addr, 1);
+			set_hs_mskping0o(base_addr, 1);
+			set_hs_mskstalled0(base_addr, 0);
+			set_hs_msknack0(base_addr, 1);
+			set_hs_mskclstall0(base_addr, 0);
+			set_hs_mskep(base_addr, ep_channel, 0);
+		} else {
+			/* check endpoint transfer direction */
+			if (((endpoint->ep.address & USB_DIR_IN) &&
+				 (in_fifo)) || ((!(endpoint->ep.address &
+				  USB_DIR_IN)) && (out_fifo)))
+				/* initialize FIFO */
+				set_hs_init(base_addr, ep_channel);
+
+			/* initialize endpoint control / status register */
+			set_hs_nackresp(base_addr, ep_channel, 0);
+			set_hs_nullresp(base_addr, ep_channel, 0);
+			set_hs_toggledis(base_addr, ep_channel, 0);
+			set_hs_stalldis(base_addr, ep_channel, 0);
+			set_hs_seltx(base_addr, ep_channel, 0);
+			set_hs_enspr(base_addr, ep_channel, 0);
+			set_hs_enspdd(base_addr, ep_channel, 0);
+			set_hs_mskdmareq(base_addr, ep_channel, 1);
+			set_hs_dmamode(base_addr, ep_channel, 0);
+			clear_hs_intready(base_addr, ep_channel);
+			clear_hs_intempty(base_addr, ep_channel);
+			set_hs_mskready(base_addr, ep_channel, 1);
+			set_hs_mskspr(base_addr, ep_channel, 1);
+			set_hs_mskspdd(base_addr, ep_channel, 1);
+			set_hs_mskping(base_addr, ep_channel, 1);
+			set_hs_mskachgif(base_addr, ep_channel, 1);
+			set_hs_mskempty(base_addr, ep_channel, 1);
+			set_hs_mskstalled(base_addr, ep_channel, 0);
+			set_hs_msknack(base_addr, ep_channel, 1);
+			set_hs_mskclstall(base_addr, ep_channel, 0);
+			set_hs_mskep(base_addr, ep_channel, 0);
+		}
+	}
+
+	return;
+}
+
+static void initialize_endpoint_configure(struct f_usb30_udc *f_usb30)
+{
+	static unsigned char ep_type[] = {
+		F_USB30_TRANS_TYPE_EP0,
+		F_USB30_TRANS_TYPE_EP1,
+		F_USB30_TRANS_TYPE_EP2,
+		F_USB30_TRANS_TYPE_EP3,
+		F_USB30_TRANS_TYPE_EP4,
+		F_USB30_TRANS_TYPE_EP5,
+		F_USB30_TRANS_TYPE_EP6,
+		F_USB30_TRANS_TYPE_EP7,
+		F_USB30_TRANS_TYPE_EP8,
+		F_USB30_TRANS_TYPE_EP9,
+		F_USB30_TRANS_TYPE_EP10,
+		F_USB30_TRANS_TYPE_EP11,
+		F_USB30_TRANS_TYPE_EP12,
+		F_USB30_TRANS_TYPE_EP13,
+		F_USB30_TRANS_TYPE_EP14,
+		F_USB30_TRANS_TYPE_EP15,
+	};
+	static unsigned char ep_dir[] = {
+		F_USB30_TRANS_DIR_EP0,
+		F_USB30_TRANS_DIR_EP1,
+		F_USB30_TRANS_DIR_EP2,
+		F_USB30_TRANS_DIR_EP3,
+		F_USB30_TRANS_DIR_EP4,
+		F_USB30_TRANS_DIR_EP5,
+		F_USB30_TRANS_DIR_EP6,
+		F_USB30_TRANS_DIR_EP7,
+		F_USB30_TRANS_DIR_EP8,
+		F_USB30_TRANS_DIR_EP9,
+		F_USB30_TRANS_DIR_EP10,
+		F_USB30_TRANS_DIR_EP11,
+		F_USB30_TRANS_DIR_EP12,
+		F_USB30_TRANS_DIR_EP13,
+		F_USB30_TRANS_DIR_EP14,
+		F_USB30_TRANS_DIR_EP15,
+	};
+	static signed char ep_intf[] = {
+		F_USB30_INTF_CHANNEL_EP0,
+		F_USB30_INTF_CHANNEL_EP1,
+		F_USB30_INTF_CHANNEL_EP2,
+		F_USB30_INTF_CHANNEL_EP3,
+		F_USB30_INTF_CHANNEL_EP4,
+		F_USB30_INTF_CHANNEL_EP5,
+		F_USB30_INTF_CHANNEL_EP6,
+		F_USB30_INTF_CHANNEL_EP7,
+		F_USB30_INTF_CHANNEL_EP8,
+		F_USB30_INTF_CHANNEL_EP9,
+		F_USB30_INTF_CHANNEL_EP10,
+		F_USB30_INTF_CHANNEL_EP11,
+		F_USB30_INTF_CHANNEL_EP12,
+		F_USB30_INTF_CHANNEL_EP13,
+		F_USB30_INTF_CHANNEL_EP14,
+		F_USB30_INTF_CHANNEL_EP15,
+	};
+	static const char * const ep_name[] = {
+		F_USB30_NAME_STRING_EP0,
+		F_USB30_NAME_STRING_EP1,
+		F_USB30_NAME_STRING_EP2,
+		F_USB30_NAME_STRING_EP3,
+		F_USB30_NAME_STRING_EP4,
+		F_USB30_NAME_STRING_EP5,
+		F_USB30_NAME_STRING_EP6,
+		F_USB30_NAME_STRING_EP7,
+		F_USB30_NAME_STRING_EP8,
+		F_USB30_NAME_STRING_EP9,
+		F_USB30_NAME_STRING_EP10,
+		F_USB30_NAME_STRING_EP11,
+		F_USB30_NAME_STRING_EP12,
+		F_USB30_NAME_STRING_EP13,
+		F_USB30_NAME_STRING_EP14,
+		F_USB30_NAME_STRING_EP15,
+	};
+	static signed char ep_dma_ch[] = {
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+		F_USB30_DMAC_CHANNEL_EP0,
+		F_USB30_DMAC_CHANNEL_EP1,
+		F_USB30_DMAC_CHANNEL_EP2,
+		F_USB30_DMAC_CHANNEL_EP3,
+		F_USB30_DMAC_CHANNEL_EP4,
+		F_USB30_DMAC_CHANNEL_EP5,
+		F_USB30_DMAC_CHANNEL_EP6,
+		F_USB30_DMAC_CHANNEL_EP7,
+		F_USB30_DMAC_CHANNEL_EP8,
+		F_USB30_DMAC_CHANNEL_EP9,
+		F_USB30_DMAC_CHANNEL_EP10,
+		F_USB30_DMAC_CHANNEL_EP11,
+		F_USB30_DMAC_CHANNEL_EP12,
+		F_USB30_DMAC_CHANNEL_EP13,
+		F_USB30_DMAC_CHANNEL_EP14,
+		F_USB30_DMAC_CHANNEL_EP15,
+#else
+		-1,		/* endpoint0 */
+		-1,		/* endpoint1 */
+		-1,		/* endpoint2 */
+		-1,		/* endpoint3 */
+		-1,		/* endpoint4 */
+		-1,		/* endpoint5 */
+		-1,		/* endpoint6 */
+		-1,		/* endpoint7 */
+		-1,		/* endpoint8 */
+		-1,		/* endpoint9 */
+		-1,		/* endpoint10 */
+		-1,		/* endpoint11 */
+		-1,		/* endpoint12 */
+		-1,		/* endpoint13 */
+		-1,		/* endpoint14 */
+		-1,		/* endpoint15 */
+#endif
+	};
+	static unsigned char intf_alt[] = {
+		F_USB30_ALT_INTF0,
+		F_USB30_ALT_INTF1,
+		F_USB30_ALT_INTF2,
+		F_USB30_ALT_INTF3,
+		F_USB30_ALT_INTF4,
+		F_USB30_ALT_INTF5,
+		F_USB30_ALT_INTF6,
+		F_USB30_ALT_INTF7,
+		F_USB30_ALT_INTF8,
+		F_USB30_ALT_INTF9,
+		F_USB30_ALT_INTF10,
+		F_USB30_ALT_INTF11,
+		F_USB30_ALT_INTF12,
+		F_USB30_ALT_INTF13,
+		F_USB30_ALT_INTF14,
+		F_USB30_ALT_INTF15,
+	};
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned long counter = 0;
+
+	/* initialzie endpoint 0 configure data */
+	endpoint[ENDPOINT0].ep.name = ep_name[ENDPOINT0];
+	endpoint[ENDPOINT0].ep.maxpacket =
+			 ep_fifo_size[counter][
+			f_usb30->gadget.speed == USB_SPEED_UNKNOWN ?
+			 USB_SPEED_SUPER : f_usb30->gadget.speed][0];
+	endpoint[ENDPOINT0].ep.address = 0;
+	endpoint[ENDPOINT0].attributes = USB_ENDPOINT_XFER_CONTROL;
+	endpoint[ENDPOINT0].interface_channel = -1;
+	endpoint[ENDPOINT0].alternate_channels = 0;
+	endpoint[ENDPOINT0].dma_channel = ep_dma_ch[ENDPOINT0];
+
+	for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++) {
+		/* initialzie endpoint configure data */
+		endpoint[counter].ep.name = ep_name[counter];
+		endpoint[counter].ep.address = counter | ep_dir[counter];
+		endpoint[counter].attributes = ep_type[counter];
+		endpoint[counter].interface_channel = ep_intf[counter];
+		endpoint[counter].alternate_channels = intf_alt[endpoint[
+						counter].interface_channel];
+		endpoint[counter].dma_channel = ep_dma_ch[counter];
+		endpoint[counter].ep.maxpacket =
+			ep_fifo_size[counter][
+			 f_usb30->gadget.speed == USB_SPEED_UNKNOWN ?
+			 USB_SPEED_SUPER : f_usb30->gadget.speed][0];
+	}
+
+	return;
+}
+
+static void initialize_endpoint(struct f_usb30_udc *f_usb30,
+	 const struct usb_ep_ops *udc_ep_ops)
+{
+	unsigned long counter = 0;
+
+	initialize_endpoint_configure(f_usb30);
+	f_usb30->gadget.ep0 = &f_usb30->udc_endpoint[0].ep;
+	INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+	INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+		endpoint[counter].ep.driver_data = NULL;
+		endpoint[counter].ep.ops = udc_ep_ops;
+		list_add_tail(&endpoint[counter].ep.ep_list,
+				 counter == ENDPOINT0 ?
+				 &f_usb30->gadget.ep0->ep_list :
+				 &f_usb30->gadget.ep_list);
+		endpoint[counter].udc = f_usb30;
+		endpoint[counter].enabled = 0;
+		endpoint[counter].req = NULL;
+		INIT_LIST_HEAD(&endpoint[counter].queue);
+		endpoint[counter].halt = 0;
+		endpoint[counter].force_halt = 0;
+		endpoint[counter].dma_transfer = 0;
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+		if (endpoint[counter].dma_channel != -1) {
+			if (endpoint[counter].ep.address & USB_DIR_IN) {
+				endpoint[counter].ss_dma_fifo_addr =
+				 (void __iomem *) get_ss_epinbuf_address(
+				     f_usb30->device_resource->start, counter);
+				endpoint[counter].hs_dma_fifo_addr =
+				 (void __iomem *) get_hs_epinbuf_address(
+				     f_usb30->device_resource->start, counter);
+			} else {
+				endpoint[counter].ss_dma_fifo_addr =
+				 (void __iomem *) get_ss_epoutbuf_address(
+				     f_usb30->device_resource->start, counter);
+				endpoint[counter].hs_dma_fifo_addr =
+				 (void __iomem *) get_hs_epoutbuf_address(
+				     f_usb30->device_resource->start, counter);
+			}
+		} else {
+			endpoint[counter].ss_dma_fifo_addr = NULL;
+			endpoint[counter].hs_dma_fifo_addr = NULL;
+		}
+#endif
+	}
+}
+
+static void configure_endpoint(struct f_usb30_udc *f_usb30)
+{
+	static const unsigned char transfer_type_register[] = {
+		TYPE_CONTROL,		/* control transfer */
+		TYPE_ISOCHRONOUS,	/* isochronout transfer */
+		TYPE_BULK,		/* bulk transfer */
+		TYPE_INTERRUPT,		/* interrupt transfer */
+	};
+	unsigned long counter;
+	unsigned long alt_counter;
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		/* enable endpoint configuration */
+		set_ss_configwren(base_addr, 1);
+
+		/* configure endpoint x */
+		for (counter = F_USB30_MAX_EP - 1;
+			 counter > ENDPOINT0; counter--) {
+			set_ss_epnum(base_addr, counter, counter);
+			set_ss_epconf(base_addr, counter, 1);
+			set_ss_intf(base_addr, counter,
+					 endpoint[counter].interface_channel);
+			set_ss_altmap(base_addr, counter, 1);
+			set_ss_numpmode(base_addr, counter, 1);
+			set_ss_diseob(base_addr, counter, 1);
+			set_ss_maxburst(base_addr, counter,
+					 endpoint[counter].ep.maxburst - 1);
+		}
+
+		/* disable endpoint configuration */
+		set_ss_configwren(base_addr, 0);
+	} else {
+		/* enable endpoint configuration */
+		set_hs_configwren(base_addr, 1);
+
+		/* configure endpoint x */
+		for (counter = F_USB30_MAX_EP - 1;
+			 counter > ENDPOINT0; counter--) {
+			for (alt_counter = 0; alt_counter <
+				 endpoint[counter].alternate_channels;
+				 alt_counter++) {
+				set_hs_epnum(base_addr, counter, alt_counter,
+					 counter);
+				set_hs_io(base_addr, counter, alt_counter,
+					 endpoint[counter].ep.address &
+					 USB_DIR_IN ? 1 : 0);
+				set_hs_type(base_addr, counter, alt_counter,
+					 transfer_type_register[endpoint[
+					counter].attributes]);
+				set_hs_conf(base_addr, counter, alt_counter, 1);
+				set_hs_intf(base_addr, counter, alt_counter,
+					 endpoint[counter].interface_channel);
+				set_hs_alt(base_addr, counter, alt_counter,
+					 alt_counter);
+				set_hs_size(base_addr, counter, alt_counter,
+					ep_fifo_size[counter][
+					f_usb30->gadget.speed ==
+					 USB_SPEED_HIGH ? USB_SPEED_HIGH :
+					 USB_SPEED_FULL][0]);
+				set_hs_numtr(base_addr, counter, alt_counter);
+			}
+		}
+
+		/* configure endpoint 0 */
+		set_hs_epnum(base_addr, ENDPOINT0, 0, 0);
+		set_hs_io(base_addr, ENDPOINT0, 0, 0);
+		set_hs_type(base_addr, ENDPOINT0, 0, TYPE_CONTROL);
+		set_hs_conf(base_addr, ENDPOINT0, 0, 0);
+		set_hs_intf(base_addr, ENDPOINT0, 0, 0);
+		set_hs_alt(base_addr, ENDPOINT0, 0, 0);
+		set_hs_size(base_addr, ENDPOINT0, 0,
+				 ep_fifo_size[counter][
+				f_usb30->gadget.speed == USB_SPEED_HIGH ?
+				 USB_SPEED_HIGH : USB_SPEED_FULL][0]);
+		set_hs_numtr(base_addr, ENDPOINT0, 0);
+
+		/* disable endpoint configuration */
+		set_hs_configwren(base_addr, 0);
+	}
+
+	return;
+}
+
+static void configure_interface(struct f_usb30_udc *f_usb30)
+{
+	static unsigned char intf_alt[] = {
+		F_USB30_ALT_INTF0,
+		F_USB30_ALT_INTF1,
+		F_USB30_ALT_INTF2,
+		F_USB30_ALT_INTF3,
+		F_USB30_ALT_INTF4,
+		F_USB30_ALT_INTF5,
+		F_USB30_ALT_INTF6,
+		F_USB30_ALT_INTF7,
+		F_USB30_ALT_INTF8,
+		F_USB30_ALT_INTF9,
+		F_USB30_ALT_INTF10,
+		F_USB30_ALT_INTF11,
+		F_USB30_ALT_INTF12,
+		F_USB30_ALT_INTF13,
+		F_USB30_ALT_INTF14,
+		F_USB30_ALT_INTF15,
+	};
+	unsigned long counter;
+	void *base_addr = f_usb30->register_base_address;
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		/* configure interface & alternate */
+		set_ss_numintf(base_addr, F_USB30_MAX_INTF);
+		for (counter = INTERFACE0;
+				 counter < F_USB30_MAX_INTF; counter++)
+			set_ss_numaintf(base_addr, counter, intf_alt[counter]);
+	} else {
+		/* configure interface & alternate */
+		set_hs_numintf(base_addr, F_USB30_MAX_INTF);
+		for (counter = INTERFACE0;
+				 counter < F_USB30_MAX_INTF; counter++) {
+			set_hs_numaltintf(base_addr, counter,
+						intf_alt[counter]);
+			set_hs_mskachgif(base_addr, counter, 0);
+		}
+	}
+
+	return;
+}
+
+static void set_bus_speed(struct f_usb30_udc *f_usb30, int ss)
+{
+	unsigned long counter;
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	/* set current bus speed */
+	if (ss) {
+		/* set bus speed to super */
+		f_usb30->gadget.speed = USB_SPEED_SUPER;
+	} else {
+		switch (get_hs_crtspeed(base_addr)) {
+		case CRT_SPEED_HIGH_SPEED:
+			f_usb30->gadget.speed = USB_SPEED_HIGH;
+			break;
+		case CRT_SPEED_FULL_SPEED:
+			f_usb30->gadget.speed = USB_SPEED_FULL;
+			break;
+		default:
+			f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+			return;
+		}
+	}
+
+	/* set endpoint max packet */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		endpoint[counter].ep.maxpacket = ep_fifo_size[counter][
+						f_usb30->gadget.speed][0];
+
+	return;
+}
+
+static void set_device_state(struct f_usb30_udc *f_usb30,
+	 unsigned char device_state)
+{
+	if (f_usb30->device_state == device_state)
+		return;
+
+	/* set device state */
+	f_usb30->device_state_last = f_usb30->device_state;
+	f_usb30->device_state = device_state;
+	dev_dbg(f_usb30->gadget.dev.parent, "device state is is %u.\n",
+							 device_state);
+
+	return;
+}
+
+static unsigned short get_fifo_bytes(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	unsigned short bytes;
+	void *base_addr = f_usb30->register_base_address;
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		/* get current bytes in FIFO */
+		if (ep_channel == ENDPOINT0)
+			bytes = f_usb30->ctrl_stage ==
+				 F_USB30_STAGE_IN_DATA ?
+				 get_ss_sizerd0i(base_addr) :
+				 get_ss_sizewr0o(base_addr);
+		else
+			bytes = get_ss_sizerdwr(base_addr, ep_channel);
+	} else {
+		/* get current bytes in FIFO */
+		if (ep_channel == ENDPOINT0)
+			bytes = f_usb30->ctrl_stage ==
+				 F_USB30_STAGE_IN_DATA ?
+				 get_hs_txrxsize0i(base_addr) :
+				 get_hs_txrxsize0o(base_addr);
+		else
+			bytes = get_hs_txrxsize(base_addr, ep_channel);
+	}
+
+	return bytes;
+}
+
+static unsigned char is_setup_transferred(struct f_usb30_udc *f_usb30)
+{
+	void *base_addr = f_usb30->register_base_address;
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		return (get_ss_intsetup(base_addr)) ||
+			 (get_ss_intsswrstb(base_addr)) ||
+			 (get_ss_intsswrste(base_addr)) ||
+			 (get_ss_sswrst(base_addr)) ? 1 : 0;
+	} else {
+		return (get_hs_intsetup(base_addr)) ||
+			 (get_hs_intusbrstb(base_addr)) ||
+			 (get_hs_intusbrste(base_addr)) ||
+			 (get_hs_busreset(base_addr)) ? 1 : 0;
+	}
+}
+
+static unsigned char set_in_transfer_pio(struct f_usb30_ep *endpoint)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	unsigned long *buffer;
+	unsigned char *buffer_byte;
+	unsigned char byte_access;
+	unsigned long bytes;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check argument */
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled)))
+		return 0;
+
+	/* check new SETUP transfer */
+	if ((ep_channel == ENDPOINT0) && (is_setup_transferred(f_usb30))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"new SETUP tranfer is occurred.\n");
+		return 0;
+	}
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		/* check transfer data setup */
+		if (!(ep_channel == ENDPOINT0 ? get_ss_ready0i(base_addr) :
+			 get_ss_ready(base_addr, ep_channel))) {
+			dev_err(f_usb30->gadget.dev.parent,
+				"endpoint %u is busy.\n", ep_channel);
+			return 0;
+		}
+	} else {
+		/* check transfer data setup */
+		if (!(ep_channel == ENDPOINT0 ? get_hs_ready0i(base_addr) :
+			 get_hs_readyi(base_addr, ep_channel))) {
+			dev_err(f_usb30->gadget.dev.parent,
+				 "endpoint %u is busy.\n", ep_channel);
+			return 0;
+		}
+	}
+
+	/* get transfer buffer */
+	buffer = (unsigned long *) (request->req.buf + request->req.actual);
+	prefetch(buffer);
+
+	/* calculate this time transfer byte */
+	bytes = (request->req.length - request->req.actual) <
+		 endpoint->ep.maxpacket ? request->req.length -
+		 request->req.actual : endpoint->ep.maxpacket;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u PIO is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+	/* update actual byte */
+	request->req.actual = bytes;
+
+	/* write IN transfer data buffer */
+	for (counter = bytes; counter >= 4; counter -= 4) {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_epinbuf(base_addr, ep_channel, *buffer) :
+			 set_hs_epinbuf(base_addr, ep_channel, *buffer);
+		buffer++;
+	}
+
+	for (byte_access = 0, buffer_byte = (unsigned char *) buffer; counter;
+		 counter--, byte_access++) {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_epinbuf_byte(base_addr, ep_channel, byte_access,
+						 *buffer_byte) :
+			 set_hs_epinbuf_byte(base_addr, ep_channel, byte_access,
+						 *buffer_byte);
+		buffer_byte++;
+	}
+
+	if (ep_channel == ENDPOINT0) {
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30)) {
+			/* enable IN transfer & IntReady0i interrupt */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 enable_ss_ready0i(base_addr) :
+				 enable_hs_ready0i(base_addr);
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskready0i(base_addr, 0) :
+				 set_hs_mskready0i(base_addr, 0);
+		}
+	} else {
+		/* enable IN transfer & IntReady interrupt */
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 enable_ss_ready(base_addr, ep_channel) :
+			 enable_hs_readyi(base_addr, ep_channel);
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready(base_addr, ep_channel, 0) :
+			 set_hs_mskready(base_addr, ep_channel, 0);
+	}
+
+	return 1;
+}
+
+static unsigned char set_out_transfer_pio(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check argument */
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled)))
+		return 0;
+
+	/* check new SETUP transfer */
+	if ((ep_channel == ENDPOINT0) && (is_setup_transferred(f_usb30))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"new SETUP tranfer is occurred.\n");
+		return 0;
+	}
+
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u PIO is setup at length = %u, actual = %u, max packet = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket);
+	(void) request;
+
+	if (ep_channel == ENDPOINT0) {
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* enable IntReady0o interrupt */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskready0o(base_addr, 0) :
+				 set_hs_mskready0o(base_addr, 0);
+	} else {
+		/* enable IntReady interrupt */
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready(base_addr, ep_channel, 0) :
+			 set_hs_mskready(base_addr, ep_channel, 0);
+	}
+
+	return 1;
+}
+
+static void abort_in_transfer_pio(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* disable endpoint interrupt */
+	if (ep_channel == ENDPOINT0) {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready0i(base_addr, 1) :
+			 set_hs_mskready0i(base_addr, 1);
+	} else {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready(base_addr, ep_channel, 1) :
+			 set_hs_mskready(base_addr, ep_channel, 1);
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskempty(base_addr, ep_channel, 1) :
+			 set_hs_mskempty(base_addr, ep_channel, 1);
+	}
+
+	if (!initialize)
+		return;
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 1, 0);
+
+	return;
+}
+
+static void abort_out_transfer_pio(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* disable endpoint interrupt */
+	if (ep_channel == ENDPOINT0)
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready0o(base_addr, 1) :
+			 set_hs_mskready0o(base_addr, 1);
+	else
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_mskready(base_addr, ep_channel, 1) :
+			 set_hs_mskready(base_addr, ep_channel, 1);
+
+	if (!initialize)
+		return;
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 0, 1);
+
+	return;
+}
+
+static unsigned char end_in_transfer_pio(struct f_usb30_ep *endpoint)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	unsigned long *buffer;
+	unsigned char *buffer_byte;
+	unsigned long bytes;
+	unsigned char byte_access;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check empty wait */
+	if ((ep_channel != ENDPOINT0) &&
+		 (f_usb30->gadget.speed == USB_SPEED_SUPER ?
+		 (!get_ss_mskempty(base_addr, ep_channel)) :
+		 (!get_hs_mskempty(base_addr, ep_channel)))) {
+		/* complete request */
+		abort_in_transfer_pio(endpoint, 0);
+		return 1;
+	}
+
+	/* check transfer remain byte */
+	if (request->req.length == request->req.actual) {
+#if defined(CONFIG_USB_GADGET_F_USB30_BULK_IN_END_NOTIFY_TIMING_TO_HOST)
+		/* check bulk transfer type */
+		if (endpoint->attributes == USB_ENDPOINT_XFER_BULK) {
+			/* clear & enable IntEmpty interrupt */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 set_ss_mskempty(base_addr, ep_channel, 0) :
+				 set_hs_mskempty(base_addr, ep_channel, 0);
+			return 0;
+		}
+#endif
+
+		/* complete request */
+		abort_in_transfer_pio(endpoint, 0);
+		return 1;
+	}
+
+	if (ep_channel == ENDPOINT0) {
+		/* check transfer data write enable */
+		if (f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (!get_ss_ready0i(base_addr)) :
+			 (!get_hs_ready0i(base_addr))) {
+			/* abort IN transfer */
+			abort_in_transfer_pio(endpoint, 0);
+			dev_err(f_usb30->gadget.dev.parent,
+				"endpoint 0 IN is busy.\n");
+			return 0;
+		}
+	} else {
+		/* check transfer data write enable */
+		if (f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (!get_ss_ready(base_addr, ep_channel)) :
+			 (!get_hs_readyi(base_addr, ep_channel))) {
+			/* abort IN transfer */
+			abort_in_transfer_pio(endpoint, 1);
+			dev_err(f_usb30->gadget.dev.parent,
+				"endpoint %u is busy.\n", ep_channel);
+			return 0;
+		}
+	}
+
+	/* get transfer buffer */
+	buffer = (unsigned long *) (request->req.buf + request->req.actual);
+	prefetch(buffer);
+
+	/* calculate this time transfer byte */
+	bytes = (request->req.length - request->req.actual) <
+		 endpoint->ep.maxpacket ? request->req.length -
+		 request->req.actual : endpoint->ep.maxpacket;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u PIO is setup at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+	/* update actual bytes */
+	request->req.actual += bytes;
+
+	/* write IN transfer data buffer */
+	for (counter = bytes; counter >= 4; counter -= 4) {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_epinbuf(base_addr, ep_channel, *buffer) :
+			 set_hs_epinbuf(base_addr, ep_channel, *buffer);
+		buffer++;
+	}
+	for (byte_access = 0, buffer_byte = (unsigned char *) buffer; counter;
+		 counter--, byte_access++) {
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 set_ss_epinbuf_byte(base_addr, ep_channel, byte_access,
+						 *buffer_byte) :
+			 set_hs_epinbuf_byte(base_addr, ep_channel, byte_access,
+						 *buffer_byte);
+		buffer_byte++;
+	}
+
+	if (ep_channel == ENDPOINT0) {
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* enable IN transfer */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 enable_ss_ready0i(base_addr) :
+				 enable_hs_ready0i(base_addr);
+	} else {
+		/* enable IN transfer */
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 enable_ss_ready(base_addr, ep_channel) :
+			 enable_hs_readyi(base_addr, ep_channel);
+	}
+
+	return 0;
+}
+
+static unsigned char end_out_transfer_pio(struct f_usb30_ep *endpoint)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	unsigned long data;
+	unsigned long *buffer;
+	unsigned char *buffer_byte;
+	unsigned long bytes;
+	unsigned char byte_access;
+	void *base_addr = f_usb30->register_base_address;
+
+	if (ep_channel == ENDPOINT0) {
+		/* check transfer data read enable */
+		if (f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (!get_ss_ready0o(base_addr)) :
+			 (!get_hs_ready0o(base_addr))) {
+			/* abort OUT transfer */
+			abort_out_transfer_pio(endpoint, 0);
+			dev_err(f_usb30->gadget.dev.parent,
+				 "endpoint 0 OUT is busy.\n");
+
+			/* check new SETUP transfer */
+			if ((is_setup_transferred(f_usb30)) &&
+				 (request->req.length == 0)) {
+				request->req.actual = 0;
+				return 1;
+			}
+			return 0;
+		}
+	} else {
+		/* check transfer data read enable */
+		if (f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 (!get_ss_ready(base_addr, ep_channel)) :
+			 (!get_hs_readyo(base_addr, ep_channel))) {
+			/* abort OUT transfer */
+			abort_out_transfer_pio(endpoint, 1);
+			dev_err(f_usb30->gadget.dev.parent,
+				"endpoint %u is busy.\n",
+							 ep_channel);
+			return 0;
+		}
+	}
+
+	/* get transfer buffer */
+	buffer = (unsigned long *) (request->req.buf + request->req.actual);
+	prefetch(buffer);
+
+	/* get OUT transfer byte */
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER)
+		bytes = ep_channel == ENDPOINT0 ?
+			 get_ss_sizewr0o(base_addr) :
+			 get_ss_sizerdwr(base_addr, ep_channel);
+	else
+		bytes = ep_channel == ENDPOINT0 ?
+			 get_hs_txrxsize0o(base_addr) :
+			 get_hs_txrxsize(base_addr, ep_channel);
+
+	if (request->req.length < (request->req.actual + bytes))
+		bytes = request->req.length - request->req.actual;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u PIO is end at length = %u, actual = %u, max packet = %u, this time = %u.\n",
+		ep_channel, request->req.length, request->req.actual,
+		endpoint->ep.maxpacket, (unsigned int) bytes);
+
+	/* update actual bytes */
+	request->req.actual += bytes;
+
+	/* read OUT transfer data buffer */
+	for (counter = bytes; counter >= 4; counter -= 4) {
+		*buffer = f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 get_ss_epoutbuf(base_addr, ep_channel) :
+				 get_hs_epoutbuf(base_addr, ep_channel);
+		buffer++;
+	}
+	if (counter) {
+		data = f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 get_ss_epoutbuf(base_addr, ep_channel) :
+				 get_hs_epoutbuf(base_addr, ep_channel);
+		for (byte_access = 0, buffer_byte = (unsigned char *) buffer;
+			 counter; counter--, byte_access++) {
+			*buffer_byte =
+				 (unsigned char) (data >> (8 * byte_access));
+			buffer_byte++;
+		}
+	}
+
+	if (ep_channel == ENDPOINT0) {
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* enable next OUT transfer */
+			f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 enable_ss_ready0o(base_addr) :
+				 enable_hs_ready0o(base_addr);
+	} else {
+		/* enable next OUT transfer */
+		f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 enable_ss_ready(base_addr, ep_channel) :
+			 enable_hs_readyo(base_addr, ep_channel);
+	}
+
+	/* check transfer request complete */
+	if ((request->req.length <= request->req.actual) ||
+		 (bytes % endpoint->ep.maxpacket) || (!bytes)) {
+		/* complete request */
+		abort_out_transfer_pio(endpoint, 0);
+		return 1;
+	}
+
+	return 0;
+}
+
+static unsigned char set_in_transfer(struct f_usb30_ep *endpoint)
+{
+	static unsigned char (*const set_in_transfer_function[]) (
+		struct f_usb30_ep *) = {
+			set_in_transfer_pio, set_in_transfer_dma,};
+	struct f_usb30_request *request = endpoint->req;
+	return set_in_transfer_function[(endpoint->dma_channel != -1) &&
+		 (request->req.length) ? 1 : 0] (endpoint);
+}
+
+static unsigned char set_out_transfer(struct f_usb30_ep *endpoint)
+{
+	static unsigned char (*const set_out_transfer_function[]) (
+		struct f_usb30_ep *) = {
+			set_out_transfer_pio, set_out_transfer_dma,};
+	return set_out_transfer_function[endpoint->dma_channel != -1 ? 1 : 0]
+		 (endpoint);
+}
+
+static void abort_in_transfer(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+	static void (*const abort_in_transfer_function[]) (
+		struct f_usb30_ep *, unsigned char) = {
+			abort_in_transfer_pio, abort_in_transfer_dma,};
+	return abort_in_transfer_function[endpoint->dma_transfer ? 1 : 0]
+		 (endpoint, initialize);
+}
+
+static void abort_out_transfer(struct f_usb30_ep *endpoint,
+	 unsigned char initialize)
+{
+	static void (*const abort_out_transfer_function[]) (
+		struct f_usb30_ep *, unsigned char) = {
+			abort_out_transfer_pio, abort_out_transfer_dma,};
+	return abort_out_transfer_function[endpoint->dma_transfer ? 1 : 0]
+		 (endpoint, initialize);
+}
+
+static unsigned char end_in_transfer(struct f_usb30_ep *endpoint)
+{
+	static unsigned char (*const end_in_transfer_function[]) (
+		struct f_usb30_ep *) = {
+			end_in_transfer_pio, end_in_transfer_dma,};
+	return end_in_transfer_function[endpoint->dma_transfer ? 1 : 0]
+		 (endpoint);
+}
+
+static unsigned char end_out_transfer(struct f_usb30_ep *endpoint)
+{
+	static unsigned char (*const end_out_transfer_function[]) (
+		struct f_usb30_ep *) = {end_out_transfer_pio,
+							 end_out_transfer_dma,};
+	return end_out_transfer_function[endpoint->dma_transfer ? 1 : 0]
+		 (endpoint);
+}
+
+static void halt_transfer(struct f_usb30_ep *endpoint)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	void *base_addr = f_usb30->register_base_address;
+
+	if (ep_channel == ENDPOINT0) {
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30)) {
+			/* halt endpoint 0 */
+			if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+				if (!get_ss_stalled0(base_addr))
+					set_ss_reqstall0(base_addr, 1);
+			} else {
+				if (!get_hs_stalled0(base_addr))
+					set_hs_reqstall0(base_addr, 1);
+			}
+		}
+	} else {
+		/* check isochronous endpoint */
+		if (endpoint->attributes == USB_ENDPOINT_XFER_ISOC)
+			return;
+
+		/* halt endpoint x */
+		if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+			if (!get_ss_stalled(base_addr, ep_channel)) {
+				/* initialize FIFO */
+				endpoint->ep.address & USB_DIR_IN ?
+					 abort_in_transfer(endpoint, 0) :
+					 abort_out_transfer(endpoint, 0);
+				/* wait & check endpoint buffer empty */
+				for (counter = 0xFFFF; (counter &&
+					(!get_ss_empty(base_addr, ep_channel)))
+					; counter--)
+					;
+				set_ss_reqstall(base_addr, ep_channel, 1);
+			}
+		} else {
+			if (!get_hs_stalled(base_addr, ep_channel)) {
+				set_hs_reqstall(base_addr, ep_channel, 1);
+				/* initialize FIFO */
+				endpoint->ep.address & USB_DIR_IN ?
+					 abort_in_transfer(endpoint, 1) :
+					 abort_out_transfer(endpoint, 1);
+			}
+		}
+	}
+
+	return;
+}
+
+static void notify_transfer_request_complete(struct f_usb30_ep *endpoint,
+	 int status)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request = endpoint->req;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+	unsigned char halt = endpoint->halt;
+
+	/* delete and initialize list */
+	list_del_init(&request->queue);
+
+	if (request->req.status == -EINPROGRESS)
+		request->req.status = status;
+	else
+		status = request->req.status;
+
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u request is completed at request = 0x%p, length = %u, actual = %u, status = %d.\n",
+		ep_channel, &request->req, request->req.length,
+		request->req.actual, status);
+
+	/* notify request complete for gadget driver */
+	if (request->req.complete) {
+		endpoint->halt = 1;
+		spin_unlock(&f_usb30->lock);
+		request->req.complete(&endpoint->ep, &request->req);
+		spin_lock(&f_usb30->lock);
+		endpoint->halt = halt;
+	}
+
+	return;
+}
+
+static void dequeue_all_transfer_request(struct f_usb30_ep *endpoint,
+	 int status)
+{
+	unsigned long counter;
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+#endif
+	struct f_usb30_request *request;
+
+	/* dequeue all transfer request */
+	for (counter = (unsigned long) -1;
+		 (counter) && (!list_empty(&endpoint->queue)); counter--) {
+		request = list_entry(endpoint->queue.next,
+					 struct f_usb30_request, queue);
+#if !defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+		/* check DMA transfer buffer unmap */
+		if ((endpoint->dma_transfer) &&
+			 (request->req.dma != F_USB30_DMA_ADDR_INVALID)) {
+			if (request->dma_buffer_map) {
+				/*
+				 * unmap DMA transfer buffer and
+				 * synchronize DMA transfer buffer
+				 */
+				dma_unmap_single(f_usb30->gadget.dev.parent,
+						 request->req.dma,
+						 request->req.length,
+						 endpoint->ep.address &
+						 USB_DIR_IN ?
+						 DMA_TO_DEVICE :
+						 DMA_FROM_DEVICE);
+				request->req.dma = F_USB30_DMA_ADDR_INVALID;
+				request->dma_buffer_map = 0;
+			} else {
+				/* synchronize DMA transfer buffer for CPU */
+				dma_sync_single_for_cpu(f_usb30->
+							gadget.dev.parent,
+							 request->req.dma,
+							 request->req.length,
+							 endpoint->ep.address &
+							 USB_DIR_IN ?
+							 DMA_TO_DEVICE :
+							 DMA_FROM_DEVICE);
+			}
+		}
+#endif
+		endpoint->req = request;
+		notify_transfer_request_complete(endpoint, status);
+	}
+
+	return;
+}
+
+void connect_ss_host(struct f_usb30_udc *f_usb30, unsigned char connect)
+{
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned long counter;
+
+	if (connect) {
+		/* enable SS clock */
+		if (get_ssclkstp(base_addr))
+			set_ssclkstpen(base_addr, 0);
+
+		/* reset F_USB30 SS controller */
+		initialize_controller(f_usb30, 1);
+
+		set_ss_msksetup(base_addr, 0);
+		set_ss_msksetconf(base_addr, 0);
+		set_ss_mskpolltou0(base_addr, 0);
+		set_ss_mskenterpoll(base_addr, 0);
+		set_ss_mskssdisable(base_addr, 0);
+		set_ss_msksuspendb(base_addr, 0);
+		set_ss_msksuspende(base_addr, 0);
+		set_ss_msksshrstb(base_addr, 0);
+		set_ss_msksshrste(base_addr, 0);
+		set_ss_msksswrstb(base_addr, 0);
+		set_ss_msksswrste(base_addr, 0);
+		set_ss_mskdev(base_addr, 0);
+
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_POWERED);
+
+		/* initialize endpoint configure data */
+		initialize_endpoint_configure(f_usb30);
+
+		/* start superspeed link training */
+		set_ss_connect(base_addr, 1);
+	} else {
+		/* mask and clear interrupt factor */
+		set_ss_mskfncsusp(base_addr, 1);
+		set_ss_msku2inactto(base_addr, 1);
+		set_ss_msksetup(base_addr, 1);
+		set_ss_msksetconf(base_addr, 1);
+		set_ss_msksuspendb(base_addr, 1);
+		set_ss_msksuspende(base_addr, 1);
+		set_ss_mskpolltou0(base_addr, 1);
+		set_ss_mskenterpoll(base_addr, 1);
+		set_ss_mskssdisable(base_addr, 1);
+		set_ss_mskvdtest(base_addr, 1);
+		set_ss_msksshrstb(base_addr, 1);
+		set_ss_msksshrste(base_addr, 1);
+		set_ss_msksswrstb(base_addr, 1);
+		set_ss_msksswrste(base_addr, 1);
+		clear_ss_intfncsusp(base_addr);
+		clear_ss_intu2inactto(base_addr);
+		clear_ss_intsetup(base_addr);
+		clear_ss_intsetconf(base_addr);
+		clear_ss_intsuspendb(base_addr);
+		clear_ss_intsuspende(base_addr);
+		clear_ss_intpolltou0(base_addr);
+		clear_ss_intenterpoll(base_addr);
+		clear_ss_intssdisable(base_addr);
+		clear_ss_intvdtest(base_addr);
+		clear_ss_intsshrstb(base_addr);
+		clear_ss_intsshrste(base_addr);
+		clear_ss_intsswrstb(base_addr);
+		clear_ss_intsswrste(base_addr);
+
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_NOTATTACHED);
+
+		/* set bus disconnect */
+		set_ss_disconnect(base_addr, 1);
+		set_ss_softreset(base_addr, 1);
+		set_ss_softreset(base_addr, 0);
+		/* abort previous transfer */
+		abort_in_transfer(&endpoint[ENDPOINT0], 0);
+		abort_out_transfer(&endpoint[ENDPOINT0], 0);
+		endpoint[ENDPOINT0].halt = 0;
+		endpoint[ENDPOINT0].force_halt = 0;
+		for (counter = ENDPOINT1;
+				 counter < F_USB30_MAX_EP; counter++) {
+			endpoint[counter].ep.address & USB_DIR_IN ?
+				 abort_in_transfer(&endpoint[counter], 0) :
+				 abort_out_transfer(&endpoint[counter], 0);
+			endpoint[counter].halt = 0;
+			endpoint[counter].force_halt = 0;
+		}
+
+		/* reset F_USB30 SS controller */
+		initialize_controller(f_usb30, 1);
+
+		/* dequeue all previous transfer request */
+		for (counter = ENDPOINT0;
+				 counter < F_USB30_MAX_EP; counter++) {
+			endpoint[counter].halt = 1;
+			dequeue_all_transfer_request(&endpoint[counter],
+							 -ESHUTDOWN);
+		}
+
+		/* initialize endpoint list data */
+		INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+		INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+		for (counter = ENDPOINT0;
+				 counter < F_USB30_MAX_EP; counter++) {
+			list_add_tail(&endpoint[counter].ep.ep_list,
+					 counter == ENDPOINT0 ?
+					 &f_usb30->gadget.ep0->ep_list :
+					 &f_usb30->gadget.ep_list);
+			INIT_LIST_HEAD(&endpoint[counter].queue);
+		}
+
+		/* initialize F_USB30 UDC device driver structure data */
+		f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+		f_usb30->configure_value_last = 0;
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		f_usb30->ctrl_pri_dir = 1;
+	}
+
+	return;
+}
+
+void connect_hs_host(struct f_usb30_udc *f_usb30, unsigned char connect)
+{
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned long counter;
+
+	if (connect) {
+		/* enable SS clock */
+		if (get_hsclkstp(base_addr))
+			set_hsclkstpen(base_addr, 0);
+
+		/* reset F_USB30 HS/FS controller */
+		initialize_controller(f_usb30, 0);
+
+		/* unmask interrupt factor */
+		set_hs_mskerraticerr(base_addr, 0);
+		set_hs_msksof(base_addr, 1);
+		set_hs_mskusbrstb(base_addr, 0);
+		set_hs_mskusbrste(base_addr, 0);
+		set_hs_msksuspendb(base_addr, 0);
+		set_hs_msksuspende(base_addr, 0);
+		set_hs_msksetup(base_addr, 0);
+		set_hs_msksetconf(base_addr, 0);
+
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_POWERED);
+
+		/* initialize endpoint configure data */
+		initialize_endpoint_configure(f_usb30);
+
+		/* set normal mode */
+		set_hs_physusp(base_addr, 0);
+
+		/* pull-up D+ terminal */
+		set_hs_disconnect(base_addr, 0);
+
+	} else {
+		/* mask and clear interrupt factor */
+		set_hs_mskerraticerr(base_addr, 1);
+		set_hs_msksof(base_addr, 1);
+		set_hs_mskusbrstb(base_addr, 1);
+		set_hs_mskusbrste(base_addr, 1);
+		set_hs_msksuspendb(base_addr, 1);
+		set_hs_msksuspende(base_addr, 1);
+		set_hs_msksetup(base_addr, 1);
+		set_hs_msksetconf(base_addr, 1);
+		clear_hs_interraticerr(base_addr);
+		clear_hs_intsof(base_addr);
+		clear_hs_intusbrstb(base_addr);
+		clear_hs_intusbrste(base_addr);
+		clear_hs_intsuspendb(base_addr);
+		clear_hs_intsuspende(base_addr);
+		clear_hs_intsetup(base_addr);
+		clear_hs_intsetconf(base_addr);
+
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_NOTATTACHED);
+
+		/* pull-down D+ terminal */
+		set_hs_disconnect(base_addr, 1);
+
+		/* abort previous transfer */
+		abort_in_transfer(&endpoint[ENDPOINT0], 0);
+		abort_out_transfer(&endpoint[ENDPOINT0], 0);
+		endpoint[ENDPOINT0].halt = 0;
+		endpoint[ENDPOINT0].force_halt = 0;
+		for (counter = ENDPOINT1;
+				 counter < F_USB30_MAX_EP; counter++) {
+			endpoint[counter].ep.address & USB_DIR_IN ?
+				 abort_in_transfer(&endpoint[counter], 1) :
+				 abort_out_transfer(&endpoint[counter], 1);
+			endpoint[counter].halt = 0;
+			endpoint[counter].force_halt = 0;
+		}
+
+		/* reset F_USB30 HS/FS controller */
+		initialize_controller(f_usb30, 0);
+
+		/* dequeue all previous transfer request */
+		for (counter = ENDPOINT0;
+				 counter < F_USB30_MAX_EP; counter++) {
+			endpoint[counter].halt = 1;
+			dequeue_all_transfer_request(&endpoint[counter],
+							 -ESHUTDOWN);
+		}
+
+		/* initialize endpoint list data */
+		INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+		INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+		for (counter = ENDPOINT0;
+				 counter < F_USB30_MAX_EP; counter++) {
+			list_add_tail(&endpoint[counter].ep.ep_list,
+					 counter == ENDPOINT0 ?
+					 &f_usb30->gadget.ep0->ep_list :
+					 &f_usb30->gadget.ep_list);
+			INIT_LIST_HEAD(&endpoint[counter].queue);
+		}
+
+		/* initialize F_USB30 UDC device driver structure data */
+		f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+		f_usb30->configure_value_last = 0;
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		f_usb30->ctrl_pri_dir = 1;
+	}
+
+	return;
+}
+
+static void on_begin_warm_or_bus_reset(struct f_usb30_udc *f_usb30)
+{
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned long counter;
+
+	/* abort previous transfer & initialize all endpoint */
+	abort_in_transfer(&endpoint[ENDPOINT0], 1);
+	abort_out_transfer(&endpoint[ENDPOINT0], 1);
+	endpoint[ENDPOINT0].halt = 0;
+	endpoint[ENDPOINT0].force_halt = 0;
+	endpoint[ENDPOINT0].dma_transfer = 0;
+	for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++) {
+		endpoint[counter].ep.address & USB_DIR_IN ?
+			 abort_in_transfer(&endpoint[counter], 1) :
+			 abort_out_transfer(&endpoint[counter], 1);
+		endpoint[counter].halt = 0;
+		endpoint[counter].force_halt = 0;
+		endpoint[counter].dma_transfer = 0;
+	}
+
+	/* initialize F_USB30 DMA controller register */
+	initialize_dma_controller(f_usb30);
+
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		dequeue_all_transfer_request(&endpoint[counter], -ECONNABORTED);
+
+	/* initialize endpoint list data */
+	INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+	INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		list_add_tail(&endpoint[counter].ep.ep_list,
+				 counter == ENDPOINT0 ?
+				 &f_usb30->gadget.ep0->ep_list :
+				 &f_usb30->gadget.ep_list);
+		INIT_LIST_HEAD(&endpoint[counter].queue);
+	}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_BUS_RESET_DUMMY_DISCONNECT_NOTIFY)
+	/* check configured */
+	if (f_usb30->configure_value_last) {
+		/*
+		 * notify dummy disconnect to gadget driver
+		 * for gadget driver re-init
+		 */
+		if ((f_usb30->gadget_driver) &&
+			 (f_usb30->gadget_driver->disconnect)) {
+			spin_unlock(&f_usb30->lock);
+			f_usb30->gadget_driver->disconnect(
+							&f_usb30->gadget);
+			spin_lock(&f_usb30->lock);
+		}
+	}
+#endif
+
+	/* initialize F_USB30 UDC device driver structure data */
+	f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+	f_usb30->configure_value_last = 0;
+	f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+	f_usb30->ctrl_pri_dir = 1;
+
+	return;
+}
+
+static void on_end_warm_reset(struct f_usb30_udc *f_usb30)
+{
+	/* configure endpoint */
+	configure_endpoint(f_usb30);
+
+	/* configure interface */
+	configure_interface(f_usb30);
+
+	/* change link and device state */
+	f_usb30->link_state = F_USB30_LINK_STATE_TRAINING;
+	set_device_state(f_usb30, USB_STATE_POWERED);
+
+	return;
+}
+
+static void on_end_bus_reset(struct f_usb30_udc *f_usb30)
+{
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check SS link state */
+	if ((!get_ss_ssdiserr(base_addr)) && (f_usb30->ss_discnt < 3)
+		&& (f_usb30->gadget_driver->max_speed == USB_SPEED_SUPER)) {
+		/* enable interrupt factor */
+		set_ss_mskenterpoll(base_addr, 0);
+
+		/* set SS link training once more */
+		set_ss_rxdetonce(base_addr, 1);
+		set_ss_connect(base_addr, 1);
+	}
+
+	/* set current bus speed */
+	set_bus_speed(f_usb30, 0);
+
+	/* configure endpoint */
+	configure_endpoint(f_usb30);
+
+	/* configure interface */
+	configure_interface(f_usb30);
+
+	/* change device state */
+	set_device_state(f_usb30, USB_STATE_DEFAULT);
+
+	return;
+}
+
+static void on_begin_hot_reset(struct f_usb30_udc *f_usb30)
+{
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	unsigned long counter;
+
+	/* abort previous transfer & initialize all endpoint */
+	abort_in_transfer(&endpoint[ENDPOINT0], 1);
+	abort_out_transfer(&endpoint[ENDPOINT0], 1);
+	endpoint[ENDPOINT0].halt = 0;
+	endpoint[ENDPOINT0].force_halt = 0;
+	for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++) {
+		endpoint[counter].ep.address & USB_DIR_IN ?
+			 abort_in_transfer(&endpoint[counter], 1) :
+			 abort_out_transfer(&endpoint[counter], 1);
+		endpoint[counter].halt = 0;
+		endpoint[counter].force_halt = 0;
+		endpoint[counter].dma_transfer = 0;
+	}
+
+	/* initialize F_USB30 DMA controller register */
+	initialize_dma_controller(f_usb30);
+
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		dequeue_all_transfer_request(&endpoint[counter], -ECONNABORTED);
+
+	/* initialize endpoint list data */
+	INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+	INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		list_add_tail(&endpoint[counter].ep.ep_list,
+				 counter == ENDPOINT0 ?
+				 &f_usb30->gadget.ep0->ep_list :
+				 &f_usb30->gadget.ep_list);
+		INIT_LIST_HEAD(&endpoint[counter].queue);
+	}
+
+	/* check configured */
+	if (f_usb30->configure_value_last) {
+		/*
+		 * notify dummy disconnect to gadget driver
+		 * for gadget driver re-init
+		 */
+		if ((f_usb30->gadget_driver) &&
+			 (f_usb30->gadget_driver->disconnect)) {
+			spin_unlock(&f_usb30->lock);
+			f_usb30->gadget_driver->disconnect(
+							&f_usb30->gadget);
+			spin_lock(&f_usb30->lock);
+		}
+	}
+
+	/* initialize F_USB30 UDC device driver structure data */
+	f_usb30->configure_value_last = 0;
+
+	return;
+}
+
+static void on_end_hot_reset(struct f_usb30_udc *f_usb30)
+{
+	/* current non process */
+	return;
+}
+
+static void on_ss_disabled(struct f_usb30_udc *f_usb30)
+{
+	void *base_addr = f_usb30->register_base_address;
+
+	/* change link state */
+	f_usb30->link_state = F_USB30_LINK_STATE_DISABLED;
+
+	/* increase disable counter */
+	f_usb30->ss_discnt++;
+
+	if (f_usb30->ss_discnt == 1) {
+		/* disconnect SS host */
+		connect_ss_host(f_usb30, 0);
+
+		/* connect HS/FS host */
+		connect_hs_host(f_usb30, 1);
+	}
+
+	if (f_usb30->ss_discnt == 3) {
+		/* change link state */
+		f_usb30->link_state = F_USB30_LINK_STATE_HSENABLED;
+
+		/* disable SS clock */
+		if (!get_ssclkstp(base_addr)) {
+			set_ssclkstpen(base_addr, 1);
+			set_ssclkstp(base_addr, 1);
+		}
+
+		/* disable Enterpoll interrupt factor */
+		set_ss_mskenterpoll(base_addr, 1);
+	}
+
+	return;
+}
+
+static void on_enterpoll(struct f_usb30_udc *f_usb30)
+{
+	void *base_addr = f_usb30->register_base_address;
+
+	/* change link state */
+	f_usb30->link_state = F_USB30_LINK_STATE_TRAINING;
+
+	/* reset HS/FS controller */
+	set_hs_softreset(base_addr);
+
+	/* mask HS/FS interrupt factor */
+	set_hs_mskerraticerr(base_addr, 1);
+	set_hs_msksof(base_addr, 1);
+	set_hs_mskusbrstb(base_addr, 1);
+	set_hs_mskusbrste(base_addr, 1);
+	set_hs_msksuspendb(base_addr, 1);
+	set_hs_msksuspende(base_addr, 1);
+	set_hs_msksetup(base_addr, 1);
+	set_hs_msksetconf(base_addr, 1);
+
+	/* connect SS host */
+	connect_ss_host(f_usb30, 1);
+
+	return;
+}
+
+static void on_polltou0(struct f_usb30_udc *f_usb30)
+{
+	void *base_addr = f_usb30->register_base_address;
+
+	/* disable interrupt factor */
+	set_ss_mskenterpoll(base_addr, 1);
+
+	/* clear disable counter */
+	f_usb30->ss_discnt = 0;
+
+	/* disable HS/FS clock */
+	if (!get_hsclkstp(base_addr)) {
+		set_hsclkstpen(base_addr, 1);
+		set_hsclkstp(base_addr, 1);
+	}
+
+	/* set bus speed to superspeed */
+	set_bus_speed(f_usb30, 1);
+
+	/* configure endpoint */
+	configure_endpoint(f_usb30);
+
+	/* configure interface */
+	configure_interface(f_usb30);
+
+	/* enable SS endpoint0 */
+	initialize_endpoint_hw(&f_usb30->udc_endpoint[0], 0, 0);
+
+	/* change link and device state */
+	f_usb30->link_state = F_USB30_LINK_STATE_SSENABLED;
+	set_device_state(f_usb30, USB_STATE_DEFAULT);
+
+	return;
+}
+
+static void on_suspend(struct f_usb30_udc *f_usb30)
+{
+	/* check parameter */
+	if (f_usb30->gadget.speed == USB_SPEED_UNKNOWN)
+		return;
+
+	/* change device state */
+	set_device_state(f_usb30, USB_STATE_SUSPENDED);
+
+	/* notify suspend to gadget driver */
+	if ((f_usb30->gadget_driver) &&
+		 (f_usb30->gadget_driver->suspend)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->suspend(&f_usb30->gadget);
+		spin_lock(&f_usb30->lock);
+	}
+
+	return;
+}
+
+static void on_wakeup(struct f_usb30_udc *f_usb30)
+{
+	/* check parameter */
+	if (f_usb30->gadget.speed == USB_SPEED_UNKNOWN)
+		return;
+
+	/* change device state */
+	set_device_state(f_usb30, f_usb30->device_state_last);
+
+	/* notify resume to gadget driver */
+	if ((f_usb30->gadget_driver) &&
+		 (f_usb30->gadget_driver->resume)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->resume(&f_usb30->gadget);
+		spin_lock(&f_usb30->lock);
+	}
+
+	return;
+}
+
+static void on_configure(struct f_usb30_udc *f_usb30)
+{
+	unsigned long counter;
+	unsigned char cfg_value = f_usb30->gadget.speed == USB_SPEED_SUPER ?
+			 get_ss_conf(f_usb30->register_base_address) :
+			 get_hs_conf(f_usb30->register_base_address);
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+	struct usb_ctrlrequest ctrlreq;
+
+	/* check configure value change */
+	if (cfg_value == f_usb30->configure_value_last)
+		return;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "configure value is %u.\n",
+						 cfg_value);
+
+	f_usb30->configure_value_last = cfg_value;
+
+	/* check configure value */
+	if (cfg_value) {
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_CONFIGURED);
+	} else {
+		for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++)
+			/* abort transfer */
+			endpoint[counter].ep.address & USB_DIR_IN ?
+				 abort_in_transfer(&endpoint[counter], 1) :
+				 abort_out_transfer(&endpoint[counter], 1);
+
+		/* change device state */
+		set_device_state(f_usb30, USB_STATE_ADDRESS);
+	}
+
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		dequeue_all_transfer_request(&endpoint[counter], -ECONNABORTED);
+
+	/* create SET_CONFIGURATION SETUP data */
+	ctrlreq.bRequestType = USB_DIR_OUT | USB_TYPE_STANDARD |
+				 USB_RECIP_DEVICE;
+	ctrlreq.bRequest = USB_REQ_SET_CONFIGURATION;
+	ctrlreq.wValue = cfg_value;
+	ctrlreq.wIndex = 0;
+	ctrlreq.wLength = 0;
+
+	/* update control transfer stage */
+	f_usb30->ctrl_stage = F_USB30_STAGE_SPECIAL;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"next control transfer stage is special stage.\n");
+
+	/* notify SET_CONFIGURATION to gadget driver */
+	if ((f_usb30->gadget_driver) && (f_usb30->gadget_driver->setup)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->setup(&f_usb30->gadget, &ctrlreq);
+		spin_lock(&f_usb30->lock);
+	}
+
+	return;
+}
+
+static void on_end_control_setup_transfer(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	union {
+		struct usb_ctrlrequest ctrlreq;
+		unsigned long word[2];
+	} setup;
+	unsigned long bytes;
+	int result;
+	void *base_addr = f_usb30->register_base_address;
+
+	/* check transfer data get disable */
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		if (!get_ss_ready0o(base_addr))
+			return;
+		/* get SS SETUP transfer byte */
+		bytes = get_ss_sizewr0o(base_addr);
+	} else {
+		if (!get_hs_ready0o(base_addr))
+			return;
+		/* get HS/FS SETUP transfer byte */
+		bytes = get_hs_txrxsize0o(base_addr);
+	}
+
+	/* check SETUP transfer byte */
+	if (bytes != 8) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"SETUP tranfer byte is invalid at bytes = %u.\n",
+			(unsigned int) bytes);
+		/* check new setup transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* protocol stall */
+			halt_transfer(endpoint);
+		return;
+	}
+
+	/* clear transfer halt */
+	endpoint->halt = 0;
+	endpoint->force_halt = 0;
+
+	/* dequeue all previous control transfer request */
+	dequeue_all_transfer_request(endpoint, -EPROTO);
+
+	/* read SETUP transfer data */
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		setup.word[0] = get_ss_epoutbuf(base_addr, ENDPOINT0);
+		setup.word[1] = get_ss_epoutbuf(base_addr, ENDPOINT0);
+	} else {
+		setup.word[0] = get_hs_epoutbuf(base_addr, ENDPOINT0);
+		setup.word[1] = get_hs_epoutbuf(base_addr, ENDPOINT0);
+	}
+
+	/* check new setup transfer */
+	if (is_setup_transferred(f_usb30))
+		return;
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		/* enable next SS OUT transfer */
+		enable_ss_ready0o(base_addr);
+
+		/* disable SS IntReady0i / IntReady0o interrupt */
+		set_ss_mskready0i(base_addr, 1);
+		set_ss_mskready0o(base_addr, 1);
+	} else {
+		/* enable HS/FS next OUT transfer */
+		enable_hs_ready0o(base_addr);
+
+		/* disable HS/FS IntReady0i / IntReady0o interrupt */
+		set_hs_mskready0i(base_addr, 1);
+		set_hs_mskready0o(base_addr, 1);
+	}
+
+	/* update control transfer stage */
+	if (setup.ctrlreq.bRequestType & USB_DIR_IN) {
+		f_usb30->ctrl_stage = F_USB30_STAGE_IN_DATA;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is IN data stage.\n");
+	} else {
+		if (setup.ctrlreq.wLength) {
+			f_usb30->ctrl_stage = F_USB30_STAGE_OUT_DATA;
+			dev_dbg(f_usb30->gadget.dev.parent,
+			 "next control transfer stage is OUT data stage.\n");
+		} else {
+			f_usb30->ctrl_stage = F_USB30_STAGE_IN_STATUS;
+			dev_dbg(f_usb30->gadget.dev.parent,
+			 "next control transfer stage is IN status stage.\n");
+		}
+	}
+
+	/* notify SETUP transfer to gadget driver */
+	if ((f_usb30->gadget_driver) && (f_usb30->gadget_driver->setup)) {
+		spin_unlock(&f_usb30->lock);
+		result = f_usb30->gadget_driver->setup(&f_usb30->gadget,
+							 &setup.ctrlreq);
+		if (result < 0) {
+			if (!is_setup_transferred(f_usb30))
+				/* protocol stall */
+				halt_transfer(endpoint);
+			dev_err(f_usb30->gadget.dev.parent,
+			 "SETUP transfer to gadget driver is failed at %d.\n",
+			 result);
+			spin_lock(&f_usb30->lock);
+			return;
+		}
+		spin_lock(&f_usb30->lock);
+	} else {
+		if (!is_setup_transferred(f_usb30))
+			/* protocol stall */
+			halt_transfer(endpoint);
+		return;
+	}
+
+	if (is_setup_transferred(f_usb30))
+		return;
+
+	/* enable status stage transfer */
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		if (setup.ctrlreq.bRequestType & USB_DIR_IN) {
+			f_usb30->ctrl_pri_dir = 1;
+			set_ss_mskready0o(base_addr, 0);
+		} else {
+			f_usb30->ctrl_pri_dir =
+					 setup.ctrlreq.wLength ? 0 : 1;
+			enable_ss_ready0i(base_addr);
+			set_ss_mskready0i(base_addr, 0);
+		}
+	} else {
+		if (setup.ctrlreq.bRequestType & USB_DIR_IN) {
+			f_usb30->ctrl_pri_dir = 1;
+			set_hs_mskready0o(base_addr, 0);
+		} else {
+			f_usb30->ctrl_pri_dir =
+					 setup.ctrlreq.wLength ? 0 : 1;
+			enable_hs_ready0i(base_addr);
+			set_hs_mskready0i(base_addr, 0);
+		}
+	}
+
+	return;
+}
+
+static void on_end_control_in_transfer(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+
+	/* process control transfer stage */
+	switch (f_usb30->ctrl_stage) {
+	case F_USB30_STAGE_IN_DATA:
+		/* check new SETUP transfer */
+		if (is_setup_transferred(f_usb30))
+			break;
+
+		/* check IN transfer continue */
+		if (!end_in_transfer_pio(endpoint))
+			break;
+
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_OUT_STATUS;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is OUT status stage.\n");
+
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30)) {
+			if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+				/* setup NULL packet for USB host */
+				set_ss_ctldone0(f_usb30->
+						register_base_address, 1);
+			} else {
+				/* setup NULL packet for USB host */
+				enable_hs_ready0i(f_usb30->
+						register_base_address);
+			}
+		}
+		break;
+	case F_USB30_STAGE_OUT_DATA:
+	case F_USB30_STAGE_IN_STATUS:
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is SETUP stage.\n");
+
+		/* notify request complete */
+		notify_transfer_request_complete(endpoint, 0);
+		break;
+	case F_USB30_STAGE_OUT_STATUS:
+	case F_USB30_STAGE_SPECIAL:
+		/* non process */
+		break;
+	default:
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* protocol stall */
+			halt_transfer(endpoint);
+
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is SETUP stage.\n");
+		break;
+	}
+
+	return;
+}
+
+static void on_end_control_out_transfer(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+
+	/* process control transfer stage */
+	switch (f_usb30->ctrl_stage) {
+	case F_USB30_STAGE_OUT_DATA:
+		/* check new SETUP transfer */
+		if (is_setup_transferred(f_usb30))
+			break;
+
+		/* check OUT transfer continue */
+		if (!end_out_transfer_pio(endpoint))
+			break;
+
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_IN_STATUS;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is IN status stage.\n");
+		break;
+	case F_USB30_STAGE_IN_DATA:
+	case F_USB30_STAGE_OUT_STATUS:
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is SETUP stage.\n");
+
+		/* notify request complete */
+		notify_transfer_request_complete(endpoint, 0);
+		break;
+	case F_USB30_STAGE_SPECIAL:
+		/* non process */
+		break;
+	default:
+		/* check new SETUP transfer */
+		if (!is_setup_transferred(f_usb30))
+			/* protocol stall */
+			halt_transfer(endpoint);
+
+		/* update control transfer stage */
+		f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"next control transfer stage is SETUP stage.\n");
+		break;
+	}
+
+	return;
+}
+
+static void on_end_in_transfer(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+
+	/* check IN transfer continue */
+	if (!end_in_transfer(endpoint))
+		/* to be continued */
+		return;
+
+	/* notify request complete */
+	notify_transfer_request_complete(endpoint, 0);
+
+	/* check next queue empty */
+	if (list_empty(&endpoint->queue))
+		return;
+
+	/* get next request */
+	request = list_entry(endpoint->queue.next,
+				 struct f_usb30_request, queue);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u is next queue at request = 0x%p, length = %u, buf = 0x%p.\n",
+		ep_channel, request, request->req.length, request->req.buf);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint halt status is %u.\n", endpoint->halt);
+
+	/* save request */
+	endpoint->req = request;
+
+	/* set endpoint x IN trasfer request */
+	if (!set_in_transfer(endpoint)) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"set_in_transfer() of endpoint %u is failed.\n",
+			 ep_channel);
+		dequeue_all_transfer_request(endpoint, -EL2HLT);
+	}
+
+	return;
+}
+
+static void on_end_out_transfer(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+
+	/* check OUT transfer continue */
+	if (!end_out_transfer(endpoint))
+		/* to be continued */
+		return;
+
+	/* notify request complete */
+	notify_transfer_request_complete(endpoint, 0);
+
+	/* check next queue empty */
+	if (list_empty(&endpoint->queue))
+		return;
+
+	/* get next request */
+	request = list_entry(endpoint->queue.next,
+				 struct f_usb30_request, queue);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u is next queue at request = 0x%p, length = %u, buf = 0x%p.\n",
+		ep_channel, request, request->req.length, request->req.buf);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint halt status is %u.\n", endpoint->halt);
+
+	/* save request */
+	endpoint->req = request;
+
+	/* set endpoint x OUT transfer request */
+	if (!set_out_transfer(endpoint)) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"set_in_transfer() of endpoint %u is failed.\n",
+			ep_channel);
+		dequeue_all_transfer_request(endpoint, -EL2HLT);
+	}
+
+	return;
+}
+
+static void on_halt_transfer(struct f_usb30_ep *endpoint)
+{
+	/* set transfer halt */
+	endpoint->halt = 1;
+
+	return;
+}
+
+static void on_clear_transfer_halt(struct f_usb30_ep *endpoint)
+{
+	struct f_usb30_udc *f_usb30 = endpoint->udc;
+	struct f_usb30_request *request;
+	unsigned char ep_channel = endpoint->ep.address &
+						 USB_ENDPOINT_NUMBER_MASK;
+
+	/* check force halt */
+	if (endpoint->force_halt) {
+		/* re-halt transfer */
+		halt_transfer(endpoint);
+		return;
+	}
+
+	/* clear transfer halt */
+	endpoint->halt = 0;
+
+	/* check next queue empty */
+	if (list_empty(&endpoint->queue))
+		return;
+
+	/* get next request */
+	request = list_entry(endpoint->queue.next,
+				 struct f_usb30_request, queue);
+
+	/* check the got next request a request under current execution */
+	if (request->req.actual)
+		return;
+
+	/* save request */
+	endpoint->req = request;
+
+	/* set endpoint x transfer request */
+	if (!(endpoint->ep.address & USB_DIR_IN ? set_in_transfer(endpoint) :
+		 set_out_transfer(endpoint))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"set_in_transfer() of endpoint %u is failed.\n",
+			ep_channel);
+		dequeue_all_transfer_request(endpoint, -EL2HLT);
+	}
+
+	return;
+}
+
+static void on_set_alternate(struct f_usb30_udc *f_usb30,
+	 unsigned char interface_channel)
+{
+	unsigned long counter;
+	struct usb_ctrlrequest ctrlreq;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	/* setup NULL packet for USB host */
+	unsigned char alternate = f_usb30->gadget.speed == USB_SPEED_SUPER ?
+				 get_ss_alt(f_usb30->
+				register_base_address, interface_channel) :
+				 get_hs_curaif(f_usb30->
+				register_base_address, interface_channel);
+
+
+	/* set endpoint max packet */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		if (endpoint[counter].interface_channel ==
+					 (signed char) interface_channel)
+			endpoint[counter].ep.maxpacket =
+				ep_fifo_size[counter][
+				f_usb30->gadget.speed][alternate];
+
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++)
+		if (endpoint[counter].interface_channel ==
+					 (signed char) interface_channel)
+			dequeue_all_transfer_request(&endpoint[counter],
+							 -ECONNABORTED);
+
+	if (f_usb30->gadget.speed == USB_SPEED_SUPER) {
+		for (counter = ENDPOINT1;
+				counter < F_USB30_MAX_EP; counter++)
+			if (endpoint[counter].ep.comp_desc->bmAttributes)
+				set_ss_enstream(f_usb30->
+						register_base_address,
+						 counter,
+						 alternate ? 1 : 0);
+	}
+
+	/* create SET_INTERFACE SETUP data */
+	ctrlreq.bRequestType = USB_DIR_OUT | USB_TYPE_STANDARD |
+				 USB_RECIP_INTERFACE;
+	ctrlreq.bRequest = USB_REQ_SET_INTERFACE;
+	ctrlreq.wValue = alternate;
+	ctrlreq.wIndex = interface_channel;
+	ctrlreq.wLength = 0;
+
+	/* update control transfer stage */
+	f_usb30->ctrl_stage = F_USB30_STAGE_SPECIAL;
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"next control transfer stage is special stage.\n");
+
+	/* notify SET_INTERFACE to gadget driver */
+	if ((f_usb30->gadget_driver) && (f_usb30->gadget_driver->setup)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->setup(&f_usb30->gadget, &ctrlreq);
+		spin_lock(&f_usb30->lock);
+	}
+	return;
+}
+
+static void on_hungup_controller(struct f_usb30_udc *f_usb30)
+{
+	/* disconnect host */
+	dev_dbg(f_usb30->gadget.dev.parent, "D+ terminal is pull-down.\n");
+	connect_hs_host(f_usb30, 0);
+
+	/* notify disconnect to gadget driver */
+	if ((f_usb30->gadget_driver) &&
+		 (f_usb30->gadget_driver->disconnect)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->disconnect(&f_usb30->gadget);
+		spin_lock(&f_usb30->lock);
+	}
+
+	/* check bus connect */
+	if (f_usb30->vbus) {
+		/* connect host */
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"D+ terminal is pull-up.\n");
+		connect_hs_host(f_usb30, 1);
+	}
+
+	return;
+}
+
+#if defined(CONFIG_ARCH_MB8AC0300)
+static irqreturn_t on_bus_connect(int irq, void *dev_id)
+{
+	struct f_usb30_udc *f_usb30;
+
+	/* check argument */
+	if (unlikely(!dev_id))
+		return IRQ_NONE;
+
+	/* get a device driver parameter */
+	f_usb30 = (struct f_usb30_udc *) dev_id;
+
+	/* F_USB30 vbus on detect external interrupt number check */
+	if (unlikely(irq != f_usb30->vbus_on_irq)) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"%s() is ended at non process.\n", __func__);
+		return IRQ_NONE;
+	}
+
+	if (f_usb30->vbus)
+		return IRQ_HANDLED;
+
+	/* get spin lock */
+	spin_lock(&f_usb30->lock);
+
+	/* vbus on */
+	f_usb30->vbus = 1;
+
+	/* clear disable counter */
+	f_usb30->ss_discnt = 0;
+
+	/* change vbus on detect external interrupt type */
+	exiu_irq_set_type(f_usb30->vbus_on_hwirq,
+				 f_usb30->vbus_active_level ?
+				 IRQF_TRIGGER_RISING : IRQF_TRIGGER_FALLING);
+
+	/* change link state */
+	f_usb30->link_state = F_USB30_LINK_STATE_ATTACHED;
+
+	/* connect host */
+	f_usb30->gadget_driver->max_speed == USB_SPEED_SUPER ?
+		 connect_ss_host(f_usb30, 1) :
+		 connect_hs_host(f_usb30, 1);
+
+	/* release spin lock */
+	spin_unlock(&f_usb30->lock);
+	dev_dbg(f_usb30->gadget.dev.parent, "Link training starts.\n");
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t on_bus_disconnect(int irq, void *dev_id)
+{
+	struct f_usb30_udc *f_usb30;
+
+	/* check argument */
+	if (unlikely(!dev_id))
+		return IRQ_NONE;
+
+	/* get a device driver parameter */
+	f_usb30 = (struct f_usb30_udc *) dev_id;
+
+	/* F_USB30 vbus off detect external interrupt number check */
+	if (unlikely(irq != f_usb30->vbus_off_irq)) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"%s() is ended at non process.\n", __func__);
+		return IRQ_NONE;
+	}
+
+	/* get spin lock */
+	spin_lock(&f_usb30->lock);
+
+	/* vbus off */
+	f_usb30->vbus = 0;
+
+	/* change link state */
+	f_usb30->link_state = F_USB30_LINK_STATE_NOTATTACHED;
+
+	/* disconnect host */
+	f_usb30->gadget.speed == USB_SPEED_SUPER ?
+		 connect_ss_host(f_usb30, 0) :
+		 connect_hs_host(f_usb30, 0);
+	/* notify disconnect to gadget driver */
+	if ((f_usb30->gadget_driver) &&
+		 (f_usb30->gadget_driver->disconnect)) {
+		spin_unlock(&f_usb30->lock);
+		f_usb30->gadget_driver->disconnect(&f_usb30->gadget);
+		spin_lock(&f_usb30->lock);
+	}
+
+	/* release spin lock */
+	spin_unlock(&f_usb30->lock);
+	dev_dbg(f_usb30->gadget.dev.parent, "vbus is off .\n");
+
+	return IRQ_HANDLED;
+}
+#else
+/* for other soc */
+#endif
+
+static irqreturn_t on_usb_hs_function_controller(int irq, void *dev_id)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = dev_id;
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	/* check argument */
+	if (unlikely(!dev_id))
+		return IRQ_NONE;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+							 __func__);
+
+	/* F_USB30 HS/FS controller interrupt request assert check */
+	if (unlikely(irq != f_usb30->hs_irq)) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"%s() is ended at non process.\n", __func__);
+		return IRQ_NONE;
+	}
+
+	/* get spin lock */
+	spin_lock(&f_usb30->lock);
+
+	/* PHY hung-up interrupt factor */
+	if ((get_hs_interraticerr(base_addr)) &&
+		 (!get_hs_mskerraticerr(base_addr))) {
+		/* clear IntErraticErr interrupt factor */
+		clear_hs_interraticerr(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntErraticErr interrrupt occurred.\n");
+
+		/* process hung-up controller */
+		on_hungup_controller(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* bus reset begin interrupt factor */
+	if ((get_hs_intusbrstb(base_addr)) && (!get_hs_mskusbrstb(base_addr))) {
+		/* clear IntUsbRstb interrupt factor */
+		clear_hs_intusbrstb(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntUsbRstb interrrupt occurred.\n");
+
+		/* process bus reset begin */
+		on_begin_warm_or_bus_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* bus reset end interrupt factor */
+	if ((get_hs_intusbrste(base_addr)) && (!get_hs_mskusbrste(base_addr))) {
+		/* clear IntUsbRste interrupt factor */
+		clear_hs_intusbrste(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntUsbRste interrrupt occurred.\n");
+
+		/* process bus reset end */
+		on_end_bus_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* suspend begin interrupt factor */
+	if ((get_hs_intsuspendb(base_addr)) &&
+		 (!get_hs_msksuspendb(base_addr))) {
+		/* clear IntSuspendb interrupt factor */
+		clear_hs_intsuspendb(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntSuspendb interrrupt occurred.\n");
+
+		/* process bus suspend */
+		on_suspend(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* suspend end interrupt factor */
+	if ((get_hs_intsuspende(base_addr)) &&
+		 (!get_hs_msksuspende(base_addr))) {
+		/* clear IntSuspende interrupt factor */
+		clear_hs_intsuspende(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntSuspende interrrupt occurred.\n");
+
+		/* process bus wakeup */
+		on_wakeup(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SETUP transfer interrupt factor */
+	if ((get_hs_intsetup(base_addr)) && (!get_hs_msksetup(base_addr))) {
+		/* clear IntSetup interrupt factor */
+		clear_hs_intsetup(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntSetup interrrupt is occurred.\n");
+
+		/* wait & check register access enable */
+		for (counter = 0xFFFF;
+			 ((counter) && ((!get_hs_intready0i(base_addr)) ||
+			(!get_hs_intready0o(base_addr)))); counter--)
+			;
+		if ((get_hs_intready0i(base_addr)) &&
+			 (get_hs_intready0o(base_addr))) {
+			clear_hs_intready0i(base_addr);
+			clear_hs_intready0o(base_addr);
+			/* process control SETUP transfer end */
+			on_end_control_setup_transfer(&endpoint[ENDPOINT0]);
+		} else {
+			/* protocol stall */
+			halt_transfer(&endpoint[ENDPOINT0]);
+		}
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SET_CONFIGURATION interrupt factor */
+	if ((get_hs_intsetconf(base_addr)) && (!get_hs_msksetconf(base_addr))) {
+		/* clear IntSetConf interrupt factor */
+		clear_hs_intsetconf(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntSetConf interrrupt occurred.\n");
+
+		/* process configure to host */
+		on_configure(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* endpoint x interrupt factor */
+	for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++) {
+		if ((get_hs_intep(base_addr, counter)) &&
+			 (!get_hs_mskep(base_addr, counter))) {
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "IntEp%u interrrupt occurred.\n",
+				 (unsigned int) counter);
+
+			if ((get_hs_intready(base_addr, counter)) &&
+				 (!get_hs_mskready(base_addr, counter))) {
+				/* clear IntReady x interrupt factor */
+				clear_hs_intready(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "IntReady interrrupt occurred.\n");
+
+				/* process IN / OUT transfer end */
+				endpoint[counter].ep.address & USB_DIR_IN ?
+					on_end_in_transfer(&endpoint[counter]) :
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intempty(base_addr, counter)) &&
+				 (!get_hs_mskempty(base_addr, counter))) {
+				/* clear IntEmpty x interrupt factor */
+				clear_hs_intempty(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "IntEmpty interrrupt occurred.\n");
+
+				/* process IN transfer end */
+				if (endpoint[counter].ep.address & USB_DIR_IN)
+					on_end_in_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intdend(base_addr, counter)) &&
+				 (!get_hs_mskdend(base_addr, counter))) {
+				/* clear IntDEnd x interrupt factor */
+				clear_hs_intdend(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "IntDEnd interrrupt occurred.\n");
+
+				/* process IN / OUT transfer end */
+				endpoint[counter].ep.address & USB_DIR_IN ?
+					on_end_in_transfer(&endpoint[counter]) :
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intspdd(base_addr, counter)) &&
+				 (!get_hs_mskspdd(base_addr, counter))) {
+				/* clear IntSPDD x interrupt factor */
+				clear_hs_intspdd(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "IntSPDD interrrupt occurred.\n");
+
+				/* process OUT transfer end */
+				if (!(endpoint[counter].ep.address &
+								 USB_DIR_IN))
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intstalled(base_addr, counter)) &&
+				 (!get_hs_mskstalled(base_addr, counter))) {
+				/* clear IntStalled x interrupt factor */
+				clear_hs_intstalled(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+							 "IntStalled interrrupt"
+							" is occurred.\n");
+
+				/* process transfer halt */
+				on_halt_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intclstall(base_addr, counter)) &&
+				 (!get_hs_mskclstall(base_addr, counter))) {
+				/* clear IntClStall x interrupt factor */
+				clear_hs_intclstall(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "IntClStall interrrupt "
+						 "is occurred.\n");
+
+				/* process transfer halt clear */
+				on_clear_transfer_halt(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+					 "other interrrupt occurred.\n");
+
+			/* clear endpoint x interrupt */
+			clear_hs_intspr(base_addr, counter);
+			clear_hs_intspdd(base_addr, counter);
+			clear_hs_intready(base_addr, counter);
+			clear_hs_intping(base_addr, counter);
+			clear_hs_intdend(base_addr, counter);
+			clear_hs_intempty(base_addr, counter);
+			clear_hs_intstalled(base_addr, counter);
+			clear_hs_intnack(base_addr, counter);
+			clear_hs_intclstall(base_addr, counter);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "%s() is ended.\n", __func__);
+
+			return IRQ_HANDLED;
+		}
+	}
+
+	/* interface x interrupt factor */
+	for (counter = INTERFACE0; counter < F_USB30_MAX_EP; counter++) {
+		if ((get_hs_intachgif(base_addr, counter)) &&
+			 (!get_hs_mskachgif(base_addr, counter))) {
+			/* clear IntAChgIf x interrupt factor */
+			clear_hs_intachgif(base_addr, counter);
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"IntAChgIf%u interrrupt occurred.\n",
+				(unsigned int) counter);
+
+			/* process alternate set */
+			on_set_alternate(f_usb30, counter);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+			return IRQ_HANDLED;
+		}
+	}
+
+	/* endpoint 0 interrupt factor */
+	if ((get_hs_intep(base_addr, ENDPOINT0)) &&
+		 (!get_hs_mskep(base_addr, ENDPOINT0))) {
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntEp0 interrrupt is occurred.\n");
+
+		/* check priority direction */
+		if (f_usb30->ctrl_pri_dir) {
+			/* priority is given to IN transfer */
+			if ((get_hs_intready0i(base_addr)) &&
+				 (!get_hs_mskready0i(base_addr))) {
+				/* clear IntReady0i interrupt factor */
+				clear_hs_intready0i(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "IntReady0i interrrupt "
+						 "is occurred.\n");
+
+				/* process control IN transfer end */
+				on_end_control_in_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intready0o(base_addr)) &&
+				 (!get_hs_mskready0o(base_addr))) {
+				/* clear IntReady0o interrupt factor */
+				clear_hs_intready0o(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "IntReady0o interrrupt "
+						 "is occurred.\n");
+
+				/* process control OUT transfer end */
+				on_end_control_out_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+		} else {
+			/* priority is given to OUT transfer */
+			if ((get_hs_intready0o(base_addr)) &&
+				 (!get_hs_mskready0o(base_addr))) {
+				/* clear IntReady0o interrupt factor */
+				clear_hs_intready0o(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "IntReady0o interrrupt "
+						 "is occurred.\n");
+
+				/* process control OUT transfer end */
+				on_end_control_out_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_hs_intready0i(base_addr)) &&
+				 (!get_hs_mskready0i(base_addr))) {
+				/* clear IntReady0i interrupt factor */
+				clear_hs_intready0i(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "IntReady0i interrrupt "
+						 "is occurred.\n");
+
+				/* process control IN transfer end */
+				on_end_control_in_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+		}
+
+		if ((get_hs_intstalled0(base_addr)) &&
+			 (!get_hs_mskstalled0(base_addr))) {
+			/* clear IntStalled0 interrupt factor */
+			clear_hs_intstalled0(base_addr);
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"IntStalled0 interrrupt occurred.\n");
+
+			/* process transfer halt */
+			on_halt_transfer(&endpoint[ENDPOINT0]);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "%s() is ended.\n", __func__);
+
+			return IRQ_HANDLED;
+		}
+
+		if ((get_hs_intclstall0(base_addr)) &&
+			 (!get_hs_mskclstall0(base_addr))) {
+			/* clear IntClStall0 interrupt factor */
+			clear_hs_intclstall0(base_addr);
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"IntClStall0 interrrupt occurred.\n");
+
+			/* process transfer halt clear */
+			on_clear_transfer_halt(
+				&endpoint[ENDPOINT0]);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n"
+								, __func__);
+
+			return IRQ_HANDLED;
+		}
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"other interrrupt is occurred.\n");
+
+		/* clear endpoint 0 interrupt */
+		clear_hs_intready0o(base_addr);
+		clear_hs_intready0i(base_addr);
+		clear_hs_intping0o(base_addr);
+		clear_hs_intstalled0(base_addr);
+		clear_hs_intnack0(base_addr);
+		clear_hs_intclstall0(base_addr);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	spin_unlock(&f_usb30->lock);
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n"
+						, __func__);
+
+	return IRQ_HANDLED;
+}
+
+static irqreturn_t on_usb_ss_function_controller(int irq, void *dev_id)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = dev_id;
+	void *base_addr = f_usb30->register_base_address;
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	/* check argument */
+	if (unlikely(!dev_id))
+		return IRQ_NONE;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+							 __func__);
+
+	/* F_USB30 SS controller interrupt request assert check */
+	if (unlikely(irq != f_usb30->ss_irq)) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"%s() is ended at non process.\n", __func__);
+		return IRQ_NONE;
+	}
+
+	/* get spin lock */
+	spin_lock(&f_usb30->lock);
+
+	/* warm reset begin interrupt factor */
+	if ((get_ss_intsswrstb(base_addr)) && (!get_ss_msksswrstb(base_addr))) {
+		/* clear IntSSWRstb interrupt factor */
+		clear_ss_intsswrstb(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSSWRstb interrrupt occurred.\n");
+
+		/* process warm reset begin */
+		on_begin_warm_or_bus_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* warm reset end interrupt factor */
+	if ((get_ss_intsswrste(base_addr)) && (!get_ss_msksswrste(base_addr))) {
+		/* clear IntSSWRste interrupt factor */
+		clear_ss_intsswrste(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSSWRste interrrupt occurred.\n");
+
+		/* process warm reset end */
+		on_end_warm_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* hot reset begin interrupt factor */
+	if ((get_ss_intsshrstb(base_addr)) && (!get_ss_msksshrstb(base_addr))) {
+		/* clear IntSSHRstb interrupt factor */
+		clear_ss_intsshrstb(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSSHRstb interrrupt occurred.\n");
+
+		if (f_usb30->link_state == F_USB30_LINK_STATE_SSENABLED)
+			/* process hot reset begin */
+			on_begin_hot_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* hot reset end interrupt factor */
+	if ((get_ss_intsshrste(base_addr)) && (!get_ss_msksshrste(base_addr))) {
+		/* clear IntSSHRste interrupt factor */
+		clear_ss_intsshrste(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSSHRste interrrupt occurred.\n");
+
+		/* process hot reset end */
+		on_end_hot_reset(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SS.Disabled interrupt factor */
+	if ((get_ss_intssdisable(base_addr)) &&
+		 (!get_ss_mskssdisable(base_addr))) {
+		/* clear IntSSDisable interrupt factor */
+		clear_ss_intssdisable(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSSDisable interrrupt occurred.\n");
+
+		/* process moving to SS.Disabled state */
+		on_ss_disabled(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* EnterPoll interrupt factor */
+	if ((get_ss_intenterpoll(base_addr)) &&
+		 (!get_ss_mskenterpoll(base_addr))) {
+		/* clear IntEnterPoll interrupt factor */
+		clear_ss_intenterpoll(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntEnterPoll interrrupt occurred.\n");
+
+		/* process moving HS/FS to Poll state */
+		on_enterpoll(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* PolltoU0 interrupt factor */
+	if ((get_ss_intpolltou0(base_addr)) &&
+		 (!get_ss_mskpolltou0(base_addr))) {
+		/* clear IntPolltoU0 interrupt factor */
+		clear_ss_intpolltou0(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntPolltoU0 interrrupt occurred.\n");
+
+		/* process moving Poll to U0 state */
+		on_polltou0(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SS Suspend begin interrupt factor */
+	if ((get_ss_intsuspendb(base_addr)) &&
+		 (!get_ss_msksuspendb(base_addr))) {
+		/* clear IntSuspendb interrupt factor */
+		clear_ss_intsuspendb(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSuspendb interrrupt occurred.\n");
+
+		/* process bus suspend */
+		on_suspend(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SS Suspend end interrupt factor */
+	if ((get_ss_intsuspende(base_addr)) &&
+		 (!get_ss_msksuspende(base_addr))) {
+		/* clear IntSuspende interrupt factor */
+		clear_ss_intsuspende(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSuspende interrrupt occurred.\n");
+
+		/* process bus wakeup */
+		on_wakeup(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SS SETUP transfer interrupt factor */
+	if ((get_ss_intsetup(base_addr)) && (!get_ss_msksetup(base_addr))) {
+		/* clear SS IntSetup interrupt factor */
+		clear_ss_intsetup(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSetup interrrupt is occurred.\n");
+
+		/* wait & check register access enable */
+		for (counter = 0xFFFF;
+			 ((counter) && ((!get_ss_intready0i(base_addr)) ||
+			(!get_ss_intready0o(base_addr)))); counter--)
+			;
+		if ((get_ss_intready0i(base_addr)) &&
+			 (get_ss_intready0o(base_addr))) {
+			clear_ss_intready0i(base_addr);
+			clear_ss_intready0o(base_addr);
+			/* process control SETUP transfer end */
+			on_end_control_setup_transfer(&endpoint[ENDPOINT0]);
+		} else {
+			/* protocol stall */
+			halt_transfer(&endpoint[ENDPOINT0]);
+		}
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* SS SET_CONFIGURATION interrupt factor */
+	if ((get_ss_intsetconf(base_addr)) && (!get_ss_msksetconf(base_addr))) {
+		/* clear SS IntSetConf interrupt factor */
+		clear_ss_intsetconf(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSetConf interrrupt occurred.\n");
+
+		/* process configure to host */
+		on_configure(f_usb30);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* endpoint x interrupt factor */
+	for (counter = ENDPOINT1; counter < F_USB30_MAX_EP; counter++) {
+		if ((get_ss_intep(base_addr, counter)) &&
+			 (!get_ss_mskep(base_addr, counter))) {
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "SS IntEp%u interrrupt occurred.\n",
+				 (unsigned int) counter);
+
+			if ((get_ss_intready(base_addr, counter)) &&
+				 (!get_ss_mskready(base_addr, counter))) {
+				/* clear SS IntReady x interrupt factor */
+				clear_ss_intready(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "SS IntReady interrrupt occurred.\n");
+
+				/* process IN / OUT transfer end */
+				endpoint[counter].ep.address & USB_DIR_IN ?
+					on_end_in_transfer(&endpoint[counter]) :
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intempty(base_addr, counter)) &&
+				 (!get_ss_mskempty(base_addr, counter))) {
+				/* clear SS IntEmpty x interrupt factor */
+				clear_ss_intempty(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "SS IntEmpty interrrupt occurred.\n");
+
+				/* process IN transfer end */
+				if (endpoint[counter].ep.address & USB_DIR_IN)
+					on_end_in_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intdend(base_addr, counter)) &&
+				 (!get_ss_mskdend(base_addr, counter))) {
+				/* clear SS IntDEnd x interrupt factor */
+				clear_ss_intdend(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "SS IntDEnd interrrupt occurred.\n");
+
+				/* process IN / OUT transfer end */
+				endpoint[counter].ep.address & USB_DIR_IN ?
+					on_end_in_transfer(&endpoint[counter]) :
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intspd(base_addr, counter)) &&
+				 (!get_ss_mskspd(base_addr, counter))) {
+				/* clear IntSPDD x interrupt factor */
+				clear_ss_intspd(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "SS IntSPDD interrrupt occurred.\n");
+
+				/* process OUT transfer end */
+				if (!(endpoint[counter].ep.address&USB_DIR_IN))
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intstalled(base_addr, counter)) &&
+				 (!get_ss_mskstalled(base_addr, counter))) {
+				/* clear IntStalled x interrupt factor */
+				clear_ss_intstalled(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					"IntStalled interrrupt is occurred.\n");
+
+				/* process transfer halt */
+				on_halt_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intclstall(base_addr, counter)) &&
+				 (!get_ss_mskclstall(base_addr, counter))) {
+				/* clear IntClStall x interrupt factor */
+				clear_ss_intclstall(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					"IntClStall interrrupt is occurred.\n");
+
+				/* process transfer halt clear */
+				on_clear_transfer_halt(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intsdend(base_addr, counter)) &&
+				 (!get_ss_msksdend(base_addr, counter))) {
+				/* clear IntSDend x interrupt factor */
+				clear_ss_intsdend(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "SS IntSDend interrrupt occurred.\n");
+
+				/* process OUT transfer end */
+				if (!(endpoint[counter].ep.address&USB_DIR_IN))
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intclstream(base_addr, counter)) &&
+				 (!get_ss_mskclstream(base_addr, counter))) {
+				/* clear IntClStream x interrupt factor */
+				clear_ss_intclstream(base_addr, counter);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+					"SS IntClStream interrrupt occurred\n");
+
+				/* process OUT transfer end */
+				if (!(endpoint[counter].ep.address&USB_DIR_IN))
+					on_end_out_transfer(&endpoint[counter]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+					 "other interrrupt occurred.\n");
+
+			/* clear endpoint x interrupt */
+			clear_ss_intspr(base_addr, counter);
+			clear_ss_intspd(base_addr, counter);
+			clear_ss_intdend(base_addr, counter);
+			clear_ss_intsdend(base_addr, counter);
+			clear_ss_intempty(base_addr, counter);
+			clear_ss_intclstream(base_addr, counter);
+			clear_ss_intready(base_addr, counter);
+			clear_ss_intpktpnd(base_addr, counter);
+			clear_ss_intstalled(base_addr, counter);
+			clear_ss_intnrdy(base_addr, counter);
+			clear_ss_intclstall(base_addr, counter);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "%s() is ended.\n", __func__);
+
+			return IRQ_HANDLED;
+		}
+	}
+
+	/* SS SET_INTERFACE interrupt factor */
+	if ((get_ss_intsetintf(base_addr)) && (!get_ss_msksetintf(base_addr))) {
+		/* clear SS IntSetIntf interrupt factor */
+		clear_ss_intsetintf(base_addr);
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"SS IntSetIntf interrrupt occurred.\n");
+
+		/* process alternate set */
+		on_set_alternate(f_usb30, get_ss_achg(base_addr, 0));
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+			 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	/* endpoint 0 interrupt factor */
+	if ((get_ss_intep(base_addr, ENDPOINT0)) &&
+		 (!get_ss_mskep(base_addr, ENDPOINT0))) {
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"IntEp0 interrrupt is occurred.\n");
+
+		/* check priority direction */
+		if (f_usb30->ctrl_pri_dir) {
+			/* priority is given to IN transfer */
+			if ((get_ss_intready0i(base_addr)) &&
+				 (!get_ss_mskready0i(base_addr))) {
+				/* clear SS IntReady0i interrupt factor */
+				clear_ss_intready0i(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "SS IntReady0i interrrupt "
+						 "is occurred.\n");
+
+				/* process control IN transfer end */
+				on_end_control_in_transfer(
+							&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intready0o(base_addr)) &&
+				 (!get_ss_mskready0o(base_addr))) {
+				/* clear SS IntReady0o interrupt factor */
+				clear_ss_intready0o(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "SS IntReady0o interrrupt "
+						 "is occurred.\n");
+
+				/* process control OUT transfer end */
+				on_end_control_out_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+		} else {
+			/* priority is given to OUT transfer */
+			if ((get_ss_intready0o(base_addr)) &&
+				 (!get_ss_mskready0o(base_addr))) {
+				/* clear SS IntReady0o interrupt factor */
+				clear_ss_intready0o(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "SS IntReady0o interrrupt "
+						 "is occurred.\n");
+
+				/* process control OUT transfer end */
+				on_end_control_out_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+
+			if ((get_ss_intready0i(base_addr)) &&
+				 (!get_ss_mskready0i(base_addr))) {
+				/* clear SS IntReady0i interrupt factor */
+				clear_ss_intready0i(base_addr);
+
+				dev_dbg(f_usb30->gadget.dev.parent,
+						 "SS IntReady0i interrrupt "
+						 "is occurred.\n");
+
+				/* process control IN transfer end */
+				on_end_control_in_transfer(
+					&endpoint[ENDPOINT0]);
+
+				spin_unlock(&f_usb30->lock);
+				dev_dbg(f_usb30->gadget.dev.parent,
+					 "%s() is ended.\n", __func__);
+
+				return IRQ_HANDLED;
+			}
+		}
+
+		if ((get_ss_intstalled0(base_addr)) &&
+			 (!get_ss_mskstalled0(base_addr))) {
+			/* clear SS IntStalled0 interrupt factor */
+			clear_ss_intstalled0(base_addr);
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"SS IntStalled0 interrrupt occurred.\n");
+
+			/* process transfer halt */
+			on_halt_transfer(&endpoint[ENDPOINT0]);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent,
+				 "%s() is ended.\n", __func__);
+
+			return IRQ_HANDLED;
+		}
+
+		if ((get_ss_intclstall0(base_addr)) &&
+			 (!get_ss_mskclstall0(base_addr))) {
+			/* clear SS IntClStall0 interrupt factor */
+			clear_ss_intclstall0(base_addr);
+
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"SS IntClStall0 interrrupt occurred.\n");
+
+			/* process transfer halt clear */
+			on_clear_transfer_halt(&endpoint[ENDPOINT0]);
+
+			spin_unlock(&f_usb30->lock);
+			dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n"
+								, __func__);
+
+			return IRQ_HANDLED;
+		}
+
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"other interrrupt is occurred.\n");
+
+		/* clear endpoint 0 interrupt */
+		clear_ss_intready0o(base_addr);
+		clear_ss_intready0i(base_addr);
+		clear_ss_intpktpnd0(base_addr);
+		clear_ss_intstalled0(base_addr);
+		clear_ss_intnrdy0(base_addr);
+		clear_ss_intclstall0(base_addr);
+
+		spin_unlock(&f_usb30->lock);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+
+		return IRQ_HANDLED;
+	}
+
+	spin_unlock(&f_usb30->lock);
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n"
+						, __func__);
+
+	return IRQ_HANDLED;
+}
+
+#define to_fusb30_udc(g)  (container_of((g), struct f_usb30_udc, gadget))
+
+/**
+ * f_usb30_udc_start - probe a gadget driver
+ * @driver: the driver being registered
+ *
+ * When a driver is successfully registered, it will receive control requests
+ * including set_configuration, which enables non-control requests.  Then
+ * usb traffic follows until a disconnect is reported.  Then a host may connect
+ * again, or the driver might get unbound.
+ *
+ * Returns 0 if no error, -EINVAL -EBUSY or other negative errno on failure
+ */
+static int f_usb30_udc_start(struct usb_gadget *g,
+		struct usb_gadget_driver *driver)
+{
+	struct f_usb30_udc *f_usb30 = to_fusb30_udc(g);
+	int result;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* initialize endpoint configure data */
+	initialize_endpoint_configure(f_usb30);
+
+	/* entry gadget driver structure */
+	f_usb30->gadget_driver = driver;
+
+	/* initialize F_USB30 UDC device driver structure data */
+	f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+	f_usb30->device_state = USB_STATE_NOTATTACHED;
+	f_usb30->device_state_last = USB_STATE_NOTATTACHED;
+	f_usb30->configure_value_last = 0;
+	f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+	f_usb30->ctrl_pri_dir = 1;
+
+#if defined(CONFIG_ARCH_MB8AC0300)
+	f_usb30->vbus = 0;
+	/* set vbus on detect external interrupt type */
+	exiu_irq_set_type(f_usb30->vbus_on_hwirq,
+				 f_usb30->vbus_active_level ?
+				 IRQF_TRIGGER_HIGH : IRQF_TRIGGER_LOW);
+	exiu_irq_set_type(f_usb30->vbus_off_hwirq,
+				 f_usb30->vbus_active_level ?
+				 IRQF_TRIGGER_FALLING : IRQF_TRIGGER_RISING);
+	/* entry a F_USB30 vbus off IRQ */
+	result = request_irq(f_usb30->vbus_off_irq, on_bus_disconnect,
+				 0, "f_usb30_udc_vbus_off", f_usb30);
+	if (result) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"request_irq() for F_USB30 vbus off is failed at %d.\n",
+			result);
+		free_irq(f_usb30->vbus_on_irq, f_usb30);
+		return result;
+	}
+
+	/* entry a F_USB30 vbus on IRQ */
+	result = request_irq(f_usb30->vbus_on_irq, on_bus_connect,
+				 0, "f_usb30_udc_vbus_on", f_usb30);
+	if (result) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"request_irq() for F_USB30 vbus on is failed at %d.\n",
+			result);
+		return result;
+	}
+#else
+/* for other soc */
+#endif
+	return 0;
+}
+
+/**
+ * f_usb30_stop - Unregister the gadget driver
+ * @driver:	gadget driver
+ *
+ * Returns 0 if no error, -ENODEV -EINVAL or other negative errno on failure
+ */
+static int f_usb30_udc_stop(struct usb_gadget *g,
+		struct usb_gadget_driver *driver)
+{
+	unsigned long flags;
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30 = to_fusb30_udc(g);
+	struct f_usb30_ep *endpoint = &f_usb30->udc_endpoint[0];
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* initialize F_USB30 controller */
+	initialize_controller(f_usb30, f_usb30->gadget.speed ==
+				 USB_SPEED_SUPER ? 1 : 0);
+
+#if defined(CONFIG_ARCH_MB8AC0300)
+	/* disable vbus on external interrupt */
+	free_irq(f_usb30->vbus_on_irq, f_usb30);
+	free_irq(f_usb30->vbus_off_irq, f_usb30);
+#else
+/* for other soc */
+#endif
+
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		endpoint[counter].halt = 1;
+		dequeue_all_transfer_request(&endpoint[counter], -ESHUTDOWN);
+	}
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	/* initialize endpoint list data */
+	INIT_LIST_HEAD(&f_usb30->gadget.ep0->ep_list);
+	INIT_LIST_HEAD(&f_usb30->gadget.ep_list);
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		list_add_tail(&endpoint[counter].ep.ep_list,
+			 counter == ENDPOINT0 ?
+			 &f_usb30->gadget.ep0->ep_list :
+			 &f_usb30->gadget.ep_list);
+		INIT_LIST_HEAD(&endpoint[counter].queue);
+		endpoint[counter].halt = 0;
+		endpoint[counter].force_halt = 0;
+	}
+
+	f_usb30->gadget_driver = NULL;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return 0;
+}
+
+/**
+ * f_usb30_udc_ep_enable - Enables usb endpoint
+ * @ep:		usb endpoint
+ * @desc:	usb endpoint descriptor
+ *
+ * Returns 0 if no error, -EINVAL -ESHUTDOWN -ERANGE on failure
+ */
+static int f_usb30_udc_ep_enable(struct usb_ep *ep,
+	 const struct usb_endpoint_descriptor *desc)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely((!ep) || (!desc)))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((ep_channel == ENDPOINT0) || (endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			 "endpoint parameter is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EINVAL;
+	}
+
+	/* check endpoint descriptor parameter */
+	if (unlikely((desc->bDescriptorType != USB_DT_ENDPOINT) ||
+		 (endpoint->ep.address != desc->bEndpointAddress) ||
+		/*
+		 * This additional check is necessary since ep_matches()
+		 * in epautoconf.c may choose BULK-type endpoint
+		 * in case of INTERRUPT transfer.
+		 */
+		 (desc->bmAttributes == USB_ENDPOINT_XFER_INT ?
+			 ((endpoint->attributes != USB_ENDPOINT_XFER_INT) &&
+			 (endpoint->attributes != USB_ENDPOINT_XFER_BULK)) :
+			 desc->bmAttributes != endpoint->attributes))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint descriptor is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EINVAL;
+	}
+
+	/* check gadget driver parameter */
+	if (unlikely(!f_usb30->gadget_driver)) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"device state is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -ESHUTDOWN;
+	}
+
+	/* check max packet size */
+	if (unlikely(endpoint->ep.maxpacket <  usb_endpoint_maxp(desc))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint %u max packet size is invalid.\n",
+			ep_channel);
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -ERANGE;
+	}
+
+	/* set endpoint parameter */
+	endpoint->ep.desc = desc;
+	dev_dbg(f_usb30->gadget.dev.parent, "end point %u is enabled.\n",
+							 ep_channel);
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 0, 0);
+
+	/* set endpoint parameter */
+	endpoint->enabled = 1;
+	endpoint->halt = 0;
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return 0;
+}
+
+/**
+ * f_usb30_udc_ep_disable - Disable usb endpoint
+ * @ep:	usb endpoint
+ *
+ * Returns 0 if no error, -EINVAL on failure
+ */
+static int f_usb30_udc_ep_disable(struct usb_ep *ep)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely(!ep))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((ep_channel == ENDPOINT0) || (!endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint parameter is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EINVAL;
+	}
+
+	dev_dbg(f_usb30->gadget.dev.parent, "end point %u is disabled.\n",
+							 ep_channel);
+
+	/* disable DMA transfer */
+	enable_dma_transfer(endpoint, 0);
+
+	/* initialize endpoint */
+	initialize_endpoint_hw(endpoint, 1, 1);
+
+	/* set endpoint parameter */
+	endpoint->enabled = 0;
+	endpoint->halt = 1;
+
+	/* dequeue all previous transfer request */
+	dequeue_all_transfer_request(endpoint, -ESHUTDOWN);
+
+	endpoint->ep.desc = NULL;
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return 0;
+}
+
+/**
+ * f_usb30_udc_ep_alloc_request - Allocate usb request
+ * @ep:	usb endpoint
+ * @gfp_flags:	memory flag
+ *
+ * Returns request if no error, 0 on failure
+ */
+static struct usb_request *f_usb30_udc_ep_alloc_request(struct usb_ep *ep,
+	 gfp_t gfp_flags)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_request *request;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+
+	/* check argument */
+	if (unlikely(!ep))
+		return 0;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* allocate memory and zero clear memory */
+	request = kzalloc(sizeof(*request), gfp_flags);
+	if (!request) {
+		dev_err(f_usb30->gadget.dev.parent, "kzalloc is failed.\n");
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return 0;
+	}
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u allocate memory is 0x%p.\n", ep_channel, request);
+
+	/* initialize list data */
+	INIT_LIST_HEAD(&request->queue);
+	request->req.dma = F_USB30_DMA_ADDR_INVALID;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return &request->req;
+}
+
+/**
+ * f_usb30_udc_ep_free_request - Free usb request
+ * @ep:	usb endpoint
+ * @req:	usb request
+ *
+ * Wrapper around kfree to free request
+ */
+static void f_usb30_udc_ep_free_request(struct usb_ep *ep,
+	 struct usb_request *req)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_request *request;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+
+	/* check argument */
+	if (unlikely((!ep) || (!req)))
+		return;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	request = container_of(req, struct f_usb30_request, req);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* free memory */
+	WARN_ON(!list_empty(&request->queue));
+	kfree(request);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u free memory is 0x%p.\n", ep_channel, request);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return;
+}
+
+/**
+ * f_usb30_udc_ep_queue - Queue a request into an IN endpoint
+ * @ep:	usb endpoint
+ * @req:	usb request
+ * @gfp_flags:	flags
+ *
+ * Returns 0 if no error, -EINVAL -ESHUTDOWN -EL2HLT on failure
+ */
+static int f_usb30_udc_ep_queue(struct usb_ep *ep,
+	 struct usb_request *req, gfp_t gfp_flags)
+{
+	int ret = 0;
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_request *request;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely((!ep) || (!req)))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	request = container_of(req, struct f_usb30_request, req);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((!req->complete) || (!req->buf) ||
+		 (!list_empty(&request->queue)) ||
+		 ((ep_channel != ENDPOINT0) &&
+		 (!req->length) && (!req->zero)))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"request parameter is invalid.\n");
+		ret = -EINVAL;
+		goto done;
+	}
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint parameter is invalid.\n");
+		ret = -EINVAL;
+		goto done;
+	}
+
+	dev_dbg(f_usb30->gadget.dev.parent,
+	 "endpoint %u is queue at request = 0x%p, length = %u, buf = 0x%p.\n",
+	 ep_channel, request, request->req.length, request->req.buf);
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint halt status is %u.\n", endpoint->halt);
+
+	/* check state */
+	if ((!f_usb30->gadget_driver) ||
+		 (f_usb30->gadget.speed == USB_SPEED_UNKNOWN)) {
+		dev_dbg(f_usb30->gadget.dev.parent, "device state is reset.\n");
+		ret = -ESHUTDOWN;
+		goto done;
+	}
+
+	/* initialize request parameter */
+	request->req.status = -EINPROGRESS;
+	request->req.actual = 0;
+
+	/* check current queue execute */
+	if ((!list_empty(&endpoint->queue)) || (endpoint->halt))
+		goto queue;
+
+	/* save request */
+	endpoint->req = request;
+
+	if (ep_channel == ENDPOINT0) {
+		/* request endpoint 0 */
+		switch (f_usb30->ctrl_stage) {
+		case F_USB30_STAGE_IN_DATA:
+			if (!set_in_transfer_pio(endpoint)) {
+				dev_err(f_usb30->gadget.dev.parent,
+					 "set_in_transfer_pio() of endpoint 0 "
+					 "is failed.\n");
+				ret = -EL2HLT;
+				goto done;
+			}
+			break;
+		case F_USB30_STAGE_OUT_DATA:
+			if (!set_out_transfer_pio(endpoint)) {
+				dev_err(f_usb30->gadget.dev.parent,
+					 "set_out_transfer_pio() of endpoint 0 "
+					 "is failed.\n");
+				ret = -EL2HLT;
+				goto done;
+			}
+			break;
+		case F_USB30_STAGE_IN_STATUS:
+			ret = 0;
+			goto done;
+		case F_USB30_STAGE_SPECIAL:
+			f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+			dev_dbg(f_usb30->gadget.dev.parent,
+			 "next control transfer stage is SETUP stage.\n");
+			notify_transfer_request_complete(endpoint, 0);
+			ret = 0;
+			goto done;
+		default:
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"control transfer stage is changed at %u.\n",
+				f_usb30->ctrl_stage);
+			ret = -EL2HLT;
+			goto done;
+		}
+	} else {
+		/* request endpoint x */
+		if (endpoint->ep.address & USB_DIR_IN) {
+			if (!set_in_transfer(endpoint)) {
+				dev_err(f_usb30->gadget.dev.parent,
+					 "set_in_transfer() of endpoint %u is "
+					 "failed.\n", ep_channel);
+				ret = -EL2HLT;
+				goto done;
+			}
+		} else {
+			if (!set_out_transfer(endpoint)) {
+				dev_err(f_usb30->gadget.dev.parent,
+					 "set_out_transfer() of endpoint %u is "
+					 "failed.\n", ep_channel);
+				ret = -EL2HLT;
+				goto done;
+			}
+		}
+	}
+
+queue:
+	/* add list tail */
+	list_add_tail(&request->queue, &endpoint->queue);
+
+done:
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return ret;
+}
+
+/**
+ * f_usb30_udc_ep_dequeue - Dequeue one request
+ * @ep:	usb endpoint
+ * @req:	usb request
+ *
+ * Returns 0 if no error, -EINVAL on failure
+ */
+static int f_usb30_udc_ep_dequeue(struct usb_ep *ep,
+	 struct usb_request *req)
+{
+	int ret = 0;
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_request *request;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely((!ep) || (!req)))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	request = container_of(req, struct f_usb30_request, req);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent, "endpoint is disabled.\n");
+		ret = -EINVAL;
+		goto done;
+	}
+
+	/* check queue empty */
+	if (!list_empty(&endpoint->queue)) {
+		/* check list entry */
+		list_for_each_entry(request, &endpoint->queue, queue) {
+			if ((request) && (&request->req == req))
+				break;
+		}
+
+		/* check dequeue request mismatch */
+		if (unlikely((!request) || (&request->req != req))) {
+			dev_err(f_usb30->gadget.dev.parent,
+				"endpoint %u request is mismatch.\n",
+				ep_channel);
+			ret = -EINVAL;
+			goto done;
+		}
+	} else {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint %u request queue is empty.\n", ep_channel);
+		ret = -EINVAL;
+		goto done;
+	}
+
+	/* abort request transfer */
+	endpoint->req = request;
+	if (ep_channel == ENDPOINT0) {
+		abort_in_transfer(endpoint, 0);
+		abort_out_transfer(endpoint, 0);
+	} else {
+		endpoint->ep.address & USB_DIR_IN ?
+		 abort_in_transfer(endpoint, 0) :
+		 abort_out_transfer(endpoint, 0);
+	}
+
+	notify_transfer_request_complete(endpoint, -ECONNRESET);
+
+	/* check next queue empty */
+	if (list_empty(&endpoint->queue))
+		goto done;
+
+	/* get next request */
+	request = list_entry(endpoint->queue.next,
+				 struct f_usb30_request, queue);
+
+	/* check the got next request a request under current execution */
+	if (request->req.actual)
+		goto done;
+
+	/* save request */
+	endpoint->req = request;
+
+	/* set endpoint x transfer request */
+	if (!(endpoint->ep.address & USB_DIR_IN ? set_in_transfer(endpoint) :
+		 set_out_transfer(endpoint))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"set_in_transfer() of endpoint %u is failed.\n",
+			 ep_channel);
+		dequeue_all_transfer_request(endpoint, -EL2HLT);
+	}
+
+done:
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return ret;
+}
+
+/**
+ * f_usb30_udc_ep_set_halt - Halts operations on one endpoint
+ * @ep:	usb endpoint
+ * @value:1--set halt  0--clear halt
+ *
+ * Returns 0 if no error, -EINVAL -EPROTO -EAGAIN on failure
+ */
+static int f_usb30_udc_ep_set_halt(struct usb_ep *ep, int value)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely((!ep) || ((value != 0) && (value != 1))))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint parameter is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EINVAL;
+	}
+
+	/* check isochronous endpoint */
+	if (unlikely(endpoint->attributes == USB_ENDPOINT_XFER_ISOC)) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint %u is isochoronous transfer.\n", ep_channel);
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EPROTO;
+	}
+
+	if (value) {
+		/* check current queue execute */
+		if (!list_empty(&endpoint->queue)) {
+			dev_dbg(f_usb30->gadget.dev.parent,
+				"endpoint %u is execute queue.\n", ep_channel);
+			spin_unlock_irqrestore(&f_usb30->lock, flags);
+			dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n"
+								, __func__);
+			return -EAGAIN;
+		}
+
+		/* set halt */
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"endpoint %u transfer is halted.\n", ep_channel);
+		halt_transfer(endpoint);
+		endpoint->halt = 1;
+	} else {
+		/* clear halt */
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"endpoint %u transfer halt is cleared.\n", ep_channel);
+		initialize_endpoint_hw(endpoint, 1, 1);
+		endpoint->halt = 0;
+		endpoint->force_halt = 0;
+	}
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return 0;
+}
+
+/**
+ * f_usb30_udc_ep_set_wedge - Wedges operations on one endpoint
+ * @ep:	usb endpoint
+ *
+ * Returns 0 if no error, -EINVAL -EPROTO -EAGAIN on failure
+ */
+static int f_usb30_udc_ep_set_wedge(struct usb_ep *ep)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely(!ep))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* check parameter */
+	if (unlikely((ep_channel != ENDPOINT0) && (!endpoint->enabled))) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint parameter is invalid.\n");
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EINVAL;
+	}
+
+	/* check isochronous endpoint */
+	if (unlikely(endpoint->attributes == USB_ENDPOINT_XFER_ISOC)) {
+		dev_err(f_usb30->gadget.dev.parent,
+			"endpoint %u is isochoronous transfer.\n", ep_channel);
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EPROTO;
+	}
+
+	/* check current queue execute */
+	if (!list_empty(&endpoint->queue)) {
+		dev_dbg(f_usb30->gadget.dev.parent,
+			"endpoint %u queue is execute.\n", ep_channel);
+		spin_unlock_irqrestore(&f_usb30->lock, flags);
+		dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+							 __func__);
+		return -EAGAIN;
+	}
+
+	/* set force halt */
+	dev_dbg(f_usb30->gadget.dev.parent,
+		"endpoint %u transfer is halted.\n", ep_channel);
+	halt_transfer(endpoint);
+	endpoint->halt = 1;
+	endpoint->force_halt = 1;
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return 0;
+}
+
+/**
+ * f_usb30_udc_ep_fifo_status - Get how many bytes in physical endpoint
+ * @ep:	usb endpoint
+ *
+ * Returns number of bytes in OUT fifos. -EINVAL on failure
+ */
+static int f_usb30_udc_ep_fifo_status(struct usb_ep *ep)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned short bytes;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely(!ep))
+		return -EINVAL;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* get current bytes in FIFO */
+	bytes = get_fifo_bytes(endpoint);
+	dev_dbg(f_usb30->gadget.dev.parent, "bytes in FIFO is %u.\n", bytes);
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return (int) bytes;
+}
+
+/**
+ * f_usb30_udc_ep_fifo_flush - Flushes one endpoint
+ * @ep:	usb endpoint
+ *
+ * Discards all data in one endpoint(IN or OUT).
+ */
+static void f_usb30_udc_ep_fifo_flush(struct usb_ep *ep)
+{
+	struct f_usb30_udc *f_usb30;
+	struct f_usb30_ep *endpoint;
+	unsigned char ep_channel;
+	unsigned long flags;
+
+	/* check argument */
+	if (unlikely(!ep))
+		return;
+
+	/* get parameter */
+	endpoint = container_of(ep, struct f_usb30_ep, ep);
+	ep_channel = endpoint->ep.address & USB_ENDPOINT_NUMBER_MASK;
+	f_usb30 = endpoint->udc;
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is started.\n",
+						 __func__);
+
+	/* get spin lock and disable interrupt, save interrupt status */
+	spin_lock_irqsave(&f_usb30->lock, flags);
+
+	/* initialize endpoint FIFO */
+	dev_dbg(f_usb30->gadget.dev.parent, "endpoint %u FIFO is flush.\n",
+						 ep_channel);
+	initialize_endpoint_hw(endpoint, 1, 1);
+
+	/* release spin lock and enable interrupt, return interrupt status */
+	spin_unlock_irqrestore(&f_usb30->lock, flags);
+
+	dev_dbg(f_usb30->gadget.dev.parent, "%s() is ended.\n",
+						 __func__);
+
+	return;
+}
+
+static struct usb_ep_ops f_usb30_udc_ep_ops = {
+	.enable = f_usb30_udc_ep_enable,
+	.disable = f_usb30_udc_ep_disable,
+	.alloc_request = f_usb30_udc_ep_alloc_request,
+	.free_request = f_usb30_udc_ep_free_request,
+	.queue = f_usb30_udc_ep_queue,
+	.dequeue = f_usb30_udc_ep_dequeue,
+	.set_halt = f_usb30_udc_ep_set_halt,
+	.set_wedge = f_usb30_udc_ep_set_wedge,
+	.fifo_status = f_usb30_udc_ep_fifo_status,
+	.fifo_flush = f_usb30_udc_ep_fifo_flush,
+};
+
+static struct usb_gadget_ops f_usb30_udc_gadget_ops = {
+	.udc_start	= f_usb30_udc_start,
+	.udc_stop	= f_usb30_udc_stop,
+};
+
+/**
+ * f_usb30_udc_probe - Probes the udc device
+ * @pdev:	platform device
+ *
+ * Perform basic init : allocates memory, initialize, entry IRQ
+ *
+ * Returns 0 if no error, -EINVAL -ENODEV -EBUSY -ENOMEM -EIO or other negative
+ * errno on failure
+ */
+static int __init f_usb30_udc_probe(struct platform_device *pdev)
+{
+	int ret = 0;
+	struct f_usb30_udc *f_usb30;
+	struct resource *device_resource;
+	resource_size_t device_resource_size;
+	struct resource *f_usb30_resource;
+	void __iomem *register_base_address;
+	int hs_irq;
+	int ss_irq;
+	struct clk *clk;
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	unsigned long counter;
+	int dmac_in_irq;
+	int dmac_out_irq;
+#endif
+	int result;
+
+	/* check argument */
+	if (unlikely(!pdev)) {
+		ret = -EINVAL;
+		goto done;
+	}
+
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+
+	/* get clock for F_USB30 device */
+	clk = clk_get(&pdev->dev, NULL);
+	if (IS_ERR(clk)) {
+		dev_dbg(&pdev->dev, "clock not found.\n");
+		ret = PTR_ERR(clk);
+		goto done;
+	}
+	clk_prepare_enable(clk);
+
+	/* get a resource for a F_USB30 device */
+	device_resource = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!device_resource) {
+		dev_err(&pdev->dev, "platform_get_resource() is failed.\n");
+		ret = -ENODEV;
+		goto err_res;
+	}
+	device_resource_size = device_resource->end -
+					 device_resource->start + 1;
+
+	/* reserve a F_USB30 device resource region */
+	f_usb30_resource = request_mem_region(device_resource->start,
+				 device_resource_size, "f_usb30_udc");
+	if (!f_usb30_resource) {
+		dev_err(&pdev->dev, "request_mem_region() is failed.\n");
+		ret = -EBUSY;
+		goto err_res;
+	}
+
+	/* get a register base address for a F_USB30 device */
+	register_base_address = remap_iomem_region(device_resource->start,
+						   device_resource_size);
+	if (!register_base_address) {
+		dev_err(&pdev->dev, "remap_iomem_region() is failed.\n");
+		ret = -ENODEV;
+		goto err_mem;
+	}
+
+	/* get an IRQ for a F_USB30 HS/FS device */
+	hs_irq = platform_get_irq(pdev, 0);
+	if (hs_irq < 0) {
+		dev_err(&pdev->dev,
+		 "platform_get_irq() for F_USB30 HS/FS is failed at %d.\n",
+		 hs_irq);
+		ret = -ENODEV;
+		goto err_map;
+	}
+
+	/* get an IRQ for a F_USB30 SS device */
+	ss_irq = platform_get_irq(pdev, 1);
+	if (ss_irq < 0) {
+		dev_err(&pdev->dev,
+			"platform_get_irq() for F_USB30 SS is failed at %d.\n",
+			ss_irq);
+		ret = -ENODEV;
+		goto err_map;
+	}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	/* get an IRQ for a F_USB30 DMAC in */
+	dmac_in_irq = platform_get_irq(pdev, 2);
+	if (dmac_in_irq < 0) {
+		dev_err(&pdev->dev,
+		 "platform_get_irq() for F_USB30 DMAC in is failed at %d.\n",
+		 dmac_in_irq);
+		ret = -ENODEV;
+		goto err_map;
+	}
+
+	/* get an IRQ for a F_USB30 DMAC out */
+	dmac_out_irq = platform_get_irq(pdev, 3);
+	if (dmac_out_irq < 0) {
+		dev_err(&pdev->dev,
+		 "platform_get_irq() for F_USB30 DMAC out is failed at %d.\n",
+		 dmac_out_irq);
+		ret = -ENODEV;
+		goto err_map;
+	}
+#endif
+
+	/* allocate F_USB30 UDC device driver structure data memory */
+	f_usb30 = kzalloc(sizeof(*f_usb30), GFP_KERNEL);
+	if (!f_usb30) {
+		dev_err(&pdev->dev, "kzalloc() is failed.\n");
+		ret = -ENOMEM;
+		goto err_map;
+	}
+
+	/* initialize a F_USB30 UDC device driver structure */
+	f_usb30->gadget.ops = &f_usb30_udc_gadget_ops;
+	f_usb30->gadget.speed = USB_SPEED_UNKNOWN;
+	f_usb30->gadget.max_speed = USB_SPEED_SUPER;
+	f_usb30->gadget.name = "f_usb30_udc";
+	dev_set_name(&f_usb30->gadget.dev, "gadget");
+	f_usb30->gadget_driver = NULL;
+	spin_lock_init(&f_usb30->lock);
+	f_usb30->device_resource = device_resource;
+	f_usb30->register_base_address = register_base_address;
+	f_usb30->hs_irq = hs_irq;
+	f_usb30->ss_irq = ss_irq;
+	f_usb30->clk = clk;
+	initialize_endpoint(f_usb30, &f_usb30_udc_ep_ops);
+#if defined(CONFIG_ARCH_MB8AC0300)
+	f_usb30->vbus = 0;
+	f_usb30->vbus_active_level = F_USB30_VBUS_ACTIVE_LEVEL;
+	of_property_read_u32(pdev->dev.of_node, "vbus_on_hwirq",
+		 &f_usb30->vbus_on_hwirq);
+	of_property_read_u32(pdev->dev.of_node, "vbus_off_hwirq",
+		 &f_usb30->vbus_off_hwirq);
+	f_usb30->vbus_on_irq = platform_get_irq(pdev, 4);
+	f_usb30->vbus_off_irq = platform_get_irq(pdev, 5);
+#else
+/* for other soc */
+#endif
+	f_usb30->link_state = F_USB30_LINK_STATE_NOTATTACHED;
+	f_usb30->device_state = USB_STATE_NOTATTACHED;
+	f_usb30->device_state_last = USB_STATE_NOTATTACHED;
+	f_usb30->configure_value_last = 0;
+	f_usb30->ctrl_stage = F_USB30_STAGE_SETUP;
+	f_usb30->ctrl_pri_dir = 1;
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	f_usb30->dmac_in_irq = dmac_in_irq;
+	f_usb30->dmac_out_irq = dmac_out_irq;
+	for (counter = 0; counter < F_USB30_MAX_DMAC; counter++)
+		f_usb30->dma_ep[counter] = 0;
+#endif
+
+	f_usb30_data = f_usb30;
+
+	/* setup the private data of a device driver */
+	platform_set_drvdata(pdev, f_usb30);
+
+	/* initialize F_USB30 controller */
+	initialize_controller(f_usb30, 1);
+
+	/* entry a F_USB30 SS device IRQ */
+	result = request_irq(f_usb30->ss_irq, on_usb_ss_function_controller,
+				 0, "f_usb30_udc_ss", f_usb30);
+	if (result) {
+		dev_err(&pdev->dev,
+			"request_irq() for F_USB30 SS is failed at %d.\n",
+			result);
+		ret = result;
+		goto err_ssirq;
+	}
+
+	/* entry a F_USB30 HS/FS device IRQ */
+	result = request_irq(f_usb30->hs_irq, on_usb_hs_function_controller,
+				 0, "f_usb30_udc_hs", f_usb30);
+	if (result) {
+		dev_err(&pdev->dev,
+			"request_irq() for F_USB30 HS/FS is failed at %d.\n",
+			result);
+		ret = result;
+		goto err_hsirq;
+	}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+	/* entry a F_USB30 DMAC in IRQ */
+	result = request_irq(f_usb30->dmac_in_irq, on_dma_transfer_contoller,
+				 0, "f_usb30_dmac_in_udc", f_usb30);
+	if (result) {
+		dev_err(&pdev->dev,
+			"request_irq() for F_USB30 DMAC IN is failed at %d.\n",
+			result);
+		ret = result;
+		goto err_dmain;
+	}
+	/* entry a F_USB30 DMAC out IRQ */
+	result = request_irq(f_usb30->dmac_out_irq, on_dma_transfer_contoller,
+				 0, "f_usb30_dmac_out_udc", f_usb30);
+	if (result) {
+		dev_err(&pdev->dev,
+		 "request_irq() for F_USB30 DMAC OUT is failed at %d.\n",
+		 result);
+		ret = result;
+		goto err_dmaout;
+	}
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	/* allocate noncachable buffer for the F_USB20HDC DMAC device */
+	for (counter = F_USB30_IN_DMAC; counter < F_USB30_MAX_DMAC; counter++) {
+		f_usb30->buffer[counter] = dma_alloc_coherent(NULL,
+						F_USB30_DMAC_TRANS_MAX_BYTES,
+						&f_usb30->dma_buffer[counter],
+						GFP_KERNEL);
+		if (f_usb30->buffer[counter] == NULL) {
+			dev_err(&pdev->dev,
+			 "%s():DMA channel %ld dma_alloc_coherent() failed.\n",
+			 __func__, counter);
+			result = -ENOMEM;
+			goto err_non;
+		}
+	}
+#endif
+#endif
+
+	result = usb_add_gadget_udc(&pdev->dev, &f_usb30->gadget);
+	if (result) {
+		ret = result;
+		dev_err(&pdev->dev, "usb_add_gadget_udc failed!\n");
+		goto err_non;
+	}
+
+	/* driver registering log output */
+	dev_info(&pdev->dev,
+		 "F_USB30 UDC driver (version %s) is registered.\n",
+		 F_USB30_DRIVER_VERSION);
+
+	dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+
+	return ret;
+
+err_non:
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	for (counter = F_USB30_IN_DMAC; counter < F_USB30_MAX_DMAC; counter++) {
+		if (f_usb30->buffer[counter] != NULL)
+			dma_free_coherent(NULL, F_USB30_DMAC_TRANS_MAX_BYTES,
+						f_usb30->buffer[counter],
+						f_usb30->dma_buffer[counter]);
+	}
+#endif
+err_dmaout:
+	free_irq(f_usb30->dmac_in_irq, f_usb30);
+err_dmain:
+	free_irq(f_usb30->hs_irq, f_usb30);
+#endif
+err_hsirq:
+	free_irq(f_usb30->ss_irq, f_usb30);
+err_ssirq:
+	platform_set_drvdata(pdev, NULL);
+	kfree(f_usb30);
+err_map:
+	unmap_iomem_region(register_base_address);
+err_mem:
+	release_mem_region(device_resource->start, device_resource_size);
+err_res:
+	clk_disable_unprepare(clk);
+	clk_put(clk);
+done:
+	dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+	return ret;
+}
+
+/**
+ * f_usb30_udc_remove - Removes the udc device driver
+ * @pdev:	platform device
+ *
+ * Returns 0 if no error, -EINVAL on failure
+ */
+static int __exit f_usb30_udc_remove(struct platform_device *pdev)
+{
+	unsigned long counter;
+	struct f_usb30_udc *f_usb30;
+
+	/* check argument */
+	if (unlikely(!pdev))
+		return -EINVAL;
+
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+
+	/* get a device driver parameter */
+	f_usb30 = platform_get_drvdata(pdev);
+	if (!f_usb30) {
+		dev_err(&pdev->dev, "platform_get_drvdata() is failed.\n");
+		dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+		return -EINVAL;
+	}
+
+	usb_del_gadget_udc(&f_usb30->gadget);
+
+	/* disable F_USB30 controller interrupt */
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+	for (counter = F_USB30_IN_DMAC; counter < F_USB30_MAX_DMAC; counter++)
+		dma_free_coherent(NULL, F_USB30_DMAC_TRANS_MAX_BYTES,
+					f_usb30->buffer[counter],
+					f_usb30->dma_buffer[counter]);
+#endif
+	free_irq(f_usb30->dmac_in_irq, f_usb30);
+	free_irq(f_usb30->dmac_out_irq, f_usb30);
+#endif
+	free_irq(f_usb30->hs_irq, f_usb30);
+	free_irq(f_usb30->ss_irq, f_usb30);
+
+	spin_lock(&f_usb30->lock);
+	/* dequeue all previous transfer request */
+	for (counter = ENDPOINT0; counter < F_USB30_MAX_EP; counter++) {
+		f_usb30->udc_endpoint[counter].halt = 1;
+		dequeue_all_transfer_request(&f_usb30->udc_endpoint[counter],
+						 -ESHUTDOWN);
+	}
+	spin_unlock(&f_usb30->lock);
+
+	/* notify disconnect to gadget driver */
+	if ((f_usb30->gadget_driver) &&
+		 (f_usb30->gadget_driver->disconnect))
+		f_usb30->gadget_driver->disconnect(&f_usb30->gadget);
+
+	/* notify unbind to gadget driver */
+	if ((f_usb30->gadget_driver) && (f_usb30->gadget_driver->unbind))
+		f_usb30->gadget_driver->unbind(&f_usb30->gadget);
+
+	/* disable F_USB30 device clock */
+	clk_disable_unprepare(f_usb30->clk);
+	clk_put(f_usb30->clk);
+
+	/* release device resource */
+	platform_set_drvdata(pdev, NULL);
+	unmap_iomem_region(f_usb30->register_base_address);
+	release_mem_region(f_usb30->device_resource->start,
+			   f_usb30->device_resource->end -
+			   f_usb30->device_resource->start + 1);
+
+	/* free F_USB30 UDC device driver structure data memory */
+	kfree(f_usb30);
+
+	dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+
+	return 0;
+}
+
+static const struct of_device_id f_usb30_dt_ids[] = {
+	{ .compatible = "fujitsu,f_usb30_udc" },
+	{ /* sentinel */ }
+};
+
+MODULE_DEVICE_TABLE(of, f_usb30_dt_ids);
+
+/* F_USB30 UDC device driver structure */
+static struct platform_driver f_usb30_udc_driver = {
+	.remove = __exit_p(f_usb30_udc_remove),
+	.driver = {
+		.name = "f_usb30_udc",
+		.owner = THIS_MODULE,
+		.of_match_table = f_usb30_dt_ids,
+		},
+};
+
+/**
+ * f_usb30_udc_init - initialize module
+ *
+ * Returns 0 if no error, negative errno on failure
+ */
+static int __init f_usb30_udc_init(void)
+{
+	return platform_driver_probe(&f_usb30_udc_driver,
+					 f_usb30_udc_probe);
+}
+
+module_init(f_usb30_udc_init);
+
+/**
+ * f_usb30_udc_exit - exit module
+ */
+static void __exit f_usb30_udc_exit(void)
+{
+	platform_driver_unregister(&f_usb30_udc_driver);
+}
+
+module_exit(f_usb30_udc_exit);
+
+/* F_USB30 UDC device module definition */
+MODULE_AUTHOR("Fujitsu Semiconductor Limited");
+MODULE_DESCRIPTION("F_USB30 USB3.0 Function Controller Driver");
+MODULE_ALIAS("platform:f_usb30_udc");
+MODULE_LICENSE("GPL");
diff --git a/drivers/usb/gadget/f_usb30_udc.h b/drivers/usb/gadget/f_usb30_udc.h
new file mode 100644
index 0000000..131871c
--- /dev/null
+++ b/drivers/usb/gadget/f_usb30_udc.h
@@ -0,0 +1,7195 @@
+/*
+ * linux/drivers/usb/gadget/f_usb30_udc.h - F_USB30 USB3.0 Function
+ * Controller Driver
+ *
+ * based on F_USB20LP USB2.0 Function Controller Driver
+ *
+ * Copyright (C) 2003 Robert Schwebel <r.schwebel@pengutronix.de>, Pengutronix
+ * Copyright (C) 2003 David Brownell
+ * Copyright (C) 2006 Lineo Solutions, Inc.
+ * Copyright (C) 2006 - 2007 Lineo Solutions, Inc.
+ * Copyright (C) FUJITSU ELECTRONICS INC. 2011. All rights reserved.
+ * Copyright (C) 2011 - 2012 FUJITSU SEMICONDUCTOR LIMITED
+ */
+
+#ifndef __F_USB30_UDC_H
+#define __F_USB30_UDC_H
+
+#define F_USB30_DRIVER_VERSION		"1.0.0"
+
+enum f_usb30_ctrl_stage {
+	F_USB30_STAGE_SETUP = 0,		/* SETUP stage                */
+	F_USB30_STAGE_IN_DATA,			/* IN data stage              */
+	F_USB30_STAGE_OUT_DATA,			/* OUT data stage             */
+	F_USB30_STAGE_IN_STATUS,		/* IN status stage            */
+	F_USB30_STAGE_OUT_STATUS,		/* OUT status stage           */
+	F_USB30_STAGE_SPECIAL,			/*
+						 * special stage for
+						 * SET_CONFIGURATION
+						 * or SET_INTERFACE request
+						 */
+	F_USB30_STAGE_MAX,			/* max value                  */
+};
+
+enum f_usb30_ss_link_state {
+	F_USB30_LINK_STATE_NOTATTACHED = 0,	/* vbus is invalid state      */
+	F_USB30_LINK_STATE_ATTACHED,		/* vbus is valid state        */
+	F_USB30_LINK_STATE_TRAINING,		/* ss link training state     */
+	F_USB30_LINK_STATE_SSENABLED,		/* SuperSpeed enumerate state */
+	F_USB30_LINK_STATE_HSENABLED,		/* HS/FS enumerated state     */
+	F_USB30_LINK_STATE_DISABLED,		/* link training fail state   */
+};
+
+#if defined(CONFIG_ARCH_MB8AC0300)
+#define F_USB30_VBUS_ON_IRQ		62	/* vbus on detect extint num  */
+#define F_USB30_VBUS_OFF_IRQ		64	/* vbus off detect extint num */
+#define F_USB30_VBUS_ACTIVE_LEVEL	1	/* vbus detect active level   */
+#else
+/* for other soc */
+#endif
+
+#define F_USB30_DMA_ADDR_INVALID	(~(dma_addr_t)0)
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+#define F_USB30_DMAC_REG_OFFSET		0x1100
+#endif
+
+/* interface count */
+#define F_USB30_MAX_INTF		1
+
+/* alternate count */
+#define F_USB30_MAX_ALT			1
+
+/* interface alternate channel count */
+#define F_USB30_ALT_INTF0		1
+#define F_USB30_ALT_INTF1		0
+#define F_USB30_ALT_INTF2		0
+#define F_USB30_ALT_INTF3		0
+#define F_USB30_ALT_INTF4		0
+#define F_USB30_ALT_INTF5		0
+#define F_USB30_ALT_INTF6		0
+#define F_USB30_ALT_INTF7		0
+#define F_USB30_ALT_INTF8		0
+#define F_USB30_ALT_INTF9		0
+#define F_USB30_ALT_INTF10		0
+#define F_USB30_ALT_INTF11		0
+#define F_USB30_ALT_INTF12		0
+#define F_USB30_ALT_INTF13		0
+#define F_USB30_ALT_INTF14		0
+#define F_USB30_ALT_INTF15		0
+
+/* endpoint count */
+#define F_USB30_MAX_EP			6
+
+/* endpoint FIFO size table array */
+static const unsigned short ep_fifo_size[F_USB30_MAX_EP][USB_SPEED_SUPER + 1]
+					[F_USB30_MAX_ALT] = {
+	/*unkown,low,    full ,   high, wireless,super */
+	{{},	{},	{64,},	{64,},	{},	{512,},},	/* endpoint 0 */
+	{{},	{},	{64,},	{512,},	{},	{1024,},},	/* endpoint 1 */
+	{{},	{},	{64,},	{512,},	{},	{1024,},},	/* endpoint 2 */
+	{{},	{},	{8,},	{64,},	{},	{64,},},	/* endpoint 3 */
+	{{},	{},	{64,},	{512,},	{},	{1024,},},	/* endpoint 4 */
+	{{},	{},	{64,},	{512,},	{},	{1024,},},	/* endpoint 5 */
+};
+
+/*
+ * endpoint transfer type
+ * [notice]:The following constant definition is used.
+ *	unused		= 0
+ *	control transfer	= USB_ENDPOINT_XFER_CONTROL
+ *	isochronous transfer	= USB_ENDPOINT_XFER_ISOC
+ *	interrupt transfer	= USB_ENDPOINT_XFER_INT
+ *	bulk transfer		= USB_ENDPOINT_XFER_BULK
+ */
+#define F_USB30_TRANS_TYPE_EP0 USB_ENDPOINT_XFER_CONTROL
+#define F_USB30_TRANS_TYPE_EP1 USB_ENDPOINT_XFER_BULK
+#define F_USB30_TRANS_TYPE_EP2 USB_ENDPOINT_XFER_BULK
+#define F_USB30_TRANS_TYPE_EP3 USB_ENDPOINT_XFER_INT
+#define F_USB30_TRANS_TYPE_EP4 USB_ENDPOINT_XFER_BULK
+#define F_USB30_TRANS_TYPE_EP5 USB_ENDPOINT_XFER_BULK
+#define F_USB30_TRANS_TYPE_EP6	0
+#define F_USB30_TRANS_TYPE_EP7	0
+#define F_USB30_TRANS_TYPE_EP8	0
+#define F_USB30_TRANS_TYPE_EP9	0
+#define F_USB30_TRANS_TYPE_EP10	0
+#define F_USB30_TRANS_TYPE_EP11	0
+#define F_USB30_TRANS_TYPE_EP12	0
+#define F_USB30_TRANS_TYPE_EP13	0
+#define F_USB30_TRANS_TYPE_EP14	0
+#define F_USB30_TRANS_TYPE_EP15	0
+
+/*
+ * endpoint transfer direction
+ * [notice]:The following constant definition is used.
+ *	unused	= 0
+ *	IN transfer	= USB_DIR_IN
+ *	OUT transfer	= USB_DIR_OUT
+ */
+#define F_USB30_TRANS_DIR_EP0 USB_DIR_OUT
+#define F_USB30_TRANS_DIR_EP1 USB_DIR_IN
+#define F_USB30_TRANS_DIR_EP2 USB_DIR_OUT
+#define F_USB30_TRANS_DIR_EP3 USB_DIR_IN
+#define F_USB30_TRANS_DIR_EP4 USB_DIR_OUT
+#define F_USB30_TRANS_DIR_EP5 USB_DIR_IN
+#define F_USB30_TRANS_DIR_EP6	0
+#define F_USB30_TRANS_DIR_EP7	0
+#define F_USB30_TRANS_DIR_EP8	0
+#define F_USB30_TRANS_DIR_EP9	0
+#define F_USB30_TRANS_DIR_EP10	0
+#define F_USB30_TRANS_DIR_EP11	0
+#define F_USB30_TRANS_DIR_EP12	0
+#define F_USB30_TRANS_DIR_EP13	0
+#define F_USB30_TRANS_DIR_EP14	0
+#define F_USB30_TRANS_DIR_EP15	0
+
+/*
+ * endpoint name string
+ * [notice]:The following constant definition is used.
+ *	unused	= ""
+ *	used		= "ep(number)(in or out)-(type)"
+ */
+#define F_USB30_NAME_STRING_EP0		"ep0"
+#define F_USB30_NAME_STRING_EP1		"ep1in-bulk"
+#define F_USB30_NAME_STRING_EP2		"ep2out-bulk"
+#define F_USB30_NAME_STRING_EP3		"ep3in-int"
+#define F_USB30_NAME_STRING_EP4		"ep4out-bulk"
+#define F_USB30_NAME_STRING_EP5		"ep5in-bulk"
+#define F_USB30_NAME_STRING_EP6		""
+#define F_USB30_NAME_STRING_EP7		""
+#define F_USB30_NAME_STRING_EP8		""
+#define F_USB30_NAME_STRING_EP9		""
+#define F_USB30_NAME_STRING_EP10	""
+#define F_USB30_NAME_STRING_EP11	""
+#define F_USB30_NAME_STRING_EP12	""
+#define F_USB30_NAME_STRING_EP13	""
+#define F_USB30_NAME_STRING_EP14	""
+#define F_USB30_NAME_STRING_EP15	""
+
+/*
+ * endpoint interface channel
+ * [notice]:unusing it is referred to as '-1'
+ */
+#define F_USB30_INTF_CHANNEL_EP0	-1
+#define F_USB30_INTF_CHANNEL_EP1	0
+#define F_USB30_INTF_CHANNEL_EP2	0
+#define F_USB30_INTF_CHANNEL_EP3	0
+#define F_USB30_INTF_CHANNEL_EP4	0
+#define F_USB30_INTF_CHANNEL_EP5	0
+#define F_USB30_INTF_CHANNEL_EP6	-1
+#define F_USB30_INTF_CHANNEL_EP7	-1
+#define F_USB30_INTF_CHANNEL_EP8	-1
+#define F_USB30_INTF_CHANNEL_EP9	-1
+#define F_USB30_INTF_CHANNEL_EP10	-1
+#define F_USB30_INTF_CHANNEL_EP11	-1
+#define F_USB30_INTF_CHANNEL_EP12	-1
+#define F_USB30_INTF_CHANNEL_EP13	-1
+#define F_USB30_INTF_CHANNEL_EP14	-1
+#define F_USB30_INTF_CHANNEL_EP15	-1
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+/* F_USB30 DMA controller channel count */
+#define F_USB30_IN_DMAC			0
+#define F_USB30_OUT_DMAC		1
+#define F_USB30_MAX_DMAC		2
+
+/* F_USB30 DMA controller transfer max byte */
+#if defined(CONFIG_USB_GADGET_F_USB30_DMA_USE_BOUNCE_BUF)
+#define F_USB30_DMAC_TRANS_MAX_BYTES	16384
+#else
+#define F_USB30_DMAC_TRANS_MAX_BYTES	4294967295U
+#endif
+
+/*
+ * DMA controller channel for endpoint
+ * [notice]:unusing it is referred to as '-1'
+ */
+#define F_USB30_DMAC_CHANNEL_EP0	-1
+#define F_USB30_DMAC_CHANNEL_EP1	0
+#define F_USB30_DMAC_CHANNEL_EP2	1
+#define F_USB30_DMAC_CHANNEL_EP3	-1
+#define F_USB30_DMAC_CHANNEL_EP4	-1
+#define F_USB30_DMAC_CHANNEL_EP5	-1
+#define F_USB30_DMAC_CHANNEL_EP6	-1
+#define F_USB30_DMAC_CHANNEL_EP7	-1
+#define F_USB30_DMAC_CHANNEL_EP8	-1
+#define F_USB30_DMAC_CHANNEL_EP9	-1
+#define F_USB30_DMAC_CHANNEL_EP10	-1
+#define F_USB30_DMAC_CHANNEL_EP11	-1
+#define F_USB30_DMAC_CHANNEL_EP12	-1
+#define F_USB30_DMAC_CHANNEL_EP13	-1
+#define F_USB30_DMAC_CHANNEL_EP14	-1
+#define F_USB30_DMAC_CHANNEL_EP15	-1
+#endif
+
+/* F_USB30 controller register ID Enumeration constant */
+#define F_USB30_REG_HSCPAC		0   /* HS/FS CPU Access Control       */
+#define F_USB30_REG_HSDVC		1   /* HS/FS Device Control           */
+#define F_USB30_REG_HSDVS		2   /* HS/FS Device Status            */
+#define F_USB30_REG_HSEPIC		3   /* HS/FS EP Interrupt Control     */
+#define F_USB30_REG_HSEPIS		4   /* HS/FS EP Interrupt Status      */
+#define F_USB30_REG_HSEPDC		5   /* HS/FS EP DMA Control           */
+#define F_USB30_REG_HSEPDS		6   /* HS/FS EP DMA Status            */
+#define F_USB30_REG_HSTSTAMP		7   /* HS/FS Time Stamp               */
+#define F_USB30_REG_HSEPTCSEL		8   /* HS/FS EP Byte Count Select     */
+#define F_USB30_REG_HSEPTC1		9   /* HS/FS EP1 Total Byte Count     */
+#define F_USB30_REG_HSEPTC2		10  /* HS/FS EP2 Total Byte Count     */
+#define F_USB30_REG_HSEPRS0		11  /* HS/FS EP0 Rx Size              */
+#define F_USB30_REG_HSEPRS1		12  /* HS/FS EP1 Rx Size              */
+#define F_USB30_REG_HSEPRS2		13  /* HS/FS EP2 Rx Size              */
+#define F_USB30_REG_HSEPRS3		14  /* HS/FS EP3 Rx Size              */
+#define F_USB30_REG_HSEPRS4		15  /* HS/FS EP4 Rx Size              */
+#define F_USB30_REG_HSEPRS5		16  /* HS/FS EP5 Rx Size              */
+#define F_USB30_REG_HSCUSTOMC		17  /* HS/FS Custom Control           */
+#define F_USB30_REG_HSCALIB		18  /* HS/FS Calibration              */
+#define F_USB30_REG_HSEPLPBK		19  /* HS/FS Loop Back Selector       */
+#define F_USB30_REG_HSINTFALTNUM	20  /* HS/FS Interface Alt Number     */
+#define F_USB30_REG_HSEPC0		21  /* HS/FS EP0 Control              */
+#define F_USB30_REG_HSEPS0		22  /* HS/FS EP0 Status               */
+#define F_USB30_REG_HSEPC1		23  /* HS/FS EP1 Control              */
+#define F_USB30_REG_HSEPS1		24  /* HS/FS EP1 Status               */
+#define F_USB30_REG_HSEPC2		25  /* HS/FS EP2 Control              */
+#define F_USB30_REG_HSEPS2		26  /* HS/FS EP2 Status               */
+#define F_USB30_REG_HSEPC3		27  /* HS/FS EP3 Control              */
+#define F_USB30_REG_HSEPS3		28  /* HS/FS EP3 Status               */
+#define F_USB30_REG_HSEPC4		29  /* HS/FS EP4 Control              */
+#define F_USB30_REG_HSEPS4		30  /* HS/FS EP4 Status               */
+#define F_USB30_REG_HSEPC5		31  /* HS/FS EP5 Control              */
+#define F_USB30_REG_HSEPS5		32  /* HS/FS EP5 Status               */
+#define F_USB30_REG_HSALTC		33  /* HS/FS Alternate Control        */
+#define F_USB30_REG_HSALTS		34  /* HS/FS Alternate Status         */
+#define F_USB30_REG_HSEPIB0		35  /* HS/FS EP0 In Buffer            */
+#define F_USB30_REG_HSEPIB1		36  /* HS/FS EP1 In Buffer            */
+#define F_USB30_REG_HSEPIB3		37  /* HS/FS EP3 In Buffer            */
+#define F_USB30_REG_HSEPIB5		38  /* HS/FS EP5 In Buffer            */
+#define F_USB30_REG_HSEPOB0		39  /* HS/FS EP0 Out Buffer           */
+#define F_USB30_REG_HSEPOB2		40  /* HS/FS EP2 Out Buffer           */
+#define F_USB30_REG_HSEPOB4		41  /* HS/FS EP4 Out Buffer           */
+#define F_USB30_REG_MAKEUP_DATA		42  /* HS/FS MAKE-UP DATA AREA        */
+#define F_USB30_REG_MAKEUP0		43  /* HS/FS EP0 MAKE-UP AREA         */
+#define F_USB30_REG_MAKEUP1		44  /* HS/FS EP1 MAKE-UP AREA         */
+#define F_USB30_REG_MAKEUP2		45  /* HS/FS EP2 MAKE-UP AREA         */
+#define F_USB30_REG_MAKEUP3		46  /* HS/FS EP3 MAKE-UP AREA         */
+#define F_USB30_REG_MAKEUP4		47  /* HS/FS EP4 MAKE-UP AREA         */
+#define F_USB30_REG_MAKEUP5		48  /* HS/FS EP5 MAKE-UP AREA         */
+#define F_USB30_REG_CLKC		49  /* Clock Control                  */
+#define F_USB30_REG_CLKCE		50  /* Clock Control Enable           */
+#define F_USB30_REG_SSCPAC		51  /* SS CPU Access Control          */
+#define F_USB30_REG_SSDVC		52  /* SS Device Control              */
+#define F_USB30_REG_SSDVS		53  /* SS Device Status               */
+#define F_USB30_REG_SSIRQC		54  /* SS Interrupt Request Control   */
+#define F_USB30_REG_SSIRQS		55  /* SS Interrupt Request Status    */
+#define F_USB30_REG_SSEPDC		56  /* SS EP DMA Control              */
+#define F_USB30_REG_SSEPDS		57  /* SS EP DMA Status               */
+#define F_USB30_REG_SSEPTCSEL		58  /* SS EP Byte Count Status Select */
+#define F_USB30_REG_SSEPTC1		59  /* SS EP1 Total Byte Count        */
+#define F_USB30_REG_SSEPTC2		60  /* SS EP2 Total Byte Count        */
+#define F_USB30_REG_SSEPSZ0I		61  /* SS EP0 In Size                 */
+#define F_USB30_REG_SSEPSZ0O		62  /* SS EP0 Out Size                */
+#define F_USB30_REG_SSEPSZ1I		63  /* SS EP1 In Size                 */
+#define F_USB30_REG_SSEPSZ2O		64  /* SS EP2 Out Size                */
+#define F_USB30_REG_SSEPSZ3I		65  /* SS EP3 In Size                 */
+#define F_USB30_REG_SSEPSZ4O		66  /* SS EP4 Out Size                */
+#define F_USB30_REG_SSEPSZ5I		67  /* SS EP5 In Size                 */
+#define F_USB30_REG_SSCUSC		68  /* SS Custom Control              */
+#define F_USB30_REG_SSEPLPBK		69  /* SS Loopback Selector           */
+#define F_USB30_REG_SSINTFC		70  /* SS Interface Control           */
+#define F_USB30_REG_SSINTFS		71  /* SS Interface Status            */
+#define F_USB30_REG_SSEPC0		72  /* SS EP0 Control                 */
+#define F_USB30_REG_SSEPS0		73  /* SS EP0 Status                  */
+#define F_USB30_REG_SSEPC1		74  /* SS EP1 Control                 */
+#define F_USB30_REG_SSEPS1		75  /* SS EP1 Status                  */
+#define F_USB30_REG_SSEPC2		76  /* SS EP2 Control                 */
+#define F_USB30_REG_SSEPS2		77  /* SS EP2 Status                  */
+#define F_USB30_REG_SSEPC3		78  /* SS EP3 Control                 */
+#define F_USB30_REG_SSEPS3		79  /* SS EP3 Status                  */
+#define F_USB30_REG_SSEPC4		80  /* SS EP4 Control                 */
+#define F_USB30_REG_SSEPS4		81  /* SS EP4 Status                  */
+#define F_USB30_REG_SSEPC5		82  /* SS EP5 Control                 */
+#define F_USB30_REG_SSEPS5		83  /* SS EP5 Status                  */
+#define F_USB30_REG_SSEPIB0		84  /* SS EP0 In Buffer               */
+#define F_USB30_REG_SSEPIB1		85  /* SS EP1 In Buffer               */
+#define F_USB30_REG_SSEPIB3		86  /* SS EP3 In Buffer               */
+#define F_USB30_REG_SSEPIB5		87  /* SS EP5 In Buffer               */
+#define F_USB30_REG_SSEPOB0		88  /* SS EP0 Out Buffer              */
+#define F_USB30_REG_SSEPOB2		89  /* SS EP2 Out Buffer              */
+#define F_USB30_REG_SSEPOB4		90  /* SS EP4 Out Buffer              */
+#define F_USB30_REG_SSCFGEP1		91  /* SS EP1 Makeup                  */
+#define F_USB30_REG_SSCFGEP2		92  /* SS EP2 Makeup                  */
+#define F_USB30_REG_SSCFGEP3		93  /* SS EP3 Makeup                  */
+#define F_USB30_REG_SSCFGEP4		94  /* SS EP4 Makeup                  */
+#define F_USB30_REG_SSCFGEP5		95  /* SS EP5 Makeup                  */
+#define F_USB30_REG_SSEXCFGDV		96  /* SS Extended Device Makeup      */
+#define F_USB30_REG_SSEXCFGEP1		97  /* SS Extended EP1 Makeup         */
+#define F_USB30_REG_SSEXCFGEP2		98  /* SS Extended EP2 Makeup         */
+#define F_USB30_REG_SSEXCFGEP4		99  /* SS Extended EP4 Makeup         */
+#define F_USB30_REG_SSEXCFGEP5		100 /* SS Extended EP5 Makeup         */
+#define F_USB30_REG_SSBSC1		101 /* SS EP1 Bulk Stream Control     */
+#define F_USB30_REG_SSBSC2		102 /* SS EP2 Bulk Stream Control     */
+#define F_USB30_REG_SSBSC5		103 /* SS EP5 Bulk Stream Control     */
+#define F_USB30_REG_SSEPSTC1		104 /* SS EP1 Stream Total Byte Count */
+#define F_USB30_REG_SSEPSTC2		105 /* SS EP2 Stream Total Byte Count */
+#define F_USB30_REG_SSERCC		106 /* SS Error Counter Control       */
+#define F_USB30_REG_SSDlECNT1		107 /* SS Data Link Error Count 1     */
+#define F_USB30_REG_SSDlECNT2		108 /* SS Data Link Error Count 2     */
+#define F_USB30_REG_SSDlECNT3		109 /* SS Data Link Error Count 3     */
+#define F_USB30_REG_SSDlECNT4		110 /* SS Data Link Error Count 4     */
+#define F_USB30_REG_SSPlECNT		111 /* SS PHY Error Count             */
+#define F_USB30_REG_SSLNC		112 /* SS Link Control                */
+#define F_USB30_REG_SSTRC1		113 /* SS Transaction Control1        */
+#define F_USB30_REG_SSTRC2		114 /* SS Transaction Control2        */
+#define F_USB30_REG_SSTRS1		115 /* SS Transaction Status1         */
+#define F_USB30_REG_SSTRS2		116 /* SS Transaction Status2         */
+#define F_USB30_REG_SSDlC1		117 /* SS Data Link Control1          */
+#define F_USB30_REG_SSDlC2		118 /* SS Data Link Control2          */
+#define F_USB30_REG_SSDlC3		119 /* SS Data Link Control3          */
+#define F_USB30_REG_SSDlC4		120 /* SS Data Link Control4          */
+#define F_USB30_REG_SSPMC1		121 /* SS PM Control1                 */
+#define F_USB30_REG_SSPMC2		122 /* SS PM Control2                 */
+#define F_USB30_REG_SSPMC3		123 /* SS PM Control3                 */
+#define F_USB30_REG_SSPMC4		124 /* SS PM Control4                 */
+#define F_USB30_REG_SSPMC5		125 /* SS PM Control5                 */
+#define F_USB30_REG_SSVDTC1		126 /* SS Vendor Device Test Control1 */
+#define F_USB30_REG_SSVDTC2		127 /* SS Vendor Device Test Control2 */
+#define F_USB30_REG_SSVDTS1		128 /* SS Vendor Device Test Status1  */
+#define F_USB30_REG_SSVDTS2		129 /* SS Vendor Device Test Status2  */
+#define F_USB30_REG_SSPlC1		130 /* SS PHY Control1                */
+#define F_USB30_REG_SSPlC2		131 /* SS PHY Control2                */
+#define F_USB30_REG_SSPlC3		132 /* SS PHY Control3                */
+#define F_USB30_REG_SSPlC4		133 /* SS PHY Control4                */
+#define F_USB30_REG_SSPlC5		134 /* SS PHY Control5                */
+#define F_USB30_REG_SSPlC6		135 /* SS PHY Control6                */
+#define F_USB30_REG_SSPlC7		136 /* SS PHY Control7                */
+#define F_USB30_REG_SSPlC8		137 /* SS PHY Control8                */
+#define F_USB30_REG_SSPlC9		138 /* SS PHY Control9                */
+#define F_USB30_REG_SSPlC10		139 /* SS PHY Control10               */
+#define F_USB30_REG_SSPlC11		140 /* SS PHY Control11               */
+#define F_USB30_REG_SSPlC12		141 /* SS PHY Control12               */
+#define F_USB30_REG_SSPlS		142 /* SS PHY Status                  */
+#define F_USB30_REG_MAX			143 /* Max Value                      */
+
+/* HS/FS END POINT x CONTROL register ID table array */
+static const unsigned long hsepc_register[] = {
+	F_USB30_REG_HSEPC0,		/* HS/FS EP0 Control */
+	F_USB30_REG_HSEPC1,		/* HS/FS EP1 Control */
+	F_USB30_REG_HSEPC2,		/* HS/FS EP2 Control */
+	F_USB30_REG_HSEPC3,		/* HS/FS EP3 Control */
+	F_USB30_REG_HSEPC4,		/* HS/FS EP4 Control */
+	F_USB30_REG_HSEPC5,		/* HS/FS EP5 Control */
+};
+
+/* HS/FS END POINT x STATUS register ID table array */
+static const unsigned long hseps_register[] = {
+	F_USB30_REG_HSEPS0,		/* HS/FS EP0 Status  */
+	F_USB30_REG_HSEPS1,		/* HS/FS EP1 Status  */
+	F_USB30_REG_HSEPS2,		/* HS/FS EP2 Status  */
+	F_USB30_REG_HSEPS3,		/* HS/FS EP3 Status  */
+	F_USB30_REG_HSEPS4,		/* HS/FS EP4 Status  */
+	F_USB30_REG_HSEPS5,		/* HS/FS EP5 Status  */
+};
+
+/* SS END POINT x CONTROL register ID table array */
+static const unsigned long ssepc_register[] = {
+	F_USB30_REG_SSEPC0,		/* SS EP0 Control */
+	F_USB30_REG_SSEPC1,		/* SS EP1 Control */
+	F_USB30_REG_SSEPC2,		/* SS EP2 Control */
+	F_USB30_REG_SSEPC3,		/* SS EP3 Control */
+	F_USB30_REG_SSEPC4,		/* SS EP4 Control */
+	F_USB30_REG_SSEPC5,		/* SS EP5 Control */
+};
+
+/* SS END POINT x STATUS register ID table array */
+static const unsigned long sseps_register[] = {
+	F_USB30_REG_SSEPS0,		/* SS EP0 Status  */
+	F_USB30_REG_SSEPS1,		/* SS EP1 Status  */
+	F_USB30_REG_SSEPS2,		/* SS EP2 Status  */
+	F_USB30_REG_SSEPS3,		/* SS EP3 Status  */
+	F_USB30_REG_SSEPS4,		/* SS EP4 Status  */
+	F_USB30_REG_SSEPS5,		/* SS EP5 Status  */
+};
+
+/* SS END POINT x STATUS register ID table array */
+static const unsigned long ssepsz_register[] = {
+	F_USB30_REG_SSEPSZ1I,		/* SS EP1 In Size  */
+	F_USB30_REG_SSEPSZ2O,		/* SS EP2 Out Size */
+	F_USB30_REG_SSEPSZ3I,		/* SS EP3 In Size  */
+	F_USB30_REG_SSEPSZ4O,		/* SS EP4 Out Size */
+	F_USB30_REG_SSEPSZ5I,		/* SS EP5 In Size  */
+};
+
+#define U30_R (1 << 0)
+#define U30_W (1 << 1)
+
+/* F_USB30 controller register structures array */
+static const struct {
+	unsigned short address_offset;	/* register address offset */
+	unsigned char access;	/* register access flags  */
+} f_usb30_register[F_USB30_REG_MAX] = {
+	{ 0x0000, U30_R | U30_W },	/* F_USB30_REG_HSCPAC      */
+	{ 0x0004, U30_R | U30_W },	/* F_USB30_REG_HSDVC       */
+	{ 0x0008, U30_R | U30_W },	/* F_USB30_REG_HSDVS       */
+	{ 0x000c, U30_R | U30_W },	/* F_USB30_REG_HSEPIC      */
+	{ 0x0010, U30_R },	/* F_USB30_REG_HSEPIS      */
+	{ 0x0014, U30_R | U30_W },	/* F_USB30_REG_HSEPDC      */
+	{ 0x0018, U30_R },	/* F_USB30_REG_HSEPDS      */
+	{ 0x001c, U30_R },	/* F_USB30_REG_HSTSTAMP    */
+	{ 0x0020, U30_R | U30_W },	/* F_USB30_REG_HSEPTCSEL   */
+	{ 0x0024, U30_R | U30_W },	/* F_USB30_REG_HSEPTC1     */
+	{ 0x0028, U30_R | U30_W },	/* F_USB30_REG_HSEPTC2     */
+	{ 0x0070, U30_R | U30_W },	/* F_USB30_REG_HSEPRS0     */
+	{ 0x0078, U30_R | U30_W },	/* F_USB30_REG_HSEPRS1     */
+	{ 0x0080, U30_R | U30_W },	/* F_USB30_REG_HSEPRS2     */
+	{ 0x0088, U30_R | U30_W },	/* F_USB30_REG_HSEPRS3     */
+	{ 0x0090, U30_R | U30_W },	/* F_USB30_REG_HSEPRS4     */
+	{ 0x0098, U30_R | U30_W },	/* F_USB30_REG_HSEPRS5     */
+	{ 0x00f0, U30_R | U30_W },	/* F_USB30_REG_HSCUSTOMC   */
+	{ 0x00f4, U30_R | U30_W },	/* F_USB30_REG_HSCALIB     */
+	{ 0x00f8, U30_R | U30_W },	/* F_USB30_REG_HSEPLPBK    */
+	{ 0x00fc, U30_R | U30_W },	/* F_USB30_REG_HSINTFALTNUM*/
+	{ 0x0100, U30_R | U30_W },	/* F_USB30_REG_HSEPC0      */
+	{ 0x0104, U30_R | U30_W },	/* F_USB30_REG_HSEPS0      */
+	{ 0x0108, U30_R | U30_W },	/* F_USB30_REG_HSEPC1      */
+	{ 0x010c, U30_R | U30_W },	/* F_USB30_REG_HSEPS1      */
+	{ 0x0110, U30_R | U30_W },	/* F_USB30_REG_HSEPC2      */
+	{ 0x0114, U30_R | U30_W },	/* F_USB30_REG_HSEPS2      */
+	{ 0x0118, U30_R | U30_W },	/* F_USB30_REG_HSEPC3      */
+	{ 0x011c, U30_R | U30_W },	/* F_USB30_REG_HSEPS3      */
+	{ 0x0120, U30_R | U30_W },	/* F_USB30_REG_HSEPC4      */
+	{ 0x0124, U30_R | U30_W },	/* F_USB30_REG_HSEPS4      */
+	{ 0x0128, U30_R | U30_W },	/* F_USB30_REG_HSEPC5      */
+	{ 0x012c, U30_R | U30_W },	/* F_USB30_REG_HSEPS5      */
+	{ 0x0178, U30_R | U30_W },	/* F_USB30_REG_HSALTC      */
+	{ 0x017c, U30_R | U30_W },	/* F_USB30_REG_HSALTS      */
+	{ 0x0180, U30_R | U30_W },	/* F_USB30_REG_HSEPIB0     */
+	{ 0x0184, U30_R | U30_W },	/* F_USB30_REG_HSEPIB1     */
+	{ 0x018c, U30_R | U30_W },	/* F_USB30_REG_HSEPIB3     */
+	{ 0x0194, U30_R | U30_W },	/* F_USB30_REG_HSEPIB5     */
+	{ 0x01c0, U30_R },	/* F_USB30_REG_HSEPOB0     */
+	{ 0x01c8, U30_R },	/* F_USB30_REG_HSEPOB2     */
+	{ 0x01d0, U30_R },	/* F_USB30_REG_HSEPOB4     */
+	{ 0x0200, U30_R | U30_W },	/* F_USB30_REG_MAKEUP_DATA */
+	{ 0x0204, U30_R | U30_W },	/* F_USB30_REG_MAKEUP_EP0  */
+	/* F_USB30_REG_MAKEUP_EP1  */
+	{ 0x0208 + (F_USB30_MAX_ALT * 0), U30_R | U30_W },
+	/* F_USB30_REG_MAKEUP_EP2  */
+	{ 0x0218 + (F_USB30_MAX_ALT * 4), U30_R | U30_W },
+	{ 0x0228, U30_R | U30_W },	/* F_USB30_REG_MAKEUP_EP3  */
+	{ 0x023c, U30_R | U30_W },	/* F_USB30_REG_MAKEUP_EP4  */
+	{ 0x024c, U30_R | U30_W },	/* F_USB30_REG_MAKEUP_EP5  */
+	{ 0x0400, U30_R | U30_W },	/* F_USB30_REG_CLKC        */
+	{ 0x0404, U30_R | U30_W },	/* F_USB30_REG_CLKCE       */
+	{ 0x0800, U30_R | U30_W },	/* F_USB30_REG_SSCPAC      */
+	{ 0x0804, U30_R | U30_W },	/* F_USB30_REG_SSDVC       */
+	{ 0x0808, U30_R | U30_W },	/* F_USB30_REG_SSDVS       */
+	{ 0x080c, U30_R | U30_W },	/* F_USB30_REG_SSIRQC      */
+	{ 0x0810, U30_R },	/* F_USB30_REG_SSIRQS      */
+	{ 0x0814, U30_R | U30_W },	/* F_USB30_REG_SSEPDC      */
+	{ 0x0818, U30_R },	/* F_USB30_REG_SSEPDS      */
+	{ 0x0820, U30_R | U30_W },	/* F_USB30_REG_SSEPTCSEL   */
+	{ 0x0824, U30_R | U30_W },	/* F_USB30_REG_SSEPTC1     */
+	{ 0x0828, U30_R | U30_W },	/* F_USB30_REG_SSEPTC2     */
+	{ 0x0870, U30_R },	/* F_USB30_REG_SSEPSZ0I    */
+	{ 0x0874, U30_R },	/* F_USB30_REG_SSEPSZ0O    */
+	{ 0x0878, U30_R },	/* F_USB30_REG_SSEPSZ1I    */
+	{ 0x0884, U30_R },	/* F_USB30_REG_SSEPSZ2O    */
+	{ 0x0888, U30_R },	/* F_USB30_REG_SSEPSZ3I    */
+	{ 0x0894, U30_R },	/* F_USB30_REG_SSEPSZ4O    */
+	{ 0x0898, U30_R },	/* F_USB30_REG_SSEPSZ5I    */
+	{ 0x08f0, U30_R | U30_W },	/* F_USB30_REG_SSCUSC      */
+	{ 0x08f4, U30_R | U30_W },	/* F_USB30_REG_SSEPLPBK    */
+	{ 0x08f8, U30_R | U30_W },	/* F_USB30_REG_SSINTFC     */
+	{ 0x08fc, U30_R | U30_W },	/* F_USB30_REG_SSINTFS     */
+	{ 0x0900, U30_R | U30_W },	/* F_USB30_REG_SSEPC0      */
+	{ 0x0904, U30_R | U30_W },	/* F_USB30_REG_SSEPS0      */
+	{ 0x0908, U30_R | U30_W },	/* F_USB30_REG_SSEPC1      */
+	{ 0x090c, U30_R | U30_W },	/* F_USB30_REG_SSEPS1      */
+	{ 0x0910, U30_R | U30_W },	/* F_USB30_REG_SSEPC2      */
+	{ 0x0914, U30_R | U30_W },	/* F_USB30_REG_SSEPS2      */
+	{ 0x0918, U30_R | U30_W },	/* F_USB30_REG_SSEPC3      */
+	{ 0x091c, U30_R | U30_W },	/* F_USB30_REG_SSEPS3      */
+	{ 0x0920, U30_R | U30_W },	/* F_USB30_REG_SSEPC4      */
+	{ 0x0924, U30_R | U30_W },	/* F_USB30_REG_SSEPS4      */
+	{ 0x0928, U30_R | U30_W },	/* F_USB30_REG_SSEPC5      */
+	{ 0x092c, U30_R | U30_W },	/* F_USB30_REG_SSEPS5      */
+	{ 0x0980, U30_R | U30_W },	/* F_USB30_REG_SSEPIB0     */
+	{ 0x0984, U30_R | U30_W },	/* F_USB30_REG_SSEPIB1     */
+	{ 0x098c, U30_R | U30_W },	/* F_USB30_REG_SSEPIB3     */
+	{ 0x0994, U30_R | U30_W },	/* F_USB30_REG_SSEPIB5     */
+	{ 0x09c0, U30_R },	/* F_USB30_REG_SSEPOB0     */
+	{ 0x09c8, U30_R },	/* F_USB30_REG_SSEPOB2     */
+	{ 0x09d0, U30_R },	/* F_USB30_REG_SSEPOB4     */
+	{ 0x0a08, U30_R | U30_W },	/* F_USB30_REG_SSCFGEP1    */
+	{ 0x0a0c, U30_R | U30_W },	/* F_USB30_REG_SSCFGEP2    */
+	{ 0x0a10, U30_R | U30_W },	/* F_USB30_REG_SSCFGEP3    */
+	{ 0x0a14, U30_R | U30_W },	/* F_USB30_REG_SSCFGEP4    */
+	{ 0x0a18, U30_R | U30_W },	/* F_USB30_REG_SSCFGEP5    */
+	{ 0x0b00, U30_R | U30_W },	/* F_USB30_REG_SSEXCFGDV   */
+	{ 0x0b08, U30_R | U30_W },	/* F_USB30_REG_SSEXCFGEP1  */
+	{ 0x0b0c, U30_R | U30_W },	/* F_USB30_REG_SSEXCFGEP2  */
+	{ 0x0b14, U30_R | U30_W },	/* F_USB30_REG_SSEXCFGEP4  */
+	{ 0x0b18, U30_R | U30_W },	/* F_USB30_REG_SSEXCFGEP5  */
+	{ 0x0b54, U30_R | U30_W },	/* F_USB30_REG_SSBSC1      */
+	{ 0x0b58, U30_R | U30_W },	/* F_USB30_REG_SSBSC2      */
+	{ 0x0b64, U30_R | U30_W },	/* F_USB30_REG_SSBSC5      */
+	{ 0x0b94, U30_R | U30_W },	/* F_USB30_REG_SSEPSTC1    */
+	{ 0x0b98, U30_R | U30_W },	/* F_USB30_REG_SSEPSTC2    */
+	{ 0x0c00, U30_R | U30_W },	/* F_USB30_REG_SSERCC      */
+	{ 0x0c04, U30_R },	/* F_USB30_REG_SSDlECNT1   */
+	{ 0x0c08, U30_R },	/* F_USB30_REG_SSDlECNT2   */
+	{ 0x0c0c, U30_R },	/* F_USB30_REG_SSDlECNT3   */
+	{ 0x0c10, U30_R },	/* F_USB30_REG_SSDlECNT4   */
+	{ 0x0c14, U30_R },	/* F_USB30_REG_SSPlECNT    */
+	{ 0x0c20, U30_R | U30_W },	/* F_USB30_REG_SSLNC       */
+	{ 0x0c40, U30_R | U30_W },	/* F_USB30_REG_SSTRC1      */
+	{ 0x0c44, U30_R | U30_W },	/* F_USB30_REG_SSTRC2      */
+	{ 0x0c48, U30_R },	/* F_USB30_REG_SSTRS1      */
+	{ 0x0c4c, U30_R },	/* F_USB30_REG_SSTRS2      */
+	{ 0x0c50, U30_R | U30_W },	/* F_USB30_REG_SSDlC1      */
+	{ 0x0c54, U30_R | U30_W },	/* F_USB30_REG_SSDlC2      */
+	{ 0x0c58, U30_R | U30_W },	/* F_USB30_REG_SSDlC3      */
+	{ 0x0c5c, U30_R | U30_W },	/* F_USB30_REG_SSDlC4      */
+	{ 0x0c60, U30_R | U30_W },	/* F_USB30_REG_SSPMC1      */
+	{ 0x0c64, U30_R | U30_W },	/* F_USB30_REG_SSPMC2      */
+	{ 0x0c68, U30_R | U30_W },	/* F_USB30_REG_SSPMC3      */
+	{ 0x0c6c, U30_R | U30_W },	/* F_USB30_REG_SSPMC4      */
+	{ 0x0c70, U30_R | U30_W },	/* F_USB30_REG_SSPMC5      */
+	{ 0x0c80, U30_R | U30_W },	/* F_USB30_REG_SSVDTC1     */
+	{ 0x0c84, U30_R | U30_W },	/* F_USB30_REG_SSVDTC2     */
+	{ 0x0c88, U30_R },	/* F_USB30_REG_SSVDTS1     */
+	{ 0x0c8c, U30_R },	/* F_USB30_REG_SSVDTS2     */
+	{ 0x0c90, U30_R | U30_W },	/* F_USB30_REG_SSPlC1      */
+	{ 0x0c94, U30_R | U30_W },	/* F_USB30_REG_SSPlC2      */
+	{ 0x0c98, U30_R | U30_W },	/* F_USB30_REG_SSPlC3      */
+	{ 0x0c9c, U30_R | U30_W },	/* F_USB30_REG_SSPlC4      */
+	{ 0x0ca0, U30_R | U30_W },	/* F_USB30_REG_SSPlC5      */
+	{ 0x0ca4, U30_R | U30_W },	/* F_USB30_REG_SSPlC6      */
+	{ 0x0ca8, U30_R | U30_W },	/* F_USB30_REG_SSPlC7      */
+	{ 0x0cac, U30_R | U30_W },	/* F_USB30_REG_SSPlC8      */
+	{ 0x0cb0, U30_R | U30_W },	/* F_USB30_REG_SSPlC9      */
+	{ 0x0cb4, U30_R | U30_W },	/* F_USB30_REG_SSPlC10     */
+	{ 0x0cb8, U30_R | U30_W },	/* F_USB30_REG_SSPlC11     */
+	{ 0x0cbc, U30_R | U30_W },	/* F_USB30_REG_SSPlC12     */
+	{ 0x0cd0, U30_R },	/* F_USB30_REG_SSPlS       */
+};
+
+static inline void control_f_usb30_default_register_cache_bits(
+	unsigned long *register_cache)
+{
+	return;
+}
+
+static inline void control_f_usb30_hsdvs_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* DEVICE STATUS register bit feild position */
+#define F_USB30_REGISTER_HSDVS_BIT_INTSUSPENDE		24 /* IntSuspende   */
+#define F_USB30_REGISTER_HSDVS_BIT_INTSUSPENDB		25 /* IntSuspendb   */
+#define F_USB30_REGISTER_HSDVS_BIT_INTSOF		26 /* IntSof        */
+#define F_USB30_REGISTER_HSDVS_BIT_INTSETUP		27 /* IntSetup      */
+#define F_USB30_REGISTER_HSDVS_BIT_INTUSBRSTE		28 /* IntUsbRste    */
+#define F_USB30_REGISTER_HSDVS_BIT_INTUSBRSTB		29 /* IntUsbRstb    */
+#define F_USB30_REGISTER_HSDVS_BIT_INTSETCONF		30 /* IntSetConf    */
+#define F_USB30_REGISTER_HSDVS_BIT_INTERRATICERR	31 /* IntErraticErr */
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTSUSPENDE) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTSUSPENDB) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTSOF) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTSETUP) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTUSBRSTE) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTUSBRSTB) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTSETCONF) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSDVS_BIT_INTERRATICERR));
+	return;
+}
+
+static inline void control_f_usb30_hseps0_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT 0 STATUS register bit feild position */
+#define F_USB30_REGISTER_HSEPS0_BIT_READY0I		3  /* Ready0i     */
+#define F_USB30_REGISTER_HSEPS0_BIT_READY0O		4  /* Ready0o     */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTREADY0I		16 /* IntReady0i  */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTREADY0O		17 /* IntReady0o  */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTPING0		18 /* IntPing0    */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTSTALLED0		21 /* IntStalled0 */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTNACK0		22 /* IntNack0    */
+#define F_USB30_REGISTER_HSEPS0_BIT_INTCLSTALL0		23 /* IntClStall0 */
+
+	*register_cache &=
+	      ~(((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_READY0I) |
+		((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_READY0O));
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTREADY0I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTREADY0O) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTPING0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTSTALLED0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTNACK0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSEPS0_BIT_INTCLSTALL0));
+
+	return;
+}
+
+static inline void control_f_usb30_hseps_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_HSEPS_BIT_READYI		2	/* Readyi     */
+#define F_USB30_REGISTER_HSEPS_BIT_READYO		3	/* Readyo     */
+#define F_USB30_REGISTER_HSEPS_BIT_INTSPR		13	/* IntSPR     */
+#define F_USB30_REGISTER_HSEPS_BIT_INTSPDD		14	/* IntSPDD    */
+#define F_USB30_REGISTER_HSEPS_BIT_INTREADY		16	/* IntReady   */
+#define F_USB30_REGISTER_HSEPS_BIT_INTPING		17	/* IntPing    */
+#define F_USB30_REGISTER_HSEPS_BIT_INTACHG		18	/* IntAChg    */
+#define F_USB30_REGISTER_HSEPS_BIT_INTDEND		19	/* IntDEnd    */
+#define F_USB30_REGISTER_HSEPS_BIT_INTEMPTY		20	/* IntEmpty   */
+#define F_USB30_REGISTER_HSEPS_BIT_INTSTALLED		21	/* IntStalled */
+#define F_USB30_REGISTER_HSEPS_BIT_INTNACK		22	/* IntNack    */
+#define F_USB30_REGISTER_HSEPS_BIT_INTCLSTALL		23	/* IntClStall */
+
+	*register_cache &=
+	     ~(((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_READYI) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_READYO));
+
+	*register_cache |=
+	     (((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTSPR) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTSPDD) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTREADY) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTPING) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTACHG) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTDEND) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTEMPTY) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTSTALLED) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTNACK) |
+	      ((unsigned long) 1 << F_USB30_REGISTER_HSEPS_BIT_INTCLSTALL));
+
+	return;
+}
+
+static inline void control_f_usb30_hsalts_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* ALTERNATE STATUS register bit feild position */
+#define F_USB30_REGISTER_HSALTS_BIT_INTACHGIF0	0      /* intAChgIf0 */
+#define F_USB30_REGISTER_HSALTS_BIT_INTACHGIF1	1      /* intAChgIf1 */
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_HSALTS_BIT_INTACHGIF0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_HSALTS_BIT_INTACHGIF1));
+
+	return;
+}
+
+static inline void control_f_usb30_ssdvs_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* DEVICE STATUS register bit feild position */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSSWRSTE		12 /* IntSSWRste      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSSWRSTB		13 /* IntSSWRstb      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSSHRSTE		14 /* IntSSHRste      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSSHRSTB		15 /* IntSSHRstb      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTVDTEST		20 /* IntVDTest       */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSSDISABLE		21 /* IntSSDisable    */
+#define F_USB30_REGISTER_SSDVS_BIT_INTENTERPOLL		22 /* IntEnterPoll    */
+#define F_USB30_REGISTER_SSDVS_BIT_INTPOLLTOU0		23 /* IntPolltoU0     */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSUSPENDE		24 /* IntSuspende     */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSUSPENDB		25 /* IntSuspendb     */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSETINTF		26 /* IntSetIntf      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSETUP		27 /* IntSetup        */
+#define F_USB30_REGISTER_SSDVS_BIT_INTU2INACTTO		29 /* IntU2InactTO    */
+#define F_USB30_REGISTER_SSDVS_BIT_INTSETCONF		30 /* IntSetConf      */
+#define F_USB30_REGISTER_SSDVS_BIT_INTFNCSUSP		31 /* IntFncSusp      */
+
+	*register_cache |=
+	    (((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSSWRSTE) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSSWRSTB) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSSHRSTE) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSSHRSTB) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTVDTEST) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSSDISABLE) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTENTERPOLL) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTPOLLTOU0) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSUSPENDE) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSUSPENDB) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSETINTF) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSETUP) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTU2INACTTO) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTSETCONF) |
+	     ((unsigned long) 1 << F_USB30_REGISTER_SSDVS_BIT_INTFNCSUSP));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps0_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT 0 STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS0_BIT_READY0I		3  /* Ready0i     */
+#define F_USB30_REGISTER_SSEPS0_BIT_READY0O		4  /* Ready0o     */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTREADY0I		16 /* IntReady0i  */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTREADY0O		17 /* IntReady0o  */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTPKTPND0		18 /* IntPktPnd0  */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTSTALLED0		21 /* IntStalled0 */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTNRDY0		22 /* IntNrdy0    */
+#define F_USB30_REGISTER_SSEPS0_BIT_INTCLSTALL0		23 /* IntClStall0 */
+
+	*register_cache &=
+	   ~(((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_READY0I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_READY0O));
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTREADY0I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTREADY0O) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTPKTPND0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTSTALLED0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTNRDY0) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS0_BIT_INTCLSTALL0));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps1_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS_BIT_READY1I		2  /* Ready1i       */
+#define F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE1	7  /* StreamActive1 */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM1		15 /* IntClStream1  */
+#define F_USB30_REGISTER_SSEPS_BIT_INTREADY1I		16 /* IntReady1i    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTPKTPND1		17 /* IntPktPnd1    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSDEND1		18 /* IntSDEnd1     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTDEND1		19 /* IntDEnd1      */
+#define F_USB30_REGISTER_SSEPS_BIT_INTEMPTY1		20 /* IntEmpty1     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSTALLED1		21 /* IntStalled1   */
+#define F_USB30_REGISTER_SSEPS_BIT_INTNRDY1		22 /* IntNrdy1      */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL1		23 /* IntClStall1   */
+
+	*register_cache &=
+	   ~((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_READY1I);
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTREADY1I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTPKTPND1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSDEND1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTDEND1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTEMPTY1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSTALLED1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTNRDY1) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL1));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps2_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS_BIT_READY2O		3  /* Ready2o       */
+#define F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE2	7  /* StreamActive2 */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSPR2		13 /* IntSPR2       */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSPD2		14 /* IntSPD2       */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM2		15 /* IntClStream2  */
+#define F_USB30_REGISTER_SSEPS_BIT_INTREADY2O		16 /* IntReady2o    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTPKTPND2		17 /* IntPktPnd2    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSDEND2		18 /* IntSDEnd2     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTDEND2		19 /* IntDEnd2      */
+#define F_USB30_REGISTER_SSEPS_BIT_INTEMPTY2		20 /* IntEmpty2     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSTALLED2		21 /* IntStalled2   */
+#define F_USB30_REGISTER_SSEPS_BIT_INTNRDY2		22 /* IntNrdy2      */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL2		23 /* IntClStall2   */
+
+	*register_cache &=
+	   ~((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_READY2O);
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSPR2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSPD2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTREADY2O) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTPKTPND2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSDEND2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTDEND2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTEMPTY2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSTALLED2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTNRDY2) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL2));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps3_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS_BIT_READY3I		2  /* Ready3i        */
+#define F_USB30_REGISTER_SSEPS_BIT_INTREADY3I		16 /* IntReady3i     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTPKTPND3		17 /* IntPktPnd3     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSTALLED3		21 /* IntStalled3    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTNRDY3		22 /* IntNrdy3       */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL3		23 /* IntClStall3    */
+
+	*register_cache &=
+	   ~((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_READY3I);
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTREADY3I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTPKTPND3) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSTALLED3) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTNRDY3) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL3));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps4_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS_BIT_READY4O		3  /* Ready4o        */
+#define F_USB30_REGISTER_SSEPS_BIT_INTREADY4O		16 /* IntReady4o     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTPKTPND4		17 /* IntPktPnd4     */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSTALLED4		21 /* IntStalled4    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTNRDY4		22 /* IntNrdy4       */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL4		23 /* IntClStall4    */
+
+	*register_cache &=
+	   ~((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_READY4O);
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTREADY4O) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTPKTPND4) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSTALLED4) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTNRDY4) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL4));
+
+	return;
+}
+
+static inline void control_f_usb30_sseps5_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* END POINT STATUS register bit feild position */
+#define F_USB30_REGISTER_SSEPS_BIT_READY5I		2  /* Ready5i       */
+#define F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE5	7  /* StreamActive5 */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM5		15 /* IntClStream5  */
+#define F_USB30_REGISTER_SSEPS_BIT_INTREADY5I		16 /* IntReady5i    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTPKTPND5		17 /* IntPktPnd5    */
+#define F_USB30_REGISTER_SSEPS_BIT_INTSTALLED5		21 /* IntStalled5   */
+#define F_USB30_REGISTER_SSEPS_BIT_INTNRDY5		22 /* IntNrdy5      */
+#define F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL5		23 /* IntClStall5   */
+
+	*register_cache &=
+	   ~((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_READY5I);
+
+	*register_cache |=
+	   (((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_STREAMACTIVE5) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTREAM5) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTREADY5I) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTPKTPND5) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTSTALLED5) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTNRDY5) |
+	    ((unsigned long) 1 << F_USB30_REGISTER_SSEPS_BIT_INTCLSTALL5));
+
+	return;
+}
+
+static inline void control_f_usb30_register_cache_bits(
+	unsigned long register_id, unsigned long *register_cache)
+{
+	static const unsigned char register_cache_bits_control[] = {
+		0,		/* HS/FS CPU Access Control       */
+		0,		/* HS/FS Device Control           */
+		1,		/* HS/FS Device Status            */
+		0,		/* HS/FS EP Interrupt Control     */
+		0,		/* HS/FS EP Interrupt Status      */
+		0,		/* HS/FS EP DMA Control           */
+		0,		/* HS/FS EP DMA Status            */
+		0,		/* HS/FS Time Stamp               */
+		0,		/* HS/FS EP Byte Count Select     */
+		0,		/* HS/FS EP1 Total Byte Count     */
+		0,		/* HS/FS EP2 Total Byte Count     */
+		0,		/* HS/FS EP0 Rx Size              */
+		0,		/* HS/FS EP1 Rx Size              */
+		0,		/* HS/FS EP2 Rx Size              */
+		0,		/* HS/FS EP3 Rx Size              */
+		0,		/* HS/FS EP4 Rx Size              */
+		0,		/* HS/FS EP5 Rx Size              */
+		0,		/* HS/FS Custom Control           */
+		0,		/* HS/FS Calibration              */
+		0,		/* HS/FS Loop Back Selector       */
+		0,		/* HS/FS Interface Alt Number     */
+		0,		/* HS/FS EP0 Control              */
+		2,		/* HS/FS EP0 Status               */
+		0,		/* HS/FS EP1 Control              */
+		3,		/* HS/FS EP1 Status               */
+		0,		/* HS/FS EP2 Control              */
+		3,		/* HS/FS EP2 Status               */
+		0,		/* HS/FS EP3 Control              */
+		3,		/* HS/FS EP3 Status               */
+		0,		/* HS/FS EP4 Control              */
+		3,		/* HS/FS EP4 Status               */
+		0,		/* HS/FS EP5 Control              */
+		3,		/* HS/FS EP5 Status               */
+		0,		/* HS/FS Alternate Control        */
+		4,		/* HS/FS Alternate Status         */
+		0,		/* HS/FS EP0 In Buffer            */
+		0,		/* HS/FS EP1 In Buffer            */
+		0,		/* HS/FS EP3 In Buffer            */
+		0,		/* HS/FS EP5 In Buffer            */
+		0,		/* HS/FS EP0 Out Buffer           */
+		0,		/* HS/FS EP2 Out Buffer           */
+		0,		/* HS/FS EP4 Out Buffer           */
+		0,		/* HS/FS Make-Up Area             */
+		0,		/* HS/FS EP0 Make-Up Area         */
+		0,		/* HS/FS EP1 Make-Up Area         */
+		0,		/* HS/FS EP2 Make-Up Area         */
+		0,		/* HS/FS EP3 Make-Up Area         */
+		0,		/* HS/FS EP4 Make-Up Area         */
+		0,		/* HS/FS EP5 Make-Up Area         */
+		0,		/* Clock Control                  */
+		0,		/* Clock Control Enable           */
+		0,		/* SS CPU Access Control          */
+		0,		/* SS Device Control              */
+		5,		/* SS Device Status               */
+		0,		/* SS Interrupt Request Control   */
+		0,		/* SS Interrupt Request Status    */
+		0,		/* SS EP DMA Control              */
+		0,		/* SS EP DMA Status               */
+		0,		/* SS EP Byte Count Status Select */
+		0,		/* SS EP1 Total Byte Count        */
+		0,		/* SS EP2 Total Byte Count        */
+		0,		/* SS EP0 In Size                 */
+		0,		/* SS EP0 Out Size                */
+		0,		/* SS EP1 In Size                 */
+		0,		/* SS EP2 Out Size                */
+		0,		/* SS EP3 In Size                 */
+		0,		/* SS EP4 Out Size                */
+		0,		/* SS EP5 In Size                 */
+		0,		/* SS Custom Control              */
+		0,		/* SS Loopback Selector           */
+		0,		/* SS Interface Control           */
+		0,		/* SS Interface Status            */
+		0,		/* SS EP0 Control                 */
+		6,		/* SS EP0 Status                  */
+		0,		/* SS EP1 Control                 */
+		7,		/* SS EP1 Status                  */
+		0,		/* SS EP2 Control                 */
+		8,		/* SS EP2 Status                  */
+		0,		/* SS EP3 Control                 */
+		9,		/* SS EP3 Status                  */
+		0,		/* SS EP4 Control                 */
+		10,		/* SS EP4 Status                  */
+		0,		/* SS EP5 Control                 */
+		11,		/* SS EP5 Status                  */
+		0,		/* SS EP0 In Buffer               */
+		0,		/* SS EP1 In Buffer               */
+		0,		/* SS EP3 In Buffer               */
+		0,		/* SS EP5 In Buffer               */
+		0,		/* SS EP0 Out Buffer              */
+		0,		/* SS EP2 Out Buffer              */
+		0,		/* SS EP4 Out Buffer              */
+		0,		/* SS EP1 Makeup                  */
+		0,		/* SS EP2 Makeup                  */
+		0,		/* SS EP3 Makeup                  */
+		0,		/* SS EP4 Makeup                  */
+		0,		/* SS EP5 Makeup                  */
+		0,		/* SS Extended Device Makeup      */
+		0,		/* SS Extended EP1 Makeup         */
+		0,		/* SS Extended EP2 Makeup         */
+		0,		/* SS Extended EP4 Makeup         */
+		0,		/* SS Extended EP5 Makeup         */
+		0,		/* SS EP1 Bulk Stream Control     */
+		0,		/* SS EP2 Bulk Stream Control     */
+		0,		/* SS EP5 Bulk Stream Control     */
+		0,		/* SS EP1 Stream Total Byte Count */
+		0,		/* SS EP2 Stream Total Byte Count */
+		0,		/* SS Error Counter Control       */
+		0,		/* SS Data Link Error Count 1     */
+		0,		/* SS Data Link Error Count 2     */
+		0,		/* SS Data Link Error Count 3     */
+		0,		/* SS Data Link Error Count 4     */
+		0,		/* SS PHY Error Count             */
+		0,		/* SS Link Control                */
+		0,		/* SS Transaction Control1        */
+		0,		/* SS Transaction Control2        */
+		0,		/* SS Transaction Status1         */
+		0,		/* SS Transaction Status2         */
+		0,		/* SS Data Link Control1          */
+		0,		/* SS Data Link Control2          */
+		0,		/* SS Data Link Control3          */
+		0,		/* SS Data Link Control4          */
+		0,		/* SS PM Control1                 */
+		0,		/* SS PM Control2                 */
+		0,		/* SS PM Control3                 */
+		0,		/* SS PM Control4                 */
+		0,		/* SS PM Control5                 */
+		0,		/* SS Vendor Device Test Control1 */
+		0,		/* SS Vendor Device Test Control2 */
+		0,		/* SS Vendor Device Test Status1  */
+		0,		/* SS Vendor Device Test Status2  */
+		0,		/* SS PHY Control1                */
+		0,		/* SS PHY Control2                */
+		0,		/* SS PHY Control3                */
+		0,		/* SS PHY Control4                */
+		0,		/* SS PHY Control5                */
+		0,		/* SS PHY Control6                */
+		0,		/* SS PHY Control7                */
+		0,		/* SS PHY Control8                */
+		0,		/* SS PHY Control9                */
+		0,		/* SS PHY Control10               */
+		0,		/* SS PHY Control11               */
+		0,		/* SS PHY Control12               */
+		0,		/* SS PHY Status                  */
+	};
+
+	static void (*const register_cache_bits_control_function[])(
+		unsigned long *) = {
+			control_f_usb30_default_register_cache_bits,
+			control_f_usb30_hsdvs_register_cache_bits,
+			control_f_usb30_hseps0_register_cache_bits,
+			control_f_usb30_hseps_register_cache_bits,
+			control_f_usb30_hsalts_register_cache_bits,
+			control_f_usb30_ssdvs_register_cache_bits,
+			control_f_usb30_sseps0_register_cache_bits,
+			control_f_usb30_sseps1_register_cache_bits,
+			control_f_usb30_sseps2_register_cache_bits,
+			control_f_usb30_sseps3_register_cache_bits,
+			control_f_usb30_sseps4_register_cache_bits,
+			control_f_usb30_sseps5_register_cache_bits,};
+
+	register_cache_bits_control_function[
+		register_cache_bits_control[register_id]](register_cache);
+	return;
+}
+
+static inline void set_f_usb30_register_bits(void __iomem *base_address,
+	 unsigned long register_id, unsigned char start_bit,
+	 unsigned char bit_length, unsigned long value)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	value &= mask;
+
+	if (f_usb30_register[register_id].access & U30_R)
+		register_cache = __raw_readl(base_address +
+			f_usb30_register[register_id].address_offset);
+
+	control_f_usb30_register_cache_bits(register_id, &register_cache);
+
+	register_cache &= ~(mask << start_bit);
+	register_cache |= (value << start_bit);
+
+	if (f_usb30_register[register_id].access & U30_W)
+		__raw_writel(register_cache, base_address +
+			f_usb30_register[register_id].address_offset);
+
+	return;
+}
+
+static inline unsigned long get_f_usb30_register_bits(
+	void __iomem *base_address, unsigned long register_id,
+	 unsigned char start_bit, unsigned char bit_length)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	if (f_usb30_register[register_id].access & U30_R)
+		register_cache = __raw_readl(base_address +
+			f_usb30_register[register_id].address_offset);
+
+	return register_cache >> start_bit & mask;
+}
+
+static inline void set_f_usb30_makeup_register_bits(
+	void __iomem *base_address, unsigned long register_id,
+	unsigned char alternate, unsigned char start_bit,
+	 unsigned char bit_length, unsigned long value)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	value &= mask;
+
+	if (f_usb30_register[register_id].access & U30_R)
+		register_cache = __raw_readl(base_address +
+			 f_usb30_register[register_id].address_offset +
+			 alternate * 4);
+
+	register_cache &= ~(mask << start_bit);
+	register_cache |= (value << start_bit);
+
+	if (f_usb30_register[register_id].access & U30_W)
+		__raw_writel(register_cache, base_address +
+			 f_usb30_register[register_id].address_offset +
+			 alternate * 4);
+
+	return;
+}
+
+static inline unsigned long get_f_usb30_makeup_register_bits(
+	void __iomem *base_address, unsigned long register_id,
+	 unsigned char alternate, unsigned char start_bit,
+	 unsigned char bit_length)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	if (f_usb30_register[register_id].access & U30_R)
+		register_cache = __raw_readl(base_address +
+			 f_usb30_register[register_id].address_offset +
+			 alternate * 4);
+
+	return register_cache >> start_bit & mask;
+}
+
+static inline void set_hs_softreset(void __iomem *base_address)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCPAC, 3, 1, 1);
+	for (counter = 50; counter; counter--)
+		;
+	for (counter = 0xffff; ((counter) &&
+		 (get_f_usb30_register_bits(base_address,
+		 F_USB30_REG_HSCPAC, 3, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_hs_configwren(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCPAC, 16, 1, enable);
+	return;
+}
+
+static inline void set_hs_reqspeed(void __iomem *base_address,
+	 unsigned char bus_speed)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 0, 2, bus_speed);
+	return;
+}
+
+	/* bus speed request symbolic constant */
+#define REQ_SPEED_HIGH_SPEED	0	/* high-speed request */
+#define REQ_SPEED_FULL_SPEED	1	/* full-speed request */
+
+static inline void set_hs_reqresume(void __iomem *base_address,
+	 unsigned char request)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 2, 1, request);
+	return;
+}
+
+static inline void set_hs_enrmtwkup(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 3, 1, enable);
+	return;
+}
+
+static inline unsigned char get_hs_enrmtwkup(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 3, 1);
+}
+
+static inline void set_hs_selfpwr(void __iomem *base_address,
+	 unsigned char self_power)
+{
+	set_f_usb30_register_bits(base_address,
+					F_USB30_REG_HSDVC, 4, 1, self_power);
+	return;
+}
+
+static inline void set_hs_disconnect(void __iomem *base_address,
+	 unsigned char dis_connect)
+{
+	set_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSDVC, 5, 1, dis_connect);
+	return;
+}
+
+static inline void set_hs_physusp(void __iomem *base_address,
+	 unsigned char force)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 14, 1, force);
+	return;
+}
+
+static inline void set_hs_lpbkphy(void __iomem *base_address,
+	 unsigned char test)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 17, 1, test);
+	return;
+}
+
+static inline void set_hs_pmode(void __iomem *base_address,
+	 unsigned char phy_mode)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 18, 1, phy_mode);
+	return;
+}
+
+static inline void set_hs_lmode(void __iomem *base_address,
+	 unsigned char link_mode)
+{
+	set_f_usb30_register_bits(base_address,
+					F_USB30_REG_HSDVC, 19, 1, link_mode);
+	return;
+}
+
+static inline void set_hs_sofsel(void __iomem *base_address,
+	 unsigned char palse)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 23, 1, palse);
+	return;
+}
+
+static inline void set_hs_msksuspende(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 24, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msksuspende(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 24, 1);
+}
+
+static inline void set_hs_msksuspendb(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 25, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msksuspendb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 25, 1);
+}
+
+static inline void set_hs_msksof(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 26, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msksof(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 26, 1);
+}
+
+static inline void set_hs_msksetup(void __iomem *base_address,
+	unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 27, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msksetup(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 27, 1);
+}
+
+static inline void set_hs_mskusbrste(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 28, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskusbrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 28, 1);
+}
+
+static inline void set_hs_mskusbrstb(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 29, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskusbrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 29, 1);
+}
+
+static inline void set_hs_msksetconf(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 30, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msksetconf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 30, 1);
+}
+
+static inline void set_hs_mskerraticerr(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVC, 31, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskerraticerr(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVC, 31, 1);
+}
+
+static inline unsigned char get_hs_suspend(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 8, 1);
+}
+
+static inline unsigned char get_hs_busreset(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 9, 1);
+}
+
+static inline unsigned char get_hs_phyreset(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 15, 1);
+}
+
+static inline unsigned char get_hs_crtspeed(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 16, 2);
+}
+
+	/* bus speed symbolic constant */
+#define CRT_SPEED_HIGH_SPEED	0	/* high-speed */
+#define CRT_SPEED_FULL_SPEED	1	/* full-speed */
+#define CRT_SPEED_LOW_SPEED	2	/* low-speed */
+
+static inline unsigned char get_hs_conf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 20, 4);
+}
+
+static inline void clear_hs_intsuspende(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 24, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intsuspende(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 24, 1);
+}
+
+static inline void clear_hs_intsuspendb(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 25, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intsuspendb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 25, 1);
+}
+
+static inline void clear_hs_intsof(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 26, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intsof(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 26, 1);
+}
+
+static inline void clear_hs_intsetup(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 27, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intsetup(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 27, 1);
+}
+
+static inline void clear_hs_intusbrste(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 28, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intusbrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 28, 1);
+}
+
+static inline void clear_hs_intusbrstb(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 29, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intusbrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 29, 1);
+}
+
+static inline void clear_hs_intsetconf(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 30, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intsetconf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 30, 1);
+}
+
+static inline void clear_hs_interraticerr(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSDVS, 31, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_interraticerr(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSDVS, 31, 1);
+}
+
+static inline void set_hs_mskep(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPIC,
+					 endpoint_channel, 1, mask);
+	return;
+}
+
+	/* endpoint channel symbolic constant */
+#define ENDPOINT0	0	/* endpoint 0 */
+#define ENDPOINT1	1	/* endpoint 1 */
+#define ENDPOINT2	2	/* endpoint 2 */
+#define ENDPOINT3	3	/* endpoint 3 */
+#define ENDPOINT4	4	/* endpoint 4 */
+#define ENDPOINT5	5	/* endpoint 5 */
+
+static inline unsigned char get_hs_mskep(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSEPIC, endpoint_channel, 1);
+}
+
+static inline unsigned char get_hs_intep(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSEPIS, endpoint_channel, 1);
+}
+
+static inline void set_hs_mskdmareq(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPDC,
+					 endpoint_channel, 1, mask);
+	return;
+}
+
+static inline void set_hs_dmamode(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usage)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPDC,
+					 endpoint_channel + 16, 1, usage);
+	return;
+}
+
+static inline unsigned char get_hs_dmamode(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPDC,
+						 endpoint_channel + 16, 1);
+}
+
+static inline unsigned char get_hs_dmareq(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSEPDS, endpoint_channel, 1);
+}
+
+static inline unsigned short get_hs_timstamp(void __iomem *base_address)
+{
+	return (unsigned short) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSTSTAMP, 0, 11);
+}
+
+static inline void set_hs_tcselusb(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usb)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPTCSEL,
+					 endpoint_channel, 1, usb);
+	return;
+}
+
+static inline void set_hs_tcnt(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned long count)
+{
+	set_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSEPTC1 + endpoint_channel - 1,
+				 0, 32, count);
+	return;
+}
+
+static inline unsigned long get_hs_tcnt(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return get_f_usb30_register_bits(base_address,
+			 F_USB30_REG_HSEPTC1 + endpoint_channel - 1, 0, 32);
+}
+
+static inline unsigned char get_hs_txrxsize0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPRS0, 0, 7);
+}
+
+static inline void set_hs_seltx0o(void __iomem *base_address,
+	 unsigned char tx_size0o)
+{
+	set_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSEPRS0, 7, 1, tx_size0o);
+	return;
+}
+
+static inline unsigned char get_hs_txrxsize0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPRS0, 8, 7);
+}
+
+static inline void set_hs_seltx0i(void __iomem *base_address,
+	 unsigned char tx_size0i)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPRS0,
+					 15, 1, tx_size0i);
+	return;
+}
+
+static inline unsigned short get_hs_txrxsize(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned short) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPRS0 +
+						 endpoint_channel, 0, 11);
+}
+
+static inline void set_hs_seltx(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char tx_size)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPRS0 +
+					 endpoint_channel, 15, 1, tx_size);
+	return;
+}
+
+static inline void set_hs_cuscnt(void __iomem *base_address,
+	 unsigned long cus_cnt)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCUSTOMC,
+					 0, 32, cus_cnt);
+	return;
+}
+
+static inline unsigned long get_hs_cuscnt(void __iomem *base_address)
+{
+	return get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCUSTOMC, 0, 32);
+}
+
+static inline void set_hs_fscalib(void __iomem *base_address,
+	 unsigned char calib)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCALIB, 0, 3, calib);
+	return;
+}
+
+static inline void set_hs_hscalib(void __iomem *base_address,
+	 unsigned char calib)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSCALIB, 4, 3, calib);
+	return;
+}
+
+static inline void set_hs_eplpbki0(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPLPBK,
+					 0, 4, endpoint_channel);
+	return;
+}
+
+static inline void set_hs_eplpbko0(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPLPBK,
+					 4, 4, endpoint_channel);
+	return;
+}
+
+static inline void set_hs_numaltintf(void __iomem *base_address,
+	 unsigned char interface, unsigned char usage_number)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSINTFALTNUM,
+					 interface * 4, 3, usage_number);
+	return;
+}
+
+static inline void set_hs_numintf(void __iomem *base_address,
+	 unsigned char usage_number)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSINTFALTNUM,
+					 24, 4, usage_number);
+	return;
+}
+
+static inline void set_hs_init0i(void __iomem *base_address)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 0, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 F_USB30_REG_HSEPC0, 0, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_hs_init0o(void __iomem *base_address)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 1, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 F_USB30_REG_HSEPC0, 1, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_hs_reqstall0(void __iomem *base_address,
+	 unsigned char stall0)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 2, 1, stall0);
+	return;
+}
+
+static inline unsigned char get_hs_reqstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 2, 1);
+}
+
+static inline void set_hs_testmode0(void __iomem *base_address,
+	 unsigned char test0)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 7, 1, test0);
+	return;
+}
+
+static inline void set_hs_inififo0i(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 14, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_inififo0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 14, 1);
+}
+
+static inline void set_hs_inififo0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 15, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_inififo0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 15, 1);
+}
+
+static inline void set_hs_mskready0i(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 16, 1);
+}
+
+static inline void set_hs_mskready0o(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 17, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 17, 1);
+}
+
+static inline void set_hs_mskping0o(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 18, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskping0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 18, 1);
+}
+
+static inline void set_hs_mskstalled0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskstalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 21, 1);
+}
+
+static inline void set_hs_msknack0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 22, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msknack0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 22, 1);
+}
+
+static inline void set_hs_mskclstall0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPC0, 23, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskclstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPC0, 23, 1);
+}
+
+static inline unsigned char get_hs_stalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 2, 1);
+}
+
+static inline void enable_hs_ready0i(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 3, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_ready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 3, 1);
+}
+
+static inline void enable_hs_ready0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 4, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_ready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 4, 1);
+}
+
+static inline void clear_hs_intready0i(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 16, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 16, 1);
+}
+
+static inline void clear_hs_intready0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 17, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 17, 1);
+}
+
+static inline void clear_hs_intping0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 18, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intping0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 18, 1);
+}
+
+static inline void clear_hs_intstalled0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intstalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 21, 1);
+}
+
+static inline void clear_hs_intnack0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 22, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intnack0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 22, 1);
+}
+
+static inline void clear_hs_intclstall0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSEPS0, 23, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intclstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSEPS0, 23, 1);
+}
+
+static inline void set_hs_init(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 0, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 hsepc_register[endpoint_channel], 0, 1))); counter--)
+		;
+	return;
+}
+
+static inline unsigned char get_hs_init(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					hsepc_register[endpoint_channel], 0, 1);
+}
+
+static inline void set_hs_reqstall(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char stall)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 1, 1, stall);
+	return;
+}
+
+static inline unsigned char get_hs_reqstall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					hsepc_register[endpoint_channel], 1, 1);
+}
+
+static inline void set_hs_initoggle(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 3, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 hsepc_register[endpoint_channel], 3, 1)));
+		 counter--)
+		;
+	return;
+}
+
+static inline void set_hs_inistall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 4, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 hsepc_register[endpoint_channel], 4, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_hs_toggledis(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char auto_disable)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 5, 1, auto_disable);
+	return;
+}
+
+static inline void set_hs_stalldis(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char auto_disable)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 6, 1, auto_disable);
+	return;
+}
+
+static inline void set_hs_nullresp(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char null)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 8, 1, null);
+	return;
+}
+
+static inline void set_hs_nackresp(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char nack)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 9, 1, nack);
+	return;
+}
+
+static inline void set_hs_enspr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usage)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 10, 1, usage);
+	return;
+}
+
+static inline void set_hs_enspdd(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usage)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 11, 1, usage);
+	return;
+}
+
+static inline void set_hs_mskspr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 13, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskspr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 13, 1);
+}
+
+static inline void set_hs_mskspdd(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 14, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskspdd(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 14, 1);
+}
+
+static inline void set_hs_inififo(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 15, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_inififo(void __iomem *base_address,
+		 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 15, 1);
+}
+
+static inline void set_hs_mskready(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskready(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 16, 1);
+}
+
+static inline void set_hs_mskping(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 17, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskping(void __iomem *base_address,
+		 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 17, 1);
+}
+
+static inline void set_hs_mskdend(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 19, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskdend(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 19, 1);
+}
+
+static inline void set_hs_mskempty(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 20, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskempty(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 20, 1);
+}
+
+static inline void set_hs_mskstalled(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskstalled(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 21, 1);
+}
+
+static inline void set_hs_msknack(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 22, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_msknack(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 22, 1);
+}
+
+static inline void set_hs_mskclstall(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 hsepc_register[endpoint_channel],
+					 23, 1, mask);
+	return;
+}
+
+static inline unsigned char get_hs_mskclstall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hsepc_register[endpoint_channel], 23, 1);
+}
+
+static inline unsigned char get_hs_stalled(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 1, 1);
+}
+
+static inline void enable_hs_readyi(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 2, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_readyi(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 2, 1);
+}
+
+static inline void enable_hs_readyo(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 3, 1, 1);
+	return;
+}
+
+static inline unsigned char get_hs_readyo(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 3, 1);
+}
+
+static inline unsigned char get_hs_empty(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 12, 1);
+}
+
+static inline void clear_hs_intspr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 13, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intspr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 13, 1);
+}
+
+static inline void clear_hs_intspdd(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 14, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intspdd(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 14, 1);
+}
+
+static inline void clear_hs_intready(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 16, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intready(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 16, 1);
+}
+
+static inline void clear_hs_intping(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 17, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intping(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 17, 1);
+}
+
+static inline void clear_hs_intdend(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 19, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intdend(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 19, 1);
+}
+
+static inline void clear_hs_intempty(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 20, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intempty(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 20, 1);
+}
+
+static inline void clear_hs_intstalled(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intstalled(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 21, 1);
+}
+
+static inline void clear_hs_intnack(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 22, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intnack(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 22, 1);
+}
+
+static inline void clear_hs_intclstall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 hseps_register[endpoint_channel],
+					 23, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intclstall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 23, 1);
+}
+
+static inline unsigned char get_hs_crtalt(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 24, 4);
+}
+
+static inline unsigned char get_hs_crtintf(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 hseps_register[endpoint_channel], 28, 4);
+}
+
+static inline void set_hs_mskachgif(void __iomem *base_address,
+	 unsigned char interface_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSALTC,
+					 interface_channel, 1, mask);
+	return;
+}
+
+	/* interface channel symbolic constant */
+#define INTERFACE0		0	/* interface 0 */
+#define INTERFACE1		1	/* interface 1 */
+
+static inline unsigned char get_hs_mskachgif(void __iomem *base_address,
+	 unsigned char interface_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSALTC, interface_channel, 1);
+}
+
+static inline void set_hs_testmodeif(void __iomem *base_address,
+	 unsigned char test)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSALTC, 16, 1, test);
+	return;
+}
+
+static inline void set_hs_testaltif(void __iomem *base_address,
+	 unsigned char alternate)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSALTC,
+					 24, 4, alternate);
+	return;
+}
+
+static inline void set_hs_testintf(void __iomem *base_address,
+	 unsigned char interface)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSALTC,
+					 28, 4, interface);
+	return;
+}
+
+static inline void clear_hs_intachgif(void __iomem *base_address,
+	 unsigned char interface_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_HSALTS,
+					 interface_channel, 1, 0);
+	return;
+}
+
+static inline unsigned char get_hs_intachgif(void __iomem *base_address,
+	 unsigned char interface_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_HSALTS, interface_channel, 1);
+}
+
+static inline unsigned char get_hs_curaif(void __iomem *base_address,
+	 unsigned char interface_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_HSALTS,
+						 16 + interface_channel * 2, 2);
+}
+
+static inline void set_hs_epinbuf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned long data)
+{
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 endpoint_channel].address_offset);
+		break;
+	case 3:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 2].address_offset);
+		break;
+	case 5:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 3].address_offset);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline void set_hs_epinbuf_byte(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char byte, unsigned char data)
+{
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 endpoint_channel].address_offset + byte);
+		break;
+	case 3:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 2].address_offset + byte);
+		break;
+	case 5:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 3].address_offset + byte);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned long get_hs_epinbuf_address(phys_addr_t base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned char ep_channel = 0;
+
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		ep_channel = endpoint_channel;
+		break;
+	case 3:
+		ep_channel = endpoint_channel - 1;
+		break;
+	case 5:
+		ep_channel = endpoint_channel - 2;
+		break;
+	default:
+		break;
+	}
+
+	return (unsigned long) (base_address +
+				 f_usb30_register[F_USB30_REG_HSEPIB0 +
+				 ep_channel].address_offset);
+}
+
+static inline unsigned long get_hs_epoutbuf(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return __raw_readl(base_address +
+				 f_usb30_register[F_USB30_REG_HSEPOB0 +
+				 (endpoint_channel / 2)].address_offset);
+}
+
+static inline unsigned long get_hs_epoutbuf_address(phys_addr_t base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned long) (base_address +
+				 f_usb30_register[F_USB30_REG_HSEPOB0 +
+				 (endpoint_channel / 2)].address_offset);
+}
+
+static inline void set_hs_makeupdata(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_MAKEUP_DATA,
+					 0, 32, 0x01200120);
+	return;
+}
+
+static inline void set_hs_epnum(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char number)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel,
+						 0, 4, number);
+	return;
+}
+
+static inline unsigned char get_hs_epnum(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel)
+{
+	return (unsigned char) get_f_usb30_makeup_register_bits(
+				base_address, F_USB30_REG_MAKEUP0 +
+				 endpoint_channel, alternate_channel, 0, 4);
+}
+
+static inline void set_hs_io(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char in)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel, 4, 1, in);
+	return;
+}
+
+static inline void set_hs_type(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char type)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel, 5, 2, type);
+	return;
+}
+
+/* transfer type symbolic constant */
+#define TYPE_UNUSED		0	/* unused */
+#define TYPE_CONTROL		0	/* control transfer */
+#define TYPE_ISOCHRONOUS	1	/* isochronous transfer */
+#define TYPE_BULK		2	/* bulk transfer */
+#define TYPE_INTERRUPT		3	/* interrupt transfer */
+
+static inline void set_hs_conf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char configure)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel,
+						 7, 4, configure);
+	return;
+}
+
+static inline void set_hs_intf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char interface)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel,
+						 11, 4, interface);
+	return;
+}
+
+static inline void set_hs_alt(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned char alternate)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel,
+						 15, 4, alternate);
+	return;
+}
+
+static inline void set_hs_size(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel,
+	 unsigned short size)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel,
+						 19, 11, size);
+	return;
+}
+
+static inline unsigned short get_hs_size(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel)
+{
+	return (unsigned short) get_f_usb30_makeup_register_bits(
+					base_address, F_USB30_REG_MAKEUP0 +
+					 endpoint_channel,
+					 alternate_channel, 19, 11);
+}
+
+static inline void set_hs_numtr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate_channel)
+{
+	set_f_usb30_makeup_register_bits(base_address,
+						 F_USB30_REG_MAKEUP0 +
+						 endpoint_channel,
+						 alternate_channel, 30, 2, 0);
+	return;
+}
+
+static inline void set_hsclkstp(void __iomem *base_address, unsigned char stop)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_CLKC, 0, 1, stop);
+	return;
+}
+
+static inline unsigned char get_hsclkstp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_CLKC, 0, 1);
+}
+
+static inline void set_ssclkstp(void __iomem *base_address, unsigned char stop)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_CLKC, 1, 1, stop);
+	return;
+}
+
+static inline unsigned char get_ssclkstp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_CLKC, 1, 1);
+}
+
+static inline void set_hsclkstpen(void __iomem *base_address,
+	 unsigned char stop)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_CLKCE, 0, 1, stop);
+	return;
+}
+
+static inline unsigned char get_hsclkstpen(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_CLKCE, 0, 1);
+}
+
+static inline void set_ssclkstpen(void __iomem *base_address,
+	 unsigned char stop)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_CLKCE, 1, 1, stop);
+	return;
+}
+
+static inline unsigned char get_ssclkstpen(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_CLKCE, 1, 1);
+}
+
+static inline void set_ss_softreset(void __iomem *base_address,
+	 unsigned char reset)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCPAC, 3, 1, reset);
+	return;
+}
+
+static inline void set_ss_configwren(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCPAC, 16, 1, enable);
+	return;
+}
+
+static inline void set_ss_ssovrden(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCPAC, 17, 1, enable);
+	return;
+}
+
+static inline void set_ss_ltactive(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 0, 1, 1);
+	return;
+}
+
+static inline void set_ss_selfpw(void __iomem *base_address,
+	 unsigned char self_power)
+{
+	set_f_usb30_register_bits(base_address,
+					F_USB30_REG_SSDVC, 4, 1, self_power);
+	return;
+}
+
+static inline void set_ss_disconnect(void __iomem *base_address,
+	 unsigned char dis_connect)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC,
+					 5, 1, dis_connect);
+	return;
+}
+
+static inline void set_ss_connect(void __iomem *base_address,
+	 unsigned char connect)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 6, 1, connect);
+	return;
+}
+
+static inline void set_ss_rxdetonce(void __iomem *base_address,
+	 unsigned char detection)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 7, 1, detection);
+	return;
+}
+
+static inline void set_ss_msksswrste(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 12, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksswrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 12, 1);
+}
+
+static inline void set_ss_msksswrstb(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 13, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksswrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 13, 1);
+}
+
+static inline void set_ss_msksshrste(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 14, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksshrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 14, 1);
+}
+
+static inline void set_ss_msksshrstb(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 15, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksshrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 15, 1);
+}
+
+static inline void set_ss_mskvdtest(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 20, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskvdtest(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 20, 1);
+}
+
+static inline void set_ss_mskssdisable(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskssdisable(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 21, 1);
+}
+
+static inline void set_ss_mskenterpoll(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 22, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskenterpoll(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 22, 1);
+}
+
+static inline void set_ss_mskpolltou0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 23, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskpolltou0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 23, 1);
+}
+
+static inline void set_ss_msksuspende(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 24, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksuspende(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 24, 1);
+}
+
+static inline void set_ss_msksuspendb(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 25, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksuspendb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 25, 1);
+}
+
+static inline void set_ss_msksetintf(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 26, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksetintf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 26, 1);
+}
+
+static inline void set_ss_msksetup(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 27, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksetup(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 27, 1);
+}
+
+static inline void set_ss_msku2inactto(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 29, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msku2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 29, 1);
+}
+
+static inline void set_ss_msksetconf(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 30, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksetconf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 30, 1);
+}
+
+static inline void set_ss_mskfncsusp(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVC, 31, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskfncsusp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVC, 31, 1);
+}
+
+static inline unsigned char get_ss_u0up(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 0, 1);
+}
+
+static inline unsigned char get_ss_ltstate(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 4, 1);
+}
+
+static inline unsigned char get_ss_suspend(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 8, 1);
+}
+
+static inline unsigned char get_ss_sswrst(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 9, 1);
+}
+
+static inline unsigned char get_ss_sshrst(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 10, 1);
+}
+
+static inline unsigned char get_ss_ssdiserr(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 11, 1);
+}
+
+static inline unsigned char get_ss_intsswrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 12, 1);
+}
+
+static inline void clear_ss_intsswrste(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 12, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsswrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 13, 1);
+}
+
+static inline void clear_ss_intsswrstb(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 13, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsshrste(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 14, 1);
+}
+
+static inline void clear_ss_intsshrste(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 14, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsshrstb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 15, 1);
+}
+
+static inline void clear_ss_intsshrstb(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+		 F_USB30_REG_SSDVS, 15, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_conf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 16, 4);
+}
+
+static inline unsigned char get_ss_intvdtest(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 20, 1);
+}
+
+static inline void clear_ss_intvdtest(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 20, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intssdisable(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 21, 1);
+}
+
+static inline void clear_ss_intssdisable(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intenterpoll(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 22, 1);
+}
+
+static inline void clear_ss_intenterpoll(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 22, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intpolltou0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 23, 1);
+}
+
+static inline void clear_ss_intpolltou0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 23, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsuspende(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 24, 1);
+}
+
+static inline void clear_ss_intsuspende(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 24, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsuspendb(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 25, 1);
+}
+
+static inline void clear_ss_intsuspendb(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 25, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsetintf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 26, 1);
+}
+
+static inline void clear_ss_intsetintf(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 26, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsetup(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 27, 1);
+}
+
+static inline void clear_ss_intsetup(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 27, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intu2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 29, 1);
+}
+
+static inline void clear_ss_intu2inactto(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 29, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsetconf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 30, 1);
+}
+
+static inline void clear_ss_intsetconf(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 30, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intfncsusp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDVS, 31, 1);
+}
+
+static inline void clear_ss_intfncsusp(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDVS, 31, 1, 0);
+	return;
+}
+
+static inline void set_ss_mskep(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSIRQC,
+					 endpoint_channel, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskep(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_SSIRQC, endpoint_channel, 1);
+}
+
+static inline void set_ss_mskdev(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSIRQC, 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskdev(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSIRQC, 16, 1);
+}
+
+static inline unsigned char get_ss_intep(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 F_USB30_REG_SSIRQS, endpoint_channel, 1);
+}
+
+static inline unsigned char get_ss_intdev(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSIRQS, 16, 1);
+}
+
+static inline void set_ss_mskdmareq(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPDC,
+						 endpoint_channel, 1, mask);
+	return;
+}
+
+static inline void set_ss_dmamode(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usage)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPDC,
+						 endpoint_channel + 16,
+						 1, usage);
+	return;
+}
+
+static inline unsigned char get_ss_dmamode(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPDC,
+						 endpoint_channel + 16, 1);
+}
+
+static inline unsigned char get_ss_dmareq(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+							 F_USB30_REG_SSEPDS,
+							 endpoint_channel, 1);
+}
+
+static inline void set_ss_tcsel(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char usb)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPTCSEL,
+					 endpoint_channel, 1, usb);
+	return;
+}
+
+static inline void set_ss_tcnt(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned long count)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPTC1 +
+					 endpoint_channel - 1, 0, 32, count);
+	return;
+}
+
+static inline unsigned long get_ss_tcnt(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPTC1 +
+						 endpoint_channel - 1, 0, 32);
+}
+
+static inline unsigned char get_ss_sizerd0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ0I, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ0I, 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ0O, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ0O, 16, 11);
+}
+
+static inline unsigned long get_ss_sizerdwr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned long) get_f_usb30_register_bits(base_address,
+					 ssepsz_register[endpoint_channel - 1],
+					 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd1i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ1I, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr1i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ1I, 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd2o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ2O, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr2o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ2O, 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd3i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ3I, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr3i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ3I, 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd4o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ4O, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr4o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ4O, 16, 11);
+}
+
+static inline unsigned char get_ss_sizerd5i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ5I, 0, 11);
+}
+
+static inline unsigned char get_ss_sizewr5i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPSZ5I, 16, 11);
+}
+
+static inline void set_ss_bulktest(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCUSC, 0, 1, 1);
+	return;
+}
+
+static inline unsigned long get_ss_bulktest(void __iomem *base_address)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCUSC, 0, 1);
+}
+
+static inline void set_ss_setadd(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCUSC, 16, 1, 1);
+	return;
+}
+
+static inline unsigned long get_ss_setadd(void __iomem *base_address)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCUSC, 16, 1);
+}
+
+static inline void set_ss_setconfig(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCUSC, 17, 1, 1);
+	return;
+}
+
+static inline unsigned long get_ss_setconfig(void __iomem *base_address)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCUSC, 17, 1);
+}
+
+static inline void set_ss_tconf(void __iomem *base_address,
+	 unsigned char config)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCUSC, 20, 3, config);
+	return;
+}
+
+static inline void set_ss_tadd(void __iomem *base_address,
+	 unsigned char address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCUSC, 24, 7, address);
+	return;
+}
+
+static inline void set_ss_dlllpbk(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPLPBK, 0, 1, 1);
+	return;
+}
+
+static inline void set_ss_numaintf(void __iomem *base_address,
+	 unsigned char interface, unsigned char usage_number)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSINTFC,
+					 interface * 4, 3, usage_number);
+	return;
+}
+
+static inline void set_ss_firstintf(void __iomem *base_address,
+	 unsigned char interface, unsigned char firstintf)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSINTFC,
+					 interface * 4 + 3, 1, firstintf);
+	return;
+}
+
+static inline unsigned long get_ss_firstintf(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSINTFC,
+						 interface * 4 + 3, 1);
+}
+
+static inline void set_ss_numintf(void __iomem *base_address,
+	unsigned char usage_number)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSINTFC,
+					 16, 3, usage_number);
+	return;
+}
+
+static inline unsigned long get_ss_enfncwake(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSINTFC,
+						 interface + 24, 1);
+}
+
+static inline void set_ss_reqfncwake(void __iomem *base_address,
+	 unsigned char interface, unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSINTFC,
+					 interface + 28, 1, enable);
+	return;
+}
+
+static inline unsigned long get_ss_alt(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSINTFS,
+						 interface * 4, 3);
+}
+
+static inline unsigned long get_ss_achg(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSINTFS,
+						 interface * 4 + 3, 1);
+}
+
+static inline void clear_ss_achg(void __iomem *base_address,
+	 unsigned char interface)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSINTFS,
+					 interface * 4 + 3, 1, 0);
+	return;
+}
+
+static inline unsigned long get_ss_fncsusp(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSINTFS,
+						 interface + 16, 1);
+}
+
+static inline void set_ss_init0i(void __iomem *base_address)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 0, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 F_USB30_REG_SSEPC0, 0, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_ss_init0o(void __iomem *base_address)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 1, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 F_USB30_REG_SSEPC0, 1, 1))); counter--)
+		;
+	return;
+}
+
+static inline void set_ss_reqstall0(void __iomem *base_address,
+	 unsigned char stall0)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 2, 1, stall0);
+	return;
+}
+
+static inline unsigned char get_ss_reqstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 2, 1);
+}
+
+static inline void set_ss_rewdifo0(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 3, 1, enable);
+	return;
+}
+
+static inline void set_ss_nrdyresp0(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 9, 1, enable);
+	return;
+}
+
+static inline void set_ss_mskready0i(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 16, 1);
+}
+
+static inline void set_ss_mskready0o(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 17, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 17, 1);
+}
+
+static inline void set_ss_mskpktpnd0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 18, 1, mask);
+	return;
+}
+
+static inline void set_ss_mskstalled0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskstalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 21, 1);
+}
+
+static inline void set_ss_msknrdy0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 22, 1, mask);
+	return;
+}
+
+static inline void set_ss_mskclstall0(void __iomem *base_address,
+	 unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPC0, 23, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskclstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 23, 1);
+}
+
+static inline unsigned char get_ss_appactiv0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC0, 24, 1);
+}
+
+static inline void set_ss_ctldone0(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 0, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_stalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 2, 1);
+}
+
+static inline void enable_ss_ready0i(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 3, 1, 1);
+	return;
+}
+
+static inline unsigned char get_ss_ready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 3, 1);
+}
+
+static inline void enable_ss_ready0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 4, 1, 1);
+	return;
+}
+
+static inline unsigned char get_ss_ready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 4, 1);
+}
+
+static inline unsigned char get_ss_pktpnd0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 8, 1);
+}
+
+static inline void clear_ss_intready0i(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 16, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intready0i(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 16, 1);
+}
+
+static inline void clear_ss_intready0o(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 17, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intready0o(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 17, 1);
+}
+
+static inline void clear_ss_intpktpnd0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 18, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intpktpnd0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 18, 1);
+}
+
+static inline void clear_ss_intstalled0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intstalled0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 21, 1);
+}
+
+static inline void clear_ss_intnrdy0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 22, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intnrdy0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 22, 1);
+}
+
+static inline void clear_ss_intclstall0(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS0, 23, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intclstall0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS0, 23, 1);
+}
+
+static inline void set_ss_init(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned long counter;
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 0, 1, 1);
+	for (counter = 0xffff;
+		 ((counter) && (get_f_usb30_register_bits(base_address,
+		 ssepc_register[endpoint_channel], 0, 1))); counter--)
+		;
+	return;
+}
+
+static inline unsigned char get_ss_init(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 0, 1);
+}
+
+static inline void set_ss_reqstall(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char stall)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 1, 1, stall);
+	return;
+}
+
+static inline unsigned char get_ss_reqstall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 1, 1);
+}
+
+static inline void set_ss_rewdifo(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char rew)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 3, 1, rew);
+	return;
+}
+
+static inline unsigned char get_ss_rewdifo(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 3, 1);
+}
+
+static inline void set_ss_inistall(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char stall)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 4, 1, stall);
+	return;
+}
+
+static inline unsigned char get_ss_inistall(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 4, 1);
+}
+
+static inline void set_ss_stalldis(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char stall)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 6, 1, stall);
+	return;
+}
+
+static inline unsigned char get_ss_stalldis(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 6, 1);
+}
+
+static inline void set_ss_autostreamclr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 7, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_autostreamclr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 7, 1);
+}
+
+static inline void set_ss_enstream(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 8, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_enstream(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 8, 1);
+}
+
+static inline void set_ss_nrdyresp(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 9, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_nrdyresp(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 9, 1);
+}
+
+static inline void set_ss_mskclstream(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 15, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskclstream(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 15, 1);
+}
+
+static inline void set_ss_mskready(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskready(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 16, 1);
+}
+
+static inline void set_ss_mskpktpnd(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 17, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskpktpnd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 17, 1);
+}
+
+static inline void set_ss_msksdend(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 18, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msksdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 1;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 18, 1);
+}
+
+static inline void set_ss_mskdend(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 19, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 1;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 19, 1);
+}
+
+static inline void set_ss_mskempty(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 20, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskempty(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 1;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 20, 1);
+}
+
+static inline void set_ss_mskstalled(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskstalled(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 21, 1);
+}
+
+static inline void set_ss_msknrdy(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 22, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_msknrdy(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 22, 1);
+}
+
+static inline void set_ss_mskclstall(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	set_f_usb30_register_bits(base_address,
+					 ssepc_register[endpoint_channel],
+					 23, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskclstall(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 23, 1);
+}
+
+static inline unsigned char get_ss_appactiv(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 ssepc_register[endpoint_channel], 24, 1);
+}
+
+static inline void set_ss_enspr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char spr)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2,
+						 10, 1, spr);
+	return;
+}
+
+static inline void set_ss_enspd(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char spd)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2,
+						 11, 1, spd);
+	return;
+}
+
+static inline void set_ss_mskspr(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2,
+						 13, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskspr(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT2)
+		return 1;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2, 13, 1);
+}
+
+static inline void set_ss_mskspd(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mask)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2,
+						 14, 1, mask);
+	return;
+}
+
+static inline unsigned char get_ss_mskspd(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT2)
+		return 1;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2, 14, 1);
+}
+
+static inline void set_ss_spunit(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char length)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2,
+						 28, 3, length);
+	return;
+}
+
+static inline unsigned char get_ss_spunit(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPC2, 28, 3);
+}
+
+static inline unsigned char get_ss_stalled(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 1, 1);
+}
+
+static inline void enable_ss_ready(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	switch (endpoint_channel) {
+	case 1:
+	case 3:
+	case 5:
+		set_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 2, 1, 1);
+		break;
+	case 2:
+	case 4:
+		set_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 3, 1, 1);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned char get_ss_ready(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned char flag = 0;
+	switch (endpoint_channel) {
+	case 1:
+	case 3:
+	case 5:
+		flag = (unsigned char) get_f_usb30_register_bits(
+					base_address, sseps_register[
+					endpoint_channel], 2, 1);
+		break;
+	case 2:
+	case 4:
+		flag = (unsigned char) get_f_usb30_register_bits(
+					base_address, sseps_register[
+					endpoint_channel], 3, 1);
+		break;
+	default:
+		break;
+	}
+
+	return flag;
+}
+
+static inline void set_ss_streamactive(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char active)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 7, 1, active);
+	return;
+}
+
+static inline unsigned char get_ss_streamactive(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 7, 1);
+}
+
+static inline unsigned char get_ss_pktpnd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 8, 1);
+}
+
+static inline unsigned char get_ss_empty(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 12, 1);
+}
+
+static inline unsigned char get_ss_intspr(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS2, 13, 1);
+}
+
+static inline void clear_ss_intspr(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS2, 13, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intspd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPS2, 14, 1);
+}
+
+static inline void clear_ss_intspd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEPS2, 14, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intclstream(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned char flag = 0;
+	switch (endpoint_channel) {
+	case 1:
+	case 2:
+	case 5:
+		flag = (unsigned char) get_f_usb30_register_bits(
+					base_address, sseps_register[
+					endpoint_channel], 15, 1);
+		break;
+	default:
+		break;
+	}
+
+	return flag;
+}
+
+static inline void clear_ss_intclstream(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	switch (endpoint_channel) {
+	case 1:
+	case 2:
+	case 5:
+		set_f_usb30_register_bits(base_address,
+						 sseps_register[
+						 endpoint_channel], 15, 1, 0);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned char get_ss_intready(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 16, 1);
+}
+
+static inline void clear_ss_intready(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 16, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intpktpnd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 17, 1);
+}
+
+static inline void clear_ss_intpktpnd(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 17, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intsdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 18, 1);
+}
+
+static inline void clear_ss_intsdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 sseps_register[
+						 endpoint_channel], 18, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 19, 1);
+}
+
+static inline void clear_ss_intdend(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 sseps_register[
+						 endpoint_channel], 19, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intempty(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel != ENDPOINT1 && endpoint_channel != ENDPOINT2)
+		return 0;
+
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 20, 1);
+}
+
+static inline void clear_ss_intempty(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	if (endpoint_channel == ENDPOINT1 || endpoint_channel == ENDPOINT2)
+		set_f_usb30_register_bits(base_address,
+						 sseps_register[
+						 endpoint_channel], 20, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intstalled(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 21, 1);
+}
+
+static inline void clear_ss_intstalled(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intnrdy(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 22, 1);
+}
+
+static inline void clear_ss_intnrdy(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 22, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_intclstall(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+				 sseps_register[endpoint_channel], 23, 1);
+}
+
+static inline void clear_ss_intclstall(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	set_f_usb30_register_bits(base_address,
+					 sseps_register[endpoint_channel],
+					 23, 1, 0);
+	return;
+}
+
+static inline void set_ss_epinbuf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned long data)
+{
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 endpoint_channel].address_offset);
+		break;
+	case 3:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 2].address_offset);
+		break;
+	case 5:
+		__raw_writel(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 3].address_offset);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline void set_ss_epinbuf_byte(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char byte, unsigned char data)
+{
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 endpoint_channel].address_offset + byte);
+		break;
+	case 3:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 2].address_offset + byte);
+		break;
+	case 5:
+		__raw_writeb(data, base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 3].address_offset + byte);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned long get_ss_epinbuf_address(phys_addr_t base_address,
+	 unsigned char endpoint_channel)
+{
+	unsigned char ep_channel = 0;
+
+	switch (endpoint_channel) {
+	case 0:
+	case 1:
+		ep_channel = endpoint_channel;
+		break;
+	case 3:
+		ep_channel = endpoint_channel - 1;
+		break;
+	case 5:
+		ep_channel = endpoint_channel - 2;
+		break;
+	default:
+		break;
+	}
+
+	return (unsigned long) (base_address +
+				 f_usb30_register[F_USB30_REG_SSEPIB0 +
+				 ep_channel].address_offset);
+}
+
+static inline unsigned long get_ss_epoutbuf(void __iomem *base_address,
+	 unsigned char endpoint_channel)
+{
+	return __raw_readl(base_address +
+				 f_usb30_register[F_USB30_REG_SSEPOB0 +
+				 (endpoint_channel / 2)].address_offset);
+}
+
+static inline unsigned long get_ss_epoutbuf_address(phys_addr_t base_address,
+	 unsigned char endpoint_channel)
+{
+	return (unsigned long) (base_address +
+				 f_usb30_register[F_USB30_REG_SSEPOB0 +
+				 (endpoint_channel / 2)].address_offset);
+}
+
+static inline void set_ss_epnum(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char number)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCFGEP1 +
+					 endpoint_channel - 1, 0, 4, number);
+	return;
+}
+
+static inline unsigned char get_ss_epnum(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCFGEP1 +
+						 endpoint_channel - 1, 0, 4);
+}
+
+static inline void set_ss_epconf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char config)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSCFGEP1 +
+					 endpoint_channel - 1, 7, 4, config);
+	return;
+}
+
+static inline unsigned char get_ss_epconf(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCFGEP1 +
+						 endpoint_channel - 1, 7, 4);
+}
+
+static inline void set_ss_intf(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char interface)
+{
+	set_f_usb30_register_bits(base_address, F_USB30_REG_SSCFGEP1 +
+				 endpoint_channel - 1, 11 + interface, 1, 1);
+	return;
+}
+
+static inline unsigned char get_ss_intf(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCFGEP1 +
+						 endpoint_channel - 1, 11, 4);
+}
+
+static inline void set_ss_altmap(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char alternate)
+{
+	set_f_usb30_register_bits(base_address, F_USB30_REG_SSCFGEP1 +
+					endpoint_channel - 1, 15, 4, alternate);
+	return;
+}
+
+static inline unsigned char get_ss_altmap(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSCFGEP1 +
+						 endpoint_channel - 1, 15, 4);
+}
+
+static inline void set_ss_capltm(void __iomem *base_address,
+	 unsigned char ltm)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEXCFGDV, 0, 1, ltm);
+	return;
+}
+
+static inline unsigned long get_ss_capltm(void __iomem *base_address)
+{
+	return get_f_usb30_register_bits(base_address,
+						F_USB30_REG_SSEXCFGDV, 0, 1);
+}
+
+static inline void set_ss_capfncwake(void __iomem *base_address,
+	 unsigned char interface, unsigned char wakeup)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSEXCFGDV,
+					 interface + 3, 1, wakeup);
+	return;
+}
+
+static inline unsigned long get_ss_capfncwake(void __iomem *base_address,
+	 unsigned char interface)
+{
+	return get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGDV,
+						 interface + 3, 1);
+}
+
+static inline void set_ss_maxburst(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char burst)
+{
+	switch (endpoint_channel) {
+	case 1:
+	case 2:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP1 +
+						 endpoint_channel - 1,
+						 0, 4, burst);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned char get_ss_maxburst(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned char burst;
+	switch (endpoint_channel) {
+	case 1:
+	case 2:
+		burst = (unsigned char) get_f_usb30_register_bits(
+					base_address, F_USB30_REG_SSEXCFGEP1
+					 + endpoint_channel - 1, 0, 4);
+	default:
+		break;
+	}
+
+	return burst;
+}
+
+static inline void set_ss_diseob(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char disable)
+{
+	switch (endpoint_channel) {
+	case 1:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP1,
+						 8, 1, disable);
+		 break;
+	case 5:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP5,
+						 8, 1, disable);
+		 break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned char get_ss_diseob(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned char disable;
+	switch (endpoint_channel) {
+	case 1:
+		disable = (unsigned char) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEXCFGEP1,
+						 8, 1);
+		break;
+	case 5:
+		disable = (unsigned char) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEXCFGEP5,
+						 8, 1);
+		break;
+	default:
+		break;
+	}
+	return disable;
+}
+
+static inline void set_ss_numpmode(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned char mode)
+{
+	switch (endpoint_channel) {
+	case 1:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP1,
+						 16, 2, mode);
+		break;
+	case 2:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP2,
+						 16, 2, mode);
+		break;
+	case 4:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEXCFGEP4,
+						 16, 2, mode);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned char get_ss_numpmode(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned char mode;
+	switch (endpoint_channel) {
+	case 1:
+		mode = (unsigned char) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEXCFGEP1,
+						 16, 2);
+		break;
+	case 2:
+		mode = (unsigned char) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEXCFGEP2,
+						 16, 2);
+		break;
+	case 4:
+		mode = (unsigned char) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEXCFGEP4,
+						 16, 2);
+		break;
+	default:
+		break;
+	}
+
+	return mode;
+}
+
+static inline void set_ss_streamid(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned int id)
+{
+	switch (endpoint_channel) {
+	case 1:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSBSC1,
+						 0, 16, id);
+		break;
+	case 2:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSBSC2,
+						 0, 16, id);
+		break;
+	case 5:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSBSC5,
+						 0, 16, id);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned int get_ss_streamid(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned int id;
+	switch (endpoint_channel) {
+	case 1:
+		id = (unsigned int) get_f_usb30_register_bits(
+					base_address, F_USB30_REG_SSBSC1,
+					 0, 16);
+		break;
+	case 2:
+		id = (unsigned int) get_f_usb30_register_bits(
+					base_address, F_USB30_REG_SSBSC2,
+					 0, 16);
+		break;
+	case 5:
+		id = (unsigned int) get_f_usb30_register_bits(
+					base_address, F_USB30_REG_SSBSC5,
+					 0, 16);
+		break;
+	default:
+		break;
+	}
+
+	return id;
+}
+
+static inline void set_ss_stcnt(void __iomem *base_address,
+	 unsigned char endpoint_channel, unsigned long count)
+{
+	switch (endpoint_channel) {
+	case 1:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPSTC1,
+						 0, 32, count);
+		break;
+	case 2:
+		set_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSEPSTC2,
+						 0, 32, count);
+		break;
+	default:
+		break;
+	}
+
+	return;
+}
+
+static inline unsigned long get_ss_stcnt(void __iomem *base_address,
+	unsigned char endpoint_channel)
+{
+	unsigned long count;
+	switch (endpoint_channel) {
+	case 1:
+		count = (unsigned long) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEPSTC1,
+						 0, 32);
+		break;
+	case 2:
+		count = (unsigned long) get_f_usb30_register_bits(
+						base_address,
+						 F_USB30_REG_SSEPSTC2,
+						 0, 32);
+		break;
+	default:
+		break;
+	}
+
+	return count;
+}
+
+static inline void clear_ss_cldllerrcnt(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSERCC, 0, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_cldllerrcnt(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSERCC, 0, 1);
+}
+
+static inline void clear_ss_clphyerrcnt(void __iomem *base_address)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSERCC, 1, 1, 0);
+	return;
+}
+
+static inline unsigned char get_ss_clphyerrcnt(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSERCC, 1, 1);
+}
+
+static inline unsigned char get_ss_derc_hbad(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT1, 0, 8);
+}
+
+static inline unsigned char get_ss_derc_hrcv(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT1, 8, 8);
+}
+
+static inline unsigned char get_ss_derc_drcv(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT1, 16, 8);
+}
+
+static inline unsigned char get_ss_derc_ssia(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT1, 24, 8);
+}
+
+static inline unsigned char get_ss_derc_lgod(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT2, 0, 8);
+}
+
+static inline unsigned char get_ss_derc_crdt(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT2, 8, 8);
+}
+
+static inline unsigned char get_ss_derc_fotr(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT2, 16, 8);
+}
+
+static inline unsigned char get_ss_derc_frer(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT2, 24, 8);
+}
+
+static inline unsigned char get_ss_derc_pdhp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT3, 0, 8);
+}
+
+static inline unsigned char get_ss_derc_cdhp(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT3, 8, 8);
+}
+
+static inline unsigned char get_ss_derc_pmlc(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT3, 16, 8);
+}
+
+static inline unsigned char get_ss_derc_pent(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT3, 24, 8);
+}
+
+static inline unsigned char get_ss_derc_ldtm(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSDlECNT4, 0, 8);
+}
+
+static inline unsigned int get_ss_perc_8b10ber(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSPlECNT, 0, 16);
+}
+
+static inline unsigned char get_ss_perc_ltsmto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSPlECNT, 16, 8);
+}
+
+static inline unsigned char get_ss_perc_elbufof(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(
+				base_address, F_USB30_REG_SSPlECNT, 24, 8);
+}
+
+static inline void set_ss_enu1(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSLNC, 0, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_enu1(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSLNC, 0, 1);
+}
+
+static inline void set_ss_enu2(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSLNC, 1, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_enu2(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSLNC, 1, 1);
+}
+
+static inline void set_ss_frclpma(void __iomem *base_address,
+	 unsigned char assert)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSLNC, 2, 1, assert);
+	return;
+}
+
+static inline unsigned char get_ss_frclpma(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSLNC, 2, 1);
+}
+
+static inline void set_ss_enltm(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSLNC, 3, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_enltm(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSLNC, 3, 1);
+}
+
+static inline void set_ss_u2inactto(void __iomem *base_address,
+	 unsigned char timeout)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSLNC, 8, 8, timeout);
+	return;
+}
+
+static inline unsigned char get_ss_u2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSLNC, 8, 8);
+}
+
+static inline void set_ss_reqpkt(void __iomem *base_address,
+	 unsigned char request)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 0, 1, request);
+	return;
+}
+
+static inline unsigned char get_ss_reqpkt(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 0, 1);
+}
+
+static inline void set_ss_pktcode(void __iomem *base_address,
+	 unsigned char code)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 1, 2, code);
+	return;
+}
+
+static inline unsigned char get_ss_pktcode(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 1, 2);
+}
+
+static inline void set_ss_vsdtest(void __iomem *base_address,
+	 unsigned char data)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 8, 8, data);
+	return;
+}
+
+static inline unsigned char get_ss_vsdtest(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 8, 8);
+}
+
+static inline void set_ss_tnump(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 16, 5, value);
+	return;
+}
+
+static inline unsigned char get_ss_tnump(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 16, 5);
+}
+
+static inline void set_ss_tseqnum(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 24, 5, value);
+	return;
+}
+
+static inline unsigned char get_ss_tseqnum(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 24, 5);
+}
+
+static inline void set_ss_tretry(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC1, 29, 1, value);
+	return;
+}
+
+static inline unsigned char get_ss_tretry(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC1, 29, 1);
+}
+
+static inline void set_ss_beltdef_value(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC2, 0, 10, value);
+	return;
+}
+
+static inline unsigned int get_ss_beltdef_value(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC2, 0, 10);
+}
+
+static inline void set_ss_beltdef_scale(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC2, 10, 2, value);
+	return;
+}
+
+static inline unsigned char get_ss_beltdef_scale(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC2, 10, 2);
+}
+
+static inline void set_ss_beltmin_value(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC2, 16, 10, value);
+	return;
+}
+
+static inline unsigned int get_ss_beltmin_value(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC2, 16, 10);
+}
+
+static inline void set_ss_beltmin_scale(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSTRC2, 26, 2, value);
+	return;
+}
+
+static inline unsigned char get_ss_beltmin_scale(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRC2, 26, 2);
+}
+
+static inline unsigned char get_ss_pcfgto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRS1, 8, 1);
+}
+
+static inline unsigned char get_ss_rcvvsdtest(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRS1, 24, 8);
+}
+
+static inline unsigned char get_ss_tnxtseq(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRS2, 0, 5);
+}
+
+static inline unsigned char get_ss_tackdseq(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRS2, 8, 5);
+}
+
+static inline unsigned char get_ss_tlimseq(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSTRS2, 16, 5);
+}
+
+static inline void set_ss_pdhp_tov(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC1, 0, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_pdhp_tov(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC1, 0, 12);
+}
+
+static inline void set_ss_cdhp_tov(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC1, 16, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_cdhp_tov(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC1, 16, 12);
+}
+
+static inline void set_ss_pmlc_tov(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC2, 0, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_pmlc_tov(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC2, 0, 12);
+}
+
+static inline void set_ss_pmet_tov(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC2, 16, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_pmet_tov(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC2, 16, 12);
+}
+
+static inline void set_ss_ldtm_tov(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC3, 0, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_ldtm_tov(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC3, 0, 12);
+}
+
+static inline void set_ss_ldtm_cntv(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC3, 16, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_ldtm_cntv(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC3, 16, 8);
+}
+
+static inline void set_ss_luptim(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC4, 0, 12, value);
+	return;
+}
+
+static inline unsigned int get_ss_luptim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC4, 0, 12);
+}
+
+static inline void set_ss_skptim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSDlC4, 16, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_ldtm_skptim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSDlC4, 16, 8);
+}
+
+static inline void set_ss_cnt_u1inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC1, 0, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_cnt_u1inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC1, 0, 8);
+}
+
+static inline void set_ss_cnt_u2inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC1, 8, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_cnt_u2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC1, 8, 8);
+}
+
+static inline void set_ss_blki_u1inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC1, 16, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_blki_u1inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC1, 16, 8);
+}
+
+static inline void set_ss_blki_u2inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC1, 24, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_blki_u2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC1, 24, 8);
+}
+
+static inline void set_ss_blko_u1inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC2, 0, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_blko_u1inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC2, 0, 8);
+}
+
+static inline void set_ss_blko_u2inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC2, 8, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_blko_u2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC2, 8, 8);
+}
+
+static inline void set_ss_inti_u1inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC2, 16, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_inti_u1inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC2, 16, 8);
+}
+
+static inline void set_ss_inti_u2inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC2, 24, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_inti_u2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC2, 24, 8);
+}
+
+static inline void set_ss_u1tou2inactto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC3, 0, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_u1tou2inactto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC3, 0, 8);
+}
+
+static inline void set_ss_exitto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC3, 16, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_exitto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC3, 16, 8);
+}
+
+static inline void set_ss_suspto(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC3, 24, 8, value);
+	return;
+}
+
+static inline unsigned char get_ss_suspto(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC3, 24, 8);
+}
+
+static inline void set_ss_lt_to(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC4, 0, 16, value);
+	return;
+}
+
+static inline unsigned int get_ss_lt_to(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC4, 0, 16);
+}
+
+static inline void set_ss_preu2_to(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC4, 16, 8, value);
+	return;
+}
+
+static inline unsigned int get_ss_preu2_to(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC4, 16, 8);
+}
+
+static inline void set_ss_rjct_rcvu1(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC4, 28, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_rjct_rcvu1(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC4, 28, 1);
+}
+
+static inline void set_ss_rjct_rcvu2(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC4, 29, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_rjct_rcvu2(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC4, 29, 1);
+}
+
+static inline void set_ss_tim_256us(void __iomem *base_address,
+	 unsigned int value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPMC5, 0, 10, value);
+	return;
+}
+
+static inline unsigned int get_ss_tim_256us(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPMC5, 0, 10);
+}
+
+static inline void set_ss_vddata_l(void __iomem *base_address,
+	 unsigned long value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSVDTC1, 0, 32, value);
+	return;
+}
+
+static inline unsigned long get_ss_vddata_l(void __iomem *base_address)
+{
+	return (unsigned long) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSVDTC1, 0, 32);
+}
+
+static inline void set_ss_vddata_h(void __iomem *base_address,
+	 unsigned long value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSVDTC2, 0, 32, value);
+	return;
+}
+
+static inline unsigned long get_ss_vddata_h(void __iomem *base_address)
+{
+	return (unsigned long) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSVDTC2, 0, 32);
+}
+
+static inline unsigned long get_ss_rcvvddata_l(void __iomem *base_address)
+{
+	return (unsigned long) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSVDTS1, 0, 32);
+}
+
+static inline unsigned long get_ss_rcvvddata_h(void __iomem *base_address)
+{
+	return (unsigned long) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSVDTS2, 0, 32);
+}
+
+/* PLL divided mode select */
+#define SS_PLL_CLOCK_20MHZ		0	/* 20Mhz */
+#define SS_PLL_CLOCK_40MHZ		1	/* 40Mhz */
+#define SS_PLL_CLOCK_25MHZ		8	/* 25Mhz */
+
+static inline void set_ss_plfvdcnt(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC1, 0, 4, value);
+	return;
+}
+
+static inline unsigned char get_ss_plfvdcnt(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC1, 0, 4);
+}
+
+static inline void set_ss_sscon(void __iomem *base_address,
+	 unsigned char enable)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC1, 8, 1, enable);
+	return;
+}
+
+static inline unsigned char get_ss_sscon(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC1, 8, 1);
+}
+
+static inline void set_ss_ssccen(void __iomem *base_address,
+	 unsigned char spread)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC1, 9, 1, spread);
+	return;
+}
+
+static inline unsigned char get_ss_ssccen(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC1, 9, 1);
+}
+
+static inline void set_ss_txmargin(void __iomem *base_address,
+	 unsigned char level)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC1, 16, 3, level);
+	return;
+}
+
+static inline unsigned char get_ss_txmargin(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC1, 16, 3);
+}
+
+static inline void set_ss_txswing(void __iomem *base_address,
+	 unsigned char level)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC1, 24, 1, level);
+	return;
+}
+
+static inline unsigned char get_ss_txswing(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC1, 24, 1);
+}
+
+static inline void set_ss_pll_locktim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 0, 4, time);
+	return;
+}
+
+static inline unsigned char get_ss_pll_locktim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 0, 4);
+}
+
+static inline void set_ss_cdr_locktim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 4, 4, time);
+	return;
+}
+
+static inline unsigned char get_ss_cdr_locktim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 4, 4);
+}
+
+static inline void set_ss_pll_rstentim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 8, 4, time);
+	return;
+}
+
+static inline unsigned char get_ss_pll_rstentim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 8, 4);
+}
+
+static inline void set_ss_polen_in(void __iomem *base_address,
+	 unsigned char assert)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 12, 1, assert);
+	return;
+}
+
+static inline unsigned char get_ss_polen_in(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 12, 1);
+}
+
+static inline void set_ss_rxden_in(void __iomem *base_address,
+	 unsigned char assert)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 13, 1, assert);
+	return;
+}
+
+static inline unsigned char get_ss_rxden_in(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 13, 1);
+}
+
+static inline void set_ss_sslfsel(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 14, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_sslfsel(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 14, 1);
+}
+
+static inline void set_ss_msklpbslf(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 15, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_msklpbslf(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 15, 1);
+}
+
+static inline void set_ss_pollfp_p1(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 20, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_pollfp_p1(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 20, 1);
+}
+
+static inline void set_ss_comp4_p1(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 21, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_comp4_p1(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 21, 1);
+}
+
+static inline void set_ss_txei_pls(void __iomem *base_address,
+	 unsigned char assert)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 22, 1, assert);
+	return;
+}
+
+static inline unsigned char get_ss_txei_pls(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 22, 1);
+}
+
+static inline void set_ss_prsu12_p0(void __iomem *base_address,
+	 unsigned char assert)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 23, 1, assert);
+	return;
+}
+
+static inline unsigned char get_ss_prsu12_p0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 23, 1);
+}
+
+static inline void set_ss_tts_func(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 24, 4, value);
+	return;
+}
+
+static inline unsigned char get_ss_tts_func(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 24, 4);
+}
+
+static inline void set_ss_dir_lpb_ext(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC2, 28, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_dir_lpb_ext(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC2, 28, 1);
+}
+
+static inline void set_ss_tim_tseqtr(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC3, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_tseqtr(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC3, 0, 10);
+}
+
+static inline void set_ss_txopcnt0(void __iomem *base_address,
+	 unsigned char code)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC3, 12, 4, code);
+	return;
+}
+
+static inline unsigned char get_ss_txopcnt0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC3, 12, 4);
+}
+
+static inline void set_ss_txodcnt0(void __iomem *base_address,
+	 unsigned char code)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC3, 24, 4, code);
+	return;
+}
+
+static inline unsigned char get_ss_txodcnt0(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC3, 24, 4);
+}
+
+static inline void set_ss_txdeemph(void __iomem *base_address,
+	 unsigned char sel)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC3, 31, 1, sel);
+	return;
+}
+
+static inline unsigned char get_ss_txdeemph(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC3, 31, 1);
+}
+
+static inline void set_ss_tim_2ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC4, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_2ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC4, 0, 10);
+}
+
+static inline void set_ss_tim_6ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC4, 16, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_6ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC4, 16, 10);
+}
+
+static inline void set_ss_tim_10ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC5, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_10ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC5, 0, 10);
+}
+
+static inline void set_ss_tim_12ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC5, 16, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_12ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC5, 16, 10);
+}
+
+static inline void set_ss_tim_100ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC6, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_100ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC6, 0, 10);
+}
+
+static inline void set_ss_tim_300ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC6, 16, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_300ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC6, 16, 10);
+}
+
+static inline void set_ss_tim_360ms(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC7, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_tim_360ms(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC7, 0, 10);
+}
+
+static inline void set_ss_ux_ent_tim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC7, 16, 5, value);
+	return;
+}
+
+static inline unsigned char get_ss_ux_ent_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC7, 16, 5);
+}
+
+static inline void set_ss_polf_rmin_tim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC8, 0, 4, value);
+	return;
+}
+
+static inline unsigned char get_ss_polf_rmin_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC8, 0, 4);
+}
+
+static inline void set_ss_polf_rmax_tim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC8, 8, 6, value);
+	return;
+}
+
+static inline unsigned char get_ss_polf_rmax_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC8, 8, 6);
+}
+
+static inline void set_ss_polf_bmin_tim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC8, 16, 3, value);
+	return;
+}
+
+static inline unsigned char get_ss_polf_bmin_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC8, 16, 3);
+}
+
+static inline void set_ss_polf_bmax_tim(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC8, 24, 5, value);
+	return;
+}
+
+static inline unsigned char get_ss_polf_bmax_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC8, 24, 5);
+}
+
+static inline void set_ss_u1lftr_tim(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC9, 0, 15, time);
+	return;
+}
+
+static inline unsigned int get_ss_u1lftr_tim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC9, 0, 15);
+}
+
+static inline void set_ss_tim_wrstdet(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC9, 16, 7, value);
+	return;
+}
+
+static inline unsigned char get_ss_tim_wrstdet(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC9, 16, 7);
+}
+
+static inline void set_ss_tim_lfpsdet(void __iomem *base_address,
+	 unsigned char value)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC9, 24, 4, value);
+	return;
+}
+
+static inline unsigned char get_ss_tim_lfpsdet(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC9, 24, 4);
+}
+
+static inline void set_ss_u3lftr_tim(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC10, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_u3lftr_tim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC10, 0, 10);
+}
+
+static inline void set_ss_u2lftr_tim(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC10, 16, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_u2lftr_tim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						F_USB30_REG_SSPlC10, 16, 10);
+}
+
+static inline void set_ss_rxeqtre_tim(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC11, 0, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_rxeqtre_tim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC11, 0, 10);
+}
+
+static inline void set_ss_rxeqtrs_tim(void __iomem *base_address,
+	 unsigned int time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC11, 16, 10, time);
+	return;
+}
+
+static inline unsigned int get_ss_rxeqtrs_tim(void __iomem *base_address)
+{
+	return (unsigned int) get_f_usb30_register_bits(base_address,
+						F_USB30_REG_SSPlC11, 16, 10);
+}
+
+static inline void set_ss_polfeitr_tim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC12, 0, 7, time);
+	return;
+}
+
+static inline unsigned char get_ss_polfeitr_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC12, 0, 7);
+}
+
+static inline void set_ss_polfbutr_tim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC12, 8, 5, time);
+	return;
+}
+
+static inline unsigned char get_ss_polfbutr_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC12, 8, 5);
+}
+
+static inline void set_ss_pnglf_min_tim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC12, 16, 7, time);
+	return;
+}
+
+static inline unsigned char get_ss_pnglf_min_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC12, 16, 7);
+}
+
+static inline void set_ss_pnglf_max_tim(void __iomem *base_address,
+	 unsigned char time)
+{
+	set_f_usb30_register_bits(base_address,
+					 F_USB30_REG_SSPlC12, 24, 7, time);
+	return;
+}
+
+static inline unsigned char get_ss_pnglf_max_tim(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlC12, 24, 7);
+}
+
+static inline unsigned char get_ss_ltssm(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlS, 0, 6);
+}
+
+static inline unsigned char get_ss_rts_func(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlS, 8, 4);
+}
+
+static inline unsigned char get_ss_ltssm_to_st(void __iomem *base_address)
+{
+	return (unsigned char) get_f_usb30_register_bits(base_address,
+						 F_USB30_REG_SSPlS, 16, 6);
+}
+
+#if defined(CONFIG_USB_GADGET_F_USB30_USED_DMA_TRANSFER)
+/* F_USB30 DMA controller register ID Enumeration constant */
+#define F_USB30_DMAC_REG_AXICFG1	0 /* DMA1 I/F AXI configuration */
+#define F_USB30_DMAC_REG_AXIADR1	1
+				/* DMA1 I/F AXI Master Read Start Address */
+#define F_USB30_DMAC_REG_DMADS1		2 /* DMA1 I/F Data Size */
+#define F_USB30_DMAC_REG_DMAC1		3 /* DMA1 I/F Control   */
+#define F_USB30_DMAC_REG_DMAS1		4 /* DMA1 I/F Status    */
+#define F_USB30_DMAC_REG_AXIDRC1	5
+				/* AXI Master Read Data Receive Count   */
+#define F_USB30_DMAC_REG_USBDTC1	6 /* USB Data In Tranfer Count  */
+#define F_USB30_DMAC_REG_AXICFG2	7 /* DMA2 I/F AXI configuration */
+#define F_USB30_DMAC_REG_AXIADR2	8
+				/* DMA2 I/F AXI Master Write Start Address */
+#define F_USB30_DMAC_REG_DMADS2		9 /* DMA2 I/F Data Size */
+#define F_USB30_DMAC_REG_DMAC2		10 /* DMA2 I/F Control  */
+#define F_USB30_DMAC_REG_DMAS2		11 /* DMA2 I/F Status   */
+#define F_USB30_DMAC_REG_USBDRC2	12 /* USB Data Out Receive Count */
+#define F_USB30_DMAC_REG_AXIDTC2	13
+				/* AXI Master Write Data Transfer Count */
+#define F_USB30_DMAC_REG_MAX		14 /* Max Value */
+
+/* I/F AXI configuration register ID table array */
+static const unsigned long axicfg_register[] = {
+	F_USB30_DMAC_REG_AXICFG1,	/* DMA1 I/F AXI configuration */
+	F_USB30_DMAC_REG_AXICFG2,	/* DMA2 I/F AXI configuration */
+};
+
+/* I/F AXI Master Read/write Start Address register ID table array */
+static const unsigned long axiar_register[] = {
+	F_USB30_DMAC_REG_AXIADR1,	/*
+					 * DMA1 I/F AXI Master Read
+					 * Start Address
+					 */
+	F_USB30_DMAC_REG_AXIADR2,	/*
+					 * DMA2 I/F AXI Master Write
+					 * Start Address
+					 */
+};
+
+/* I/F Data Size register ID table array */
+static const unsigned long dmads_register[] = {
+	F_USB30_DMAC_REG_DMADS1,	/* DMA1 I/F Data Size */
+	F_USB30_DMAC_REG_DMADS2,	/* DMA2 I/F Data Size */
+};
+
+/* I/F Control register ID table array */
+static const unsigned long dmac_register[] = {
+	F_USB30_DMAC_REG_DMAC1,		/* DMA1 I/F Control */
+	F_USB30_DMAC_REG_DMAC2,		/* DMA2 I/F Control */
+};
+
+/* I/F Status register ID table array */
+static const unsigned long dmas_register[] = {
+	F_USB30_DMAC_REG_DMAS1,		/* DMA1 I/F Status */
+	F_USB30_DMAC_REG_DMAS2,		/* DMA2 I/F Status */
+};
+
+/* AXI Master Read/write Data Receive Count register ID table array */
+static const unsigned long axidrc_register[] = {
+	F_USB30_DMAC_REG_AXIDRC1,	/* DMA1 I/F AXI Transferred Data Size */
+	F_USB30_DMAC_REG_AXIDTC2,	/* DMA2 I/F AXI Received Data Size */
+};
+
+/* USB Data In/Out Tranfer Count register ID table array */
+static const unsigned long usbdtc_register[] = {
+	F_USB30_DMAC_REG_USBDTC1,	/* DMA1 USB Buffer Received Data Size */
+	F_USB30_DMAC_REG_USBDRC2,	/*
+					 * DMA2 USB Buffer Transferred
+					 * Data Size
+					 */
+};
+
+/* F_USB30 DMA controller register structures array */
+static const struct {
+	unsigned long address_offset;	/* register address offset */
+	unsigned char readable;	/* register readable flag */
+	unsigned char writable;	/* register writable flag */
+} f_usb30_dma_register[F_USB30_DMAC_REG_MAX] = {
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0000), 1, 1 },
+					/* F_USB30_DMAC_REG_AXICFG1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0004), 1, 1 },
+					/* F_USB30_DMAC_REG_AXIADR1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0008), 1, 1 },
+					/* F_USB30_DMAC_REG_DMADS1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x000c), 1, 1 },
+					/* F_USB30_DMAC_REG_DMAC1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0010), 1, 1 },
+					/* F_USB30_DMAC_REG_DMAS1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0014), 1, 1 },
+					/* F_USB30_DMAC_REG_AXIDRC1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0018), 1, 1 },
+					/* F_USB30_DMAC_REG_USBDTC1 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0100), 1, 1 },
+					/* F_USB30_DMAC_REG_AXICFG2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0104), 1, 1 },
+					/* F_USB30_DMAC_REG_AXIADR2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0108), 1, 1 },
+					/* F_USB30_DMAC_REG_DMADS2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x010c), 1, 1 },
+					/* F_USB30_DMAC_REG_DMAC2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0110), 1, 1 },
+					/* F_USB30_DMAC_REG_DMAS2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0114), 1, 1 },
+					/* F_USB30_DMAC_REG_USBDRC2 */
+	{
+	(unsigned long)
+	(F_USB30_DMAC_REG_OFFSET + 0x0118), 1, 1 },
+					/* F_USB30_DMAC_REG_AXIDTC2 */
+};
+
+static inline void control_f_usb30_dmac_default_register_cache_bits(
+	unsigned long *register_cache)
+{
+	return;
+}
+
+static inline void control_f_usb30_dmac_dmas1_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* I/F Status register bit feild position */
+#define F_USB30_DMAC_REG_DMAS_BIT_INTREMAINFIN1		16
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUSBFIN1		17
+#define F_USB30_DMAC_REG_DMAS_BIT_INTNULL1		18
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUEXDREQ1		19
+#define F_USB30_DMAC_REG_DMAS_BIT_INTDECERR1		24
+#define F_USB30_DMAC_REG_DMAS_BIT_INTSLVERR1		25
+#define F_USB30_DMAC_REG_DMAS_BIT_INTEXOKAY1		26
+#define F_USB30_DMAC_REG_DMAS_BIT_INTAXIFIN1		27
+#define F_USB30_DMAC_REG_DMAS_BIT_INTSTARTERR1		28
+
+	*register_cache |=
+	    (((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTREMAINFIN1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUSBFIN1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTNULL1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUEXDREQ1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTDECERR1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTSLVERR1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTEXOKAY1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTAXIFIN1) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTSTARTERR1));
+	return;
+}
+
+static inline void control_f_usb30_dmac_dmas2_register_cache_bits(
+	unsigned long *register_cache)
+{
+	/* I/F Status register bit feild position */
+#define F_USB30_DMAC_REG_DMAS_BIT_INTREMAINFIN2		16
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUSBFIN2		17
+#define F_USB30_DMAC_REG_DMAS_BIT_INTNULL2		18
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUEXDREQ2		19
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUSBUNDER2		20
+#define F_USB30_DMAC_REG_DMAS_BIT_INTUSBOVER2		21
+#define F_USB30_DMAC_REG_DMAS_BIT_INTDECERR2		24
+#define F_USB30_DMAC_REG_DMAS_BIT_INTSLVERR2		25
+#define F_USB30_DMAC_REG_DMAS_BIT_INTEXOKAY2		26
+#define F_USB30_DMAC_REG_DMAS_BIT_INTAXIFIN2		27
+#define F_USB30_DMAC_REG_DMAS_BIT_INTSTARTERR2		28
+
+	*register_cache |=
+	    (((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTREMAINFIN2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUSBFIN2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTNULL2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUEXDREQ2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUSBUNDER2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTUSBOVER2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTDECERR2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTSLVERR2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTEXOKAY2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTAXIFIN2) |
+	     ((unsigned long) 1 << F_USB30_DMAC_REG_DMAS_BIT_INTSTARTERR2));
+	return;
+}
+
+static inline void control_f_usb30_dmac_register_cache_bits(
+	unsigned long register_id, unsigned long *register_cache)
+{
+	static const unsigned char register_cache_bits_control[] = {
+		0,	/* DMA1 I/F AXI configuration */
+		0,	/* DMA1 I/F AXI Master Read Start Address */
+		0,	/* DMA1 I/F Data Size */
+		0,	/* DMA1 I/F Control   */
+		1,	/* DMA1 I/F Status    */
+		0,	/* AXI Master Read Data Receive Count   */
+		0,	/* USB Data In Tranfer Count  */
+		0,	/* DMA2 I/F AXI configuration */
+		0,	/* DMA2 I/F AXI Master Write Start Address */
+		0,	/* DMA2 I/F Data Size */
+		0,	/* DMA2 I/F Control  */
+		2,	/* DMA2 I/F Status   */
+		0,	/* USB Data Out Receive Count */
+		0,	/* AXI Master Write Data Transfer Count */
+	};
+
+	static void (*const register_cache_bits_control_function[])
+		(unsigned long *) = {
+		control_f_usb30_dmac_default_register_cache_bits,
+		control_f_usb30_dmac_dmas1_register_cache_bits,
+		control_f_usb30_dmac_dmas2_register_cache_bits,};
+
+	register_cache_bits_control_function[
+		register_cache_bits_control[register_id]] (register_cache);
+	return;
+}
+
+static inline void set_f_usb30_dmac_register_bits(
+	void __iomem *base_address, unsigned long register_id,
+	 unsigned char start_bit, unsigned char bit_length,
+	 unsigned long value)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	value &= mask;
+
+	if (f_usb30_dma_register[register_id].readable)
+		register_cache = __raw_readl(base_address +
+			 f_usb30_dma_register[register_id].address_offset);
+
+	control_f_usb30_dmac_register_cache_bits(register_id,
+							 &register_cache);
+
+	register_cache &= ~(mask << start_bit);
+	register_cache |= (value << start_bit);
+
+	if (f_usb30_dma_register[register_id].writable)
+		__raw_writel(register_cache, base_address +
+			 f_usb30_dma_register[register_id].address_offset);
+
+	return;
+}
+
+static inline unsigned long get_f_usb30_dmac_register_bits(
+	void __iomem *base_address, unsigned long register_id,
+	 unsigned char start_bit, unsigned char bit_length)
+{
+	unsigned long register_cache;
+	unsigned long mask = (unsigned long) -1 >> (32 - bit_length);
+
+	if (f_usb30_dma_register[register_id].readable)
+		register_cache = __raw_readl(base_address +
+			 f_usb30_dma_register[register_id].address_offset);
+
+	return register_cache >> start_bit & mask;
+}
+
+/* DMAC burst type select */
+#define DMAC_BURST_FIXED		0	/* fixed burst */
+#define DMAC_BURST_INCREMENT		1	/* increment burst */
+#define DMAC_BURST_WRAPPING		2	/* wrapping burst */
+
+static inline void set_bursttype(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char type)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+		 axicfg_register[dma_channel], 0, 2, type);
+	return;
+}
+
+/* DMAC max burst size select */
+#define DMAC_MAX_BURST_4		3	/* 4 burst */
+#define DMAC_MAX_BURST_8		7	/* 8 burst */
+#define DMAC_MAX_BURST_16		15	/* 16 burst */
+
+static inline void set_maxlen(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char length)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 axicfg_register[dma_channel], 8, 4, length);
+	return;
+}
+
+static inline void set_issue(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char sel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 axicfg_register[dma_channel], 24, 1, sel);
+	return;
+}
+
+static inline void set_axiaddress(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned long address)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 axiar_register[dma_channel], 0, 32, address);
+	return;
+}
+
+static inline void set_dmadatasize(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned long size)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmads_register[dma_channel], 0, 32, size);
+	return;
+}
+
+static inline void set_dmastart(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char enable)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 0, 1, enable);
+	return;
+}
+
+static inline void set_dmaabort(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char enable)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 1, 1, enable);
+	return;
+}
+
+static inline void set_dmainit(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char enable)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 2, 1, enable);
+	return;
+}
+
+static inline void set_abortcntl(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char sel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 9, 1, sel);
+	return;
+}
+
+static inline void set_nullcntl(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char sel)
+{
+	if (dma_channel == F_USB30_IN_DMAC)
+		set_f_usb30_dmac_register_bits(base_address,
+					dmac_register[dma_channel], 10, 1, sel);
+	return;
+}
+
+static inline void set_decerrcntl(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char sel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 14, 1, sel);
+	return;
+}
+
+static inline void set_slverrcntl(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char sel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 15, 1, sel);
+	return;
+}
+
+static inline void set_mskintremainfin(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 16, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintremainfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 16, 1);
+}
+
+static inline void set_mskintusbfin(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 17, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintusbfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 17, 1);
+}
+
+static inline void set_mskintnull(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 18, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintnull(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 18, 1);
+}
+
+static inline void set_mskintuexdreq(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 19, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintuexdreq(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 19, 1);
+}
+
+static inline void set_mskintusbunder(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	if (dma_channel == F_USB30_OUT_DMAC)
+		set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 20, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintusbunder(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 20, 1);
+}
+
+static inline void set_mskintusbover(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	if (dma_channel == F_USB30_OUT_DMAC)
+		set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 21, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintusbover(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(
+		base_address, dmac_register[dma_channel], 21, 1);
+}
+
+static inline void set_mskintdecerr(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 24, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintdecerr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 24, 1);
+}
+
+static inline void set_mskintslverr(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 25, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintslverr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 25, 1);
+}
+
+static inline void set_mskintexokay(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 26, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintexokay(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 26, 1);
+}
+
+static inline void set_mskintaxifin(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 27, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintaxifin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 27, 1);
+}
+
+static inline void set_mskintstarterr(void __iomem *base_address,
+	 unsigned char dma_channel, unsigned char mask)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+				 dmac_register[dma_channel], 28, 1, mask);
+	return;
+}
+
+static inline unsigned char get_mskintstarterr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmac_register[dma_channel], 28, 1);
+}
+
+static inline unsigned char get_dmaactive(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 0, 1);
+}
+
+static inline unsigned char get_abortactive(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 1, 1);
+}
+
+static inline unsigned char get_axiactive(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	if (dma_channel == F_USB30_OUT_DMAC)
+		return 0;
+
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 4, 1);
+}
+
+static inline unsigned char get_usbactive(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	if (dma_channel == F_USB30_IN_DMAC)
+		return 0;
+
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 3, 1);
+}
+
+static inline void clear_intremainfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 16, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intremainfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 16, 1);
+}
+
+static inline void clear_intusbfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 17, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intusbfin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 17, 1);
+}
+
+static inline void clear_intnull(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 18, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intnull(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 18, 1);
+}
+
+static inline void clear_intuexdreq(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 19, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intuexdreq(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 19, 1);
+}
+
+static inline void clear_intusbunder(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	if (dma_channel == F_USB30_OUT_DMAC)
+		set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 20, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intusbunder(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 20, 1);
+}
+
+static inline void clear_intusbover(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	if (dma_channel == F_USB30_OUT_DMAC)
+		set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 21, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intusbover(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 21, 1);
+}
+
+static inline void clear_intdecerr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 24, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intdecerr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 24, 1);
+}
+
+static inline void clear_intslverr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 25, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intslverr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 25, 1);
+}
+
+static inline void clear_intexokay(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					dmas_register[dma_channel], 26, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intexokay(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 26, 1);
+}
+
+static inline void clear_intaxifin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 27, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intaxifin(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 27, 1);
+}
+
+static inline void clear_intstarterr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	set_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 28, 1, 0);
+	return;
+}
+
+static inline unsigned char get_intstarterr(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned char) get_f_usb30_dmac_register_bits(base_address,
+					 dmas_register[dma_channel], 28, 1);
+}
+
+static inline unsigned long get_axicount(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned long) get_f_usb30_dmac_register_bits(base_address,
+					 axidrc_register[dma_channel], 0, 32);
+}
+
+static inline unsigned long get_usbcount(void __iomem *base_address,
+	 unsigned char dma_channel)
+{
+	return (unsigned long) get_f_usb30_dmac_register_bits(base_address,
+					 usbdtc_register[dma_channel], 0, 32);
+}
+#endif
+
+static inline void __iomem *remap_iomem_region(unsigned long base_address,
+	unsigned long size)
+{
+	return (void __iomem *) ioremap(base_address, size);
+}
+
+static inline void unmap_iomem_region(void __iomem *base_address)
+{
+	iounmap(base_address);
+	return;
+}
+
+#endif
diff --git a/drivers/usb/host/Kconfig b/drivers/usb/host/Kconfig
index bb2415c..9ca07f7 100644
--- a/drivers/usb/host/Kconfig
+++ b/drivers/usb/host/Kconfig
@@ -17,7 +17,6 @@ config USB_C67X00_HCD
 
 config USB_XHCI_HCD
 	tristate "xHCI HCD (USB 3.0) support"
-	depends on USB_ARCH_HAS_XHCI
 	---help---
 	  The eXtensible Host Controller Interface (xHCI) is standard for USB 3.0
 	  "SuperSpeed" host controller hardware.
@@ -43,7 +42,6 @@ endif # USB_XHCI_HCD
 
 config USB_EHCI_HCD
 	tristate "EHCI HCD (USB 2.0) support"
-	depends on USB_ARCH_HAS_EHCI
 	---help---
 	  The Enhanced Host Controller Interface (EHCI) is standard for USB 2.0
 	  "high speed" (480 Mbit/sec, 60 Mbyte/sec) host controller hardware.
@@ -192,6 +190,13 @@ config USB_EHCI_HCD_SPEAR
           Enables support for the on-chip EHCI controller on
           ST SPEAr chips.
 
+config USB_EHCI_HCD_SYNOPSYS
+       tristate "Support for Synopsys Host-AHB USB 2.0 controller"
+	depends on USB_EHCI_HCD
+	---help---
+	  Enable support for onchip USB controllers based on DesignWare USB 2.0
+	  Host-AHB Controller IP from Synopsys.
+
 config USB_EHCI_HCD_AT91
         tristate  "Support for Atmel on-chip EHCI USB controller"
         depends on USB_EHCI_HCD && ARCH_AT91
@@ -362,7 +367,6 @@ config USB_ISP1362_HCD
 
 config USB_OHCI_HCD
 	tristate "OHCI HCD support"
-	depends on USB_ARCH_HAS_OHCI
 	select ISP1301_OMAP if MACH_OMAP_H2 || MACH_OMAP_H3
 	depends on USB_ISP1301 || !ARCH_LPC32XX
 	---help---
@@ -718,3 +722,15 @@ config USB_HCD_SSB
 
 	  If unsure, say N.
 
+config USB_F_USB20HO_HCD
+	tristate "F_USB20HO USB Host Controller"
+	depends on USB && ARCH_MB86S70
+	default y
+	help
+	  This driver enables support for USB20HO USB Host Controller,
+	  the driver supports High, Full and Low Speed USB.
+
+	  To compile this driver a module, choose M here: the module
+	  will be called "f_usb20ho-hcd".
+
+
diff --git a/drivers/usb/host/Makefile b/drivers/usb/host/Makefile
index 3c227f1..9b2629b 100644
--- a/drivers/usb/host/Makefile
+++ b/drivers/usb/host/Makefile
@@ -33,6 +33,7 @@ obj-$(CONFIG_USB_EHCI_HCD_OMAP)	+= ehci-omap.o
 obj-$(CONFIG_USB_EHCI_HCD_ORION)	+= ehci-orion.o
 obj-$(CONFIG_USB_EHCI_HCD_SPEAR)	+= ehci-spear.o
 obj-$(CONFIG_USB_EHCI_S5P)	+= ehci-s5p.o
+obj-$(CONFIG_USB_EHCI_HCD_SYNOPSYS)	+= ehci-h20ahb.o
 obj-$(CONFIG_USB_EHCI_HCD_AT91) += ehci-atmel.o
 obj-$(CONFIG_USB_EHCI_MSM)	+= ehci-msm.o
 
@@ -55,3 +56,5 @@ obj-$(CONFIG_USB_XUSBPS_DR_OF)	+= xusbps-dr-of.o
 obj-$(CONFIG_USB_OCTEON2_COMMON) += octeon2-common.o
 obj-$(CONFIG_USB_HCD_BCMA)	+= bcma-hcd.o
 obj-$(CONFIG_USB_HCD_SSB)	+= ssb-hcd.o
+obj-$(CONFIG_USB_FUSBH200_HCD)	+= fusbh200-hcd.o
+obj-$(CONFIG_USB_F_USB20HO_HCD)+= f_usb20ho_hcd.o
diff --git a/drivers/usb/host/Makefile.orig b/drivers/usb/host/Makefile.orig
new file mode 100644
index 0000000..3c227f1
--- /dev/null
+++ b/drivers/usb/host/Makefile.orig
@@ -0,0 +1,57 @@
+#
+# Makefile for USB Host Controller Drivers
+#
+
+ccflags-$(CONFIG_USB_DEBUG) := -DDEBUG
+
+isp1760-y := isp1760-hcd.o isp1760-if.o
+
+fhci-y := fhci-hcd.o fhci-hub.o fhci-q.o
+fhci-y += fhci-mem.o fhci-tds.o fhci-sched.o
+
+fhci-$(CONFIG_FHCI_DEBUG) += fhci-dbg.o
+
+xhci-hcd-y := xhci.o xhci-mem.o
+xhci-hcd-y += xhci-ring.o xhci-hub.o xhci-dbg.o
+xhci-hcd-$(CONFIG_PCI)	+= xhci-pci.o
+
+ifneq ($(CONFIG_USB_XHCI_PLATFORM), )
+	xhci-hcd-y		+= xhci-plat.o
+endif
+
+obj-$(CONFIG_USB_WHCI_HCD)	+= whci/
+
+obj-$(CONFIG_PCI)		+= pci-quirks.o
+
+obj-$(CONFIG_IPROC_USB2H)	+= usb2h/
+
+obj-$(CONFIG_USB_EHCI_HCD)	+= ehci-hcd.o
+obj-$(CONFIG_USB_EHCI_PCI)	+= ehci-pci.o
+obj-$(CONFIG_USB_EHCI_HCD_PLATFORM)	+= ehci-platform.o
+obj-$(CONFIG_USB_EHCI_MXC)	+= ehci-mxc.o
+obj-$(CONFIG_USB_EHCI_HCD_OMAP)	+= ehci-omap.o
+obj-$(CONFIG_USB_EHCI_HCD_ORION)	+= ehci-orion.o
+obj-$(CONFIG_USB_EHCI_HCD_SPEAR)	+= ehci-spear.o
+obj-$(CONFIG_USB_EHCI_S5P)	+= ehci-s5p.o
+obj-$(CONFIG_USB_EHCI_HCD_AT91) += ehci-atmel.o
+obj-$(CONFIG_USB_EHCI_MSM)	+= ehci-msm.o
+
+obj-$(CONFIG_USB_OXU210HP_HCD)	+= oxu210hp-hcd.o
+obj-$(CONFIG_USB_ISP116X_HCD)	+= isp116x-hcd.o
+obj-$(CONFIG_USB_ISP1362_HCD)	+= isp1362-hcd.o
+obj-$(CONFIG_USB_OHCI_HCD)	+= ohci-hcd.o
+obj-$(CONFIG_USB_UHCI_HCD)	+= uhci-hcd.o
+obj-$(CONFIG_USB_FHCI_HCD)	+= fhci.o
+obj-$(CONFIG_USB_XHCI_HCD)	+= xhci-hcd.o
+obj-$(CONFIG_USB_SL811_HCD)	+= sl811-hcd.o
+obj-$(CONFIG_USB_SL811_CS)	+= sl811_cs.o
+obj-$(CONFIG_USB_U132_HCD)	+= u132-hcd.o
+obj-$(CONFIG_USB_R8A66597_HCD)	+= r8a66597-hcd.o
+obj-$(CONFIG_USB_ISP1760_HCD)	+= isp1760.o
+obj-$(CONFIG_USB_HWA_HCD)	+= hwa-hc.o
+obj-$(CONFIG_USB_IMX21_HCD)	+= imx21-hcd.o
+obj-$(CONFIG_USB_FSL_MPH_DR_OF)	+= fsl-mph-dr-of.o
+obj-$(CONFIG_USB_XUSBPS_DR_OF)	+= xusbps-dr-of.o
+obj-$(CONFIG_USB_OCTEON2_COMMON) += octeon2-common.o
+obj-$(CONFIG_USB_HCD_BCMA)	+= bcma-hcd.o
+obj-$(CONFIG_USB_HCD_SSB)	+= ssb-hcd.o
diff --git a/drivers/usb/host/ehci-h20ahb.c b/drivers/usb/host/ehci-h20ahb.c
new file mode 100644
index 0000000..7724bab
--- /dev/null
+++ b/drivers/usb/host/ehci-h20ahb.c
@@ -0,0 +1,341 @@
+/*
+ * Copyright (C) 2007-2013 Texas Instruments, Inc.
+ *	Author: Vikram Pandita <vikram.pandita@ti.com>
+ *	Author: Anand Gadiyar <gadiyar@ti.com>
+ *	Author: Keshava Munegowda <keshava_mgowda@ti.com>
+ *	Author: Roger Quadros <rogerq@ti.com>
+ *
+ * Copyright (C) 2009 Nokia Corporation
+ *	Contact: Felipe Balbi <felipe.balbi@nokia.com>
+ *
+ * Based on ehci-omap.c - driver for USBHOST on OMAP3/4 processors
+ *
+ * This file is subject to the terms and conditions of the GNU General Public
+ * License.  See the file COPYING in the main directory of this archive
+ * for more details.
+ *
+ */
+
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/io.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/usb/ulpi.h>
+#include <linux/pm_runtime.h>
+#include <linux/gpio.h>
+#include <linux/clk.h>
+#include <linux/usb.h>
+#include <linux/usb/hcd.h>
+#include <linux/of.h>
+#include <linux/dma-mapping.h>
+
+#include "ehci.h"
+
+#define H20AHB_HS_USB_PORTS	1
+
+/* EHCI Synopsys-specific Register Set */
+#define EHCI_INSNREG04					(0xA0)
+#define EHCI_INSNREG04_DISABLE_UNSUSPEND		(1 << 5)
+#define	EHCI_INSNREG05_ULPI				(0xA4)
+#define	EHCI_INSNREG05_ULPI_CONTROL_SHIFT		31
+#define	EHCI_INSNREG05_ULPI_PORTSEL_SHIFT		24
+#define	EHCI_INSNREG05_ULPI_OPSEL_SHIFT			22
+#define	EHCI_INSNREG05_ULPI_REGADD_SHIFT		16
+#define	EHCI_INSNREG05_ULPI_EXTREGADD_SHIFT		8
+#define	EHCI_INSNREG05_ULPI_WRDATA_SHIFT		0
+
+#define DRIVER_DESC "H20AHB-EHCI Host Controller driver"
+
+static const char hcd_name[] = "ehci-h20ahb";
+
+/*-------------------------------------------------------------------------*/
+
+struct h20ahb_hcd {
+	struct usb_phy *phy[H20AHB_HS_USB_PORTS]; /* one PHY for each port */
+	int nports;
+};
+
+static inline void ehci_write(void __iomem *base, u32 reg, u32 val)
+{
+	writel_relaxed(val, base + reg);
+}
+
+static inline u32 ehci_read(void __iomem *base, u32 reg)
+{
+	return readl_relaxed(base + reg);
+}
+
+/* configure so an HC device and id are always provided */
+/* always called with process context; sleeping is OK */
+
+static struct hc_driver __read_mostly ehci_h20ahb_hc_driver;
+
+static const struct ehci_driver_overrides ehci_h20ahb_overrides __initdata = {
+	.extra_priv_size = sizeof(struct h20ahb_hcd),
+};
+
+static int ehci_h20ahb_phy_read(struct usb_phy *x, u32 reg)
+{
+	u32 val = (1 << EHCI_INSNREG05_ULPI_CONTROL_SHIFT) |
+		(1 << EHCI_INSNREG05_ULPI_PORTSEL_SHIFT) |
+		(3 << EHCI_INSNREG05_ULPI_OPSEL_SHIFT) |
+		(reg << EHCI_INSNREG05_ULPI_REGADD_SHIFT);
+	ehci_write(x->io_priv, 0, val);
+	while ((val = ehci_read(x->io_priv, 0)) &
+		(1 << EHCI_INSNREG05_ULPI_CONTROL_SHIFT));
+	return val & 0xff;
+}
+
+static int ehci_h20ahb_phy_write(struct usb_phy *x, u32 val, u32 reg)
+{
+	u32 v = (1 << EHCI_INSNREG05_ULPI_CONTROL_SHIFT) |
+		(1 << EHCI_INSNREG05_ULPI_PORTSEL_SHIFT) |
+		(2 << EHCI_INSNREG05_ULPI_OPSEL_SHIFT) |
+		(reg << EHCI_INSNREG05_ULPI_REGADD_SHIFT) |
+		(val & 0xff);
+	ehci_write(x->io_priv, 0, v);
+	while ((v = ehci_read(x->io_priv, 0)) &
+		(1 << EHCI_INSNREG05_ULPI_CONTROL_SHIFT));
+	return 0;
+}
+
+static struct usb_phy_io_ops ehci_h20ahb_phy_io_ops = {
+	.read = ehci_h20ahb_phy_read,
+	.write = ehci_h20ahb_phy_write,
+};
+
+
+/**
+ * ehci_hcd_h20ahb_probe - initialize Synopsis-based HCDs
+ *
+ * Allocates basic resources for this USB host controller, and
+ * then invokes the start() method for the HCD associated with it
+ * through the hotplug entry's driver_data.
+ */
+static int ehci_hcd_h20ahb_probe(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct resource	*res;
+	struct usb_hcd	*hcd;
+	void __iomem *regs;
+	int ret;
+	int irq;
+	int i;
+	struct h20ahb_hcd	*h20ahb;
+
+	if (usb_disabled())
+		return -ENODEV;
+
+	/* if (!dev->parent) {
+		dev_err(dev, "Missing parent device\n");
+		return -ENODEV;
+		}*/
+
+	/* For DT boot, get platform data from parent. i.e. usbhshost */
+	/*if (dev->of_node) {
+		pdata = dev_get_platdata(dev->parent);
+		dev->platform_data = pdata;
+	}
+
+	if (!pdata) {
+		dev_err(dev, "Missing platform data\n");
+		return -ENODEV;
+		}*/
+
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(dev, "EHCI irq failed\n");
+		return -ENODEV;
+	}
+
+	res =  platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	regs = devm_ioremap_resource(dev, res);
+	if (IS_ERR(regs))
+		return PTR_ERR(regs);
+
+	/*
+	 * Right now device-tree probed devices don't get dma_mask set.
+	 * Since shared usb code relies on it, set it here for now.
+	 * Once we have dma capability bindings this can go away.
+	 */
+	ret = dma_coerce_mask_and_coherent(dev, DMA_BIT_MASK(32));
+	if (ret)
+		return ret;
+
+	ret = -ENODEV;
+	hcd = usb_create_hcd(&ehci_h20ahb_hc_driver, dev,
+			dev_name(dev));
+	if (!hcd) {
+		dev_err(dev, "Failed to create HCD\n");
+		return -ENOMEM;
+	}
+
+	hcd->rsrc_start = res->start;
+	hcd->rsrc_len = resource_size(res);
+	hcd->regs = regs;
+	hcd_to_ehci(hcd)->caps = regs;
+
+	h20ahb = (struct h20ahb_hcd *)hcd_to_ehci(hcd)->priv;
+	h20ahb->nports = 1;
+
+	platform_set_drvdata(pdev, hcd);
+
+	/* get the PHY devices if needed */
+	for (i = 0 ; i < h20ahb->nports ; i++) {
+		struct usb_phy *phy;
+
+		/* get the PHY device */
+#if 0
+		if (dev->of_node)
+			phy = devm_usb_get_phy_by_phandle(dev, "phys", i);
+		else
+			phy = devm_usb_get_phy_dev(dev, i);
+#endif
+		phy = otg_ulpi_create(&ehci_h20ahb_phy_io_ops, 0);
+		if (IS_ERR(phy)) {
+			ret = PTR_ERR(phy);
+			dev_err(dev, "Can't get PHY device for port %d: %d\n",
+					i, ret);
+			goto err_phy;
+		}
+		phy->dev = dev;
+		usb_add_phy_dev(phy);
+
+		h20ahb->phy[i] = phy;
+		phy->io_priv = hcd->regs + EHCI_INSNREG05_ULPI;
+
+#if 0
+		usb_phy_init(h20ahb->phy[i]);
+		/* bring PHY out of suspend */
+		usb_phy_set_suspend(h20ahb->phy[i], 0);
+#endif
+	}
+
+	/* make the first port's phy the one used by hcd as well */
+	hcd->phy = h20ahb->phy[0];
+
+	pm_runtime_enable(dev);
+	pm_runtime_get_sync(dev);
+
+	/*
+	 * An undocumented "feature" in the H20AHB EHCI controller,
+	 * causes suspended ports to be taken out of suspend when
+	 * the USBCMD.Run/Stop bit is cleared (for example when
+	 * we do ehci_bus_suspend).
+	 * This breaks suspend-resume if the root-hub is allowed
+	 * to suspend. Writing 1 to this undocumented register bit
+	 * disables this feature and restores normal behavior.
+	 */
+	ehci_write(regs, EHCI_INSNREG04,
+				EHCI_INSNREG04_DISABLE_UNSUSPEND);
+
+	ret = usb_add_hcd(hcd, irq, IRQF_SHARED);
+	if (ret) {
+		dev_err(dev, "failed to add hcd with err %d\n", ret);
+		goto err_pm_runtime;
+	}
+	device_wakeup_enable(hcd->self.controller);
+
+	/*
+	 * Bring PHYs out of reset for non PHY modes.
+	 * Even though HSIC mode is a PHY-less mode, the reset
+	 * line exists between the chips and can be modelled
+	 * as a PHY device for reset control.
+	 */
+	for (i = 0; i < h20ahb->nports; i++) {
+		usb_phy_init(h20ahb->phy[i]);
+		/* bring PHY out of suspend */
+		usb_phy_set_suspend(h20ahb->phy[i], 0);
+	}
+
+	return 0;
+
+err_pm_runtime:
+	pm_runtime_put_sync(dev);
+
+err_phy:
+	for (i = 0; i < h20ahb->nports; i++) {
+		if (h20ahb->phy[i])
+			usb_phy_shutdown(h20ahb->phy[i]);
+	}
+
+	usb_put_hcd(hcd);
+
+	return ret;
+}
+
+
+/**
+ * ehci_hcd_h20ahb_remove - shutdown processing for EHCI HCDs
+ * @pdev: USB Host Controller being removed
+ *
+ * Reverses the effect of usb_ehci_hcd_h20ahb_probe(), first invoking
+ * the HCD's stop() method.  It is always called from a thread
+ * context, normally "rmmod", "apmd", or something similar.
+ */
+static int ehci_hcd_h20ahb_remove(struct platform_device *pdev)
+{
+	struct device *dev = &pdev->dev;
+	struct usb_hcd *hcd = dev_get_drvdata(dev);
+	struct h20ahb_hcd *h20ahb = (struct h20ahb_hcd *)hcd_to_ehci(hcd)->priv;
+	int i;
+
+	usb_remove_hcd(hcd);
+
+	for (i = 0; i < h20ahb->nports; i++) {
+		if (h20ahb->phy[i])
+			usb_phy_shutdown(h20ahb->phy[i]);
+	}
+
+	usb_put_hcd(hcd);
+	pm_runtime_put_sync(dev);
+	pm_runtime_disable(dev);
+
+	return 0;
+}
+
+static const struct of_device_id h20ahb_ehci_dt_ids[] = {
+	{ .compatible = "snps,ehci-h20ahb" },
+	{ }
+};
+
+MODULE_DEVICE_TABLE(of, h20ahb_ehci_dt_ids);
+
+static struct platform_driver ehci_hcd_h20ahb_driver = {
+	.probe			= ehci_hcd_h20ahb_probe,
+	.remove			= ehci_hcd_h20ahb_remove,
+	.shutdown		= usb_hcd_platform_shutdown,
+	/*.suspend		= ehci_hcd_h20ahb_suspend, */
+	/*.resume		= ehci_hcd_h20ahb_resume, */
+	.driver = {
+		.name		= hcd_name,
+		.of_match_table = h20ahb_ehci_dt_ids,
+	}
+};
+
+/*-------------------------------------------------------------------------*/
+
+static int __init ehci_h20ahb_init(void)
+{
+	if (usb_disabled())
+		return -ENODEV;
+
+	pr_info("%s: " DRIVER_DESC "\n", hcd_name);
+
+	ehci_init_driver(&ehci_h20ahb_hc_driver, &ehci_h20ahb_overrides);
+	return platform_driver_register(&ehci_hcd_h20ahb_driver);
+}
+module_init(ehci_h20ahb_init);
+
+static void __exit ehci_h20ahb_cleanup(void)
+{
+	platform_driver_unregister(&ehci_hcd_h20ahb_driver);
+}
+module_exit(ehci_h20ahb_cleanup);
+
+MODULE_ALIAS("platform:ehci-h20ahb");
+MODULE_AUTHOR("Liviu Dudau <Liviu.Dudau@arm.com>");
+
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_LICENSE("GPL");
diff --git a/drivers/usb/host/f_usb20ho_hcd.c b/drivers/usb/host/f_usb20ho_hcd.c
new file mode 100644
index 0000000..5bf714b
--- /dev/null
+++ b/drivers/usb/host/f_usb20ho_hcd.c
@@ -0,0 +1,375 @@
+/**
+ * f_usb20ho_lap.c - Fujitsu EHCI platform driver
+ *
+ * Copyright (c) 2013 FUJITSU SEMICONDUCTOR LIMITED
+ *		http://jp.fujitsu.com/group/fsl
+ *
+ * based on bcma-hcd.c
+ *
+ * Author: FUJITSU SEMICONDUCTOR
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+/* #define DEBUG */
+#include <linux/clk.h>
+#include <linux/delay.h>
+#include <linux/device.h>
+#include <linux/platform_device.h>
+#include <linux/module.h>
+#include <linux/errno.h>
+#include <linux/slab.h>
+#include <linux/kernel.h>
+#include <linux/usb/ehci_pdriver.h>
+#include <linux/usb/ohci_pdriver.h>
+#include <linux/dma-mapping.h>
+#include <linux/of.h>
+#include <linux/pm_runtime.h>
+#include <linux/usb.h>
+#include <linux/usb/hcd.h>
+#include "f_usb20ho_hcd.h"
+
+/* to be confirmed this unknow value */
+#define F_EHCI_OFFSET	0x40000
+#define F_EHCI_SIZE		0x1000
+#define F_OHCI_OFFSET	0x41000
+#define F_OHCI_SIZE		0x1000
+#define F_OTHER_OFFSET	0x42000
+#define F_OTHER_SIZE		0x1000
+
+MODULE_AUTHOR("FUJITSU SEMICONDUCTOR");
+MODULE_DESCRIPTION("USB platform driver for f_usb20ho_lap IP ");
+MODULE_LICENSE("GPL");
+
+static const struct usb_ehci_pdata ehci_pdata = {
+/* TO-DO: power management callback might be useful */
+};
+
+static const struct usb_ohci_pdata ohci_pdata = {
+/* TO-DO: power management callback might be useful */
+};
+
+/* return 0 means successful */
+static int dwc3_mb86s70_clk_control(struct device *dev, bool on)
+{
+	int ret, i;
+	struct clk *clk;
+
+	dev_dbg(dev, "%s() is started (on:%d).\n", __func__, on);
+
+	if (!on)
+		goto clock_off;
+
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		ret = clk_prepare_enable(clk);
+		if (ret) {
+			dev_err(dev, "failed to enable clock[%d]\n", i);
+			goto clock_off;
+		}
+		dev_info(dev, "enabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+
+clock_off:
+	for (i = 0;; i++) {
+		clk = of_clk_get(dev->of_node, i);
+		if (IS_ERR(clk))
+			break;
+
+		clk_disable_unprepare(clk);
+		dev_info(dev, "disabled_clk_num[%d]\n", i+1);
+	}
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return on;
+}
+
+static struct platform_device *f_usb20ho_hcd_create_pdev(
+		struct platform_device *pdev, bool ohci)
+{
+	struct resource *resource;
+	struct platform_device *hci_dev;
+	struct resource hci_res[2];
+	int ret = -ENOMEM;
+	int irq;
+	resource_size_t resource_size;
+
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+
+	memset(hci_res, 0, sizeof(hci_res));
+
+	/* get a resource for a F_USB20HO device */
+	resource = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!resource) {
+		dev_err(&pdev->dev, "%s():platform_get_resource() failed.\n"
+			, __func__);
+		ret = -ENODEV;
+		goto err_res;
+	}
+	resource_size = resource->end - resource->start + 1;
+
+	/* get an IRQ for a F_USB20HO device */
+	irq = platform_get_irq(pdev, 0);
+	if (irq < 0) {
+		dev_err(&pdev->dev,
+			"%s():platform_get_irq() for F_USB20HO failed at %d.\n",
+			__func__, irq);
+		ret = -ENODEV;
+		goto err_res;
+	}
+
+	hci_res[0].start = ohci ? resource->start + F_OHCI_OFFSET :
+		resource->start + F_EHCI_OFFSET;
+	hci_res[0].end = ohci ? resource->start + F_OHCI_OFFSET
+			+ F_OHCI_SIZE - 1 :
+		resource->start + F_EHCI_OFFSET + F_EHCI_SIZE - 1;
+	hci_res[0].flags = IORESOURCE_MEM;
+
+	hci_res[1].start = irq;
+	hci_res[1].flags = IORESOURCE_IRQ;
+
+	hci_dev = platform_device_alloc(ohci ? "ohci-platform" :
+					"ehci-platform" , 0);
+	if (!hci_dev) {
+		dev_err(&pdev->dev, "platform_device_alloc() failed.\n");
+		ret = -ENODEV;
+		goto err_res;
+	}
+
+	dma_set_coherent_mask(&hci_dev->dev, pdev->dev.coherent_dma_mask);
+	hci_dev->dev.parent = &pdev->dev;
+	hci_dev->dev.dma_mask = pdev->dev.dma_mask;
+
+	ret = platform_device_add_resources(hci_dev, hci_res,
+					    ARRAY_SIZE(hci_res));
+	if (ret) {
+		dev_err(&pdev->dev
+			, "platform_device_add_resources() failed.\n");
+		goto err_alloc;
+	}
+
+	ret = platform_device_add_data(hci_dev,
+			ohci ? (void *)&ohci_pdata : (void *)&ehci_pdata,
+			ohci ? sizeof(ohci_pdata) : sizeof(ehci_pdata));
+	if (ret) {
+		dev_err(&pdev->dev, "platform_device_add_data() failed.\n");
+		goto err_alloc;
+	}
+
+	ret = platform_device_add(hci_dev);
+	if (ret) {
+		dev_err(&pdev->dev, "platform_device_add() failed.\n");
+		goto err_alloc;
+	}
+
+	dev_info(&pdev->dev, "%s() is ended.\n", __func__);
+	return hci_dev;
+
+err_alloc:
+	platform_device_put(hci_dev);
+err_res:
+	dev_dbg(&pdev->dev, "%s() is ended with error.\n", __func__);
+	dev_err(&pdev->dev, "%s(): fail and return ERR_PTR.\n", __func__);
+	return ERR_PTR(ret);
+}
+
+static u64 f_usb20ho_dma_mask =  DMA_BIT_MASK(32);
+
+static int f_usb20ho_hcd_probe(struct platform_device *pdev)
+{
+	int err;
+	struct f_usb20ho_hcd *usb_dev;
+	struct device *dev = &pdev->dev;
+
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+
+	dev->dma_mask = &f_usb20ho_dma_mask;
+	if (!dev->coherent_dma_mask)
+		dev->coherent_dma_mask = DMA_BIT_MASK(32);
+	dev_info(&pdev->dev, "%s(): coherent_dma_mask is 0x%llx .\n"
+		, __func__, dev->coherent_dma_mask);
+
+	usb_dev = kzalloc(sizeof(struct f_usb20ho_hcd), GFP_KERNEL);
+	if (!usb_dev) {
+		dev_err(&pdev->dev, "kzalloc() failed.\n");
+		return -ENOMEM;
+	}
+	usb_dev->dev = &pdev->dev;
+	platform_set_drvdata(pdev, usb_dev);
+
+	/* get an IRQ for a F_USB20HO device */
+	usb_dev->irq = platform_get_irq(pdev, 0);
+	if (usb_dev->irq < 0) {
+		dev_err(&pdev->dev,
+			"%s():platform_get_irq() for F_USB20HO failed at %d.\n",
+			__func__, usb_dev->irq);
+		err = -ENODEV;
+		goto err_free_usb_dev;
+	}
+	disable_irq(usb_dev->irq);
+
+	/* resume driver for clock, power, irq */
+	pm_runtime_enable(&pdev->dev);
+	err = pm_runtime_get_sync(&pdev->dev);
+	if (err < 0) {
+		dev_err(&pdev->dev, "get_sync failed with err %d\n", err);
+		goto err_unregister_ohci_dev;
+	}
+
+	usb_dev->ehci_dev = f_usb20ho_hcd_create_pdev(pdev, false);
+	if (IS_ERR(usb_dev->ehci_dev)) {
+		dev_err(&pdev->dev, "failed to create EHCI driver.\n");
+		err = -ENODEV;
+		goto err_free_usb_dev;
+	}
+
+	usb_dev->ohci_dev = f_usb20ho_hcd_create_pdev(pdev, true);
+	if (IS_ERR(usb_dev->ohci_dev)) {
+		dev_err(&pdev->dev, "failed to create OHCI driver.\n");
+		err = -ENODEV;
+		goto err_unregister_ehci_dev;
+	}
+
+	dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+	return 0;
+
+err_unregister_ohci_dev:
+	platform_device_unregister(usb_dev->ohci_dev);
+err_unregister_ehci_dev:
+	platform_device_unregister(usb_dev->ehci_dev);
+	pm_runtime_put_sync(&pdev->dev);
+err_free_usb_dev:
+	kfree(usb_dev);
+	dev_err(&pdev->dev, "%s() is ended with error %d.\n"
+		, __func__, err);
+	return err;
+}
+
+static int f_usb20ho_hcd_remove(struct platform_device *pdev)
+{
+	struct f_usb20ho_hcd *usb_dev = platform_get_drvdata(pdev);
+	struct platform_device *ohci_dev = usb_dev->ohci_dev;
+	struct platform_device *ehci_dev = usb_dev->ehci_dev;
+
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+
+	if (ohci_dev)
+		platform_device_unregister(ohci_dev);
+	if (ehci_dev)
+		platform_device_unregister(ehci_dev);
+
+	/* disable power,clock,irq */
+	pm_runtime_put_sync(&pdev->dev);
+	pm_runtime_disable(&pdev->dev);
+
+	dev_dbg(&pdev->dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+
+#ifdef CONFIG_PM
+#ifdef CONFIG_PM_RUNTIME
+static int  f_usb20ho_runtime_suspend(struct device *dev)
+{
+	struct f_usb20ho_hcd *usb_dev = dev_get_drvdata(dev);
+
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	disable_irq(usb_dev->irq);
+	dwc3_mb86s70_clk_control(dev, false);
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+
+static int  f_usb20ho_runtime_resume(struct device *dev)
+{
+	struct f_usb20ho_hcd *usb_dev = dev_get_drvdata(dev);
+
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	dwc3_mb86s70_clk_control(dev, true);
+	enable_irq(usb_dev->irq);
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return 0;
+}
+#endif /* CONFIG_PM_RUNTIME */
+
+static int f_usb20ho_hcd_suspend(struct device *dev)
+{
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return f_usb20ho_runtime_suspend(dev);
+}
+
+static int f_usb20ho_hcd_resume(struct device *dev)
+{
+	dev_dbg(dev, "%s() is started.\n", __func__);
+
+	if (pm_runtime_status_suspended(dev))
+		return 0;
+
+	dev_dbg(dev, "%s() is ended.\n", __func__);
+	return f_usb20ho_runtime_resume(dev);
+}
+
+static const struct dev_pm_ops f_usb20ho_hcd_ops = {
+	.suspend =  f_usb20ho_hcd_suspend,
+	.resume = f_usb20ho_hcd_resume,
+	SET_RUNTIME_PM_OPS(f_usb20ho_runtime_suspend
+		, f_usb20ho_runtime_resume, NULL)
+};
+
+#define DEV_PM (&f_usb20ho_hcd_ops)
+#else /* !CONFIG_PM */
+#define DEV_PM	NULL
+#endif /* CONFIG_PM */
+
+static void f_usb20ho_hcd_shutdown(struct platform_device *pdev)
+{
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+#ifdef CONFIG_PM
+	 f_usb20ho_hcd_suspend(&pdev->dev);
+#endif
+	dev_dbg(&pdev->dev, "%s() is started.\n", __func__);
+}
+
+static const struct of_device_id f_usb20ho_hcd_dt_ids[] = {
+	{ .compatible = "fujitsu,f_usb20ho_hcd" },
+	{ /* sentinel */ }
+};
+MODULE_DEVICE_TABLE(of, f_usb20ho_hcd_dt_ids);
+
+static struct platform_driver f_usb20ho_hcd_driver = {
+	.probe		= f_usb20ho_hcd_probe,
+	.remove		= __exit_p(f_usb20ho_hcd_remove),
+	.shutdown	= f_usb20ho_hcd_shutdown,
+	.driver = {
+		.name = "f_usb20ho_hcd",
+		.owner = THIS_MODULE,
+		.pm = DEV_PM,
+		.of_match_table = f_usb20ho_hcd_dt_ids,
+	},
+};
+
+static int f_usb20ho_hcd_init(void)
+{
+	return platform_driver_register(&f_usb20ho_hcd_driver);
+}
+module_init(f_usb20ho_hcd_init);
+
+static void __exit f_usb20ho_hcd_exit(void)
+{
+	platform_driver_unregister(&f_usb20ho_hcd_driver);
+}
+module_exit(f_usb20ho_hcd_exit);
diff --git a/drivers/usb/host/f_usb20ho_hcd.h b/drivers/usb/host/f_usb20ho_hcd.h
new file mode 100644
index 0000000..5b3d4ac
--- /dev/null
+++ b/drivers/usb/host/f_usb20ho_hcd.h
@@ -0,0 +1,33 @@
+/*
+ * linux/drivers/usb/host/f_usb20ho_hcd.h - F_USB20HDC USB
+ * host controller driver
+ *
+ * Copyright (C) FUJITSU ELECTRONICS INC. 2011. All rights reserved.
+ * Copyright (C) 2012 FUJITSU SEMICONDUCTOR LIMITED.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version 2
+ * of the License, or (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301,
+ * USA.
+ */
+
+#ifndef _F_USB20HO_HCD_H
+#define _F_USB20HO_HCD_H
+
+struct f_usb20ho_hcd {
+	struct device *dev;
+	struct platform_device *ehci_dev;
+	struct platform_device *ohci_dev;
+	int irq;
+};
+#endif
diff --git a/drivers/usb/host/pci-quirks.c b/drivers/usb/host/pci-quirks.c
index 3613e90..cfb7493 100644
--- a/drivers/usb/host/pci-quirks.c
+++ b/drivers/usb/host/pci-quirks.c
@@ -480,7 +480,8 @@ static void quirk_usb_handoff_ohci(struct pci_dev *pdev)
 {
 	void __iomem *base;
 	u32 control;
-	u32 fminterval;
+	u32 fminterval = 0;
+	bool no_fminterval = false;
 	int cnt;
 
 	if (!mmio_resource_enabled(pdev, 0))
@@ -490,6 +491,13 @@ static void quirk_usb_handoff_ohci(struct pci_dev *pdev)
 	if (base == NULL)
 		return;
 
+	/*
+	 * ULi M5237 OHCI controller locks the whole system when accessing
+	 * the OHCI_FMINTERVAL offset.
+	 */
+	if (pdev->vendor == PCI_VENDOR_ID_AL && pdev->device == 0x5237)
+		no_fminterval = true;
+
 	control = readl(base + OHCI_CONTROL);
 
 /* On PA-RISC, PDC can leave IR set incorrectly; ignore it there. */
@@ -528,7 +536,9 @@ static void quirk_usb_handoff_ohci(struct pci_dev *pdev)
 	}
 
 	/* software reset of the controller, preserving HcFmInterval */
+	if (!no_fminterval)
 	fminterval = readl(base + OHCI_FMINTERVAL);
+
 	writel(OHCI_HCR, base + OHCI_CMDSTATUS);
 
 	/* reset requires max 10 us delay */
@@ -537,6 +547,8 @@ static void quirk_usb_handoff_ohci(struct pci_dev *pdev)
 			break;
 		udelay(1);
 	}
+
+	if (!no_fminterval)
 	writel(fminterval, base + OHCI_FMINTERVAL);
 
 	/* Now the controller is safely in SUSPEND and nothing can wake it up */
diff --git a/drivers/usb/host/xhci-plat.c b/drivers/usb/host/xhci-plat.c
index 6e70ce9..1c34cff 100644
--- a/drivers/usb/host/xhci-plat.c
+++ b/drivers/usb/host/xhci-plat.c
@@ -19,6 +19,15 @@
 
 static void xhci_plat_quirks(struct device *dev, struct xhci_hcd *xhci)
 {
+	struct xhci_platform_data *platform_data = dev->platform_data;
+
+	if (platform_data !=NULL)
+		/*
+		 * vendor can specify quirks whatever he wants in
+		 * xhci_platform_data.quirks.
+		 */
+		xhci->quirks |= platform_data->quirks;
+
 	/*
 	 * As of now platform drivers don't provide MSI support so we ensure
 	 * here that the generic code does not try to make a pci_dev from our
@@ -186,11 +195,37 @@ static int xhci_plat_remove(struct platform_device *dev)
 	return 0;
 }
 
+#ifdef CONFIG_PM
+static int xhci_plat_suspend(struct device *dev)
+{
+	struct usb_hcd	*hcd = dev_get_drvdata(dev);
+	struct xhci_hcd	*xhci = hcd_to_xhci(hcd);
+
+	return xhci_suspend(xhci);
+}
+
+static int xhci_plat_resume(struct device *dev)
+{
+	struct usb_hcd	*hcd = dev_get_drvdata(dev);
+	struct xhci_hcd	*xhci = hcd_to_xhci(hcd);
+
+	return xhci_resume(xhci, 0);
+}
+
+static const struct dev_pm_ops xhci_plat_pm_ops = {
+	SET_SYSTEM_SLEEP_PM_OPS(xhci_plat_suspend, xhci_plat_resume)
+};
+#define DEV_PM_OPS	(&xhci_plat_pm_ops)
+#else
+#define DEV_PM_OPS	NULL
+#endif /* CONFIG_PM */
+
 static struct platform_driver usb_xhci_driver = {
 	.probe	= xhci_plat_probe,
 	.remove	= xhci_plat_remove,
 	.driver	= {
 		.name = "xhci-hcd",
+		.pm = DEV_PM_OPS,
 	},
 };
 MODULE_ALIAS("platform:xhci-hcd");
diff --git a/drivers/usb/host/xhci.c b/drivers/usb/host/xhci.c
index 0382803..92834ea 100644
--- a/drivers/usb/host/xhci.c
+++ b/drivers/usb/host/xhci.c
@@ -391,6 +391,10 @@ static int xhci_try_enable_msi(struct usb_hcd *hcd)
 
  legacy_irq:
 	/* fall back to legacy interrupt*/
+	#ifdef CONFIG_ARCH_MB86S70
+	snprintf(hcd->irq_descr, sizeof(hcd->irq_descr), "%s:usb%d",
+			hcd->driver->description, hcd->self.busnum);
+	#endif
 	ret = request_irq(pdev->irq, &usb_hcd_irq, IRQF_SHARED,
 			hcd->irq_descr, hcd);
 	if (ret) {
@@ -1464,6 +1468,36 @@ static struct xhci_ring *xhci_urb_to_transfer_ring(struct xhci_hcd *xhci,
 }
 
 /*
+ * get root hub port's connection state. This is for Synopsis quirks usage.
+ */
+static int xhci_get_port_connect_state(struct usb_hcd* hcd)
+{
+	int max_ports;
+	struct xhci_hcd	*xhci = hcd_to_xhci(hcd);
+	__le32 __iomem **port_array;
+	u32 temp;
+
+	/* print degbu info */
+	if (hcd->speed == HCD_USB3) {
+		max_ports = xhci->num_usb3_ports;
+		port_array = xhci->usb3_ports;
+	} else {
+		max_ports = xhci->num_usb2_ports;
+		port_array = xhci->usb2_ports;
+	}
+	temp = xhci_readl(xhci, port_array[0]);
+	if (temp & PORT_CSC)
+		xhci_dbg(xhci, "xhci_get_port_connect_state(): port change is 1.\n");
+	else
+		xhci_dbg(xhci, "xhci_get_port_connect_state(): port change is 0.\n");
+#if 0
+	if ((!(temp & PORT_CONNECT)) && (!(temp & PORT_CSC)))
+		return 0;
+#endif
+	return !!(temp & PORT_CONNECT);
+}
+
+/*
  * Remove the URB's TD from the endpoint ring.  This may cause the HC to stop
  * USB transfers, potentially stopping in the middle of a TRB buffer.  The HC
  * should pick up where it left off in the TD, unless a Set Transfer Ring
@@ -1572,6 +1606,27 @@ int xhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)
 	 */
 	if (!(ep->ep_state & EP_HALT_PENDING)) {
 		ep->ep_state |= EP_HALT_PENDING;
+		if ((xhci->quirks & XHCI_DISCONNECT_QUIRK) &&
+				!xhci_get_port_connect_state(hcd) &&
+				((urb->ep->desc.bmAttributes & 0x3) ==
+				USB_ENDPOINT_XFER_INT)) {
+			xhci_warn(xhci, "%s(): prevent to stop endpoint.\n",
+				__func__);
+
+			urb_priv = urb->hcpriv;
+			for (i = urb_priv->td_cnt; i < urb_priv->length; i++) {
+				td = urb_priv->td[i];
+				if (!list_empty(&td->td_list))
+					list_del_init(&td->td_list);
+				if (!list_empty(&td->cancelled_td_list))
+					list_del_init(&td->cancelled_td_list);
+			}
+			usb_hcd_unlink_urb_from_ep(hcd, urb);
+			spin_unlock_irqrestore(&xhci->lock, flags);
+			usb_hcd_giveback_urb(bus_to_hcd(urb->dev->bus), urb, -ESHUTDOWN);
+			xhci_urb_free_priv(xhci, urb_priv);
+			return ret;
+		}
 		ep->stop_cmds_pending++;
 		ep->stop_cmd_timer.expires = jiffies +
 			XHCI_STOP_EP_CMD_TIMEOUT * HZ;
@@ -2727,6 +2782,12 @@ int xhci_check_bandwidth(struct usb_hcd *hcd, struct usb_device *udev)
 	xhci_dbg_ctx(xhci, virt_dev->in_ctx,
 		     LAST_CTX_TO_EP_NUM(le32_to_cpu(slot_ctx->dev_info)));
 
+	if ((xhci->quirks & XHCI_DISCONNECT_QUIRK) &&
+			!xhci_get_port_connect_state(hcd)) {
+		xhci_warn(xhci, "%s(): prevent to configure endpoint.\n",
+			__func__);
+		ret = -1;
+	} else
 	ret = xhci_configure_endpoint(xhci, udev, NULL,
 			false, false);
 	if (ret) {
@@ -3523,6 +3584,9 @@ void xhci_free_dev(struct usb_hcd *hcd, struct usb_device *udev)
 {
 	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
 	struct xhci_virt_device *virt_dev;
+#ifndef CONFIG_USB_DEFAULT_PERSIST
+	struct device *dev = hcd->self.controller;
+#endif
 	unsigned long flags;
 	u32 state;
 	int i, ret;
@@ -3603,6 +3667,9 @@ static int xhci_reserve_host_control_ep_resources(struct xhci_hcd *xhci)
 int xhci_alloc_dev(struct usb_hcd *hcd, struct usb_device *udev)
 {
 	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+#ifndef CONFIG_USB_DEFAULT_PERSIST
+	struct device *dev = hcd->self.controller;
+#endif
 	unsigned long flags;
 	int timeleft;
 	int ret;
@@ -3862,13 +3929,21 @@ static int xhci_change_max_exit_latency(struct xhci_hcd *xhci,
 	int ret;
 
 	spin_lock_irqsave(&xhci->lock, flags);
-	if (max_exit_latency == xhci->devs[udev->slot_id]->current_mel) {
+
+	virt_dev = xhci->devs[udev->slot_id];
+
+	/*
+	 * virt_dev might not exists yet if xHC resumed from hibernate (S4) and
+	 * xHC was re-initialized. Exit latency will be set later after
+	 * hub_port_finish_reset() is done and xhci->devs[] are re-allocated
+	 */
+
+	if (!virt_dev || max_exit_latency == virt_dev->current_mel) {
 		spin_unlock_irqrestore(&xhci->lock, flags);
 		return 0;
 	}
 
 	/* Attempt to issue an Evaluate Context command to change the MEL. */
-	virt_dev = xhci->devs[udev->slot_id];
 	command = xhci->lpm_command;
 	xhci_slot_copy(xhci, command->in_ctx, virt_dev->out_ctx);
 	spin_unlock_irqrestore(&xhci->lock, flags);
diff --git a/drivers/usb/host/xhci.c.orig b/drivers/usb/host/xhci.c.orig
new file mode 100644
index 0000000..0382803
--- /dev/null
+++ b/drivers/usb/host/xhci.c.orig
@@ -0,0 +1,4796 @@
+/*
+ * xHCI host controller driver
+ *
+ * Copyright (C) 2008 Intel Corp.
+ *
+ * Author: Sarah Sharp
+ * Some code borrowed from the Linux EHCI driver.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope that it will be useful, but
+ * WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY
+ * or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
+ * for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software Foundation,
+ * Inc., 675 Mass Ave, Cambridge, MA 02139, USA.
+ */
+
+#include <linux/pci.h>
+#include <linux/irq.h>
+#include <linux/log2.h>
+#include <linux/module.h>
+#include <linux/moduleparam.h>
+#include <linux/slab.h>
+#include <linux/dmi.h>
+
+#include "xhci.h"
+
+#define DRIVER_AUTHOR "Sarah Sharp"
+#define DRIVER_DESC "'eXtensible' Host Controller (xHC) Driver"
+
+/* Some 0.95 hardware can't handle the chain bit on a Link TRB being cleared */
+static int link_quirk;
+module_param(link_quirk, int, S_IRUGO | S_IWUSR);
+MODULE_PARM_DESC(link_quirk, "Don't clear the chain bit on a link TRB");
+
+/* TODO: copied from ehci-hcd.c - can this be refactored? */
+/*
+ * xhci_handshake - spin reading hc until handshake completes or fails
+ * @ptr: address of hc register to be read
+ * @mask: bits to look at in result of read
+ * @done: value of those bits when handshake succeeds
+ * @usec: timeout in microseconds
+ *
+ * Returns negative errno, or zero on success
+ *
+ * Success happens when the "mask" bits have the specified value (hardware
+ * handshake done).  There are two failure modes:  "usec" have passed (major
+ * hardware flakeout), or the register reads as all-ones (hardware removed).
+ */
+int xhci_handshake(struct xhci_hcd *xhci, void __iomem *ptr,
+		      u32 mask, u32 done, int usec)
+{
+	u32	result;
+
+	do {
+		result = xhci_readl(xhci, ptr);
+		if (result == ~(u32)0)		/* card removed */
+			return -ENODEV;
+		result &= mask;
+		if (result == done)
+			return 0;
+		udelay(1);
+		usec--;
+	} while (usec > 0);
+	return -ETIMEDOUT;
+}
+
+/*
+ * Disable interrupts and begin the xHCI halting process.
+ */
+void xhci_quiesce(struct xhci_hcd *xhci)
+{
+	u32 halted;
+	u32 cmd;
+	u32 mask;
+
+	mask = ~(XHCI_IRQS);
+	halted = xhci_readl(xhci, &xhci->op_regs->status) & STS_HALT;
+	if (!halted)
+		mask &= ~CMD_RUN;
+
+	cmd = xhci_readl(xhci, &xhci->op_regs->command);
+	cmd &= mask;
+	xhci_writel(xhci, cmd, &xhci->op_regs->command);
+}
+
+/*
+ * Force HC into halt state.
+ *
+ * Disable any IRQs and clear the run/stop bit.
+ * HC will complete any current and actively pipelined transactions, and
+ * should halt within 16 ms of the run/stop bit being cleared.
+ * Read HC Halted bit in the status register to see when the HC is finished.
+ */
+int xhci_halt(struct xhci_hcd *xhci)
+{
+	int ret;
+	xhci_dbg(xhci, "// Halt the HC\n");
+	xhci_quiesce(xhci);
+
+	ret = xhci_handshake(xhci, &xhci->op_regs->status,
+			STS_HALT, STS_HALT, XHCI_MAX_HALT_USEC);
+	if (!ret) {
+		xhci->xhc_state |= XHCI_STATE_HALTED;
+		xhci->cmd_ring_state = CMD_RING_STATE_STOPPED;
+	} else
+		xhci_warn(xhci, "Host not halted after %u microseconds.\n",
+				XHCI_MAX_HALT_USEC);
+	return ret;
+}
+
+/*
+ * Set the run bit and wait for the host to be running.
+ */
+static int xhci_start(struct xhci_hcd *xhci)
+{
+	u32 temp;
+	int ret;
+
+	temp = xhci_readl(xhci, &xhci->op_regs->command);
+	temp |= (CMD_RUN);
+	xhci_dbg(xhci, "// Turn on HC, cmd = 0x%x.\n",
+			temp);
+	xhci_writel(xhci, temp, &xhci->op_regs->command);
+
+	/*
+	 * Wait for the HCHalted Status bit to be 0 to indicate the host is
+	 * running.
+	 */
+	ret = xhci_handshake(xhci, &xhci->op_regs->status,
+			STS_HALT, 0, XHCI_MAX_HALT_USEC);
+	if (ret == -ETIMEDOUT)
+		xhci_err(xhci, "Host took too long to start, "
+				"waited %u microseconds.\n",
+				XHCI_MAX_HALT_USEC);
+	if (!ret)
+		xhci->xhc_state &= ~XHCI_STATE_HALTED;
+	return ret;
+}
+
+/*
+ * Reset a halted HC.
+ *
+ * This resets pipelines, timers, counters, state machines, etc.
+ * Transactions will be terminated immediately, and operational registers
+ * will be set to their defaults.
+ */
+int xhci_reset(struct xhci_hcd *xhci)
+{
+	u32 command;
+	u32 state;
+	int ret, i;
+
+	state = xhci_readl(xhci, &xhci->op_regs->status);
+	if ((state & STS_HALT) == 0) {
+		xhci_warn(xhci, "Host controller not halted, aborting reset.\n");
+		return 0;
+	}
+
+	xhci_dbg(xhci, "// Reset the HC\n");
+	command = xhci_readl(xhci, &xhci->op_regs->command);
+	command |= CMD_RESET;
+	xhci_writel(xhci, command, &xhci->op_regs->command);
+
+	/* Existing Intel xHCI controllers require a delay of 1 mS,
+	 * after setting the CMD_RESET bit, and before accessing any
+	 * HC registers. This allows the HC to complete the
+	 * reset operation and be ready for HC register access.
+	 * Without this delay, the subsequent HC register access,
+	 * may result in a system hang very rarely.
+	 */
+	if (xhci->quirks & XHCI_INTEL_HOST)
+               udelay(1000);
+
+	ret = xhci_handshake(xhci, &xhci->op_regs->command,
+			CMD_RESET, 0, 10 * 1000 * 1000);
+	if (ret)
+		return ret;
+
+	xhci_dbg(xhci, "Wait for controller to be ready for doorbell rings\n");
+	/*
+	 * xHCI cannot write to any doorbells or operational registers other
+	 * than status until the "Controller Not Ready" flag is cleared.
+	 */
+	ret = xhci_handshake(xhci, &xhci->op_regs->status,
+			STS_CNR, 0, 10 * 1000 * 1000);
+
+	for (i = 0; i < 2; ++i) {
+		xhci->bus_state[i].port_c_suspend = 0;
+		xhci->bus_state[i].suspended_ports = 0;
+		xhci->bus_state[i].resuming_ports = 0;
+	}
+
+	return ret;
+}
+
+#ifdef CONFIG_PCI
+static int xhci_free_msi(struct xhci_hcd *xhci)
+{
+	int i;
+
+	if (!xhci->msix_entries)
+		return -EINVAL;
+
+	for (i = 0; i < xhci->msix_count; i++)
+		if (xhci->msix_entries[i].vector)
+			free_irq(xhci->msix_entries[i].vector,
+					xhci_to_hcd(xhci));
+	return 0;
+}
+
+/*
+ * Set up MSI
+ */
+static int xhci_setup_msi(struct xhci_hcd *xhci)
+{
+	int ret;
+	struct pci_dev  *pdev = to_pci_dev(xhci_to_hcd(xhci)->self.controller);
+
+	ret = pci_enable_msi(pdev);
+	if (ret) {
+		xhci_dbg(xhci, "failed to allocate MSI entry\n");
+		return ret;
+	}
+
+	ret = request_irq(pdev->irq, xhci_msi_irq,
+				0, "xhci_hcd", xhci_to_hcd(xhci));
+	if (ret) {
+		xhci_dbg(xhci, "disable MSI interrupt\n");
+		pci_disable_msi(pdev);
+	}
+
+	return ret;
+}
+
+/*
+ * Free IRQs
+ * free all IRQs request
+ */
+static void xhci_free_irq(struct xhci_hcd *xhci)
+{
+	struct pci_dev *pdev = to_pci_dev(xhci_to_hcd(xhci)->self.controller);
+	int ret;
+
+	/* return if using legacy interrupt */
+	if (xhci_to_hcd(xhci)->irq > 0)
+		return;
+
+	ret = xhci_free_msi(xhci);
+	if (!ret)
+		return;
+	if (pdev->irq > 0)
+		free_irq(pdev->irq, xhci_to_hcd(xhci));
+
+	return;
+}
+
+/*
+ * Set up MSI-X
+ */
+static int xhci_setup_msix(struct xhci_hcd *xhci)
+{
+	int i, ret = 0;
+	struct usb_hcd *hcd = xhci_to_hcd(xhci);
+	struct pci_dev *pdev = to_pci_dev(hcd->self.controller);
+
+	/*
+	 * calculate number of msi-x vectors supported.
+	 * - HCS_MAX_INTRS: the max number of interrupts the host can handle,
+	 *   with max number of interrupters based on the xhci HCSPARAMS1.
+	 * - num_online_cpus: maximum msi-x vectors per CPUs core.
+	 *   Add additional 1 vector to ensure always available interrupt.
+	 */
+	xhci->msix_count = min(num_online_cpus() + 1,
+				HCS_MAX_INTRS(xhci->hcs_params1));
+
+	xhci->msix_entries =
+		kmalloc((sizeof(struct msix_entry))*xhci->msix_count,
+				GFP_KERNEL);
+	if (!xhci->msix_entries) {
+		xhci_err(xhci, "Failed to allocate MSI-X entries\n");
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < xhci->msix_count; i++) {
+		xhci->msix_entries[i].entry = i;
+		xhci->msix_entries[i].vector = 0;
+	}
+
+	ret = pci_enable_msix(pdev, xhci->msix_entries, xhci->msix_count);
+	if (ret) {
+		xhci_dbg(xhci, "Failed to enable MSI-X\n");
+		goto free_entries;
+	}
+
+	for (i = 0; i < xhci->msix_count; i++) {
+		ret = request_irq(xhci->msix_entries[i].vector,
+				xhci_msi_irq,
+				0, "xhci_hcd", xhci_to_hcd(xhci));
+		if (ret)
+			goto disable_msix;
+	}
+
+	hcd->msix_enabled = 1;
+	return ret;
+
+disable_msix:
+	xhci_dbg(xhci, "disable MSI-X interrupt\n");
+	xhci_free_irq(xhci);
+	pci_disable_msix(pdev);
+free_entries:
+	kfree(xhci->msix_entries);
+	xhci->msix_entries = NULL;
+	return ret;
+}
+
+/* Free any IRQs and disable MSI-X */
+static void xhci_cleanup_msix(struct xhci_hcd *xhci)
+{
+	struct usb_hcd *hcd = xhci_to_hcd(xhci);
+	struct pci_dev *pdev = to_pci_dev(hcd->self.controller);
+
+	if (xhci->quirks & XHCI_PLAT)
+		return;
+
+	xhci_free_irq(xhci);
+
+	if (xhci->msix_entries) {
+		pci_disable_msix(pdev);
+		kfree(xhci->msix_entries);
+		xhci->msix_entries = NULL;
+	} else {
+		pci_disable_msi(pdev);
+	}
+
+	hcd->msix_enabled = 0;
+	return;
+}
+
+static void xhci_msix_sync_irqs(struct xhci_hcd *xhci)
+{
+	int i;
+
+	if (xhci->msix_entries) {
+		for (i = 0; i < xhci->msix_count; i++)
+			synchronize_irq(xhci->msix_entries[i].vector);
+	}
+}
+
+static int xhci_try_enable_msi(struct usb_hcd *hcd)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct pci_dev  *pdev;
+	int ret;
+
+	/* The xhci platform device has set up IRQs through usb_add_hcd. */
+	if (xhci->quirks & XHCI_PLAT)
+		return 0;
+
+	pdev = to_pci_dev(xhci_to_hcd(xhci)->self.controller);
+	/*
+	 * Some Fresco Logic host controllers advertise MSI, but fail to
+	 * generate interrupts.  Don't even try to enable MSI.
+	 */
+	if (xhci->quirks & XHCI_BROKEN_MSI)
+		goto legacy_irq;
+
+	/* unregister the legacy interrupt */
+	if (hcd->irq)
+		free_irq(hcd->irq, hcd);
+	hcd->irq = 0;
+
+	ret = xhci_setup_msix(xhci);
+	if (ret)
+		/* fall back to msi*/
+		ret = xhci_setup_msi(xhci);
+
+	if (!ret)
+		/* hcd->irq is 0, we have MSI */
+		return 0;
+
+	if (!pdev->irq) {
+		xhci_err(xhci, "No msi-x/msi found and no IRQ in BIOS\n");
+		return -EINVAL;
+	}
+
+ legacy_irq:
+	/* fall back to legacy interrupt*/
+	ret = request_irq(pdev->irq, &usb_hcd_irq, IRQF_SHARED,
+			hcd->irq_descr, hcd);
+	if (ret) {
+		xhci_err(xhci, "request interrupt %d failed\n",
+				pdev->irq);
+		return ret;
+	}
+	hcd->irq = pdev->irq;
+	return 0;
+}
+
+#else
+
+static inline int xhci_try_enable_msi(struct usb_hcd *hcd)
+{
+	return 0;
+}
+
+static inline void xhci_cleanup_msix(struct xhci_hcd *xhci)
+{
+}
+
+static inline void xhci_msix_sync_irqs(struct xhci_hcd *xhci)
+{
+}
+
+#endif
+
+static void compliance_mode_recovery(unsigned long arg)
+{
+	struct xhci_hcd *xhci;
+	struct usb_hcd *hcd;
+	u32 temp;
+	int i;
+
+	xhci = (struct xhci_hcd *)arg;
+
+	for (i = 0; i < xhci->num_usb3_ports; i++) {
+		temp = xhci_readl(xhci, xhci->usb3_ports[i]);
+		if ((temp & PORT_PLS_MASK) == USB_SS_PORT_LS_COMP_MOD) {
+			/*
+			 * Compliance Mode Detected. Letting USB Core
+			 * handle the Warm Reset
+			 */
+			xhci_dbg(xhci, "Compliance mode detected->port %d\n",
+					i + 1);
+			xhci_dbg(xhci, "Attempting compliance mode recovery\n");
+			hcd = xhci->shared_hcd;
+
+			if (hcd->state == HC_STATE_SUSPENDED)
+				usb_hcd_resume_root_hub(hcd);
+
+			usb_hcd_poll_rh_status(hcd);
+		}
+	}
+
+	if (xhci->port_status_u0 != ((1 << xhci->num_usb3_ports)-1))
+		mod_timer(&xhci->comp_mode_recovery_timer,
+			jiffies + msecs_to_jiffies(COMP_MODE_RCVRY_MSECS));
+}
+
+/*
+ * Quirk to work around issue generated by the SN65LVPE502CP USB3.0 re-driver
+ * that causes ports behind that hardware to enter compliance mode sometimes.
+ * The quirk creates a timer that polls every 2 seconds the link state of
+ * each host controller's port and recovers it by issuing a Warm reset
+ * if Compliance mode is detected, otherwise the port will become "dead" (no
+ * device connections or disconnections will be detected anymore). Becasue no
+ * status event is generated when entering compliance mode (per xhci spec),
+ * this quirk is needed on systems that have the failing hardware installed.
+ */
+static void compliance_mode_recovery_timer_init(struct xhci_hcd *xhci)
+{
+	xhci->port_status_u0 = 0;
+	init_timer(&xhci->comp_mode_recovery_timer);
+
+	xhci->comp_mode_recovery_timer.data = (unsigned long) xhci;
+	xhci->comp_mode_recovery_timer.function = compliance_mode_recovery;
+	xhci->comp_mode_recovery_timer.expires = jiffies +
+			msecs_to_jiffies(COMP_MODE_RCVRY_MSECS);
+
+	set_timer_slack(&xhci->comp_mode_recovery_timer,
+			msecs_to_jiffies(COMP_MODE_RCVRY_MSECS));
+	add_timer(&xhci->comp_mode_recovery_timer);
+	xhci_dbg(xhci, "Compliance mode recovery timer initialized\n");
+}
+
+/*
+ * This function identifies the systems that have installed the SN65LVPE502CP
+ * USB3.0 re-driver and that need the Compliance Mode Quirk.
+ * Systems:
+ * Vendor: Hewlett-Packard -> System Models: Z420, Z620 and Z820
+ */
+bool xhci_compliance_mode_recovery_timer_quirk_check(void)
+{
+	const char *dmi_product_name, *dmi_sys_vendor;
+
+	dmi_product_name = dmi_get_system_info(DMI_PRODUCT_NAME);
+	dmi_sys_vendor = dmi_get_system_info(DMI_SYS_VENDOR);
+	if (!dmi_product_name || !dmi_sys_vendor)
+		return false;
+
+	if (!(strstr(dmi_sys_vendor, "Hewlett-Packard")))
+		return false;
+
+	if (strstr(dmi_product_name, "Z420") ||
+			strstr(dmi_product_name, "Z620") ||
+			strstr(dmi_product_name, "Z820") ||
+			strstr(dmi_product_name, "Z1 Workstation"))
+		return true;
+
+	return false;
+}
+
+static int xhci_all_ports_seen_u0(struct xhci_hcd *xhci)
+{
+	return (xhci->port_status_u0 == ((1 << xhci->num_usb3_ports)-1));
+}
+
+
+/*
+ * Initialize memory for HCD and xHC (one-time init).
+ *
+ * Program the PAGESIZE register, initialize the device context array, create
+ * device contexts (?), set up a command ring segment (or two?), create event
+ * ring (one for now).
+ */
+int xhci_init(struct usb_hcd *hcd)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	int retval = 0;
+
+	xhci_dbg(xhci, "xhci_init\n");
+	spin_lock_init(&xhci->lock);
+	if (xhci->hci_version == 0x95 && link_quirk) {
+		xhci_dbg(xhci, "QUIRK: Not clearing Link TRB chain bits.\n");
+		xhci->quirks |= XHCI_LINK_TRB_QUIRK;
+	} else {
+		xhci_dbg(xhci, "xHCI doesn't need link TRB QUIRK\n");
+	}
+	retval = xhci_mem_init(xhci, GFP_KERNEL);
+	xhci_dbg(xhci, "Finished xhci_init\n");
+
+	/* Initializing Compliance Mode Recovery Data If Needed */
+	if (xhci_compliance_mode_recovery_timer_quirk_check()) {
+		xhci->quirks |= XHCI_COMP_MODE_QUIRK;
+		compliance_mode_recovery_timer_init(xhci);
+	}
+
+	return retval;
+}
+
+/*-------------------------------------------------------------------------*/
+
+
+#ifdef CONFIG_USB_XHCI_HCD_DEBUGGING
+static void xhci_event_ring_work(unsigned long arg)
+{
+	unsigned long flags;
+	int temp;
+	u64 temp_64;
+	struct xhci_hcd *xhci = (struct xhci_hcd *) arg;
+	int i, j;
+
+	xhci_dbg(xhci, "Poll event ring: %lu\n", jiffies);
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	temp = xhci_readl(xhci, &xhci->op_regs->status);
+	xhci_dbg(xhci, "op reg status = 0x%x\n", temp);
+	if (temp == 0xffffffff || (xhci->xhc_state & XHCI_STATE_DYING) ||
+			(xhci->xhc_state & XHCI_STATE_HALTED)) {
+		xhci_dbg(xhci, "HW died, polling stopped.\n");
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return;
+	}
+
+	temp = xhci_readl(xhci, &xhci->ir_set->irq_pending);
+	xhci_dbg(xhci, "ir_set 0 pending = 0x%x\n", temp);
+	xhci_dbg(xhci, "HC error bitmask = 0x%x\n", xhci->error_bitmask);
+	xhci->error_bitmask = 0;
+	xhci_dbg(xhci, "Event ring:\n");
+	xhci_debug_segment(xhci, xhci->event_ring->deq_seg);
+	xhci_dbg_ring_ptrs(xhci, xhci->event_ring);
+	temp_64 = xhci_read_64(xhci, &xhci->ir_set->erst_dequeue);
+	temp_64 &= ~ERST_PTR_MASK;
+	xhci_dbg(xhci, "ERST deq = 64'h%0lx\n", (long unsigned int) temp_64);
+	xhci_dbg(xhci, "Command ring:\n");
+	xhci_debug_segment(xhci, xhci->cmd_ring->deq_seg);
+	xhci_dbg_ring_ptrs(xhci, xhci->cmd_ring);
+	xhci_dbg_cmd_ptrs(xhci);
+	for (i = 0; i < MAX_HC_SLOTS; ++i) {
+		if (!xhci->devs[i])
+			continue;
+		for (j = 0; j < 31; ++j) {
+			xhci_dbg_ep_rings(xhci, i, j, &xhci->devs[i]->eps[j]);
+		}
+	}
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	if (!xhci->zombie)
+		mod_timer(&xhci->event_ring_timer, jiffies + POLL_TIMEOUT * HZ);
+	else
+		xhci_dbg(xhci, "Quit polling the event ring.\n");
+}
+#endif
+
+static int xhci_run_finished(struct xhci_hcd *xhci)
+{
+	if (xhci_start(xhci)) {
+		xhci_halt(xhci);
+		return -ENODEV;
+	}
+	xhci->shared_hcd->state = HC_STATE_RUNNING;
+	xhci->cmd_ring_state = CMD_RING_STATE_RUNNING;
+
+	if (xhci->quirks & XHCI_NEC_HOST)
+		xhci_ring_cmd_db(xhci);
+
+	xhci_dbg(xhci, "Finished xhci_run for USB3 roothub\n");
+	return 0;
+}
+
+/*
+ * Start the HC after it was halted.
+ *
+ * This function is called by the USB core when the HC driver is added.
+ * Its opposite is xhci_stop().
+ *
+ * xhci_init() must be called once before this function can be called.
+ * Reset the HC, enable device slot contexts, program DCBAAP, and
+ * set command ring pointer and event ring pointer.
+ *
+ * Setup MSI-X vectors and enable interrupts.
+ */
+int xhci_run(struct usb_hcd *hcd)
+{
+	u32 temp;
+	u64 temp_64;
+	int ret;
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+
+	/* Start the xHCI host controller running only after the USB 2.0 roothub
+	 * is setup.
+	 */
+
+	hcd->uses_new_polling = 1;
+	if (!usb_hcd_is_primary_hcd(hcd))
+		return xhci_run_finished(xhci);
+
+	xhci_dbg(xhci, "xhci_run\n");
+
+	ret = xhci_try_enable_msi(hcd);
+	if (ret)
+		return ret;
+
+#ifdef CONFIG_USB_XHCI_HCD_DEBUGGING
+	init_timer(&xhci->event_ring_timer);
+	xhci->event_ring_timer.data = (unsigned long) xhci;
+	xhci->event_ring_timer.function = xhci_event_ring_work;
+	/* Poll the event ring */
+	xhci->event_ring_timer.expires = jiffies + POLL_TIMEOUT * HZ;
+	xhci->zombie = 0;
+	xhci_dbg(xhci, "Setting event ring polling timer\n");
+	add_timer(&xhci->event_ring_timer);
+#endif
+
+	xhci_dbg(xhci, "Command ring memory map follows:\n");
+	xhci_debug_ring(xhci, xhci->cmd_ring);
+	xhci_dbg_ring_ptrs(xhci, xhci->cmd_ring);
+	xhci_dbg_cmd_ptrs(xhci);
+
+	xhci_dbg(xhci, "ERST memory map follows:\n");
+	xhci_dbg_erst(xhci, &xhci->erst);
+	xhci_dbg(xhci, "Event ring:\n");
+	xhci_debug_ring(xhci, xhci->event_ring);
+	xhci_dbg_ring_ptrs(xhci, xhci->event_ring);
+	temp_64 = xhci_read_64(xhci, &xhci->ir_set->erst_dequeue);
+	temp_64 &= ~ERST_PTR_MASK;
+	xhci_dbg(xhci, "ERST deq = 64'h%0lx\n", (long unsigned int) temp_64);
+
+	xhci_dbg(xhci, "// Set the interrupt modulation register\n");
+	temp = xhci_readl(xhci, &xhci->ir_set->irq_control);
+	temp &= ~ER_IRQ_INTERVAL_MASK;
+	temp |= (u32) 160;
+	xhci_writel(xhci, temp, &xhci->ir_set->irq_control);
+
+	/* Set the HCD state before we enable the irqs */
+	temp = xhci_readl(xhci, &xhci->op_regs->command);
+	temp |= (CMD_EIE);
+	xhci_dbg(xhci, "// Enable interrupts, cmd = 0x%x.\n",
+			temp);
+	xhci_writel(xhci, temp, &xhci->op_regs->command);
+
+	temp = xhci_readl(xhci, &xhci->ir_set->irq_pending);
+	xhci_dbg(xhci, "// Enabling event ring interrupter %p by writing 0x%x to irq_pending\n",
+			xhci->ir_set, (unsigned int) ER_IRQ_ENABLE(temp));
+	xhci_writel(xhci, ER_IRQ_ENABLE(temp),
+			&xhci->ir_set->irq_pending);
+	xhci_print_ir_set(xhci, 0);
+
+	if (xhci->quirks & XHCI_NEC_HOST)
+		xhci_queue_vendor_command(xhci, 0, 0, 0,
+				TRB_TYPE(TRB_NEC_GET_FW));
+
+	xhci_dbg(xhci, "Finished xhci_run for USB2 roothub\n");
+	return 0;
+}
+
+static void xhci_only_stop_hcd(struct usb_hcd *hcd)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+
+	spin_lock_irq(&xhci->lock);
+	xhci_halt(xhci);
+
+	/* The shared_hcd is going to be deallocated shortly (the USB core only
+	 * calls this function when allocation fails in usb_add_hcd(), or
+	 * usb_remove_hcd() is called).  So we need to unset xHCI's pointer.
+	 */
+	xhci->shared_hcd = NULL;
+	spin_unlock_irq(&xhci->lock);
+}
+
+/*
+ * Stop xHCI driver.
+ *
+ * This function is called by the USB core when the HC driver is removed.
+ * Its opposite is xhci_run().
+ *
+ * Disable device contexts, disable IRQs, and quiesce the HC.
+ * Reset the HC, finish any completed transactions, and cleanup memory.
+ */
+void xhci_stop(struct usb_hcd *hcd)
+{
+	u32 temp;
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+
+	if (!usb_hcd_is_primary_hcd(hcd)) {
+		xhci_only_stop_hcd(xhci->shared_hcd);
+		return;
+	}
+
+	spin_lock_irq(&xhci->lock);
+	/* Make sure the xHC is halted for a USB3 roothub
+	 * (xhci_stop() could be called as part of failed init).
+	 */
+	xhci_halt(xhci);
+	xhci_reset(xhci);
+	spin_unlock_irq(&xhci->lock);
+
+	xhci_cleanup_msix(xhci);
+
+#ifdef CONFIG_USB_XHCI_HCD_DEBUGGING
+	/* Tell the event ring poll function not to reschedule */
+	xhci->zombie = 1;
+	del_timer_sync(&xhci->event_ring_timer);
+#endif
+
+	/* Deleting Compliance Mode Recovery Timer */
+	if ((xhci->quirks & XHCI_COMP_MODE_QUIRK) &&
+			(!(xhci_all_ports_seen_u0(xhci)))) {
+		del_timer_sync(&xhci->comp_mode_recovery_timer);
+		xhci_dbg(xhci, "%s: compliance mode recovery timer deleted\n",
+				__func__);
+	}
+
+	if (xhci->quirks & XHCI_AMD_PLL_FIX)
+		usb_amd_dev_put();
+
+	xhci_dbg(xhci, "// Disabling event ring interrupts\n");
+	temp = xhci_readl(xhci, &xhci->op_regs->status);
+	xhci_writel(xhci, temp & ~STS_EINT, &xhci->op_regs->status);
+	temp = xhci_readl(xhci, &xhci->ir_set->irq_pending);
+	xhci_writel(xhci, ER_IRQ_DISABLE(temp),
+			&xhci->ir_set->irq_pending);
+	xhci_print_ir_set(xhci, 0);
+
+	xhci_dbg(xhci, "cleaning up memory\n");
+	xhci_mem_cleanup(xhci);
+	xhci_dbg(xhci, "xhci_stop completed - status = %x\n",
+		    xhci_readl(xhci, &xhci->op_regs->status));
+}
+
+/*
+ * Shutdown HC (not bus-specific)
+ *
+ * This is called when the machine is rebooting or halting.  We assume that the
+ * machine will be powered off, and the HC's internal state will be reset.
+ * Don't bother to free memory.
+ *
+ * This will only ever be called with the main usb_hcd (the USB3 roothub).
+ */
+void xhci_shutdown(struct usb_hcd *hcd)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+
+	if (xhci->quirks & XHCI_SPURIOUS_REBOOT)
+		usb_disable_xhci_ports(to_pci_dev(hcd->self.controller));
+
+	spin_lock_irq(&xhci->lock);
+	xhci_halt(xhci);
+	spin_unlock_irq(&xhci->lock);
+
+	xhci_cleanup_msix(xhci);
+
+	xhci_dbg(xhci, "xhci_shutdown completed - status = %x\n",
+		    xhci_readl(xhci, &xhci->op_regs->status));
+}
+
+#ifdef CONFIG_PM
+static void xhci_save_registers(struct xhci_hcd *xhci)
+{
+	xhci->s3.command = xhci_readl(xhci, &xhci->op_regs->command);
+	xhci->s3.dev_nt = xhci_readl(xhci, &xhci->op_regs->dev_notification);
+	xhci->s3.dcbaa_ptr = xhci_read_64(xhci, &xhci->op_regs->dcbaa_ptr);
+	xhci->s3.config_reg = xhci_readl(xhci, &xhci->op_regs->config_reg);
+	xhci->s3.erst_size = xhci_readl(xhci, &xhci->ir_set->erst_size);
+	xhci->s3.erst_base = xhci_read_64(xhci, &xhci->ir_set->erst_base);
+	xhci->s3.erst_dequeue = xhci_read_64(xhci, &xhci->ir_set->erst_dequeue);
+	xhci->s3.irq_pending = xhci_readl(xhci, &xhci->ir_set->irq_pending);
+	xhci->s3.irq_control = xhci_readl(xhci, &xhci->ir_set->irq_control);
+}
+
+static void xhci_restore_registers(struct xhci_hcd *xhci)
+{
+	xhci_writel(xhci, xhci->s3.command, &xhci->op_regs->command);
+	xhci_writel(xhci, xhci->s3.dev_nt, &xhci->op_regs->dev_notification);
+	xhci_write_64(xhci, xhci->s3.dcbaa_ptr, &xhci->op_regs->dcbaa_ptr);
+	xhci_writel(xhci, xhci->s3.config_reg, &xhci->op_regs->config_reg);
+	xhci_writel(xhci, xhci->s3.erst_size, &xhci->ir_set->erst_size);
+	xhci_write_64(xhci, xhci->s3.erst_base, &xhci->ir_set->erst_base);
+	xhci_write_64(xhci, xhci->s3.erst_dequeue, &xhci->ir_set->erst_dequeue);
+	xhci_writel(xhci, xhci->s3.irq_pending, &xhci->ir_set->irq_pending);
+	xhci_writel(xhci, xhci->s3.irq_control, &xhci->ir_set->irq_control);
+}
+
+static void xhci_set_cmd_ring_deq(struct xhci_hcd *xhci)
+{
+	u64	val_64;
+
+	/* step 2: initialize command ring buffer */
+	val_64 = xhci_read_64(xhci, &xhci->op_regs->cmd_ring);
+	val_64 = (val_64 & (u64) CMD_RING_RSVD_BITS) |
+		(xhci_trb_virt_to_dma(xhci->cmd_ring->deq_seg,
+				      xhci->cmd_ring->dequeue) &
+		 (u64) ~CMD_RING_RSVD_BITS) |
+		xhci->cmd_ring->cycle_state;
+	xhci_dbg(xhci, "// Setting command ring address to 0x%llx\n",
+			(long unsigned long) val_64);
+	xhci_write_64(xhci, val_64, &xhci->op_regs->cmd_ring);
+}
+
+/*
+ * The whole command ring must be cleared to zero when we suspend the host.
+ *
+ * The host doesn't save the command ring pointer in the suspend well, so we
+ * need to re-program it on resume.  Unfortunately, the pointer must be 64-byte
+ * aligned, because of the reserved bits in the command ring dequeue pointer
+ * register.  Therefore, we can't just set the dequeue pointer back in the
+ * middle of the ring (TRBs are 16-byte aligned).
+ */
+static void xhci_clear_command_ring(struct xhci_hcd *xhci)
+{
+	struct xhci_ring *ring;
+	struct xhci_segment *seg;
+
+	ring = xhci->cmd_ring;
+	seg = ring->deq_seg;
+	do {
+		memset(seg->trbs, 0,
+			sizeof(union xhci_trb) * (TRBS_PER_SEGMENT - 1));
+		seg->trbs[TRBS_PER_SEGMENT - 1].link.control &=
+			cpu_to_le32(~TRB_CYCLE);
+		seg = seg->next;
+	} while (seg != ring->deq_seg);
+
+	/* Reset the software enqueue and dequeue pointers */
+	ring->deq_seg = ring->first_seg;
+	ring->dequeue = ring->first_seg->trbs;
+	ring->enq_seg = ring->deq_seg;
+	ring->enqueue = ring->dequeue;
+
+	ring->num_trbs_free = ring->num_segs * (TRBS_PER_SEGMENT - 1) - 1;
+	/*
+	 * Ring is now zeroed, so the HW should look for change of ownership
+	 * when the cycle bit is set to 1.
+	 */
+	ring->cycle_state = 1;
+
+	/*
+	 * Reset the hardware dequeue pointer.
+	 * Yes, this will need to be re-written after resume, but we're paranoid
+	 * and want to make sure the hardware doesn't access bogus memory
+	 * because, say, the BIOS or an SMI started the host without changing
+	 * the command ring pointers.
+	 */
+	xhci_set_cmd_ring_deq(xhci);
+}
+
+/*
+ * Stop HC (not bus-specific)
+ *
+ * This is called when the machine transition into S3/S4 mode.
+ *
+ */
+int xhci_suspend(struct xhci_hcd *xhci)
+{
+	int			rc = 0;
+	struct usb_hcd		*hcd = xhci_to_hcd(xhci);
+	u32			command;
+
+	if (hcd->state != HC_STATE_SUSPENDED ||
+			xhci->shared_hcd->state != HC_STATE_SUSPENDED)
+		return -EINVAL;
+
+	/* Don't poll the roothubs on bus suspend. */
+	xhci_dbg(xhci, "%s: stopping port polling.\n", __func__);
+	clear_bit(HCD_FLAG_POLL_RH, &hcd->flags);
+	del_timer_sync(&hcd->rh_timer);
+
+	spin_lock_irq(&xhci->lock);
+	clear_bit(HCD_FLAG_HW_ACCESSIBLE, &hcd->flags);
+	clear_bit(HCD_FLAG_HW_ACCESSIBLE, &xhci->shared_hcd->flags);
+	/* step 1: stop endpoint */
+	/* skipped assuming that port suspend has done */
+
+	/* step 2: clear Run/Stop bit */
+	command = xhci_readl(xhci, &xhci->op_regs->command);
+	command &= ~CMD_RUN;
+	xhci_writel(xhci, command, &xhci->op_regs->command);
+	if (xhci_handshake(xhci, &xhci->op_regs->status,
+		      STS_HALT, STS_HALT, XHCI_MAX_HALT_USEC)) {
+		xhci_warn(xhci, "WARN: xHC CMD_RUN timeout\n");
+		spin_unlock_irq(&xhci->lock);
+		return -ETIMEDOUT;
+	}
+	xhci_clear_command_ring(xhci);
+
+	/* step 3: save registers */
+	xhci_save_registers(xhci);
+
+	/* step 4: set CSS flag */
+	command = xhci_readl(xhci, &xhci->op_regs->command);
+	command |= CMD_CSS;
+	xhci_writel(xhci, command, &xhci->op_regs->command);
+	if (xhci_handshake(xhci, &xhci->op_regs->status,
+				STS_SAVE, 0, 10 * 1000)) {
+		xhci_warn(xhci, "WARN: xHC save state timeout\n");
+		spin_unlock_irq(&xhci->lock);
+		return -ETIMEDOUT;
+	}
+	spin_unlock_irq(&xhci->lock);
+
+	/*
+	 * Deleting Compliance Mode Recovery Timer because the xHCI Host
+	 * is about to be suspended.
+	 */
+	if ((xhci->quirks & XHCI_COMP_MODE_QUIRK) &&
+			(!(xhci_all_ports_seen_u0(xhci)))) {
+		del_timer_sync(&xhci->comp_mode_recovery_timer);
+		xhci_dbg(xhci, "%s: compliance mode recovery timer deleted\n",
+				__func__);
+	}
+
+	/* step 5: remove core well power */
+	/* synchronize irq when using MSI-X */
+	xhci_msix_sync_irqs(xhci);
+
+	return rc;
+}
+
+/*
+ * start xHC (not bus-specific)
+ *
+ * This is called when the machine transition from S3/S4 mode.
+ *
+ */
+int xhci_resume(struct xhci_hcd *xhci, bool hibernated)
+{
+	u32			command, temp = 0, status;
+	struct usb_hcd		*hcd = xhci_to_hcd(xhci);
+	struct usb_hcd		*secondary_hcd;
+	int			retval = 0;
+	bool			comp_timer_running = false;
+
+	/* Wait a bit if either of the roothubs need to settle from the
+	 * transition into bus suspend.
+	 */
+	if (time_before(jiffies, xhci->bus_state[0].next_statechange) ||
+			time_before(jiffies,
+				xhci->bus_state[1].next_statechange))
+		msleep(100);
+
+	set_bit(HCD_FLAG_HW_ACCESSIBLE, &hcd->flags);
+	set_bit(HCD_FLAG_HW_ACCESSIBLE, &xhci->shared_hcd->flags);
+
+	spin_lock_irq(&xhci->lock);
+	if (xhci->quirks & XHCI_RESET_ON_RESUME)
+		hibernated = true;
+
+	if (!hibernated) {
+		/* step 1: restore register */
+		xhci_restore_registers(xhci);
+		/* step 2: initialize command ring buffer */
+		xhci_set_cmd_ring_deq(xhci);
+		/* step 3: restore state and start state*/
+		/* step 3: set CRS flag */
+		command = xhci_readl(xhci, &xhci->op_regs->command);
+		command |= CMD_CRS;
+		xhci_writel(xhci, command, &xhci->op_regs->command);
+		if (xhci_handshake(xhci, &xhci->op_regs->status,
+			      STS_RESTORE, 0, 10 * 1000)) {
+			xhci_warn(xhci, "WARN: xHC restore state timeout\n");
+			spin_unlock_irq(&xhci->lock);
+			return -ETIMEDOUT;
+		}
+		temp = xhci_readl(xhci, &xhci->op_regs->status);
+	}
+
+	/* If restore operation fails, re-initialize the HC during resume */
+	if ((temp & STS_SRE) || hibernated) {
+
+		if ((xhci->quirks & XHCI_COMP_MODE_QUIRK) &&
+				!(xhci_all_ports_seen_u0(xhci))) {
+			del_timer_sync(&xhci->comp_mode_recovery_timer);
+			xhci_dbg(xhci, "Compliance Mode Recovery Timer deleted!\n");
+		}
+
+		/* Let the USB core know _both_ roothubs lost power. */
+		usb_root_hub_lost_power(xhci->main_hcd->self.root_hub);
+		usb_root_hub_lost_power(xhci->shared_hcd->self.root_hub);
+
+		xhci_dbg(xhci, "Stop HCD\n");
+		xhci_halt(xhci);
+		xhci_reset(xhci);
+		spin_unlock_irq(&xhci->lock);
+		xhci_cleanup_msix(xhci);
+
+#ifdef CONFIG_USB_XHCI_HCD_DEBUGGING
+		/* Tell the event ring poll function not to reschedule */
+		xhci->zombie = 1;
+		del_timer_sync(&xhci->event_ring_timer);
+#endif
+
+		xhci_dbg(xhci, "// Disabling event ring interrupts\n");
+		temp = xhci_readl(xhci, &xhci->op_regs->status);
+		xhci_writel(xhci, temp & ~STS_EINT, &xhci->op_regs->status);
+		temp = xhci_readl(xhci, &xhci->ir_set->irq_pending);
+		xhci_writel(xhci, ER_IRQ_DISABLE(temp),
+				&xhci->ir_set->irq_pending);
+		xhci_print_ir_set(xhci, 0);
+
+		xhci_dbg(xhci, "cleaning up memory\n");
+		xhci_mem_cleanup(xhci);
+		xhci_dbg(xhci, "xhci_stop completed - status = %x\n",
+			    xhci_readl(xhci, &xhci->op_regs->status));
+
+		/* USB core calls the PCI reinit and start functions twice:
+		 * first with the primary HCD, and then with the secondary HCD.
+		 * If we don't do the same, the host will never be started.
+		 */
+		if (!usb_hcd_is_primary_hcd(hcd))
+			secondary_hcd = hcd;
+		else
+			secondary_hcd = xhci->shared_hcd;
+
+		xhci_dbg(xhci, "Initialize the xhci_hcd\n");
+		retval = xhci_init(hcd->primary_hcd);
+		if (retval)
+			return retval;
+		comp_timer_running = true;
+
+		xhci_dbg(xhci, "Start the primary HCD\n");
+		retval = xhci_run(hcd->primary_hcd);
+		if (!retval) {
+			xhci_dbg(xhci, "Start the secondary HCD\n");
+			retval = xhci_run(secondary_hcd);
+		}
+		hcd->state = HC_STATE_SUSPENDED;
+		xhci->shared_hcd->state = HC_STATE_SUSPENDED;
+		goto done;
+	}
+
+	/* step 4: set Run/Stop bit */
+	command = xhci_readl(xhci, &xhci->op_regs->command);
+	command |= CMD_RUN;
+	xhci_writel(xhci, command, &xhci->op_regs->command);
+	xhci_handshake(xhci, &xhci->op_regs->status, STS_HALT,
+		  0, 250 * 1000);
+
+	/* step 5: walk topology and initialize portsc,
+	 * portpmsc and portli
+	 */
+	/* this is done in bus_resume */
+
+	/* step 6: restart each of the previously
+	 * Running endpoints by ringing their doorbells
+	 */
+
+	spin_unlock_irq(&xhci->lock);
+
+ done:
+	if (retval == 0) {
+		/* Resume root hubs only when have pending events. */
+		status = readl(&xhci->op_regs->status);
+		if (status & STS_EINT) {
+			usb_hcd_resume_root_hub(hcd);
+			usb_hcd_resume_root_hub(xhci->shared_hcd);
+		}
+	}
+
+	/*
+	 * If system is subject to the Quirk, Compliance Mode Timer needs to
+	 * be re-initialized Always after a system resume. Ports are subject
+	 * to suffer the Compliance Mode issue again. It doesn't matter if
+	 * ports have entered previously to U0 before system's suspension.
+	 */
+	if ((xhci->quirks & XHCI_COMP_MODE_QUIRK) && !comp_timer_running)
+		compliance_mode_recovery_timer_init(xhci);
+
+	/* Re-enable port polling. */
+	xhci_dbg(xhci, "%s: starting port polling.\n", __func__);
+	set_bit(HCD_FLAG_POLL_RH, &hcd->flags);
+	usb_hcd_poll_rh_status(hcd);
+
+	return retval;
+}
+#endif	/* CONFIG_PM */
+
+/*-------------------------------------------------------------------------*/
+
+/**
+ * xhci_get_endpoint_index - Used for passing endpoint bitmasks between the core and
+ * HCDs.  Find the index for an endpoint given its descriptor.  Use the return
+ * value to right shift 1 for the bitmask.
+ *
+ * Index  = (epnum * 2) + direction - 1,
+ * where direction = 0 for OUT, 1 for IN.
+ * For control endpoints, the IN index is used (OUT index is unused), so
+ * index = (epnum * 2) + direction - 1 = (epnum * 2) + 1 - 1 = (epnum * 2)
+ */
+unsigned int xhci_get_endpoint_index(struct usb_endpoint_descriptor *desc)
+{
+	unsigned int index;
+	if (usb_endpoint_xfer_control(desc))
+		index = (unsigned int) (usb_endpoint_num(desc)*2);
+	else
+		index = (unsigned int) (usb_endpoint_num(desc)*2) +
+			(usb_endpoint_dir_in(desc) ? 1 : 0) - 1;
+	return index;
+}
+
+/* The reverse operation to xhci_get_endpoint_index. Calculate the USB endpoint
+ * address from the XHCI endpoint index.
+ */
+unsigned int xhci_get_endpoint_address(unsigned int ep_index)
+{
+	unsigned int number = DIV_ROUND_UP(ep_index, 2);
+	unsigned int direction = ep_index % 2 ? USB_DIR_OUT : USB_DIR_IN;
+	return direction | number;
+}
+
+/* Find the flag for this endpoint (for use in the control context).  Use the
+ * endpoint index to create a bitmask.  The slot context is bit 0, endpoint 0 is
+ * bit 1, etc.
+ */
+unsigned int xhci_get_endpoint_flag(struct usb_endpoint_descriptor *desc)
+{
+	return 1 << (xhci_get_endpoint_index(desc) + 1);
+}
+
+/* Find the flag for this endpoint (for use in the control context).  Use the
+ * endpoint index to create a bitmask.  The slot context is bit 0, endpoint 0 is
+ * bit 1, etc.
+ */
+unsigned int xhci_get_endpoint_flag_from_index(unsigned int ep_index)
+{
+	return 1 << (ep_index + 1);
+}
+
+/* Compute the last valid endpoint context index.  Basically, this is the
+ * endpoint index plus one.  For slot contexts with more than valid endpoint,
+ * we find the most significant bit set in the added contexts flags.
+ * e.g. ep 1 IN (with epnum 0x81) => added_ctxs = 0b1000
+ * fls(0b1000) = 4, but the endpoint context index is 3, so subtract one.
+ */
+unsigned int xhci_last_valid_endpoint(u32 added_ctxs)
+{
+	return fls(added_ctxs) - 1;
+}
+
+/* Returns 1 if the arguments are OK;
+ * returns 0 this is a root hub; returns -EINVAL for NULL pointers.
+ */
+static int xhci_check_args(struct usb_hcd *hcd, struct usb_device *udev,
+		struct usb_host_endpoint *ep, int check_ep, bool check_virt_dev,
+		const char *func) {
+	struct xhci_hcd	*xhci;
+	struct xhci_virt_device	*virt_dev;
+
+	if (!hcd || (check_ep && !ep) || !udev) {
+		printk(KERN_DEBUG "xHCI %s called with invalid args\n",
+				func);
+		return -EINVAL;
+	}
+	if (!udev->parent) {
+		printk(KERN_DEBUG "xHCI %s called for root hub\n",
+				func);
+		return 0;
+	}
+
+	xhci = hcd_to_xhci(hcd);
+	if (check_virt_dev) {
+		if (!udev->slot_id || !xhci->devs[udev->slot_id]) {
+			printk(KERN_DEBUG "xHCI %s called with unaddressed "
+						"device\n", func);
+			return -EINVAL;
+		}
+
+		virt_dev = xhci->devs[udev->slot_id];
+		if (virt_dev->udev != udev) {
+			printk(KERN_DEBUG "xHCI %s called with udev and "
+					  "virt_dev does not match\n", func);
+			return -EINVAL;
+		}
+	}
+
+	if (xhci->xhc_state & XHCI_STATE_HALTED)
+		return -ENODEV;
+
+	return 1;
+}
+
+static int xhci_configure_endpoint(struct xhci_hcd *xhci,
+		struct usb_device *udev, struct xhci_command *command,
+		bool ctx_change, bool must_succeed);
+
+/*
+ * Full speed devices may have a max packet size greater than 8 bytes, but the
+ * USB core doesn't know that until it reads the first 8 bytes of the
+ * descriptor.  If the usb_device's max packet size changes after that point,
+ * we need to issue an evaluate context command and wait on it.
+ */
+static int xhci_check_maxpacket(struct xhci_hcd *xhci, unsigned int slot_id,
+		unsigned int ep_index, struct urb *urb)
+{
+	struct xhci_container_ctx *in_ctx;
+	struct xhci_container_ctx *out_ctx;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_ep_ctx *ep_ctx;
+	int max_packet_size;
+	int hw_max_packet_size;
+	int ret = 0;
+
+	out_ctx = xhci->devs[slot_id]->out_ctx;
+	ep_ctx = xhci_get_ep_ctx(xhci, out_ctx, ep_index);
+	hw_max_packet_size = MAX_PACKET_DECODED(le32_to_cpu(ep_ctx->ep_info2));
+	max_packet_size = usb_endpoint_maxp(&urb->dev->ep0.desc);
+	if (hw_max_packet_size != max_packet_size) {
+		xhci_dbg(xhci, "Max Packet Size for ep 0 changed.\n");
+		xhci_dbg(xhci, "Max packet size in usb_device = %d\n",
+				max_packet_size);
+		xhci_dbg(xhci, "Max packet size in xHCI HW = %d\n",
+				hw_max_packet_size);
+		xhci_dbg(xhci, "Issuing evaluate context command.\n");
+
+		/* Set up the modified control endpoint 0 */
+		xhci_endpoint_copy(xhci, xhci->devs[slot_id]->in_ctx,
+				xhci->devs[slot_id]->out_ctx, ep_index);
+		in_ctx = xhci->devs[slot_id]->in_ctx;
+		ep_ctx = xhci_get_ep_ctx(xhci, in_ctx, ep_index);
+		ep_ctx->ep_info2 &= cpu_to_le32(~MAX_PACKET_MASK);
+		ep_ctx->ep_info2 |= cpu_to_le32(MAX_PACKET(max_packet_size));
+
+		/* Set up the input context flags for the command */
+		/* FIXME: This won't work if a non-default control endpoint
+		 * changes max packet sizes.
+		 */
+		ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+		ctrl_ctx->add_flags = cpu_to_le32(EP0_FLAG);
+		ctrl_ctx->drop_flags = 0;
+
+		xhci_dbg(xhci, "Slot %d input context\n", slot_id);
+		xhci_dbg_ctx(xhci, in_ctx, ep_index);
+		xhci_dbg(xhci, "Slot %d output context\n", slot_id);
+		xhci_dbg_ctx(xhci, out_ctx, ep_index);
+
+		ret = xhci_configure_endpoint(xhci, urb->dev, NULL,
+				true, false);
+
+		/* Clean up the input context for later use by bandwidth
+		 * functions.
+		 */
+		ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG);
+	}
+	return ret;
+}
+
+/*
+ * non-error returns are a promise to giveback() the urb later
+ * we drop ownership so next owner (or urb unlink) can get it
+ */
+int xhci_urb_enqueue(struct usb_hcd *hcd, struct urb *urb, gfp_t mem_flags)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct xhci_td *buffer;
+	unsigned long flags;
+	int ret = 0;
+	unsigned int slot_id, ep_index;
+	struct urb_priv	*urb_priv;
+	int size, i;
+
+	if (!urb || xhci_check_args(hcd, urb->dev, urb->ep,
+					true, true, __func__) <= 0)
+		return -EINVAL;
+
+	slot_id = urb->dev->slot_id;
+	ep_index = xhci_get_endpoint_index(&urb->ep->desc);
+
+	if (!HCD_HW_ACCESSIBLE(hcd)) {
+		if (!in_interrupt())
+			xhci_dbg(xhci, "urb submitted during PCI suspend\n");
+		ret = -ESHUTDOWN;
+		goto exit;
+	}
+
+	if (usb_endpoint_xfer_isoc(&urb->ep->desc))
+		size = urb->number_of_packets;
+	else
+		size = 1;
+
+	urb_priv = kzalloc(sizeof(struct urb_priv) +
+				  size * sizeof(struct xhci_td *), mem_flags);
+	if (!urb_priv)
+		return -ENOMEM;
+
+	buffer = kzalloc(size * sizeof(struct xhci_td), mem_flags);
+	if (!buffer) {
+		kfree(urb_priv);
+		return -ENOMEM;
+	}
+
+	for (i = 0; i < size; i++) {
+		urb_priv->td[i] = buffer;
+		buffer++;
+	}
+
+	urb_priv->length = size;
+	urb_priv->td_cnt = 0;
+	urb->hcpriv = urb_priv;
+
+	if (usb_endpoint_xfer_control(&urb->ep->desc)) {
+		/* Check to see if the max packet size for the default control
+		 * endpoint changed during FS device enumeration
+		 */
+		if (urb->dev->speed == USB_SPEED_FULL) {
+			ret = xhci_check_maxpacket(xhci, slot_id,
+					ep_index, urb);
+			if (ret < 0) {
+				xhci_urb_free_priv(xhci, urb_priv);
+				urb->hcpriv = NULL;
+				return ret;
+			}
+		}
+
+		/* We have a spinlock and interrupts disabled, so we must pass
+		 * atomic context to this function, which may allocate memory.
+		 */
+		spin_lock_irqsave(&xhci->lock, flags);
+		if (xhci->xhc_state & XHCI_STATE_DYING)
+			goto dying;
+		ret = xhci_queue_ctrl_tx(xhci, GFP_ATOMIC, urb,
+				slot_id, ep_index);
+		if (ret)
+			goto free_priv;
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	} else if (usb_endpoint_xfer_bulk(&urb->ep->desc)) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		if (xhci->xhc_state & XHCI_STATE_DYING)
+			goto dying;
+		if (xhci->devs[slot_id]->eps[ep_index].ep_state &
+				EP_GETTING_STREAMS) {
+			xhci_warn(xhci, "WARN: Can't enqueue URB while bulk ep "
+					"is transitioning to using streams.\n");
+			ret = -EINVAL;
+		} else if (xhci->devs[slot_id]->eps[ep_index].ep_state &
+				EP_GETTING_NO_STREAMS) {
+			xhci_warn(xhci, "WARN: Can't enqueue URB while bulk ep "
+					"is transitioning to "
+					"not having streams.\n");
+			ret = -EINVAL;
+		} else {
+			ret = xhci_queue_bulk_tx(xhci, GFP_ATOMIC, urb,
+					slot_id, ep_index);
+		}
+		if (ret)
+			goto free_priv;
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	} else if (usb_endpoint_xfer_int(&urb->ep->desc)) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		if (xhci->xhc_state & XHCI_STATE_DYING)
+			goto dying;
+		ret = xhci_queue_intr_tx(xhci, GFP_ATOMIC, urb,
+				slot_id, ep_index);
+		if (ret)
+			goto free_priv;
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	} else {
+		spin_lock_irqsave(&xhci->lock, flags);
+		if (xhci->xhc_state & XHCI_STATE_DYING)
+			goto dying;
+		ret = xhci_queue_isoc_tx_prepare(xhci, GFP_ATOMIC, urb,
+				slot_id, ep_index);
+		if (ret)
+			goto free_priv;
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	}
+exit:
+	return ret;
+dying:
+	xhci_dbg(xhci, "Ep 0x%x: URB %p submitted for "
+			"non-responsive xHCI host.\n",
+			urb->ep->desc.bEndpointAddress, urb);
+	ret = -ESHUTDOWN;
+free_priv:
+	xhci_urb_free_priv(xhci, urb_priv);
+	urb->hcpriv = NULL;
+	spin_unlock_irqrestore(&xhci->lock, flags);
+	return ret;
+}
+
+/* Get the right ring for the given URB.
+ * If the endpoint supports streams, boundary check the URB's stream ID.
+ * If the endpoint doesn't support streams, return the singular endpoint ring.
+ */
+static struct xhci_ring *xhci_urb_to_transfer_ring(struct xhci_hcd *xhci,
+		struct urb *urb)
+{
+	unsigned int slot_id;
+	unsigned int ep_index;
+	unsigned int stream_id;
+	struct xhci_virt_ep *ep;
+
+	slot_id = urb->dev->slot_id;
+	ep_index = xhci_get_endpoint_index(&urb->ep->desc);
+	stream_id = urb->stream_id;
+	ep = &xhci->devs[slot_id]->eps[ep_index];
+	/* Common case: no streams */
+	if (!(ep->ep_state & EP_HAS_STREAMS))
+		return ep->ring;
+
+	if (stream_id == 0) {
+		xhci_warn(xhci,
+				"WARN: Slot ID %u, ep index %u has streams, "
+				"but URB has no stream ID.\n",
+				slot_id, ep_index);
+		return NULL;
+	}
+
+	if (stream_id < ep->stream_info->num_streams)
+		return ep->stream_info->stream_rings[stream_id];
+
+	xhci_warn(xhci,
+			"WARN: Slot ID %u, ep index %u has "
+			"stream IDs 1 to %u allocated, "
+			"but stream ID %u is requested.\n",
+			slot_id, ep_index,
+			ep->stream_info->num_streams - 1,
+			stream_id);
+	return NULL;
+}
+
+/*
+ * Remove the URB's TD from the endpoint ring.  This may cause the HC to stop
+ * USB transfers, potentially stopping in the middle of a TRB buffer.  The HC
+ * should pick up where it left off in the TD, unless a Set Transfer Ring
+ * Dequeue Pointer is issued.
+ *
+ * The TRBs that make up the buffers for the canceled URB will be "removed" from
+ * the ring.  Since the ring is a contiguous structure, they can't be physically
+ * removed.  Instead, there are two options:
+ *
+ *  1) If the HC is in the middle of processing the URB to be canceled, we
+ *     simply move the ring's dequeue pointer past those TRBs using the Set
+ *     Transfer Ring Dequeue Pointer command.  This will be the common case,
+ *     when drivers timeout on the last submitted URB and attempt to cancel.
+ *
+ *  2) If the HC is in the middle of a different TD, we turn the TRBs into a
+ *     series of 1-TRB transfer no-op TDs.  (No-ops shouldn't be chained.)  The
+ *     HC will need to invalidate the any TRBs it has cached after the stop
+ *     endpoint command, as noted in the xHCI 0.95 errata.
+ *
+ *  3) The TD may have completed by the time the Stop Endpoint Command
+ *     completes, so software needs to handle that case too.
+ *
+ * This function should protect against the TD enqueueing code ringing the
+ * doorbell while this code is waiting for a Stop Endpoint command to complete.
+ * It also needs to account for multiple cancellations on happening at the same
+ * time for the same endpoint.
+ *
+ * Note that this function can be called in any context, or so says
+ * usb_hcd_unlink_urb()
+ */
+int xhci_urb_dequeue(struct usb_hcd *hcd, struct urb *urb, int status)
+{
+	unsigned long flags;
+	int ret, i;
+	u32 temp;
+	struct xhci_hcd *xhci;
+	struct urb_priv	*urb_priv;
+	struct xhci_td *td;
+	unsigned int ep_index;
+	struct xhci_ring *ep_ring;
+	struct xhci_virt_ep *ep;
+
+	xhci = hcd_to_xhci(hcd);
+	spin_lock_irqsave(&xhci->lock, flags);
+	/* Make sure the URB hasn't completed or been unlinked already */
+	ret = usb_hcd_check_unlink_urb(hcd, urb, status);
+	if (ret || !urb->hcpriv)
+		goto done;
+	temp = xhci_readl(xhci, &xhci->op_regs->status);
+	if (temp == 0xffffffff || (xhci->xhc_state & XHCI_STATE_HALTED)) {
+		xhci_dbg(xhci, "HW died, freeing TD.\n");
+		urb_priv = urb->hcpriv;
+		for (i = urb_priv->td_cnt; i < urb_priv->length; i++) {
+			td = urb_priv->td[i];
+			if (!list_empty(&td->td_list))
+				list_del_init(&td->td_list);
+			if (!list_empty(&td->cancelled_td_list))
+				list_del_init(&td->cancelled_td_list);
+		}
+
+		usb_hcd_unlink_urb_from_ep(hcd, urb);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		usb_hcd_giveback_urb(hcd, urb, -ESHUTDOWN);
+		xhci_urb_free_priv(xhci, urb_priv);
+		return ret;
+	}
+	if ((xhci->xhc_state & XHCI_STATE_DYING) ||
+			(xhci->xhc_state & XHCI_STATE_HALTED)) {
+		xhci_dbg(xhci, "Ep 0x%x: URB %p to be canceled on "
+				"non-responsive xHCI host.\n",
+				urb->ep->desc.bEndpointAddress, urb);
+		/* Let the stop endpoint command watchdog timer (which set this
+		 * state) finish cleaning up the endpoint TD lists.  We must
+		 * have caught it in the middle of dropping a lock and giving
+		 * back an URB.
+		 */
+		goto done;
+	}
+
+	ep_index = xhci_get_endpoint_index(&urb->ep->desc);
+	ep = &xhci->devs[urb->dev->slot_id]->eps[ep_index];
+	ep_ring = xhci_urb_to_transfer_ring(xhci, urb);
+	if (!ep_ring) {
+		ret = -EINVAL;
+		goto done;
+	}
+
+	urb_priv = urb->hcpriv;
+	i = urb_priv->td_cnt;
+	if (i < urb_priv->length)
+		xhci_dbg(xhci, "Cancel URB %p, dev %s, ep 0x%x, "
+				"starting at offset 0x%llx\n",
+				urb, urb->dev->devpath,
+				urb->ep->desc.bEndpointAddress,
+				(unsigned long long) xhci_trb_virt_to_dma(
+					urb_priv->td[i]->start_seg,
+					urb_priv->td[i]->first_trb));
+
+	for (; i < urb_priv->length; i++) {
+		td = urb_priv->td[i];
+		list_add_tail(&td->cancelled_td_list, &ep->cancelled_td_list);
+	}
+
+	/* Queue a stop endpoint command, but only if this is
+	 * the first cancellation to be handled.
+	 */
+	if (!(ep->ep_state & EP_HALT_PENDING)) {
+		ep->ep_state |= EP_HALT_PENDING;
+		ep->stop_cmds_pending++;
+		ep->stop_cmd_timer.expires = jiffies +
+			XHCI_STOP_EP_CMD_TIMEOUT * HZ;
+		add_timer(&ep->stop_cmd_timer);
+		xhci_queue_stop_endpoint(xhci, urb->dev->slot_id, ep_index, 0);
+		xhci_ring_cmd_db(xhci);
+	}
+done:
+	spin_unlock_irqrestore(&xhci->lock, flags);
+	return ret;
+}
+
+/* Drop an endpoint from a new bandwidth configuration for this device.
+ * Only one call to this function is allowed per endpoint before
+ * check_bandwidth() or reset_bandwidth() must be called.
+ * A call to xhci_drop_endpoint() followed by a call to xhci_add_endpoint() will
+ * add the endpoint to the schedule with possibly new parameters denoted by a
+ * different endpoint descriptor in usb_host_endpoint.
+ * A call to xhci_add_endpoint() followed by a call to xhci_drop_endpoint() is
+ * not allowed.
+ *
+ * The USB core will not allow URBs to be queued to an endpoint that is being
+ * disabled, so there's no need for mutual exclusion to protect
+ * the xhci->devs[slot_id] structure.
+ */
+int xhci_drop_endpoint(struct usb_hcd *hcd, struct usb_device *udev,
+		struct usb_host_endpoint *ep)
+{
+	struct xhci_hcd *xhci;
+	struct xhci_container_ctx *in_ctx, *out_ctx;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_slot_ctx *slot_ctx;
+	unsigned int last_ctx;
+	unsigned int ep_index;
+	struct xhci_ep_ctx *ep_ctx;
+	u32 drop_flag;
+	u32 new_add_flags, new_drop_flags, new_slot_info;
+	int ret;
+
+	ret = xhci_check_args(hcd, udev, ep, 1, true, __func__);
+	if (ret <= 0)
+		return ret;
+	xhci = hcd_to_xhci(hcd);
+	if (xhci->xhc_state & XHCI_STATE_DYING)
+		return -ENODEV;
+
+	xhci_dbg(xhci, "%s called for udev %p\n", __func__, udev);
+	drop_flag = xhci_get_endpoint_flag(&ep->desc);
+	if (drop_flag == SLOT_FLAG || drop_flag == EP0_FLAG) {
+		xhci_dbg(xhci, "xHCI %s - can't drop slot or ep 0 %#x\n",
+				__func__, drop_flag);
+		return 0;
+	}
+
+	in_ctx = xhci->devs[udev->slot_id]->in_ctx;
+	out_ctx = xhci->devs[udev->slot_id]->out_ctx;
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+	ep_index = xhci_get_endpoint_index(&ep->desc);
+	ep_ctx = xhci_get_ep_ctx(xhci, out_ctx, ep_index);
+	/* If the HC already knows the endpoint is disabled,
+	 * or the HCD has noted it is disabled, ignore this request
+	 */
+	if (((ep_ctx->ep_info & cpu_to_le32(EP_STATE_MASK)) ==
+	     cpu_to_le32(EP_STATE_DISABLED)) ||
+	    le32_to_cpu(ctrl_ctx->drop_flags) &
+	    xhci_get_endpoint_flag(&ep->desc)) {
+		xhci_warn(xhci, "xHCI %s called with disabled ep %p\n",
+				__func__, ep);
+		return 0;
+	}
+
+	ctrl_ctx->drop_flags |= cpu_to_le32(drop_flag);
+	new_drop_flags = le32_to_cpu(ctrl_ctx->drop_flags);
+
+	ctrl_ctx->add_flags &= cpu_to_le32(~drop_flag);
+	new_add_flags = le32_to_cpu(ctrl_ctx->add_flags);
+
+	last_ctx = xhci_last_valid_endpoint(le32_to_cpu(ctrl_ctx->add_flags));
+	slot_ctx = xhci_get_slot_ctx(xhci, in_ctx);
+	/* Update the last valid endpoint context, if we deleted the last one */
+	if ((le32_to_cpu(slot_ctx->dev_info) & LAST_CTX_MASK) >
+	    LAST_CTX(last_ctx)) {
+		slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
+		slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(last_ctx));
+	}
+	new_slot_info = le32_to_cpu(slot_ctx->dev_info);
+
+	xhci_endpoint_zero(xhci, xhci->devs[udev->slot_id], ep);
+
+	xhci_dbg(xhci, "drop ep 0x%x, slot id %d, new drop flags = %#x, new add flags = %#x, new slot info = %#x\n",
+			(unsigned int) ep->desc.bEndpointAddress,
+			udev->slot_id,
+			(unsigned int) new_drop_flags,
+			(unsigned int) new_add_flags,
+			(unsigned int) new_slot_info);
+	return 0;
+}
+
+/* Add an endpoint to a new possible bandwidth configuration for this device.
+ * Only one call to this function is allowed per endpoint before
+ * check_bandwidth() or reset_bandwidth() must be called.
+ * A call to xhci_drop_endpoint() followed by a call to xhci_add_endpoint() will
+ * add the endpoint to the schedule with possibly new parameters denoted by a
+ * different endpoint descriptor in usb_host_endpoint.
+ * A call to xhci_add_endpoint() followed by a call to xhci_drop_endpoint() is
+ * not allowed.
+ *
+ * The USB core will not allow URBs to be queued to an endpoint until the
+ * configuration or alt setting is installed in the device, so there's no need
+ * for mutual exclusion to protect the xhci->devs[slot_id] structure.
+ */
+int xhci_add_endpoint(struct usb_hcd *hcd, struct usb_device *udev,
+		struct usb_host_endpoint *ep)
+{
+	struct xhci_hcd *xhci;
+	struct xhci_container_ctx *in_ctx, *out_ctx;
+	unsigned int ep_index;
+	struct xhci_slot_ctx *slot_ctx;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	u32 added_ctxs;
+	unsigned int last_ctx;
+	u32 new_add_flags, new_drop_flags, new_slot_info;
+	struct xhci_virt_device *virt_dev;
+	int ret = 0;
+
+	ret = xhci_check_args(hcd, udev, ep, 1, true, __func__);
+	if (ret <= 0) {
+		/* So we won't queue a reset ep command for a root hub */
+		ep->hcpriv = NULL;
+		return ret;
+	}
+	xhci = hcd_to_xhci(hcd);
+	if (xhci->xhc_state & XHCI_STATE_DYING)
+		return -ENODEV;
+
+	added_ctxs = xhci_get_endpoint_flag(&ep->desc);
+	last_ctx = xhci_last_valid_endpoint(added_ctxs);
+	if (added_ctxs == SLOT_FLAG || added_ctxs == EP0_FLAG) {
+		/* FIXME when we have to issue an evaluate endpoint command to
+		 * deal with ep0 max packet size changing once we get the
+		 * descriptors
+		 */
+		xhci_dbg(xhci, "xHCI %s - can't add slot or ep 0 %#x\n",
+				__func__, added_ctxs);
+		return 0;
+	}
+
+	virt_dev = xhci->devs[udev->slot_id];
+	in_ctx = virt_dev->in_ctx;
+	out_ctx = virt_dev->out_ctx;
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+	ep_index = xhci_get_endpoint_index(&ep->desc);
+
+	/* If this endpoint is already in use, and the upper layers are trying
+	 * to add it again without dropping it, reject the addition.
+	 */
+	if (virt_dev->eps[ep_index].ring &&
+			!(le32_to_cpu(ctrl_ctx->drop_flags) &
+				xhci_get_endpoint_flag(&ep->desc))) {
+		xhci_warn(xhci, "Trying to add endpoint 0x%x "
+				"without dropping it.\n",
+				(unsigned int) ep->desc.bEndpointAddress);
+		return -EINVAL;
+	}
+
+	/* If the HCD has already noted the endpoint is enabled,
+	 * ignore this request.
+	 */
+	if (le32_to_cpu(ctrl_ctx->add_flags) &
+	    xhci_get_endpoint_flag(&ep->desc)) {
+		xhci_warn(xhci, "xHCI %s called with enabled ep %p\n",
+				__func__, ep);
+		return 0;
+	}
+
+	/*
+	 * Configuration and alternate setting changes must be done in
+	 * process context, not interrupt context (or so documenation
+	 * for usb_set_interface() and usb_set_configuration() claim).
+	 */
+	if (xhci_endpoint_init(xhci, virt_dev, udev, ep, GFP_NOIO) < 0) {
+		dev_dbg(&udev->dev, "%s - could not initialize ep %#x\n",
+				__func__, ep->desc.bEndpointAddress);
+		return -ENOMEM;
+	}
+
+	ctrl_ctx->add_flags |= cpu_to_le32(added_ctxs);
+	new_add_flags = le32_to_cpu(ctrl_ctx->add_flags);
+
+	/* If xhci_endpoint_disable() was called for this endpoint, but the
+	 * xHC hasn't been notified yet through the check_bandwidth() call,
+	 * this re-adds a new state for the endpoint from the new endpoint
+	 * descriptors.  We must drop and re-add this endpoint, so we leave the
+	 * drop flags alone.
+	 */
+	new_drop_flags = le32_to_cpu(ctrl_ctx->drop_flags);
+
+	slot_ctx = xhci_get_slot_ctx(xhci, in_ctx);
+	/* Update the last valid endpoint context, if we just added one past */
+	if ((le32_to_cpu(slot_ctx->dev_info) & LAST_CTX_MASK) <
+	    LAST_CTX(last_ctx)) {
+		slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
+		slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(last_ctx));
+	}
+	new_slot_info = le32_to_cpu(slot_ctx->dev_info);
+
+	/* Store the usb_device pointer for later use */
+	ep->hcpriv = udev;
+
+	xhci_dbg(xhci, "add ep 0x%x, slot id %d, new drop flags = %#x, new add flags = %#x, new slot info = %#x\n",
+			(unsigned int) ep->desc.bEndpointAddress,
+			udev->slot_id,
+			(unsigned int) new_drop_flags,
+			(unsigned int) new_add_flags,
+			(unsigned int) new_slot_info);
+	return 0;
+}
+
+static void xhci_zero_in_ctx(struct xhci_hcd *xhci, struct xhci_virt_device *virt_dev)
+{
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_ep_ctx *ep_ctx;
+	struct xhci_slot_ctx *slot_ctx;
+	int i;
+
+	/* When a device's add flag and drop flag are zero, any subsequent
+	 * configure endpoint command will leave that endpoint's state
+	 * untouched.  Make sure we don't leave any old state in the input
+	 * endpoint contexts.
+	 */
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, virt_dev->in_ctx);
+	ctrl_ctx->drop_flags = 0;
+	ctrl_ctx->add_flags = 0;
+	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx);
+	slot_ctx->dev_info &= cpu_to_le32(~LAST_CTX_MASK);
+	/* Endpoint 0 is always valid */
+	slot_ctx->dev_info |= cpu_to_le32(LAST_CTX(1));
+	for (i = 1; i < 31; ++i) {
+		ep_ctx = xhci_get_ep_ctx(xhci, virt_dev->in_ctx, i);
+		ep_ctx->ep_info = 0;
+		ep_ctx->ep_info2 = 0;
+		ep_ctx->deq = 0;
+		ep_ctx->tx_info = 0;
+	}
+}
+
+static int xhci_configure_endpoint_result(struct xhci_hcd *xhci,
+		struct usb_device *udev, u32 *cmd_status)
+{
+	int ret;
+
+	switch (*cmd_status) {
+	case COMP_ENOMEM:
+		dev_warn(&udev->dev, "Not enough host controller resources "
+				"for new device state.\n");
+		ret = -ENOMEM;
+		/* FIXME: can we allocate more resources for the HC? */
+		break;
+	case COMP_BW_ERR:
+	case COMP_2ND_BW_ERR:
+		dev_warn(&udev->dev, "Not enough bandwidth "
+				"for new device state.\n");
+		ret = -ENOSPC;
+		/* FIXME: can we go back to the old state? */
+		break;
+	case COMP_TRB_ERR:
+		/* the HCD set up something wrong */
+		dev_warn(&udev->dev, "ERROR: Endpoint drop flag = 0, "
+				"add flag = 1, "
+				"and endpoint is not disabled.\n");
+		ret = -EINVAL;
+		break;
+	case COMP_DEV_ERR:
+		dev_warn(&udev->dev, "ERROR: Incompatible device for endpoint "
+				"configure command.\n");
+		ret = -ENODEV;
+		break;
+	case COMP_SUCCESS:
+		dev_dbg(&udev->dev, "Successful Endpoint Configure command\n");
+		ret = 0;
+		break;
+	default:
+		xhci_err(xhci, "ERROR: unexpected command completion "
+				"code 0x%x.\n", *cmd_status);
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+static int xhci_evaluate_context_result(struct xhci_hcd *xhci,
+		struct usb_device *udev, u32 *cmd_status)
+{
+	int ret;
+	struct xhci_virt_device *virt_dev = xhci->devs[udev->slot_id];
+
+	switch (*cmd_status) {
+	case COMP_EINVAL:
+		dev_warn(&udev->dev, "WARN: xHCI driver setup invalid evaluate "
+				"context command.\n");
+		ret = -EINVAL;
+		break;
+	case COMP_EBADSLT:
+		dev_warn(&udev->dev, "WARN: slot not enabled for"
+				"evaluate context command.\n");
+		ret = -EINVAL;
+		break;
+	case COMP_CTX_STATE:
+		dev_warn(&udev->dev, "WARN: invalid context state for "
+				"evaluate context command.\n");
+		xhci_dbg_ctx(xhci, virt_dev->out_ctx, 1);
+		ret = -EINVAL;
+		break;
+	case COMP_DEV_ERR:
+		dev_warn(&udev->dev, "ERROR: Incompatible device for evaluate "
+				"context command.\n");
+		ret = -ENODEV;
+		break;
+	case COMP_MEL_ERR:
+		/* Max Exit Latency too large error */
+		dev_warn(&udev->dev, "WARN: Max Exit Latency too large\n");
+		ret = -EINVAL;
+		break;
+	case COMP_SUCCESS:
+		dev_dbg(&udev->dev, "Successful evaluate context command\n");
+		ret = 0;
+		break;
+	default:
+		xhci_err(xhci, "ERROR: unexpected command completion "
+				"code 0x%x.\n", *cmd_status);
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+static u32 xhci_count_num_new_endpoints(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx)
+{
+	struct xhci_input_control_ctx *ctrl_ctx;
+	u32 valid_add_flags;
+	u32 valid_drop_flags;
+
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+	/* Ignore the slot flag (bit 0), and the default control endpoint flag
+	 * (bit 1).  The default control endpoint is added during the Address
+	 * Device command and is never removed until the slot is disabled.
+	 */
+	valid_add_flags = ctrl_ctx->add_flags >> 2;
+	valid_drop_flags = ctrl_ctx->drop_flags >> 2;
+
+	/* Use hweight32 to count the number of ones in the add flags, or
+	 * number of endpoints added.  Don't count endpoints that are changed
+	 * (both added and dropped).
+	 */
+	return hweight32(valid_add_flags) -
+		hweight32(valid_add_flags & valid_drop_flags);
+}
+
+static unsigned int xhci_count_num_dropped_endpoints(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx)
+{
+	struct xhci_input_control_ctx *ctrl_ctx;
+	u32 valid_add_flags;
+	u32 valid_drop_flags;
+
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+	valid_add_flags = ctrl_ctx->add_flags >> 2;
+	valid_drop_flags = ctrl_ctx->drop_flags >> 2;
+
+	return hweight32(valid_drop_flags) -
+		hweight32(valid_add_flags & valid_drop_flags);
+}
+
+/*
+ * We need to reserve the new number of endpoints before the configure endpoint
+ * command completes.  We can't subtract the dropped endpoints from the number
+ * of active endpoints until the command completes because we can oversubscribe
+ * the host in this case:
+ *
+ *  - the first configure endpoint command drops more endpoints than it adds
+ *  - a second configure endpoint command that adds more endpoints is queued
+ *  - the first configure endpoint command fails, so the config is unchanged
+ *  - the second command may succeed, even though there isn't enough resources
+ *
+ * Must be called with xhci->lock held.
+ */
+static int xhci_reserve_host_resources(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx)
+{
+	u32 added_eps;
+
+	added_eps = xhci_count_num_new_endpoints(xhci, in_ctx);
+	if (xhci->num_active_eps + added_eps > xhci->limit_active_eps) {
+		xhci_dbg(xhci, "Not enough ep ctxs: "
+				"%u active, need to add %u, limit is %u.\n",
+				xhci->num_active_eps, added_eps,
+				xhci->limit_active_eps);
+		return -ENOMEM;
+	}
+	xhci->num_active_eps += added_eps;
+	xhci_dbg(xhci, "Adding %u ep ctxs, %u now active.\n", added_eps,
+			xhci->num_active_eps);
+	return 0;
+}
+
+/*
+ * The configure endpoint was failed by the xHC for some other reason, so we
+ * need to revert the resources that failed configuration would have used.
+ *
+ * Must be called with xhci->lock held.
+ */
+static void xhci_free_host_resources(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx)
+{
+	u32 num_failed_eps;
+
+	num_failed_eps = xhci_count_num_new_endpoints(xhci, in_ctx);
+	xhci->num_active_eps -= num_failed_eps;
+	xhci_dbg(xhci, "Removing %u failed ep ctxs, %u now active.\n",
+			num_failed_eps,
+			xhci->num_active_eps);
+}
+
+/*
+ * Now that the command has completed, clean up the active endpoint count by
+ * subtracting out the endpoints that were dropped (but not changed).
+ *
+ * Must be called with xhci->lock held.
+ */
+static void xhci_finish_resource_reservation(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx)
+{
+	u32 num_dropped_eps;
+
+	num_dropped_eps = xhci_count_num_dropped_endpoints(xhci, in_ctx);
+	xhci->num_active_eps -= num_dropped_eps;
+	if (num_dropped_eps)
+		xhci_dbg(xhci, "Removing %u dropped ep ctxs, %u now active.\n",
+				num_dropped_eps,
+				xhci->num_active_eps);
+}
+
+static unsigned int xhci_get_block_size(struct usb_device *udev)
+{
+	switch (udev->speed) {
+	case USB_SPEED_LOW:
+	case USB_SPEED_FULL:
+		return FS_BLOCK;
+	case USB_SPEED_HIGH:
+		return HS_BLOCK;
+	case USB_SPEED_SUPER:
+		return SS_BLOCK;
+	case USB_SPEED_UNKNOWN:
+	case USB_SPEED_WIRELESS:
+	default:
+		/* Should never happen */
+		return 1;
+	}
+}
+
+static unsigned int
+xhci_get_largest_overhead(struct xhci_interval_bw *interval_bw)
+{
+	if (interval_bw->overhead[LS_OVERHEAD_TYPE])
+		return LS_OVERHEAD;
+	if (interval_bw->overhead[FS_OVERHEAD_TYPE])
+		return FS_OVERHEAD;
+	return HS_OVERHEAD;
+}
+
+/* If we are changing a LS/FS device under a HS hub,
+ * make sure (if we are activating a new TT) that the HS bus has enough
+ * bandwidth for this new TT.
+ */
+static int xhci_check_tt_bw_table(struct xhci_hcd *xhci,
+		struct xhci_virt_device *virt_dev,
+		int old_active_eps)
+{
+	struct xhci_interval_bw_table *bw_table;
+	struct xhci_tt_bw_info *tt_info;
+
+	/* Find the bandwidth table for the root port this TT is attached to. */
+	bw_table = &xhci->rh_bw[virt_dev->real_port - 1].bw_table;
+	tt_info = virt_dev->tt_info;
+	/* If this TT already had active endpoints, the bandwidth for this TT
+	 * has already been added.  Removing all periodic endpoints (and thus
+	 * making the TT enactive) will only decrease the bandwidth used.
+	 */
+	if (old_active_eps)
+		return 0;
+	if (old_active_eps == 0 && tt_info->active_eps != 0) {
+		if (bw_table->bw_used + TT_HS_OVERHEAD > HS_BW_LIMIT)
+			return -ENOMEM;
+		return 0;
+	}
+	/* Not sure why we would have no new active endpoints...
+	 *
+	 * Maybe because of an Evaluate Context change for a hub update or a
+	 * control endpoint 0 max packet size change?
+	 * FIXME: skip the bandwidth calculation in that case.
+	 */
+	return 0;
+}
+
+static int xhci_check_ss_bw(struct xhci_hcd *xhci,
+		struct xhci_virt_device *virt_dev)
+{
+	unsigned int bw_reserved;
+
+	bw_reserved = DIV_ROUND_UP(SS_BW_RESERVED*SS_BW_LIMIT_IN, 100);
+	if (virt_dev->bw_table->ss_bw_in > (SS_BW_LIMIT_IN - bw_reserved))
+		return -ENOMEM;
+
+	bw_reserved = DIV_ROUND_UP(SS_BW_RESERVED*SS_BW_LIMIT_OUT, 100);
+	if (virt_dev->bw_table->ss_bw_out > (SS_BW_LIMIT_OUT - bw_reserved))
+		return -ENOMEM;
+
+	return 0;
+}
+
+/*
+ * This algorithm is a very conservative estimate of the worst-case scheduling
+ * scenario for any one interval.  The hardware dynamically schedules the
+ * packets, so we can't tell which microframe could be the limiting factor in
+ * the bandwidth scheduling.  This only takes into account periodic endpoints.
+ *
+ * Obviously, we can't solve an NP complete problem to find the minimum worst
+ * case scenario.  Instead, we come up with an estimate that is no less than
+ * the worst case bandwidth used for any one microframe, but may be an
+ * over-estimate.
+ *
+ * We walk the requirements for each endpoint by interval, starting with the
+ * smallest interval, and place packets in the schedule where there is only one
+ * possible way to schedule packets for that interval.  In order to simplify
+ * this algorithm, we record the largest max packet size for each interval, and
+ * assume all packets will be that size.
+ *
+ * For interval 0, we obviously must schedule all packets for each interval.
+ * The bandwidth for interval 0 is just the amount of data to be transmitted
+ * (the sum of all max ESIT payload sizes, plus any overhead per packet times
+ * the number of packets).
+ *
+ * For interval 1, we have two possible microframes to schedule those packets
+ * in.  For this algorithm, if we can schedule the same number of packets for
+ * each possible scheduling opportunity (each microframe), we will do so.  The
+ * remaining number of packets will be saved to be transmitted in the gaps in
+ * the next interval's scheduling sequence.
+ *
+ * As we move those remaining packets to be scheduled with interval 2 packets,
+ * we have to double the number of remaining packets to transmit.  This is
+ * because the intervals are actually powers of 2, and we would be transmitting
+ * the previous interval's packets twice in this interval.  We also have to be
+ * sure that when we look at the largest max packet size for this interval, we
+ * also look at the largest max packet size for the remaining packets and take
+ * the greater of the two.
+ *
+ * The algorithm continues to evenly distribute packets in each scheduling
+ * opportunity, and push the remaining packets out, until we get to the last
+ * interval.  Then those packets and their associated overhead are just added
+ * to the bandwidth used.
+ */
+static int xhci_check_bw_table(struct xhci_hcd *xhci,
+		struct xhci_virt_device *virt_dev,
+		int old_active_eps)
+{
+	unsigned int bw_reserved;
+	unsigned int max_bandwidth;
+	unsigned int bw_used;
+	unsigned int block_size;
+	struct xhci_interval_bw_table *bw_table;
+	unsigned int packet_size = 0;
+	unsigned int overhead = 0;
+	unsigned int packets_transmitted = 0;
+	unsigned int packets_remaining = 0;
+	unsigned int i;
+
+	if (virt_dev->udev->speed == USB_SPEED_SUPER)
+		return xhci_check_ss_bw(xhci, virt_dev);
+
+	if (virt_dev->udev->speed == USB_SPEED_HIGH) {
+		max_bandwidth = HS_BW_LIMIT;
+		/* Convert percent of bus BW reserved to blocks reserved */
+		bw_reserved = DIV_ROUND_UP(HS_BW_RESERVED * max_bandwidth, 100);
+	} else {
+		max_bandwidth = FS_BW_LIMIT;
+		bw_reserved = DIV_ROUND_UP(FS_BW_RESERVED * max_bandwidth, 100);
+	}
+
+	bw_table = virt_dev->bw_table;
+	/* We need to translate the max packet size and max ESIT payloads into
+	 * the units the hardware uses.
+	 */
+	block_size = xhci_get_block_size(virt_dev->udev);
+
+	/* If we are manipulating a LS/FS device under a HS hub, double check
+	 * that the HS bus has enough bandwidth if we are activing a new TT.
+	 */
+	if (virt_dev->tt_info) {
+		xhci_dbg(xhci, "Recalculating BW for rootport %u\n",
+				virt_dev->real_port);
+		if (xhci_check_tt_bw_table(xhci, virt_dev, old_active_eps)) {
+			xhci_warn(xhci, "Not enough bandwidth on HS bus for "
+					"newly activated TT.\n");
+			return -ENOMEM;
+		}
+		xhci_dbg(xhci, "Recalculating BW for TT slot %u port %u\n",
+				virt_dev->tt_info->slot_id,
+				virt_dev->tt_info->ttport);
+	} else {
+		xhci_dbg(xhci, "Recalculating BW for rootport %u\n",
+				virt_dev->real_port);
+	}
+
+	/* Add in how much bandwidth will be used for interval zero, or the
+	 * rounded max ESIT payload + number of packets * largest overhead.
+	 */
+	bw_used = DIV_ROUND_UP(bw_table->interval0_esit_payload, block_size) +
+		bw_table->interval_bw[0].num_packets *
+		xhci_get_largest_overhead(&bw_table->interval_bw[0]);
+
+	for (i = 1; i < XHCI_MAX_INTERVAL; i++) {
+		unsigned int bw_added;
+		unsigned int largest_mps;
+		unsigned int interval_overhead;
+
+		/*
+		 * How many packets could we transmit in this interval?
+		 * If packets didn't fit in the previous interval, we will need
+		 * to transmit that many packets twice within this interval.
+		 */
+		packets_remaining = 2 * packets_remaining +
+			bw_table->interval_bw[i].num_packets;
+
+		/* Find the largest max packet size of this or the previous
+		 * interval.
+		 */
+		if (list_empty(&bw_table->interval_bw[i].endpoints))
+			largest_mps = 0;
+		else {
+			struct xhci_virt_ep *virt_ep;
+			struct list_head *ep_entry;
+
+			ep_entry = bw_table->interval_bw[i].endpoints.next;
+			virt_ep = list_entry(ep_entry,
+					struct xhci_virt_ep, bw_endpoint_list);
+			/* Convert to blocks, rounding up */
+			largest_mps = DIV_ROUND_UP(
+					virt_ep->bw_info.max_packet_size,
+					block_size);
+		}
+		if (largest_mps > packet_size)
+			packet_size = largest_mps;
+
+		/* Use the larger overhead of this or the previous interval. */
+		interval_overhead = xhci_get_largest_overhead(
+				&bw_table->interval_bw[i]);
+		if (interval_overhead > overhead)
+			overhead = interval_overhead;
+
+		/* How many packets can we evenly distribute across
+		 * (1 << (i + 1)) possible scheduling opportunities?
+		 */
+		packets_transmitted = packets_remaining >> (i + 1);
+
+		/* Add in the bandwidth used for those scheduled packets */
+		bw_added = packets_transmitted * (overhead + packet_size);
+
+		/* How many packets do we have remaining to transmit? */
+		packets_remaining = packets_remaining % (1 << (i + 1));
+
+		/* What largest max packet size should those packets have? */
+		/* If we've transmitted all packets, don't carry over the
+		 * largest packet size.
+		 */
+		if (packets_remaining == 0) {
+			packet_size = 0;
+			overhead = 0;
+		} else if (packets_transmitted > 0) {
+			/* Otherwise if we do have remaining packets, and we've
+			 * scheduled some packets in this interval, take the
+			 * largest max packet size from endpoints with this
+			 * interval.
+			 */
+			packet_size = largest_mps;
+			overhead = interval_overhead;
+		}
+		/* Otherwise carry over packet_size and overhead from the last
+		 * time we had a remainder.
+		 */
+		bw_used += bw_added;
+		if (bw_used > max_bandwidth) {
+			xhci_warn(xhci, "Not enough bandwidth. "
+					"Proposed: %u, Max: %u\n",
+				bw_used, max_bandwidth);
+			return -ENOMEM;
+		}
+	}
+	/*
+	 * Ok, we know we have some packets left over after even-handedly
+	 * scheduling interval 15.  We don't know which microframes they will
+	 * fit into, so we over-schedule and say they will be scheduled every
+	 * microframe.
+	 */
+	if (packets_remaining > 0)
+		bw_used += overhead + packet_size;
+
+	if (!virt_dev->tt_info && virt_dev->udev->speed == USB_SPEED_HIGH) {
+		unsigned int port_index = virt_dev->real_port - 1;
+
+		/* OK, we're manipulating a HS device attached to a
+		 * root port bandwidth domain.  Include the number of active TTs
+		 * in the bandwidth used.
+		 */
+		bw_used += TT_HS_OVERHEAD *
+			xhci->rh_bw[port_index].num_active_tts;
+	}
+
+	xhci_dbg(xhci, "Final bandwidth: %u, Limit: %u, Reserved: %u, "
+		"Available: %u " "percent\n",
+		bw_used, max_bandwidth, bw_reserved,
+		(max_bandwidth - bw_used - bw_reserved) * 100 /
+		max_bandwidth);
+
+	bw_used += bw_reserved;
+	if (bw_used > max_bandwidth) {
+		xhci_warn(xhci, "Not enough bandwidth. Proposed: %u, Max: %u\n",
+				bw_used, max_bandwidth);
+		return -ENOMEM;
+	}
+
+	bw_table->bw_used = bw_used;
+	return 0;
+}
+
+static bool xhci_is_async_ep(unsigned int ep_type)
+{
+	return (ep_type != ISOC_OUT_EP && ep_type != INT_OUT_EP &&
+					ep_type != ISOC_IN_EP &&
+					ep_type != INT_IN_EP);
+}
+
+static bool xhci_is_sync_in_ep(unsigned int ep_type)
+{
+	return (ep_type == ISOC_IN_EP || ep_type == INT_IN_EP);
+}
+
+static unsigned int xhci_get_ss_bw_consumed(struct xhci_bw_info *ep_bw)
+{
+	unsigned int mps = DIV_ROUND_UP(ep_bw->max_packet_size, SS_BLOCK);
+
+	if (ep_bw->ep_interval == 0)
+		return SS_OVERHEAD_BURST +
+			(ep_bw->mult * ep_bw->num_packets *
+					(SS_OVERHEAD + mps));
+	return DIV_ROUND_UP(ep_bw->mult * ep_bw->num_packets *
+				(SS_OVERHEAD + mps + SS_OVERHEAD_BURST),
+				1 << ep_bw->ep_interval);
+
+}
+
+void xhci_drop_ep_from_interval_table(struct xhci_hcd *xhci,
+		struct xhci_bw_info *ep_bw,
+		struct xhci_interval_bw_table *bw_table,
+		struct usb_device *udev,
+		struct xhci_virt_ep *virt_ep,
+		struct xhci_tt_bw_info *tt_info)
+{
+	struct xhci_interval_bw	*interval_bw;
+	int normalized_interval;
+
+	if (xhci_is_async_ep(ep_bw->type))
+		return;
+
+	if (udev->speed == USB_SPEED_SUPER) {
+		if (xhci_is_sync_in_ep(ep_bw->type))
+			xhci->devs[udev->slot_id]->bw_table->ss_bw_in -=
+				xhci_get_ss_bw_consumed(ep_bw);
+		else
+			xhci->devs[udev->slot_id]->bw_table->ss_bw_out -=
+				xhci_get_ss_bw_consumed(ep_bw);
+		return;
+	}
+
+	/* SuperSpeed endpoints never get added to intervals in the table, so
+	 * this check is only valid for HS/FS/LS devices.
+	 */
+	if (list_empty(&virt_ep->bw_endpoint_list))
+		return;
+	/* For LS/FS devices, we need to translate the interval expressed in
+	 * microframes to frames.
+	 */
+	if (udev->speed == USB_SPEED_HIGH)
+		normalized_interval = ep_bw->ep_interval;
+	else
+		normalized_interval = ep_bw->ep_interval - 3;
+
+	if (normalized_interval == 0)
+		bw_table->interval0_esit_payload -= ep_bw->max_esit_payload;
+	interval_bw = &bw_table->interval_bw[normalized_interval];
+	interval_bw->num_packets -= ep_bw->num_packets;
+	switch (udev->speed) {
+	case USB_SPEED_LOW:
+		interval_bw->overhead[LS_OVERHEAD_TYPE] -= 1;
+		break;
+	case USB_SPEED_FULL:
+		interval_bw->overhead[FS_OVERHEAD_TYPE] -= 1;
+		break;
+	case USB_SPEED_HIGH:
+		interval_bw->overhead[HS_OVERHEAD_TYPE] -= 1;
+		break;
+	case USB_SPEED_SUPER:
+	case USB_SPEED_UNKNOWN:
+	case USB_SPEED_WIRELESS:
+		/* Should never happen because only LS/FS/HS endpoints will get
+		 * added to the endpoint list.
+		 */
+		return;
+	}
+	if (tt_info)
+		tt_info->active_eps -= 1;
+	list_del_init(&virt_ep->bw_endpoint_list);
+}
+
+static void xhci_add_ep_to_interval_table(struct xhci_hcd *xhci,
+		struct xhci_bw_info *ep_bw,
+		struct xhci_interval_bw_table *bw_table,
+		struct usb_device *udev,
+		struct xhci_virt_ep *virt_ep,
+		struct xhci_tt_bw_info *tt_info)
+{
+	struct xhci_interval_bw	*interval_bw;
+	struct xhci_virt_ep *smaller_ep;
+	int normalized_interval;
+
+	if (xhci_is_async_ep(ep_bw->type))
+		return;
+
+	if (udev->speed == USB_SPEED_SUPER) {
+		if (xhci_is_sync_in_ep(ep_bw->type))
+			xhci->devs[udev->slot_id]->bw_table->ss_bw_in +=
+				xhci_get_ss_bw_consumed(ep_bw);
+		else
+			xhci->devs[udev->slot_id]->bw_table->ss_bw_out +=
+				xhci_get_ss_bw_consumed(ep_bw);
+		return;
+	}
+
+	/* For LS/FS devices, we need to translate the interval expressed in
+	 * microframes to frames.
+	 */
+	if (udev->speed == USB_SPEED_HIGH)
+		normalized_interval = ep_bw->ep_interval;
+	else
+		normalized_interval = ep_bw->ep_interval - 3;
+
+	if (normalized_interval == 0)
+		bw_table->interval0_esit_payload += ep_bw->max_esit_payload;
+	interval_bw = &bw_table->interval_bw[normalized_interval];
+	interval_bw->num_packets += ep_bw->num_packets;
+	switch (udev->speed) {
+	case USB_SPEED_LOW:
+		interval_bw->overhead[LS_OVERHEAD_TYPE] += 1;
+		break;
+	case USB_SPEED_FULL:
+		interval_bw->overhead[FS_OVERHEAD_TYPE] += 1;
+		break;
+	case USB_SPEED_HIGH:
+		interval_bw->overhead[HS_OVERHEAD_TYPE] += 1;
+		break;
+	case USB_SPEED_SUPER:
+	case USB_SPEED_UNKNOWN:
+	case USB_SPEED_WIRELESS:
+		/* Should never happen because only LS/FS/HS endpoints will get
+		 * added to the endpoint list.
+		 */
+		return;
+	}
+
+	if (tt_info)
+		tt_info->active_eps += 1;
+	/* Insert the endpoint into the list, largest max packet size first. */
+	list_for_each_entry(smaller_ep, &interval_bw->endpoints,
+			bw_endpoint_list) {
+		if (ep_bw->max_packet_size >=
+				smaller_ep->bw_info.max_packet_size) {
+			/* Add the new ep before the smaller endpoint */
+			list_add_tail(&virt_ep->bw_endpoint_list,
+					&smaller_ep->bw_endpoint_list);
+			return;
+		}
+	}
+	/* Add the new endpoint at the end of the list. */
+	list_add_tail(&virt_ep->bw_endpoint_list,
+			&interval_bw->endpoints);
+}
+
+void xhci_update_tt_active_eps(struct xhci_hcd *xhci,
+		struct xhci_virt_device *virt_dev,
+		int old_active_eps)
+{
+	struct xhci_root_port_bw_info *rh_bw_info;
+	if (!virt_dev->tt_info)
+		return;
+
+	rh_bw_info = &xhci->rh_bw[virt_dev->real_port - 1];
+	if (old_active_eps == 0 &&
+				virt_dev->tt_info->active_eps != 0) {
+		rh_bw_info->num_active_tts += 1;
+		rh_bw_info->bw_table.bw_used += TT_HS_OVERHEAD;
+	} else if (old_active_eps != 0 &&
+				virt_dev->tt_info->active_eps == 0) {
+		rh_bw_info->num_active_tts -= 1;
+		rh_bw_info->bw_table.bw_used -= TT_HS_OVERHEAD;
+	}
+}
+
+static int xhci_reserve_bandwidth(struct xhci_hcd *xhci,
+		struct xhci_virt_device *virt_dev,
+		struct xhci_container_ctx *in_ctx)
+{
+	struct xhci_bw_info ep_bw_info[31];
+	int i;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	int old_active_eps = 0;
+
+	if (virt_dev->tt_info)
+		old_active_eps = virt_dev->tt_info->active_eps;
+
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+
+	for (i = 0; i < 31; i++) {
+		if (!EP_IS_ADDED(ctrl_ctx, i) && !EP_IS_DROPPED(ctrl_ctx, i))
+			continue;
+
+		/* Make a copy of the BW info in case we need to revert this */
+		memcpy(&ep_bw_info[i], &virt_dev->eps[i].bw_info,
+				sizeof(ep_bw_info[i]));
+		/* Drop the endpoint from the interval table if the endpoint is
+		 * being dropped or changed.
+		 */
+		if (EP_IS_DROPPED(ctrl_ctx, i))
+			xhci_drop_ep_from_interval_table(xhci,
+					&virt_dev->eps[i].bw_info,
+					virt_dev->bw_table,
+					virt_dev->udev,
+					&virt_dev->eps[i],
+					virt_dev->tt_info);
+	}
+	/* Overwrite the information stored in the endpoints' bw_info */
+	xhci_update_bw_info(xhci, virt_dev->in_ctx, ctrl_ctx, virt_dev);
+	for (i = 0; i < 31; i++) {
+		/* Add any changed or added endpoints to the interval table */
+		if (EP_IS_ADDED(ctrl_ctx, i))
+			xhci_add_ep_to_interval_table(xhci,
+					&virt_dev->eps[i].bw_info,
+					virt_dev->bw_table,
+					virt_dev->udev,
+					&virt_dev->eps[i],
+					virt_dev->tt_info);
+	}
+
+	if (!xhci_check_bw_table(xhci, virt_dev, old_active_eps)) {
+		/* Ok, this fits in the bandwidth we have.
+		 * Update the number of active TTs.
+		 */
+		xhci_update_tt_active_eps(xhci, virt_dev, old_active_eps);
+		return 0;
+	}
+
+	/* We don't have enough bandwidth for this, revert the stored info. */
+	for (i = 0; i < 31; i++) {
+		if (!EP_IS_ADDED(ctrl_ctx, i) && !EP_IS_DROPPED(ctrl_ctx, i))
+			continue;
+
+		/* Drop the new copies of any added or changed endpoints from
+		 * the interval table.
+		 */
+		if (EP_IS_ADDED(ctrl_ctx, i)) {
+			xhci_drop_ep_from_interval_table(xhci,
+					&virt_dev->eps[i].bw_info,
+					virt_dev->bw_table,
+					virt_dev->udev,
+					&virt_dev->eps[i],
+					virt_dev->tt_info);
+		}
+		/* Revert the endpoint back to its old information */
+		memcpy(&virt_dev->eps[i].bw_info, &ep_bw_info[i],
+				sizeof(ep_bw_info[i]));
+		/* Add any changed or dropped endpoints back into the table */
+		if (EP_IS_DROPPED(ctrl_ctx, i))
+			xhci_add_ep_to_interval_table(xhci,
+					&virt_dev->eps[i].bw_info,
+					virt_dev->bw_table,
+					virt_dev->udev,
+					&virt_dev->eps[i],
+					virt_dev->tt_info);
+	}
+	return -ENOMEM;
+}
+
+
+/* Issue a configure endpoint command or evaluate context command
+ * and wait for it to finish.
+ */
+static int xhci_configure_endpoint(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct xhci_command *command,
+		bool ctx_change, bool must_succeed)
+{
+	int ret;
+	int timeleft;
+	unsigned long flags;
+	struct xhci_container_ctx *in_ctx;
+	struct completion *cmd_completion;
+	u32 *cmd_status;
+	struct xhci_virt_device *virt_dev;
+	union xhci_trb *cmd_trb;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	virt_dev = xhci->devs[udev->slot_id];
+
+	if (command)
+		in_ctx = command->in_ctx;
+	else
+		in_ctx = virt_dev->in_ctx;
+
+	if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK) &&
+			xhci_reserve_host_resources(xhci, in_ctx)) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_warn(xhci, "Not enough host resources, "
+				"active endpoint contexts = %u\n",
+				xhci->num_active_eps);
+		return -ENOMEM;
+	}
+	if ((xhci->quirks & XHCI_SW_BW_CHECKING) &&
+			xhci_reserve_bandwidth(xhci, virt_dev, in_ctx)) {
+		if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK))
+			xhci_free_host_resources(xhci, in_ctx);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_warn(xhci, "Not enough bandwidth\n");
+		return -ENOMEM;
+	}
+
+	if (command) {
+		cmd_completion = command->completion;
+		cmd_status = &command->status;
+		command->command_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+		list_add_tail(&command->cmd_list, &virt_dev->cmd_list);
+	} else {
+		cmd_completion = &virt_dev->cmd_completion;
+		cmd_status = &virt_dev->cmd_status;
+	}
+	init_completion(cmd_completion);
+
+	cmd_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+	if (!ctx_change)
+		ret = xhci_queue_configure_endpoint(xhci, in_ctx->dma,
+				udev->slot_id, must_succeed);
+	else
+		ret = xhci_queue_evaluate_context(xhci, in_ctx->dma,
+				udev->slot_id, must_succeed);
+	if (ret < 0) {
+		if (command)
+			list_del(&command->cmd_list);
+		if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK))
+			xhci_free_host_resources(xhci, in_ctx);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_dbg(xhci, "FIXME allocate a new ring segment\n");
+		return -ENOMEM;
+	}
+	xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* Wait for the configure endpoint command to complete */
+	timeleft = wait_for_completion_interruptible_timeout(
+			cmd_completion,
+			XHCI_CMD_DEFAULT_TIMEOUT);
+	if (timeleft <= 0) {
+		xhci_warn(xhci, "%s while waiting for %s command\n",
+				timeleft == 0 ? "Timeout" : "Signal",
+				ctx_change == 0 ?
+					"configure endpoint" :
+					"evaluate context");
+		/* cancel the configure endpoint command */
+		ret = xhci_cancel_cmd(xhci, command, cmd_trb);
+		if (ret < 0)
+			return ret;
+		return -ETIME;
+	}
+
+	if (!ctx_change)
+		ret = xhci_configure_endpoint_result(xhci, udev, cmd_status);
+	else
+		ret = xhci_evaluate_context_result(xhci, udev, cmd_status);
+
+	if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK)) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		/* If the command failed, remove the reserved resources.
+		 * Otherwise, clean up the estimate to include dropped eps.
+		 */
+		if (ret)
+			xhci_free_host_resources(xhci, in_ctx);
+		else
+			xhci_finish_resource_reservation(xhci, in_ctx);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	}
+	return ret;
+}
+
+/* Called after one or more calls to xhci_add_endpoint() or
+ * xhci_drop_endpoint().  If this call fails, the USB core is expected
+ * to call xhci_reset_bandwidth().
+ *
+ * Since we are in the middle of changing either configuration or
+ * installing a new alt setting, the USB core won't allow URBs to be
+ * enqueued for any endpoint on the old config or interface.  Nothing
+ * else should be touching the xhci->devs[slot_id] structure, so we
+ * don't need to take the xhci->lock for manipulating that.
+ */
+int xhci_check_bandwidth(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	int i;
+	int ret = 0;
+	struct xhci_hcd *xhci;
+	struct xhci_virt_device	*virt_dev;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_slot_ctx *slot_ctx;
+
+	ret = xhci_check_args(hcd, udev, NULL, 0, true, __func__);
+	if (ret <= 0)
+		return ret;
+	xhci = hcd_to_xhci(hcd);
+	if (xhci->xhc_state & XHCI_STATE_DYING)
+		return -ENODEV;
+
+	xhci_dbg(xhci, "%s called for udev %p\n", __func__, udev);
+	virt_dev = xhci->devs[udev->slot_id];
+
+	/* See section 4.6.6 - A0 = 1; A1 = D0 = D1 = 0 */
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, virt_dev->in_ctx);
+	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+	ctrl_ctx->add_flags &= cpu_to_le32(~EP0_FLAG);
+	ctrl_ctx->drop_flags &= cpu_to_le32(~(SLOT_FLAG | EP0_FLAG));
+
+	/* Don't issue the command if there's no endpoints to update. */
+	if (ctrl_ctx->add_flags == cpu_to_le32(SLOT_FLAG) &&
+			ctrl_ctx->drop_flags == 0)
+		return 0;
+
+	xhci_dbg(xhci, "New Input Control Context:\n");
+	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx);
+	xhci_dbg_ctx(xhci, virt_dev->in_ctx,
+		     LAST_CTX_TO_EP_NUM(le32_to_cpu(slot_ctx->dev_info)));
+
+	ret = xhci_configure_endpoint(xhci, udev, NULL,
+			false, false);
+	if (ret) {
+		/* Callee should call reset_bandwidth() */
+		return ret;
+	}
+
+	xhci_dbg(xhci, "Output context after successful config ep cmd:\n");
+	xhci_dbg_ctx(xhci, virt_dev->out_ctx,
+		     LAST_CTX_TO_EP_NUM(le32_to_cpu(slot_ctx->dev_info)));
+
+	/* Free any rings that were dropped, but not changed. */
+	for (i = 1; i < 31; ++i) {
+		if ((le32_to_cpu(ctrl_ctx->drop_flags) & (1 << (i + 1))) &&
+		    !(le32_to_cpu(ctrl_ctx->add_flags) & (1 << (i + 1))))
+			xhci_free_or_cache_endpoint_ring(xhci, virt_dev, i);
+	}
+	xhci_zero_in_ctx(xhci, virt_dev);
+	/*
+	 * Install any rings for completely new endpoints or changed endpoints,
+	 * and free or cache any old rings from changed endpoints.
+	 */
+	for (i = 1; i < 31; ++i) {
+		if (!virt_dev->eps[i].new_ring)
+			continue;
+		/* Only cache or free the old ring if it exists.
+		 * It may not if this is the first add of an endpoint.
+		 */
+		if (virt_dev->eps[i].ring) {
+			xhci_free_or_cache_endpoint_ring(xhci, virt_dev, i);
+		}
+		virt_dev->eps[i].ring = virt_dev->eps[i].new_ring;
+		virt_dev->eps[i].new_ring = NULL;
+	}
+
+	return ret;
+}
+
+void xhci_reset_bandwidth(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	struct xhci_hcd *xhci;
+	struct xhci_virt_device	*virt_dev;
+	int i, ret;
+
+	ret = xhci_check_args(hcd, udev, NULL, 0, true, __func__);
+	if (ret <= 0)
+		return;
+	xhci = hcd_to_xhci(hcd);
+
+	xhci_dbg(xhci, "%s called for udev %p\n", __func__, udev);
+	virt_dev = xhci->devs[udev->slot_id];
+	/* Free any rings allocated for added endpoints */
+	for (i = 0; i < 31; ++i) {
+		if (virt_dev->eps[i].new_ring) {
+			xhci_ring_free(xhci, virt_dev->eps[i].new_ring);
+			virt_dev->eps[i].new_ring = NULL;
+		}
+	}
+	xhci_zero_in_ctx(xhci, virt_dev);
+}
+
+static void xhci_setup_input_ctx_for_config_ep(struct xhci_hcd *xhci,
+		struct xhci_container_ctx *in_ctx,
+		struct xhci_container_ctx *out_ctx,
+		u32 add_flags, u32 drop_flags)
+{
+	struct xhci_input_control_ctx *ctrl_ctx;
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, in_ctx);
+	ctrl_ctx->add_flags = cpu_to_le32(add_flags);
+	ctrl_ctx->drop_flags = cpu_to_le32(drop_flags);
+	xhci_slot_copy(xhci, in_ctx, out_ctx);
+	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+
+	xhci_dbg(xhci, "Input Context:\n");
+	xhci_dbg_ctx(xhci, in_ctx, xhci_last_valid_endpoint(add_flags));
+}
+
+static void xhci_setup_input_ctx_for_quirk(struct xhci_hcd *xhci,
+		unsigned int slot_id, unsigned int ep_index,
+		struct xhci_dequeue_state *deq_state)
+{
+	struct xhci_container_ctx *in_ctx;
+	struct xhci_ep_ctx *ep_ctx;
+	u32 added_ctxs;
+	dma_addr_t addr;
+
+	xhci_endpoint_copy(xhci, xhci->devs[slot_id]->in_ctx,
+			xhci->devs[slot_id]->out_ctx, ep_index);
+	in_ctx = xhci->devs[slot_id]->in_ctx;
+	ep_ctx = xhci_get_ep_ctx(xhci, in_ctx, ep_index);
+	addr = xhci_trb_virt_to_dma(deq_state->new_deq_seg,
+			deq_state->new_deq_ptr);
+	if (addr == 0) {
+		xhci_warn(xhci, "WARN Cannot submit config ep after "
+				"reset ep command\n");
+		xhci_warn(xhci, "WARN deq seg = %p, deq ptr = %p\n",
+				deq_state->new_deq_seg,
+				deq_state->new_deq_ptr);
+		return;
+	}
+	ep_ctx->deq = cpu_to_le64(addr | deq_state->new_cycle_state);
+
+	added_ctxs = xhci_get_endpoint_flag_from_index(ep_index);
+	xhci_setup_input_ctx_for_config_ep(xhci, xhci->devs[slot_id]->in_ctx,
+			xhci->devs[slot_id]->out_ctx, added_ctxs, added_ctxs);
+}
+
+void xhci_cleanup_stalled_ring(struct xhci_hcd *xhci,
+		struct usb_device *udev, unsigned int ep_index)
+{
+	struct xhci_dequeue_state deq_state;
+	struct xhci_virt_ep *ep;
+
+	xhci_dbg(xhci, "Cleaning up stalled endpoint ring\n");
+	ep = &xhci->devs[udev->slot_id]->eps[ep_index];
+	/* We need to move the HW's dequeue pointer past this TD,
+	 * or it will attempt to resend it on the next doorbell ring.
+	 */
+	xhci_find_new_dequeue_state(xhci, udev->slot_id,
+			ep_index, ep->stopped_stream, ep->stopped_td,
+			&deq_state);
+
+	/* HW with the reset endpoint quirk will use the saved dequeue state to
+	 * issue a configure endpoint command later.
+	 */
+	if (!(xhci->quirks & XHCI_RESET_EP_QUIRK)) {
+		xhci_dbg(xhci, "Queueing new dequeue state\n");
+		xhci_queue_new_dequeue_state(xhci, udev->slot_id,
+				ep_index, ep->stopped_stream, &deq_state);
+	} else {
+		/* Better hope no one uses the input context between now and the
+		 * reset endpoint completion!
+		 * XXX: No idea how this hardware will react when stream rings
+		 * are enabled.
+		 */
+		xhci_dbg(xhci, "Setting up input context for "
+				"configure endpoint command\n");
+		xhci_setup_input_ctx_for_quirk(xhci, udev->slot_id,
+				ep_index, &deq_state);
+	}
+}
+
+/* Deal with stalled endpoints.  The core should have sent the control message
+ * to clear the halt condition.  However, we need to make the xHCI hardware
+ * reset its sequence number, since a device will expect a sequence number of
+ * zero after the halt condition is cleared.
+ * Context: in_interrupt
+ */
+void xhci_endpoint_reset(struct usb_hcd *hcd,
+		struct usb_host_endpoint *ep)
+{
+	struct xhci_hcd *xhci;
+	struct usb_device *udev;
+	unsigned int ep_index;
+	unsigned long flags;
+	int ret;
+	struct xhci_virt_ep *virt_ep;
+
+	xhci = hcd_to_xhci(hcd);
+	udev = (struct usb_device *) ep->hcpriv;
+	/* Called with a root hub endpoint (or an endpoint that wasn't added
+	 * with xhci_add_endpoint()
+	 */
+	if (!ep->hcpriv)
+		return;
+	ep_index = xhci_get_endpoint_index(&ep->desc);
+	virt_ep = &xhci->devs[udev->slot_id]->eps[ep_index];
+	if (!virt_ep->stopped_td) {
+		xhci_dbg(xhci, "Endpoint 0x%x not halted, refusing to reset.\n",
+				ep->desc.bEndpointAddress);
+		return;
+	}
+	if (usb_endpoint_xfer_control(&ep->desc)) {
+		xhci_dbg(xhci, "Control endpoint stall already handled.\n");
+		return;
+	}
+
+	xhci_dbg(xhci, "Queueing reset endpoint command\n");
+	spin_lock_irqsave(&xhci->lock, flags);
+	ret = xhci_queue_reset_ep(xhci, udev->slot_id, ep_index);
+	/*
+	 * Can't change the ring dequeue pointer until it's transitioned to the
+	 * stopped state, which is only upon a successful reset endpoint
+	 * command.  Better hope that last command worked!
+	 */
+	if (!ret) {
+		xhci_cleanup_stalled_ring(xhci, udev, ep_index);
+		kfree(virt_ep->stopped_td);
+		xhci_ring_cmd_db(xhci);
+	}
+	virt_ep->stopped_td = NULL;
+	virt_ep->stopped_trb = NULL;
+	virt_ep->stopped_stream = 0;
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	if (ret)
+		xhci_warn(xhci, "FIXME allocate a new ring segment\n");
+}
+
+static int xhci_check_streams_endpoint(struct xhci_hcd *xhci,
+		struct usb_device *udev, struct usb_host_endpoint *ep,
+		unsigned int slot_id)
+{
+	int ret;
+	unsigned int ep_index;
+	unsigned int ep_state;
+
+	if (!ep)
+		return -EINVAL;
+	ret = xhci_check_args(xhci_to_hcd(xhci), udev, ep, 1, true, __func__);
+	if (ret <= 0)
+		return -EINVAL;
+	if (ep->ss_ep_comp.bmAttributes == 0) {
+		xhci_warn(xhci, "WARN: SuperSpeed Endpoint Companion"
+				" descriptor for ep 0x%x does not support streams\n",
+				ep->desc.bEndpointAddress);
+		return -EINVAL;
+	}
+
+	ep_index = xhci_get_endpoint_index(&ep->desc);
+	ep_state = xhci->devs[slot_id]->eps[ep_index].ep_state;
+	if (ep_state & EP_HAS_STREAMS ||
+			ep_state & EP_GETTING_STREAMS) {
+		xhci_warn(xhci, "WARN: SuperSpeed bulk endpoint 0x%x "
+				"already has streams set up.\n",
+				ep->desc.bEndpointAddress);
+		xhci_warn(xhci, "Send email to xHCI maintainer and ask for "
+				"dynamic stream context array reallocation.\n");
+		return -EINVAL;
+	}
+	if (!list_empty(&xhci->devs[slot_id]->eps[ep_index].ring->td_list)) {
+		xhci_warn(xhci, "Cannot setup streams for SuperSpeed bulk "
+				"endpoint 0x%x; URBs are pending.\n",
+				ep->desc.bEndpointAddress);
+		return -EINVAL;
+	}
+	return 0;
+}
+
+static void xhci_calculate_streams_entries(struct xhci_hcd *xhci,
+		unsigned int *num_streams, unsigned int *num_stream_ctxs)
+{
+	unsigned int max_streams;
+
+	/* The stream context array size must be a power of two */
+	*num_stream_ctxs = roundup_pow_of_two(*num_streams);
+	/*
+	 * Find out how many primary stream array entries the host controller
+	 * supports.  Later we may use secondary stream arrays (similar to 2nd
+	 * level page entries), but that's an optional feature for xHCI host
+	 * controllers. xHCs must support at least 4 stream IDs.
+	 */
+	max_streams = HCC_MAX_PSA(xhci->hcc_params);
+	if (*num_stream_ctxs > max_streams) {
+		xhci_dbg(xhci, "xHCI HW only supports %u stream ctx entries.\n",
+				max_streams);
+		*num_stream_ctxs = max_streams;
+		*num_streams = max_streams;
+	}
+}
+
+/* Returns an error code if one of the endpoint already has streams.
+ * This does not change any data structures, it only checks and gathers
+ * information.
+ */
+static int xhci_calculate_streams_and_bitmask(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct usb_host_endpoint **eps, unsigned int num_eps,
+		unsigned int *num_streams, u32 *changed_ep_bitmask)
+{
+	unsigned int max_streams;
+	unsigned int endpoint_flag;
+	int i;
+	int ret;
+
+	for (i = 0; i < num_eps; i++) {
+		ret = xhci_check_streams_endpoint(xhci, udev,
+				eps[i], udev->slot_id);
+		if (ret < 0)
+			return ret;
+
+		max_streams = usb_ss_max_streams(&eps[i]->ss_ep_comp);
+		if (max_streams < (*num_streams - 1)) {
+			xhci_dbg(xhci, "Ep 0x%x only supports %u stream IDs.\n",
+					eps[i]->desc.bEndpointAddress,
+					max_streams);
+			*num_streams = max_streams+1;
+		}
+
+		endpoint_flag = xhci_get_endpoint_flag(&eps[i]->desc);
+		if (*changed_ep_bitmask & endpoint_flag)
+			return -EINVAL;
+		*changed_ep_bitmask |= endpoint_flag;
+	}
+	return 0;
+}
+
+static u32 xhci_calculate_no_streams_bitmask(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct usb_host_endpoint **eps, unsigned int num_eps)
+{
+	u32 changed_ep_bitmask = 0;
+	unsigned int slot_id;
+	unsigned int ep_index;
+	unsigned int ep_state;
+	int i;
+
+	slot_id = udev->slot_id;
+	if (!xhci->devs[slot_id])
+		return 0;
+
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		ep_state = xhci->devs[slot_id]->eps[ep_index].ep_state;
+		/* Are streams already being freed for the endpoint? */
+		if (ep_state & EP_GETTING_NO_STREAMS) {
+			xhci_warn(xhci, "WARN Can't disable streams for "
+					"endpoint 0x%x\n, "
+					"streams are being disabled already.",
+					eps[i]->desc.bEndpointAddress);
+			return 0;
+		}
+		/* Are there actually any streams to free? */
+		if (!(ep_state & EP_HAS_STREAMS) &&
+				!(ep_state & EP_GETTING_STREAMS)) {
+			xhci_warn(xhci, "WARN Can't disable streams for "
+					"endpoint 0x%x\n, "
+					"streams are already disabled!",
+					eps[i]->desc.bEndpointAddress);
+			xhci_warn(xhci, "WARN xhci_free_streams() called "
+					"with non-streams endpoint\n");
+			return 0;
+		}
+		changed_ep_bitmask |= xhci_get_endpoint_flag(&eps[i]->desc);
+	}
+	return changed_ep_bitmask;
+}
+
+/*
+ * The USB device drivers use this function (though the HCD interface in USB
+ * core) to prepare a set of bulk endpoints to use streams.  Streams are used to
+ * coordinate mass storage command queueing across multiple endpoints (basically
+ * a stream ID == a task ID).
+ *
+ * Setting up streams involves allocating the same size stream context array
+ * for each endpoint and issuing a configure endpoint command for all endpoints.
+ *
+ * Don't allow the call to succeed if one endpoint only supports one stream
+ * (which means it doesn't support streams at all).
+ *
+ * Drivers may get less stream IDs than they asked for, if the host controller
+ * hardware or endpoints claim they can't support the number of requested
+ * stream IDs.
+ */
+int xhci_alloc_streams(struct usb_hcd *hcd, struct usb_device *udev,
+		struct usb_host_endpoint **eps, unsigned int num_eps,
+		unsigned int num_streams, gfp_t mem_flags)
+{
+	int i, ret;
+	struct xhci_hcd *xhci;
+	struct xhci_virt_device *vdev;
+	struct xhci_command *config_cmd;
+	unsigned int ep_index;
+	unsigned int num_stream_ctxs;
+	unsigned long flags;
+	u32 changed_ep_bitmask = 0;
+
+	if (!eps)
+		return -EINVAL;
+
+	/* Add one to the number of streams requested to account for
+	 * stream 0 that is reserved for xHCI usage.
+	 */
+	num_streams += 1;
+	xhci = hcd_to_xhci(hcd);
+	xhci_dbg(xhci, "Driver wants %u stream IDs (including stream 0).\n",
+			num_streams);
+
+	config_cmd = xhci_alloc_command(xhci, true, true, mem_flags);
+	if (!config_cmd) {
+		xhci_dbg(xhci, "Could not allocate xHCI command structure.\n");
+		return -ENOMEM;
+	}
+
+	/* Check to make sure all endpoints are not already configured for
+	 * streams.  While we're at it, find the maximum number of streams that
+	 * all the endpoints will support and check for duplicate endpoints.
+	 */
+	spin_lock_irqsave(&xhci->lock, flags);
+	ret = xhci_calculate_streams_and_bitmask(xhci, udev, eps,
+			num_eps, &num_streams, &changed_ep_bitmask);
+	if (ret < 0) {
+		xhci_free_command(xhci, config_cmd);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return ret;
+	}
+	if (num_streams <= 1) {
+		xhci_warn(xhci, "WARN: endpoints can't handle "
+				"more than one stream.\n");
+		xhci_free_command(xhci, config_cmd);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return -EINVAL;
+	}
+	vdev = xhci->devs[udev->slot_id];
+	/* Mark each endpoint as being in transition, so
+	 * xhci_urb_enqueue() will reject all URBs.
+	 */
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		vdev->eps[ep_index].ep_state |= EP_GETTING_STREAMS;
+	}
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* Setup internal data structures and allocate HW data structures for
+	 * streams (but don't install the HW structures in the input context
+	 * until we're sure all memory allocation succeeded).
+	 */
+	xhci_calculate_streams_entries(xhci, &num_streams, &num_stream_ctxs);
+	xhci_dbg(xhci, "Need %u stream ctx entries for %u stream IDs.\n",
+			num_stream_ctxs, num_streams);
+
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		vdev->eps[ep_index].stream_info = xhci_alloc_stream_info(xhci,
+				num_stream_ctxs,
+				num_streams, mem_flags);
+		if (!vdev->eps[ep_index].stream_info)
+			goto cleanup;
+		/* Set maxPstreams in endpoint context and update deq ptr to
+		 * point to stream context array. FIXME
+		 */
+	}
+
+	/* Set up the input context for a configure endpoint command. */
+	for (i = 0; i < num_eps; i++) {
+		struct xhci_ep_ctx *ep_ctx;
+
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		ep_ctx = xhci_get_ep_ctx(xhci, config_cmd->in_ctx, ep_index);
+
+		xhci_endpoint_copy(xhci, config_cmd->in_ctx,
+				vdev->out_ctx, ep_index);
+		xhci_setup_streams_ep_input_ctx(xhci, ep_ctx,
+				vdev->eps[ep_index].stream_info);
+	}
+	/* Tell the HW to drop its old copy of the endpoint context info
+	 * and add the updated copy from the input context.
+	 */
+	xhci_setup_input_ctx_for_config_ep(xhci, config_cmd->in_ctx,
+			vdev->out_ctx, changed_ep_bitmask, changed_ep_bitmask);
+
+	/* Issue and wait for the configure endpoint command */
+	ret = xhci_configure_endpoint(xhci, udev, config_cmd,
+			false, false);
+
+	/* xHC rejected the configure endpoint command for some reason, so we
+	 * leave the old ring intact and free our internal streams data
+	 * structure.
+	 */
+	if (ret < 0)
+		goto cleanup;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		vdev->eps[ep_index].ep_state &= ~EP_GETTING_STREAMS;
+		xhci_dbg(xhci, "Slot %u ep ctx %u now has streams.\n",
+			 udev->slot_id, ep_index);
+		vdev->eps[ep_index].ep_state |= EP_HAS_STREAMS;
+	}
+	xhci_free_command(xhci, config_cmd);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* Subtract 1 for stream 0, which drivers can't use */
+	return num_streams - 1;
+
+cleanup:
+	/* If it didn't work, free the streams! */
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		xhci_free_stream_info(xhci, vdev->eps[ep_index].stream_info);
+		vdev->eps[ep_index].stream_info = NULL;
+		/* FIXME Unset maxPstreams in endpoint context and
+		 * update deq ptr to point to normal string ring.
+		 */
+		vdev->eps[ep_index].ep_state &= ~EP_GETTING_STREAMS;
+		vdev->eps[ep_index].ep_state &= ~EP_HAS_STREAMS;
+		xhci_endpoint_zero(xhci, vdev, eps[i]);
+	}
+	xhci_free_command(xhci, config_cmd);
+	return -ENOMEM;
+}
+
+/* Transition the endpoint from using streams to being a "normal" endpoint
+ * without streams.
+ *
+ * Modify the endpoint context state, submit a configure endpoint command,
+ * and free all endpoint rings for streams if that completes successfully.
+ */
+int xhci_free_streams(struct usb_hcd *hcd, struct usb_device *udev,
+		struct usb_host_endpoint **eps, unsigned int num_eps,
+		gfp_t mem_flags)
+{
+	int i, ret;
+	struct xhci_hcd *xhci;
+	struct xhci_virt_device *vdev;
+	struct xhci_command *command;
+	unsigned int ep_index;
+	unsigned long flags;
+	u32 changed_ep_bitmask;
+
+	xhci = hcd_to_xhci(hcd);
+	vdev = xhci->devs[udev->slot_id];
+
+	/* Set up a configure endpoint command to remove the streams rings */
+	spin_lock_irqsave(&xhci->lock, flags);
+	changed_ep_bitmask = xhci_calculate_no_streams_bitmask(xhci,
+			udev, eps, num_eps);
+	if (changed_ep_bitmask == 0) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return -EINVAL;
+	}
+
+	/* Use the xhci_command structure from the first endpoint.  We may have
+	 * allocated too many, but the driver may call xhci_free_streams() for
+	 * each endpoint it grouped into one call to xhci_alloc_streams().
+	 */
+	ep_index = xhci_get_endpoint_index(&eps[0]->desc);
+	command = vdev->eps[ep_index].stream_info->free_streams_command;
+	for (i = 0; i < num_eps; i++) {
+		struct xhci_ep_ctx *ep_ctx;
+
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		ep_ctx = xhci_get_ep_ctx(xhci, command->in_ctx, ep_index);
+		xhci->devs[udev->slot_id]->eps[ep_index].ep_state |=
+			EP_GETTING_NO_STREAMS;
+
+		xhci_endpoint_copy(xhci, command->in_ctx,
+				vdev->out_ctx, ep_index);
+		xhci_setup_no_streams_ep_input_ctx(xhci, ep_ctx,
+				&vdev->eps[ep_index]);
+	}
+	xhci_setup_input_ctx_for_config_ep(xhci, command->in_ctx,
+			vdev->out_ctx, changed_ep_bitmask, changed_ep_bitmask);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* Issue and wait for the configure endpoint command,
+	 * which must succeed.
+	 */
+	ret = xhci_configure_endpoint(xhci, udev, command,
+			false, true);
+
+	/* xHC rejected the configure endpoint command for some reason, so we
+	 * leave the streams rings intact.
+	 */
+	if (ret < 0)
+		return ret;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	for (i = 0; i < num_eps; i++) {
+		ep_index = xhci_get_endpoint_index(&eps[i]->desc);
+		xhci_free_stream_info(xhci, vdev->eps[ep_index].stream_info);
+		vdev->eps[ep_index].stream_info = NULL;
+		/* FIXME Unset maxPstreams in endpoint context and
+		 * update deq ptr to point to normal string ring.
+		 */
+		vdev->eps[ep_index].ep_state &= ~EP_GETTING_NO_STREAMS;
+		vdev->eps[ep_index].ep_state &= ~EP_HAS_STREAMS;
+	}
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	return 0;
+}
+
+/*
+ * Deletes endpoint resources for endpoints that were active before a Reset
+ * Device command, or a Disable Slot command.  The Reset Device command leaves
+ * the control endpoint intact, whereas the Disable Slot command deletes it.
+ *
+ * Must be called with xhci->lock held.
+ */
+void xhci_free_device_endpoint_resources(struct xhci_hcd *xhci,
+	struct xhci_virt_device *virt_dev, bool drop_control_ep)
+{
+	int i;
+	unsigned int num_dropped_eps = 0;
+	unsigned int drop_flags = 0;
+
+	for (i = (drop_control_ep ? 0 : 1); i < 31; i++) {
+		if (virt_dev->eps[i].ring) {
+			drop_flags |= 1 << i;
+			num_dropped_eps++;
+		}
+	}
+	xhci->num_active_eps -= num_dropped_eps;
+	if (num_dropped_eps)
+		xhci_dbg(xhci, "Dropped %u ep ctxs, flags = 0x%x, "
+				"%u now active.\n",
+				num_dropped_eps, drop_flags,
+				xhci->num_active_eps);
+}
+
+/*
+ * This submits a Reset Device Command, which will set the device state to 0,
+ * set the device address to 0, and disable all the endpoints except the default
+ * control endpoint.  The USB core should come back and call
+ * xhci_address_device(), and then re-set up the configuration.  If this is
+ * called because of a usb_reset_and_verify_device(), then the old alternate
+ * settings will be re-installed through the normal bandwidth allocation
+ * functions.
+ *
+ * Wait for the Reset Device command to finish.  Remove all structures
+ * associated with the endpoints that were disabled.  Clear the input device
+ * structure?  Cache the rings?  Reset the control endpoint 0 max packet size?
+ *
+ * If the virt_dev to be reset does not exist or does not match the udev,
+ * it means the device is lost, possibly due to the xHC restore error and
+ * re-initialization during S3/S4. In this case, call xhci_alloc_dev() to
+ * re-allocate the device.
+ */
+int xhci_discover_or_reset_device(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	int ret, i;
+	unsigned long flags;
+	struct xhci_hcd *xhci;
+	unsigned int slot_id;
+	struct xhci_virt_device *virt_dev;
+	struct xhci_command *reset_device_cmd;
+	int timeleft;
+	int last_freed_endpoint;
+	struct xhci_slot_ctx *slot_ctx;
+	int old_active_eps = 0;
+
+	ret = xhci_check_args(hcd, udev, NULL, 0, false, __func__);
+	if (ret <= 0)
+		return ret;
+	xhci = hcd_to_xhci(hcd);
+	slot_id = udev->slot_id;
+	virt_dev = xhci->devs[slot_id];
+	if (!virt_dev) {
+		xhci_dbg(xhci, "The device to be reset with slot ID %u does "
+				"not exist. Re-allocate the device\n", slot_id);
+		ret = xhci_alloc_dev(hcd, udev);
+		if (ret == 1)
+			return 0;
+		else
+			return -EINVAL;
+	}
+
+	if (virt_dev->udev != udev) {
+		/* If the virt_dev and the udev does not match, this virt_dev
+		 * may belong to another udev.
+		 * Re-allocate the device.
+		 */
+		xhci_dbg(xhci, "The device to be reset with slot ID %u does "
+				"not match the udev. Re-allocate the device\n",
+				slot_id);
+		ret = xhci_alloc_dev(hcd, udev);
+		if (ret == 1)
+			return 0;
+		else
+			return -EINVAL;
+	}
+
+	/* If device is not setup, there is no point in resetting it */
+	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx);
+	if (GET_SLOT_STATE(le32_to_cpu(slot_ctx->dev_state)) ==
+						SLOT_STATE_DISABLED)
+		return 0;
+
+	xhci_dbg(xhci, "Resetting device with slot ID %u\n", slot_id);
+	/* Allocate the command structure that holds the struct completion.
+	 * Assume we're in process context, since the normal device reset
+	 * process has to wait for the device anyway.  Storage devices are
+	 * reset as part of error handling, so use GFP_NOIO instead of
+	 * GFP_KERNEL.
+	 */
+	reset_device_cmd = xhci_alloc_command(xhci, false, true, GFP_NOIO);
+	if (!reset_device_cmd) {
+		xhci_dbg(xhci, "Couldn't allocate command structure.\n");
+		return -ENOMEM;
+	}
+
+	/* Attempt to submit the Reset Device command to the command ring */
+	spin_lock_irqsave(&xhci->lock, flags);
+	reset_device_cmd->command_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+
+	list_add_tail(&reset_device_cmd->cmd_list, &virt_dev->cmd_list);
+	ret = xhci_queue_reset_device(xhci, slot_id);
+	if (ret) {
+		xhci_dbg(xhci, "FIXME: allocate a command ring segment\n");
+		list_del(&reset_device_cmd->cmd_list);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		goto command_cleanup;
+	}
+	xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* Wait for the Reset Device command to finish */
+	timeleft = wait_for_completion_interruptible_timeout(
+			reset_device_cmd->completion,
+			USB_CTRL_SET_TIMEOUT);
+	if (timeleft <= 0) {
+		xhci_warn(xhci, "%s while waiting for reset device command\n",
+				timeleft == 0 ? "Timeout" : "Signal");
+		spin_lock_irqsave(&xhci->lock, flags);
+		/* The timeout might have raced with the event ring handler, so
+		 * only delete from the list if the item isn't poisoned.
+		 */
+		if (reset_device_cmd->cmd_list.next != LIST_POISON1)
+			list_del(&reset_device_cmd->cmd_list);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		ret = -ETIME;
+		goto command_cleanup;
+	}
+
+	/* The Reset Device command can't fail, according to the 0.95/0.96 spec,
+	 * unless we tried to reset a slot ID that wasn't enabled,
+	 * or the device wasn't in the addressed or configured state.
+	 */
+	ret = reset_device_cmd->status;
+	switch (ret) {
+	case COMP_EBADSLT: /* 0.95 completion code for bad slot ID */
+	case COMP_CTX_STATE: /* 0.96 completion code for same thing */
+		xhci_info(xhci, "Can't reset device (slot ID %u) in %s state\n",
+				slot_id,
+				xhci_get_slot_state(xhci, virt_dev->out_ctx));
+		xhci_info(xhci, "Not freeing device rings.\n");
+		/* Don't treat this as an error.  May change my mind later. */
+		ret = 0;
+		goto command_cleanup;
+	case COMP_SUCCESS:
+		xhci_dbg(xhci, "Successful reset device command.\n");
+		break;
+	default:
+		if (xhci_is_vendor_info_code(xhci, ret))
+			break;
+		xhci_warn(xhci, "Unknown completion code %u for "
+				"reset device command.\n", ret);
+		ret = -EINVAL;
+		goto command_cleanup;
+	}
+
+	/* Free up host controller endpoint resources */
+	if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK)) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		/* Don't delete the default control endpoint resources */
+		xhci_free_device_endpoint_resources(xhci, virt_dev, false);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	}
+
+	/* Everything but endpoint 0 is disabled, so free or cache the rings. */
+	last_freed_endpoint = 1;
+	for (i = 1; i < 31; ++i) {
+		struct xhci_virt_ep *ep = &virt_dev->eps[i];
+
+		if (ep->ep_state & EP_HAS_STREAMS) {
+			xhci_free_stream_info(xhci, ep->stream_info);
+			ep->stream_info = NULL;
+			ep->ep_state &= ~EP_HAS_STREAMS;
+		}
+
+		if (ep->ring) {
+			xhci_free_or_cache_endpoint_ring(xhci, virt_dev, i);
+			last_freed_endpoint = i;
+		}
+		if (!list_empty(&virt_dev->eps[i].bw_endpoint_list))
+			xhci_drop_ep_from_interval_table(xhci,
+					&virt_dev->eps[i].bw_info,
+					virt_dev->bw_table,
+					udev,
+					&virt_dev->eps[i],
+					virt_dev->tt_info);
+		xhci_clear_endpoint_bw_info(&virt_dev->eps[i].bw_info);
+	}
+	/* If necessary, update the number of active TTs on this root port */
+	xhci_update_tt_active_eps(xhci, virt_dev, old_active_eps);
+
+	xhci_dbg(xhci, "Output context after successful reset device cmd:\n");
+	xhci_dbg_ctx(xhci, virt_dev->out_ctx, last_freed_endpoint);
+	ret = 0;
+
+command_cleanup:
+	xhci_free_command(xhci, reset_device_cmd);
+	return ret;
+}
+
+/*
+ * At this point, the struct usb_device is about to go away, the device has
+ * disconnected, and all traffic has been stopped and the endpoints have been
+ * disabled.  Free any HC data structures associated with that device.
+ */
+void xhci_free_dev(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct xhci_virt_device *virt_dev;
+	unsigned long flags;
+	u32 state;
+	int i, ret;
+
+#ifndef CONFIG_USB_DEFAULT_PERSIST
+	/*
+	 * We called pm_runtime_get_noresume when the device was attached.
+	 * Decrement the counter here to allow controller to runtime suspend
+	 * if no devices remain.
+	 */
+	if (xhci->quirks & XHCI_RESET_ON_RESUME)
+		pm_runtime_put_noidle(hcd->self.controller);
+#endif
+
+	ret = xhci_check_args(hcd, udev, NULL, 0, true, __func__);
+	/* If the host is halted due to driver unload, we still need to free the
+	 * device.
+	 */
+	if (ret <= 0 && ret != -ENODEV)
+		return;
+
+	virt_dev = xhci->devs[udev->slot_id];
+
+	/* Stop any wayward timer functions (which may grab the lock) */
+	for (i = 0; i < 31; ++i) {
+		virt_dev->eps[i].ep_state &= ~EP_HALT_PENDING;
+		del_timer_sync(&virt_dev->eps[i].stop_cmd_timer);
+	}
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	/* Don't disable the slot if the host controller is dead. */
+	state = xhci_readl(xhci, &xhci->op_regs->status);
+	if (state == 0xffffffff || (xhci->xhc_state & XHCI_STATE_DYING) ||
+			(xhci->xhc_state & XHCI_STATE_HALTED)) {
+		xhci_free_virt_device(xhci, udev->slot_id);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return;
+	}
+
+	if (xhci_queue_slot_control(xhci, TRB_DISABLE_SLOT, udev->slot_id)) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_dbg(xhci, "FIXME: allocate a command ring segment\n");
+		return;
+	}
+	xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+	/*
+	 * Event command completion handler will free any data structures
+	 * associated with the slot.  XXX Can free sleep?
+	 */
+}
+
+/*
+ * Checks if we have enough host controller resources for the default control
+ * endpoint.
+ *
+ * Must be called with xhci->lock held.
+ */
+static int xhci_reserve_host_control_ep_resources(struct xhci_hcd *xhci)
+{
+	if (xhci->num_active_eps + 1 > xhci->limit_active_eps) {
+		xhci_dbg(xhci, "Not enough ep ctxs: "
+				"%u active, need to add 1, limit is %u.\n",
+				xhci->num_active_eps, xhci->limit_active_eps);
+		return -ENOMEM;
+	}
+	xhci->num_active_eps += 1;
+	xhci_dbg(xhci, "Adding 1 ep ctx, %u now active.\n",
+			xhci->num_active_eps);
+	return 0;
+}
+
+
+/*
+ * Returns 0 if the xHC ran out of device slots, the Enable Slot command
+ * timed out, or allocating memory failed.  Returns 1 on success.
+ */
+int xhci_alloc_dev(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	unsigned long flags;
+	int timeleft;
+	int ret;
+	union xhci_trb *cmd_trb;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	cmd_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+	ret = xhci_queue_slot_control(xhci, TRB_ENABLE_SLOT, 0);
+	if (ret) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_dbg(xhci, "FIXME: allocate a command ring segment\n");
+		return 0;
+	}
+	xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* XXX: how much time for xHC slot assignment? */
+	timeleft = wait_for_completion_interruptible_timeout(&xhci->addr_dev,
+			XHCI_CMD_DEFAULT_TIMEOUT);
+	if (timeleft <= 0) {
+		xhci_warn(xhci, "%s while waiting for a slot\n",
+				timeleft == 0 ? "Timeout" : "Signal");
+		/* cancel the enable slot request */
+		return xhci_cancel_cmd(xhci, NULL, cmd_trb);
+	}
+
+	if (!xhci->slot_id) {
+		xhci_err(xhci, "Error while assigning device slot ID\n");
+		return 0;
+	}
+
+	if ((xhci->quirks & XHCI_EP_LIMIT_QUIRK)) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		ret = xhci_reserve_host_control_ep_resources(xhci);
+		if (ret) {
+			spin_unlock_irqrestore(&xhci->lock, flags);
+			xhci_warn(xhci, "Not enough host resources, "
+					"active endpoint contexts = %u\n",
+					xhci->num_active_eps);
+			goto disable_slot;
+		}
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	}
+	/* Use GFP_NOIO, since this function can be called from
+	 * xhci_discover_or_reset_device(), which may be called as part of
+	 * mass storage driver error handling.
+	 */
+	if (!xhci_alloc_virt_device(xhci, xhci->slot_id, udev, GFP_NOIO)) {
+		xhci_warn(xhci, "Could not allocate xHCI USB device data structures\n");
+		goto disable_slot;
+	}
+	udev->slot_id = xhci->slot_id;
+
+#ifndef CONFIG_USB_DEFAULT_PERSIST
+	/*
+	 * If resetting upon resume, we can't put the controller into runtime
+	 * suspend if there is a device attached.
+	 */
+	if (xhci->quirks & XHCI_RESET_ON_RESUME)
+		pm_runtime_get_noresume(hcd->self.controller);
+#endif
+
+	/* Is this a LS or FS device under a HS hub? */
+	/* Hub or peripherial? */
+	return 1;
+
+disable_slot:
+	/* Disable slot, if we can do it without mem alloc */
+	spin_lock_irqsave(&xhci->lock, flags);
+	if (!xhci_queue_slot_control(xhci, TRB_DISABLE_SLOT, udev->slot_id))
+		xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+	return 0;
+}
+
+/*
+ * Issue an Address Device command (which will issue a SetAddress request to
+ * the device).
+ * We should be protected by the usb_address0_mutex in khubd's hub_port_init, so
+ * we should only issue and wait on one address command at the same time.
+ *
+ * We add one to the device address issued by the hardware because the USB core
+ * uses address 1 for the root hubs (even though they're not really devices).
+ */
+int xhci_address_device(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	unsigned long flags;
+	int timeleft;
+	struct xhci_virt_device *virt_dev;
+	int ret = 0;
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct xhci_slot_ctx *slot_ctx;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	u64 temp_64;
+	union xhci_trb *cmd_trb;
+
+	if (!udev->slot_id) {
+		xhci_dbg(xhci, "Bad Slot ID %d\n", udev->slot_id);
+		return -EINVAL;
+	}
+
+	virt_dev = xhci->devs[udev->slot_id];
+
+	if (WARN_ON(!virt_dev)) {
+		/*
+		 * In plug/unplug torture test with an NEC controller,
+		 * a zero-dereference was observed once due to virt_dev = 0.
+		 * Print useful debug rather than crash if it is observed again!
+		 */
+		xhci_warn(xhci, "Virt dev invalid for slot_id 0x%x!\n",
+			udev->slot_id);
+		return -EINVAL;
+	}
+
+	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->in_ctx);
+	/*
+	 * If this is the first Set Address since device plug-in or
+	 * virt_device realloaction after a resume with an xHCI power loss,
+	 * then set up the slot context.
+	 */
+	if (!slot_ctx->dev_info)
+		xhci_setup_addressable_virt_dev(xhci, udev);
+	/* Otherwise, update the control endpoint ring enqueue pointer. */
+	else
+		xhci_copy_ep0_dequeue_into_input_ctx(xhci, udev);
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, virt_dev->in_ctx);
+	ctrl_ctx->add_flags = cpu_to_le32(SLOT_FLAG | EP0_FLAG);
+	ctrl_ctx->drop_flags = 0;
+
+	xhci_dbg(xhci, "Slot ID %d Input Context:\n", udev->slot_id);
+	xhci_dbg_ctx(xhci, virt_dev->in_ctx, 2);
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	cmd_trb = xhci_find_next_enqueue(xhci->cmd_ring);
+	ret = xhci_queue_address_device(xhci, virt_dev->in_ctx->dma,
+					udev->slot_id);
+	if (ret) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		xhci_dbg(xhci, "FIXME: allocate a command ring segment\n");
+		return ret;
+	}
+	xhci_ring_cmd_db(xhci);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	/* ctrl tx can take up to 5 sec; XXX: need more time for xHC? */
+	timeleft = wait_for_completion_interruptible_timeout(&xhci->addr_dev,
+			XHCI_CMD_DEFAULT_TIMEOUT);
+	/* FIXME: From section 4.3.4: "Software shall be responsible for timing
+	 * the SetAddress() "recovery interval" required by USB and aborting the
+	 * command on a timeout.
+	 */
+	if (timeleft <= 0) {
+		xhci_warn(xhci, "%s while waiting for address device command\n",
+				timeleft == 0 ? "Timeout" : "Signal");
+		/* cancel the address device command */
+		ret = xhci_cancel_cmd(xhci, NULL, cmd_trb);
+		if (ret < 0)
+			return ret;
+		return -ETIME;
+	}
+
+	switch (virt_dev->cmd_status) {
+	case COMP_CTX_STATE:
+	case COMP_EBADSLT:
+		xhci_err(xhci, "Setup ERROR: address device command for slot %d.\n",
+				udev->slot_id);
+		ret = -EINVAL;
+		break;
+	case COMP_TX_ERR:
+		dev_warn(&udev->dev, "Device not responding to set address.\n");
+		ret = -EPROTO;
+		break;
+	case COMP_DEV_ERR:
+		dev_warn(&udev->dev, "ERROR: Incompatible device for address "
+				"device command.\n");
+		ret = -ENODEV;
+		break;
+	case COMP_SUCCESS:
+		xhci_dbg(xhci, "Successful Address Device command\n");
+		break;
+	default:
+		xhci_err(xhci, "ERROR: unexpected command completion "
+				"code 0x%x.\n", virt_dev->cmd_status);
+		xhci_dbg(xhci, "Slot ID %d Output Context:\n", udev->slot_id);
+		xhci_dbg_ctx(xhci, virt_dev->out_ctx, 2);
+		ret = -EINVAL;
+		break;
+	}
+	if (ret) {
+		return ret;
+	}
+	temp_64 = xhci_read_64(xhci, &xhci->op_regs->dcbaa_ptr);
+	xhci_dbg(xhci, "Op regs DCBAA ptr = %#016llx\n", temp_64);
+	xhci_dbg(xhci, "Slot ID %d dcbaa entry @%p = %#016llx\n",
+		 udev->slot_id,
+		 &xhci->dcbaa->dev_context_ptrs[udev->slot_id],
+		 (unsigned long long)
+		 le64_to_cpu(xhci->dcbaa->dev_context_ptrs[udev->slot_id]));
+	xhci_dbg(xhci, "Output Context DMA address = %#08llx\n",
+			(unsigned long long)virt_dev->out_ctx->dma);
+	xhci_dbg(xhci, "Slot ID %d Input Context:\n", udev->slot_id);
+	xhci_dbg_ctx(xhci, virt_dev->in_ctx, 2);
+	xhci_dbg(xhci, "Slot ID %d Output Context:\n", udev->slot_id);
+	xhci_dbg_ctx(xhci, virt_dev->out_ctx, 2);
+	/*
+	 * USB core uses address 1 for the roothubs, so we add one to the
+	 * address given back to us by the HC.
+	 */
+	slot_ctx = xhci_get_slot_ctx(xhci, virt_dev->out_ctx);
+	/* Use kernel assigned address for devices; store xHC assigned
+	 * address locally. */
+	virt_dev->address = (le32_to_cpu(slot_ctx->dev_state) & DEV_ADDR_MASK)
+		+ 1;
+	/* Zero the input context control for later use */
+	ctrl_ctx->add_flags = 0;
+	ctrl_ctx->drop_flags = 0;
+
+	xhci_dbg(xhci, "Internal device address = %d\n", virt_dev->address);
+
+	return 0;
+}
+
+/*
+ * Transfer the port index into real index in the HW port status
+ * registers. Caculate offset between the port's PORTSC register
+ * and port status base. Divide the number of per port register
+ * to get the real index. The raw port number bases 1.
+ */
+int xhci_find_raw_port_number(struct usb_hcd *hcd, int port1)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	__le32 __iomem *base_addr = &xhci->op_regs->port_status_base;
+	__le32 __iomem *addr;
+	int raw_port;
+
+	if (hcd->speed != HCD_USB3)
+		addr = xhci->usb2_ports[port1 - 1];
+	else
+		addr = xhci->usb3_ports[port1 - 1];
+
+	raw_port = (addr - base_addr)/NUM_PORT_REGS + 1;
+	return raw_port;
+}
+
+/*
+ * Issue an Evaluate Context command to change the Maximum Exit Latency in the
+ * slot context.  If that succeeds, store the new MEL in the xhci_virt_device.
+ */
+static int xhci_change_max_exit_latency(struct xhci_hcd *xhci,
+			struct usb_device *udev, u16 max_exit_latency)
+{
+	struct xhci_virt_device *virt_dev;
+	struct xhci_command *command;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_slot_ctx *slot_ctx;
+	unsigned long flags;
+	int ret;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	if (max_exit_latency == xhci->devs[udev->slot_id]->current_mel) {
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return 0;
+	}
+
+	/* Attempt to issue an Evaluate Context command to change the MEL. */
+	virt_dev = xhci->devs[udev->slot_id];
+	command = xhci->lpm_command;
+	xhci_slot_copy(xhci, command->in_ctx, virt_dev->out_ctx);
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, command->in_ctx);
+	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+	slot_ctx = xhci_get_slot_ctx(xhci, command->in_ctx);
+	slot_ctx->dev_info2 &= cpu_to_le32(~((u32) MAX_EXIT));
+	slot_ctx->dev_info2 |= cpu_to_le32(max_exit_latency);
+
+	xhci_dbg(xhci, "Set up evaluate context for LPM MEL change.\n");
+	xhci_dbg(xhci, "Slot %u Input Context:\n", udev->slot_id);
+	xhci_dbg_ctx(xhci, command->in_ctx, 0);
+
+	/* Issue and wait for the evaluate context command. */
+	ret = xhci_configure_endpoint(xhci, udev, command,
+			true, true);
+	xhci_dbg(xhci, "Slot %u Output Context:\n", udev->slot_id);
+	xhci_dbg_ctx(xhci, virt_dev->out_ctx, 0);
+
+	if (!ret) {
+		spin_lock_irqsave(&xhci->lock, flags);
+		virt_dev->current_mel = max_exit_latency;
+		spin_unlock_irqrestore(&xhci->lock, flags);
+	}
+	return ret;
+}
+
+#ifdef CONFIG_PM_RUNTIME
+
+/* BESL to HIRD Encoding array for USB2 LPM */
+static int xhci_besl_encoding[16] = {125, 150, 200, 300, 400, 500, 1000, 2000,
+	3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000};
+
+/* Calculate HIRD/BESL for USB2 PORTPMSC*/
+static int xhci_calculate_hird_besl(struct xhci_hcd *xhci,
+					struct usb_device *udev)
+{
+	int u2del, besl, besl_host;
+	int besl_device = 0;
+	u32 field;
+
+	u2del = HCS_U2_LATENCY(xhci->hcs_params3);
+	field = le32_to_cpu(udev->bos->ext_cap->bmAttributes);
+
+	if (field & USB_BESL_SUPPORT) {
+		for (besl_host = 0; besl_host < 16; besl_host++) {
+			if (xhci_besl_encoding[besl_host] >= u2del)
+				break;
+		}
+		/* Use baseline BESL value as default */
+		if (field & USB_BESL_BASELINE_VALID)
+			besl_device = USB_GET_BESL_BASELINE(field);
+		else if (field & USB_BESL_DEEP_VALID)
+			besl_device = USB_GET_BESL_DEEP(field);
+	} else {
+		if (u2del <= 50)
+			besl_host = 0;
+		else
+			besl_host = (u2del - 51) / 75 + 1;
+	}
+
+	besl = besl_host + besl_device;
+	if (besl > 15)
+		besl = 15;
+
+	return besl;
+}
+
+/* Calculate BESLD, L1 timeout and HIRDM for USB2 PORTHLPMC */
+static int xhci_calculate_usb2_hw_lpm_params(struct usb_device *udev)
+{
+	u32 field;
+	int l1;
+	int besld = 0;
+	int hirdm = 0;
+
+	field = le32_to_cpu(udev->bos->ext_cap->bmAttributes);
+
+	/* xHCI l1 is set in steps of 256us, xHCI 1.0 section 5.4.11.2 */
+	l1 = udev->l1_params.timeout / 256;
+
+	/* device has preferred BESLD */
+	if (field & USB_BESL_DEEP_VALID) {
+		besld = USB_GET_BESL_DEEP(field);
+		hirdm = 1;
+	}
+
+	return PORT_BESLD(besld) | PORT_L1_TIMEOUT(l1) | PORT_HIRDM(hirdm);
+}
+
+int xhci_set_usb2_hardware_lpm(struct usb_hcd *hcd,
+			struct usb_device *udev, int enable)
+{
+	struct xhci_hcd	*xhci = hcd_to_xhci(hcd);
+	__le32 __iomem	**port_array;
+	__le32 __iomem	*pm_addr, *hlpm_addr;
+	u32		pm_val, hlpm_val, field;
+	unsigned int	port_num;
+	unsigned long	flags;
+	int		hird, exit_latency;
+	int		ret;
+
+	if (hcd->speed == HCD_USB3 || !xhci->hw_lpm_support ||
+			!udev->lpm_capable)
+		return -EPERM;
+
+	if (!udev->parent || udev->parent->parent ||
+			udev->descriptor.bDeviceClass == USB_CLASS_HUB)
+		return -EPERM;
+
+	if (udev->usb2_hw_lpm_capable != 1)
+		return -EPERM;
+
+	spin_lock_irqsave(&xhci->lock, flags);
+
+	port_array = xhci->usb2_ports;
+	port_num = udev->portnum - 1;
+	pm_addr = port_array[port_num] + PORTPMSC;
+	pm_val = xhci_readl(xhci, pm_addr);
+	hlpm_addr = port_array[port_num] + PORTHLPMC;
+	field = le32_to_cpu(udev->bos->ext_cap->bmAttributes);
+
+	xhci_dbg(xhci, "%s port %d USB2 hardware LPM\n",
+			enable ? "enable" : "disable", port_num);
+
+	if (enable) {
+		/* Host supports BESL timeout instead of HIRD */
+		if (udev->usb2_hw_lpm_besl_capable) {
+			/* if device doesn't have a preferred BESL value use a
+			 * default one which works with mixed HIRD and BESL
+			 * systems. See XHCI_DEFAULT_BESL definition in xhci.h
+			 */
+			if ((field & USB_BESL_SUPPORT) &&
+			    (field & USB_BESL_BASELINE_VALID))
+				hird = USB_GET_BESL_BASELINE(field);
+			else
+				hird = udev->l1_params.besl;
+
+			exit_latency = xhci_besl_encoding[hird];
+			spin_unlock_irqrestore(&xhci->lock, flags);
+
+			/* USB 3.0 code dedicate one xhci->lpm_command->in_ctx
+			 * input context for link powermanagement evaluate
+			 * context commands. It is protected by hcd->bandwidth
+			 * mutex and is shared by all devices. We need to set
+			 * the max ext latency in USB 2 BESL LPM as well, so
+			 * use the same mutex and xhci_change_max_exit_latency()
+			 */
+			mutex_lock(hcd->bandwidth_mutex);
+			ret = xhci_change_max_exit_latency(xhci, udev,
+							   exit_latency);
+			mutex_unlock(hcd->bandwidth_mutex);
+
+			if (ret < 0)
+				return ret;
+			spin_lock_irqsave(&xhci->lock, flags);
+
+			hlpm_val = xhci_calculate_usb2_hw_lpm_params(udev);
+			xhci_writel(xhci, hlpm_val, hlpm_addr);
+			/* flush write */
+			xhci_readl(xhci, hlpm_addr);
+		} else {
+			hird = xhci_calculate_hird_besl(xhci, udev);
+		}
+
+		pm_val &= ~PORT_HIRD_MASK;
+		pm_val |= PORT_HIRD(hird) | PORT_RWE | PORT_L1DS(udev->slot_id);
+		xhci_writel(xhci, pm_val, pm_addr);
+		pm_val = xhci_readl(xhci, pm_addr);
+		pm_val |= PORT_HLE;
+		xhci_writel(xhci, pm_val, pm_addr);
+		/* flush write */
+		xhci_readl(xhci, pm_addr);
+	} else {
+		pm_val &= ~(PORT_HLE | PORT_RWE | PORT_HIRD_MASK | PORT_L1DS_MASK);
+		xhci_writel(xhci, pm_val, pm_addr);
+		/* flush write */
+		xhci_readl(xhci, pm_addr);
+		if (udev->usb2_hw_lpm_besl_capable) {
+			spin_unlock_irqrestore(&xhci->lock, flags);
+			mutex_lock(hcd->bandwidth_mutex);
+			xhci_change_max_exit_latency(xhci, udev, 0);
+			mutex_unlock(hcd->bandwidth_mutex);
+			return 0;
+		}
+	}
+
+	spin_unlock_irqrestore(&xhci->lock, flags);
+	return 0;
+}
+
+/* check if a usb2 port supports a given extened capability protocol
+ * only USB2 ports extended protocol capability values are cached.
+ * Return 1 if capability is supported
+ */
+static int xhci_check_usb2_port_capability(struct xhci_hcd *xhci, int port,
+					   unsigned capability)
+{
+	u32 port_offset, port_count;
+	int i;
+
+	for (i = 0; i < xhci->num_ext_caps; i++) {
+		if (xhci->ext_caps[i] & capability) {
+			/* port offsets starts at 1 */
+			port_offset = XHCI_EXT_PORT_OFF(xhci->ext_caps[i]) - 1;
+			port_count = XHCI_EXT_PORT_COUNT(xhci->ext_caps[i]);
+			if (port >= port_offset &&
+			    port < port_offset + port_count)
+				return 1;
+		}
+	}
+	return 0;
+}
+
+int xhci_update_device(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	struct xhci_hcd	*xhci = hcd_to_xhci(hcd);
+	int		portnum = udev->portnum - 1;
+
+	if (hcd->speed == HCD_USB3 || !xhci->sw_lpm_support ||
+			!udev->lpm_capable)
+		return 0;
+
+	/* we only support lpm for non-hub device connected to root hub yet */
+	if (!udev->parent || udev->parent->parent ||
+			udev->descriptor.bDeviceClass == USB_CLASS_HUB)
+		return 0;
+
+	if (xhci->hw_lpm_support == 1 &&
+			xhci_check_usb2_port_capability(
+				xhci, portnum, XHCI_HLC)) {
+		udev->usb2_hw_lpm_capable = 1;
+		udev->l1_params.timeout = XHCI_L1_TIMEOUT;
+		udev->l1_params.besl = XHCI_DEFAULT_BESL;
+		if (xhci_check_usb2_port_capability(xhci, portnum,
+					XHCI_BLC))
+			udev->usb2_hw_lpm_besl_capable = 1;
+	}
+
+	return 0;
+}
+
+#else
+
+int xhci_set_usb2_hardware_lpm(struct usb_hcd *hcd,
+				struct usb_device *udev, int enable)
+{
+	return 0;
+}
+
+int xhci_update_device(struct usb_hcd *hcd, struct usb_device *udev)
+{
+	return 0;
+}
+
+#endif /* CONFIG_PM_RUNTIME */
+
+/*---------------------- USB 3.0 Link PM functions ------------------------*/
+
+#ifdef CONFIG_PM
+/* Service interval in nanoseconds = 2^(bInterval - 1) * 125us * 1000ns / 1us */
+static unsigned long long xhci_service_interval_to_ns(
+		struct usb_endpoint_descriptor *desc)
+{
+	return (1ULL << (desc->bInterval - 1)) * 125 * 1000;
+}
+
+static u16 xhci_get_timeout_no_hub_lpm(struct usb_device *udev,
+		enum usb3_link_state state)
+{
+	unsigned long long sel;
+	unsigned long long pel;
+	unsigned int max_sel_pel;
+	char *state_name;
+
+	switch (state) {
+	case USB3_LPM_U1:
+		/* Convert SEL and PEL stored in nanoseconds to microseconds */
+		sel = DIV_ROUND_UP(udev->u1_params.sel, 1000);
+		pel = DIV_ROUND_UP(udev->u1_params.pel, 1000);
+		max_sel_pel = USB3_LPM_MAX_U1_SEL_PEL;
+		state_name = "U1";
+		break;
+	case USB3_LPM_U2:
+		sel = DIV_ROUND_UP(udev->u2_params.sel, 1000);
+		pel = DIV_ROUND_UP(udev->u2_params.pel, 1000);
+		max_sel_pel = USB3_LPM_MAX_U2_SEL_PEL;
+		state_name = "U2";
+		break;
+	default:
+		dev_warn(&udev->dev, "%s: Can't get timeout for non-U1 or U2 state.\n",
+				__func__);
+		return USB3_LPM_DISABLED;
+	}
+
+	if (sel <= max_sel_pel && pel <= max_sel_pel)
+		return USB3_LPM_DEVICE_INITIATED;
+
+	if (sel > max_sel_pel)
+		dev_dbg(&udev->dev, "Device-initiated %s disabled "
+				"due to long SEL %llu ms\n",
+				state_name, sel);
+	else
+		dev_dbg(&udev->dev, "Device-initiated %s disabled "
+				"due to long PEL %llu\n ms",
+				state_name, pel);
+	return USB3_LPM_DISABLED;
+}
+
+/* Returns the hub-encoded U1 timeout value.
+ * The U1 timeout should be the maximum of the following values:
+ *  - For control endpoints, U1 system exit latency (SEL) * 3
+ *  - For bulk endpoints, U1 SEL * 5
+ *  - For interrupt endpoints:
+ *    - Notification EPs, U1 SEL * 3
+ *    - Periodic EPs, max(105% of bInterval, U1 SEL * 2)
+ *  - For isochronous endpoints, max(105% of bInterval, U1 SEL * 2)
+ */
+static u16 xhci_calculate_intel_u1_timeout(struct usb_device *udev,
+		struct usb_endpoint_descriptor *desc)
+{
+	unsigned long long timeout_ns;
+	int ep_type;
+	int intr_type;
+
+	ep_type = usb_endpoint_type(desc);
+	switch (ep_type) {
+	case USB_ENDPOINT_XFER_CONTROL:
+		timeout_ns = udev->u1_params.sel * 3;
+		break;
+	case USB_ENDPOINT_XFER_BULK:
+		timeout_ns = udev->u1_params.sel * 5;
+		break;
+	case USB_ENDPOINT_XFER_INT:
+		intr_type = usb_endpoint_interrupt_type(desc);
+		if (intr_type == USB_ENDPOINT_INTR_NOTIFICATION) {
+			timeout_ns = udev->u1_params.sel * 3;
+			break;
+		}
+		/* Otherwise the calculation is the same as isoc eps */
+	case USB_ENDPOINT_XFER_ISOC:
+		timeout_ns = xhci_service_interval_to_ns(desc);
+		timeout_ns = DIV_ROUND_UP_ULL(timeout_ns * 105, 100);
+		if (timeout_ns < udev->u1_params.sel * 2)
+			timeout_ns = udev->u1_params.sel * 2;
+		break;
+	default:
+		return 0;
+	}
+
+	/* The U1 timeout is encoded in 1us intervals. */
+	timeout_ns = DIV_ROUND_UP_ULL(timeout_ns, 1000);
+	/* Don't return a timeout of zero, because that's USB3_LPM_DISABLED. */
+	if (timeout_ns == USB3_LPM_DISABLED)
+		timeout_ns++;
+
+	/* If the necessary timeout value is bigger than what we can set in the
+	 * USB 3.0 hub, we have to disable hub-initiated U1.
+	 */
+	if (timeout_ns <= USB3_LPM_U1_MAX_TIMEOUT)
+		return timeout_ns;
+	dev_dbg(&udev->dev, "Hub-initiated U1 disabled "
+			"due to long timeout %llu ms\n", timeout_ns);
+	return xhci_get_timeout_no_hub_lpm(udev, USB3_LPM_U1);
+}
+
+/* Returns the hub-encoded U2 timeout value.
+ * The U2 timeout should be the maximum of:
+ *  - 10 ms (to avoid the bandwidth impact on the scheduler)
+ *  - largest bInterval of any active periodic endpoint (to avoid going
+ *    into lower power link states between intervals).
+ *  - the U2 Exit Latency of the device
+ */
+static u16 xhci_calculate_intel_u2_timeout(struct usb_device *udev,
+		struct usb_endpoint_descriptor *desc)
+{
+	unsigned long long timeout_ns;
+	unsigned long long u2_del_ns;
+
+	timeout_ns = 10 * 1000 * 1000;
+
+	if ((usb_endpoint_xfer_int(desc) || usb_endpoint_xfer_isoc(desc)) &&
+			(xhci_service_interval_to_ns(desc) > timeout_ns))
+		timeout_ns = xhci_service_interval_to_ns(desc);
+
+	u2_del_ns = le16_to_cpu(udev->bos->ss_cap->bU2DevExitLat) * 1000ULL;
+	if (u2_del_ns > timeout_ns)
+		timeout_ns = u2_del_ns;
+
+	/* The U2 timeout is encoded in 256us intervals */
+	timeout_ns = DIV_ROUND_UP_ULL(timeout_ns, 256 * 1000);
+	/* If the necessary timeout value is bigger than what we can set in the
+	 * USB 3.0 hub, we have to disable hub-initiated U2.
+	 */
+	if (timeout_ns <= USB3_LPM_U2_MAX_TIMEOUT)
+		return timeout_ns;
+	dev_dbg(&udev->dev, "Hub-initiated U2 disabled "
+			"due to long timeout %llu ms\n", timeout_ns);
+	return xhci_get_timeout_no_hub_lpm(udev, USB3_LPM_U2);
+}
+
+static u16 xhci_call_host_update_timeout_for_endpoint(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct usb_endpoint_descriptor *desc,
+		enum usb3_link_state state,
+		u16 *timeout)
+{
+	if (state == USB3_LPM_U1) {
+		if (xhci->quirks & XHCI_INTEL_HOST)
+			return xhci_calculate_intel_u1_timeout(udev, desc);
+	} else {
+		if (xhci->quirks & XHCI_INTEL_HOST)
+			return xhci_calculate_intel_u2_timeout(udev, desc);
+	}
+
+	return USB3_LPM_DISABLED;
+}
+
+static int xhci_update_timeout_for_endpoint(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct usb_endpoint_descriptor *desc,
+		enum usb3_link_state state,
+		u16 *timeout)
+{
+	u16 alt_timeout;
+
+	alt_timeout = xhci_call_host_update_timeout_for_endpoint(xhci, udev,
+		desc, state, timeout);
+
+	/* If we found we can't enable hub-initiated LPM, or
+	 * the U1 or U2 exit latency was too high to allow
+	 * device-initiated LPM as well, just stop searching.
+	 */
+	if (alt_timeout == USB3_LPM_DISABLED ||
+			alt_timeout == USB3_LPM_DEVICE_INITIATED) {
+		*timeout = alt_timeout;
+		return -E2BIG;
+	}
+	if (alt_timeout > *timeout)
+		*timeout = alt_timeout;
+	return 0;
+}
+
+static int xhci_update_timeout_for_interface(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		struct usb_host_interface *alt,
+		enum usb3_link_state state,
+		u16 *timeout)
+{
+	int j;
+
+	for (j = 0; j < alt->desc.bNumEndpoints; j++) {
+		if (xhci_update_timeout_for_endpoint(xhci, udev,
+					&alt->endpoint[j].desc, state, timeout))
+			return -E2BIG;
+		continue;
+	}
+	return 0;
+}
+
+static int xhci_check_intel_tier_policy(struct usb_device *udev,
+		enum usb3_link_state state)
+{
+	struct usb_device *parent;
+	unsigned int num_hubs;
+
+	if (state == USB3_LPM_U2)
+		return 0;
+
+	/* Don't enable U1 if the device is on a 2nd tier hub or lower. */
+	for (parent = udev->parent, num_hubs = 0; parent->parent;
+			parent = parent->parent)
+		num_hubs++;
+
+	if (num_hubs < 2)
+		return 0;
+
+	dev_dbg(&udev->dev, "Disabling U1 link state for device"
+			" below second-tier hub.\n");
+	dev_dbg(&udev->dev, "Plug device into first-tier hub "
+			"to decrease power consumption.\n");
+	return -E2BIG;
+}
+
+static int xhci_check_tier_policy(struct xhci_hcd *xhci,
+		struct usb_device *udev,
+		enum usb3_link_state state)
+{
+	if (xhci->quirks & XHCI_INTEL_HOST)
+		return xhci_check_intel_tier_policy(udev, state);
+	return -EINVAL;
+}
+
+/* Returns the U1 or U2 timeout that should be enabled.
+ * If the tier check or timeout setting functions return with a non-zero exit
+ * code, that means the timeout value has been finalized and we shouldn't look
+ * at any more endpoints.
+ */
+static u16 xhci_calculate_lpm_timeout(struct usb_hcd *hcd,
+			struct usb_device *udev, enum usb3_link_state state)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct usb_host_config *config;
+	char *state_name;
+	int i;
+	u16 timeout = USB3_LPM_DISABLED;
+
+	if (state == USB3_LPM_U1)
+		state_name = "U1";
+	else if (state == USB3_LPM_U2)
+		state_name = "U2";
+	else {
+		dev_warn(&udev->dev, "Can't enable unknown link state %i\n",
+				state);
+		return timeout;
+	}
+
+	if (xhci_check_tier_policy(xhci, udev, state) < 0)
+		return timeout;
+
+	/* Gather some information about the currently installed configuration
+	 * and alternate interface settings.
+	 */
+	if (xhci_update_timeout_for_endpoint(xhci, udev, &udev->ep0.desc,
+			state, &timeout))
+		return timeout;
+
+	config = udev->actconfig;
+	if (!config)
+		return timeout;
+
+	for (i = 0; i < USB_MAXINTERFACES; i++) {
+		struct usb_driver *driver;
+		struct usb_interface *intf = config->interface[i];
+
+		if (!intf)
+			continue;
+
+		/* Check if any currently bound drivers want hub-initiated LPM
+		 * disabled.
+		 */
+		if (intf->dev.driver) {
+			driver = to_usb_driver(intf->dev.driver);
+			if (driver && driver->disable_hub_initiated_lpm) {
+				dev_dbg(&udev->dev, "Hub-initiated %s disabled "
+						"at request of driver %s\n",
+						state_name, driver->name);
+				return xhci_get_timeout_no_hub_lpm(udev, state);
+			}
+		}
+
+		/* Not sure how this could happen... */
+		if (!intf->cur_altsetting)
+			continue;
+
+		if (xhci_update_timeout_for_interface(xhci, udev,
+					intf->cur_altsetting,
+					state, &timeout))
+			return timeout;
+	}
+	return timeout;
+}
+
+static int calculate_max_exit_latency(struct usb_device *udev,
+		enum usb3_link_state state_changed,
+		u16 hub_encoded_timeout)
+{
+	unsigned long long u1_mel_us = 0;
+	unsigned long long u2_mel_us = 0;
+	unsigned long long mel_us = 0;
+	bool disabling_u1;
+	bool disabling_u2;
+	bool enabling_u1;
+	bool enabling_u2;
+
+	disabling_u1 = (state_changed == USB3_LPM_U1 &&
+			hub_encoded_timeout == USB3_LPM_DISABLED);
+	disabling_u2 = (state_changed == USB3_LPM_U2 &&
+			hub_encoded_timeout == USB3_LPM_DISABLED);
+
+	enabling_u1 = (state_changed == USB3_LPM_U1 &&
+			hub_encoded_timeout != USB3_LPM_DISABLED);
+	enabling_u2 = (state_changed == USB3_LPM_U2 &&
+			hub_encoded_timeout != USB3_LPM_DISABLED);
+
+	/* If U1 was already enabled and we're not disabling it,
+	 * or we're going to enable U1, account for the U1 max exit latency.
+	 */
+	if ((udev->u1_params.timeout != USB3_LPM_DISABLED && !disabling_u1) ||
+			enabling_u1)
+		u1_mel_us = DIV_ROUND_UP(udev->u1_params.mel, 1000);
+	if ((udev->u2_params.timeout != USB3_LPM_DISABLED && !disabling_u2) ||
+			enabling_u2)
+		u2_mel_us = DIV_ROUND_UP(udev->u2_params.mel, 1000);
+
+	if (u1_mel_us > u2_mel_us)
+		mel_us = u1_mel_us;
+	else
+		mel_us = u2_mel_us;
+	/* xHCI host controller max exit latency field is only 16 bits wide. */
+	if (mel_us > MAX_EXIT) {
+		dev_warn(&udev->dev, "Link PM max exit latency of %lluus "
+				"is too big.\n", mel_us);
+		return -E2BIG;
+	}
+	return mel_us;
+}
+
+/* Returns the USB3 hub-encoded value for the U1/U2 timeout. */
+int xhci_enable_usb3_lpm_timeout(struct usb_hcd *hcd,
+			struct usb_device *udev, enum usb3_link_state state)
+{
+	struct xhci_hcd	*xhci;
+	u16 hub_encoded_timeout;
+	int mel;
+	int ret;
+
+	xhci = hcd_to_xhci(hcd);
+	/* The LPM timeout values are pretty host-controller specific, so don't
+	 * enable hub-initiated timeouts unless the vendor has provided
+	 * information about their timeout algorithm.
+	 */
+	if (!xhci || !(xhci->quirks & XHCI_LPM_SUPPORT) ||
+			!xhci->devs[udev->slot_id])
+		return USB3_LPM_DISABLED;
+
+	hub_encoded_timeout = xhci_calculate_lpm_timeout(hcd, udev, state);
+	mel = calculate_max_exit_latency(udev, state, hub_encoded_timeout);
+	if (mel < 0) {
+		/* Max Exit Latency is too big, disable LPM. */
+		hub_encoded_timeout = USB3_LPM_DISABLED;
+		mel = 0;
+	}
+
+	ret = xhci_change_max_exit_latency(xhci, udev, mel);
+	if (ret)
+		return ret;
+	return hub_encoded_timeout;
+}
+
+int xhci_disable_usb3_lpm_timeout(struct usb_hcd *hcd,
+			struct usb_device *udev, enum usb3_link_state state)
+{
+	struct xhci_hcd	*xhci;
+	u16 mel;
+	int ret;
+
+	xhci = hcd_to_xhci(hcd);
+	if (!xhci || !(xhci->quirks & XHCI_LPM_SUPPORT) ||
+			!xhci->devs[udev->slot_id])
+		return 0;
+
+	mel = calculate_max_exit_latency(udev, state, USB3_LPM_DISABLED);
+	ret = xhci_change_max_exit_latency(xhci, udev, mel);
+	if (ret)
+		return ret;
+	return 0;
+}
+#else /* CONFIG_PM */
+
+int xhci_enable_usb3_lpm_timeout(struct usb_hcd *hcd,
+			struct usb_device *udev, enum usb3_link_state state)
+{
+	return USB3_LPM_DISABLED;
+}
+
+int xhci_disable_usb3_lpm_timeout(struct usb_hcd *hcd,
+			struct usb_device *udev, enum usb3_link_state state)
+{
+	return 0;
+}
+#endif	/* CONFIG_PM */
+
+/*-------------------------------------------------------------------------*/
+
+/* Once a hub descriptor is fetched for a device, we need to update the xHC's
+ * internal data structures for the device.
+ */
+int xhci_update_hub_device(struct usb_hcd *hcd, struct usb_device *hdev,
+			struct usb_tt *tt, gfp_t mem_flags)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	struct xhci_virt_device *vdev;
+	struct xhci_command *config_cmd;
+	struct xhci_input_control_ctx *ctrl_ctx;
+	struct xhci_slot_ctx *slot_ctx;
+	unsigned long flags;
+	unsigned think_time;
+	int ret;
+
+	/* Ignore root hubs */
+	if (!hdev->parent)
+		return 0;
+
+	vdev = xhci->devs[hdev->slot_id];
+	if (!vdev) {
+		xhci_warn(xhci, "Cannot update hub desc for unknown device.\n");
+		return -EINVAL;
+	}
+	config_cmd = xhci_alloc_command(xhci, true, true, mem_flags);
+	if (!config_cmd) {
+		xhci_dbg(xhci, "Could not allocate xHCI command structure.\n");
+		return -ENOMEM;
+	}
+
+	spin_lock_irqsave(&xhci->lock, flags);
+	if (hdev->speed == USB_SPEED_HIGH &&
+			xhci_alloc_tt_info(xhci, vdev, hdev, tt, GFP_ATOMIC)) {
+		xhci_dbg(xhci, "Could not allocate xHCI TT structure.\n");
+		xhci_free_command(xhci, config_cmd);
+		spin_unlock_irqrestore(&xhci->lock, flags);
+		return -ENOMEM;
+	}
+
+	xhci_slot_copy(xhci, config_cmd->in_ctx, vdev->out_ctx);
+	ctrl_ctx = xhci_get_input_control_ctx(xhci, config_cmd->in_ctx);
+	ctrl_ctx->add_flags |= cpu_to_le32(SLOT_FLAG);
+	slot_ctx = xhci_get_slot_ctx(xhci, config_cmd->in_ctx);
+	slot_ctx->dev_info |= cpu_to_le32(DEV_HUB);
+	if (tt->multi)
+		slot_ctx->dev_info |= cpu_to_le32(DEV_MTT);
+	if (xhci->hci_version > 0x95) {
+		xhci_dbg(xhci, "xHCI version %x needs hub "
+				"TT think time and number of ports\n",
+				(unsigned int) xhci->hci_version);
+		slot_ctx->dev_info2 |= cpu_to_le32(XHCI_MAX_PORTS(hdev->maxchild));
+		/* Set TT think time - convert from ns to FS bit times.
+		 * 0 = 8 FS bit times, 1 = 16 FS bit times,
+		 * 2 = 24 FS bit times, 3 = 32 FS bit times.
+		 *
+		 * xHCI 1.0: this field shall be 0 if the device is not a
+		 * High-spped hub.
+		 */
+		think_time = tt->think_time;
+		if (think_time != 0)
+			think_time = (think_time / 666) - 1;
+		if (xhci->hci_version < 0x100 || hdev->speed == USB_SPEED_HIGH)
+			slot_ctx->tt_info |=
+				cpu_to_le32(TT_THINK_TIME(think_time));
+	} else {
+		xhci_dbg(xhci, "xHCI version %x doesn't need hub "
+				"TT think time or number of ports\n",
+				(unsigned int) xhci->hci_version);
+	}
+	slot_ctx->dev_state = 0;
+	spin_unlock_irqrestore(&xhci->lock, flags);
+
+	xhci_dbg(xhci, "Set up %s for hub device.\n",
+			(xhci->hci_version > 0x95) ?
+			"configure endpoint" : "evaluate context");
+	xhci_dbg(xhci, "Slot %u Input Context:\n", hdev->slot_id);
+	xhci_dbg_ctx(xhci, config_cmd->in_ctx, 0);
+
+	/* Issue and wait for the configure endpoint or
+	 * evaluate context command.
+	 */
+	if (xhci->hci_version > 0x95)
+		ret = xhci_configure_endpoint(xhci, hdev, config_cmd,
+				false, false);
+	else
+		ret = xhci_configure_endpoint(xhci, hdev, config_cmd,
+				true, false);
+
+	xhci_dbg(xhci, "Slot %u Output Context:\n", hdev->slot_id);
+	xhci_dbg_ctx(xhci, vdev->out_ctx, 0);
+
+	xhci_free_command(xhci, config_cmd);
+	return ret;
+}
+
+int xhci_get_frame(struct usb_hcd *hcd)
+{
+	struct xhci_hcd *xhci = hcd_to_xhci(hcd);
+	/* EHCI mods by the periodic size.  Why? */
+	return xhci_readl(xhci, &xhci->run_regs->microframe_index) >> 3;
+}
+
+int xhci_gen_setup(struct usb_hcd *hcd, xhci_get_quirks_t get_quirks)
+{
+	struct xhci_hcd		*xhci;
+	struct device		*dev = hcd->self.controller;
+	int			retval;
+	u32			temp;
+
+	/* Accept arbitrarily long scatter-gather lists */
+	hcd->self.sg_tablesize = ~0;
+	/* XHCI controllers don't stop the ep queue on short packets :| */
+	hcd->self.no_stop_on_short = 1;
+
+	if (usb_hcd_is_primary_hcd(hcd)) {
+		xhci = kzalloc(sizeof(struct xhci_hcd), GFP_KERNEL);
+		if (!xhci)
+			return -ENOMEM;
+		*((struct xhci_hcd **) hcd->hcd_priv) = xhci;
+		xhci->main_hcd = hcd;
+		/* Mark the first roothub as being USB 2.0.
+		 * The xHCI driver will register the USB 3.0 roothub.
+		 */
+		hcd->speed = HCD_USB2;
+		hcd->self.root_hub->speed = USB_SPEED_HIGH;
+		/*
+		 * USB 2.0 roothub under xHCI has an integrated TT,
+		 * (rate matching hub) as opposed to having an OHCI/UHCI
+		 * companion controller.
+		 */
+		hcd->has_tt = 1;
+	} else {
+		/* xHCI private pointer was set in xhci_pci_probe for the second
+		 * registered roothub.
+		 */
+		xhci = hcd_to_xhci(hcd);
+		temp = xhci_readl(xhci, &xhci->cap_regs->hcc_params);
+		if (HCC_64BIT_ADDR(temp)) {
+			xhci_dbg(xhci, "Enabling 64-bit DMA addresses.\n");
+			dma_set_mask(hcd->self.controller, DMA_BIT_MASK(64));
+		} else {
+			dma_set_mask(hcd->self.controller, DMA_BIT_MASK(32));
+		}
+		return 0;
+	}
+
+	xhci->cap_regs = hcd->regs;
+	xhci->op_regs = hcd->regs +
+		HC_LENGTH(xhci_readl(xhci, &xhci->cap_regs->hc_capbase));
+	xhci->run_regs = hcd->regs +
+		(xhci_readl(xhci, &xhci->cap_regs->run_regs_off) & RTSOFF_MASK);
+	/* Cache read-only capability registers */
+	xhci->hcs_params1 = xhci_readl(xhci, &xhci->cap_regs->hcs_params1);
+	xhci->hcs_params2 = xhci_readl(xhci, &xhci->cap_regs->hcs_params2);
+	xhci->hcs_params3 = xhci_readl(xhci, &xhci->cap_regs->hcs_params3);
+	xhci->hcc_params = xhci_readl(xhci, &xhci->cap_regs->hc_capbase);
+	xhci->hci_version = HC_VERSION(xhci->hcc_params);
+	xhci->hcc_params = xhci_readl(xhci, &xhci->cap_regs->hcc_params);
+	xhci_print_registers(xhci);
+
+	get_quirks(dev, xhci);
+
+	/* In xhci controllers which follow xhci 1.0 spec gives a spurious
+	 * success event after a short transfer. This quirk will ignore such
+	 * spurious event.
+	 */
+	if (xhci->hci_version > 0x96)
+		xhci->quirks |= XHCI_SPURIOUS_SUCCESS;
+
+	/* Make sure the HC is halted. */
+	retval = xhci_halt(xhci);
+	if (retval)
+		goto error;
+
+	xhci_dbg(xhci, "Resetting HCD\n");
+	/* Reset the internal HC memory state and registers. */
+	retval = xhci_reset(xhci);
+	if (retval)
+		goto error;
+	xhci_dbg(xhci, "Reset complete\n");
+
+	temp = xhci_readl(xhci, &xhci->cap_regs->hcc_params);
+	if (HCC_64BIT_ADDR(temp)) {
+		xhci_dbg(xhci, "Enabling 64-bit DMA addresses.\n");
+		dma_set_mask(hcd->self.controller, DMA_BIT_MASK(64));
+	} else {
+		dma_set_mask(hcd->self.controller, DMA_BIT_MASK(32));
+	}
+
+	xhci_dbg(xhci, "Calling HCD init\n");
+	/* Initialize HCD and host controller data structures. */
+	retval = xhci_init(hcd);
+	if (retval)
+		goto error;
+	xhci_dbg(xhci, "Called HCD init\n");
+	return 0;
+error:
+	kfree(xhci);
+	return retval;
+}
+
+MODULE_DESCRIPTION(DRIVER_DESC);
+MODULE_AUTHOR(DRIVER_AUTHOR);
+MODULE_LICENSE("GPL");
+
+static int __init xhci_hcd_init(void)
+{
+	int retval;
+
+	retval = xhci_register_pci();
+	if (retval < 0) {
+		printk(KERN_DEBUG "Problem registering PCI driver.");
+		return retval;
+	}
+	retval = xhci_register_plat();
+	if (retval < 0) {
+		printk(KERN_DEBUG "Problem registering platform driver.");
+		goto unreg_pci;
+	}
+	/*
+	 * Check the compiler generated sizes of structures that must be laid
+	 * out in specific ways for hardware access.
+	 */
+	BUILD_BUG_ON(sizeof(struct xhci_doorbell_array) != 256*32/8);
+	BUILD_BUG_ON(sizeof(struct xhci_slot_ctx) != 8*32/8);
+	BUILD_BUG_ON(sizeof(struct xhci_ep_ctx) != 8*32/8);
+	/* xhci_device_control has eight fields, and also
+	 * embeds one xhci_slot_ctx and 31 xhci_ep_ctx
+	 */
+	BUILD_BUG_ON(sizeof(struct xhci_stream_ctx) != 4*32/8);
+	BUILD_BUG_ON(sizeof(union xhci_trb) != 4*32/8);
+	BUILD_BUG_ON(sizeof(struct xhci_erst_entry) != 4*32/8);
+	BUILD_BUG_ON(sizeof(struct xhci_cap_regs) != 7*32/8);
+	BUILD_BUG_ON(sizeof(struct xhci_intr_reg) != 8*32/8);
+	/* xhci_run_regs has eight fields and embeds 128 xhci_intr_regs */
+	BUILD_BUG_ON(sizeof(struct xhci_run_regs) != (8+8*128)*32/8);
+	return 0;
+unreg_pci:
+	xhci_unregister_pci();
+	return retval;
+}
+module_init(xhci_hcd_init);
+
+static void __exit xhci_hcd_cleanup(void)
+{
+	xhci_unregister_pci();
+	xhci_unregister_plat();
+}
+module_exit(xhci_hcd_cleanup);
diff --git a/drivers/usb/host/xhci.h b/drivers/usb/host/xhci.h
index a1e6dd9..2ccdf20 100644
--- a/drivers/usb/host/xhci.h
+++ b/drivers/usb/host/xhci.h
@@ -40,6 +40,11 @@
 /* Section 5.3.3 - MaxPorts */
 #define MAX_HC_PORTS		127
 
+/* platform data of vendor which can be pass to xhci_hcd */
+struct xhci_platform_data {
+	u32 quirks;
+};
+
 /*
  * xHCI register interface.
  * This corresponds to the eXtensible Host Controller Interface (xHCI)
@@ -1554,6 +1559,7 @@ struct xhci_hcd {
 #define XHCI_COMP_MODE_QUIRK	(1 << 14)
 #define XHCI_AVOID_BEI		(1 << 15)
 #define XHCI_PLAT		(1 << 16)
+#define XHCI_DISCONNECT_QUIRK	(1 << 17)
 	unsigned int		num_active_eps;
 	unsigned int		limit_active_eps;
 	/* There are two roothubs to keep track of bus suspend info for */
diff --git a/include/clocksource/arm_arch_timer.h b/include/clocksource/arm_arch_timer.h
index e6bc74f..6d26b40 100644
--- a/include/clocksource/arm_arch_timer.h
+++ b/include/clocksource/arm_arch_timer.h
@@ -30,6 +30,8 @@ enum arch_timer_reg {
 
 #define ARCH_TIMER_PHYS_ACCESS		0
 #define ARCH_TIMER_VIRT_ACCESS		1
+#define ARCH_TIMER_MEM_PHYS_ACCESS	2
+#define ARCH_TIMER_MEM_VIRT_ACCESS	3
 
 #define ARCH_TIMER_USR_PCT_ACCESS_EN	(1 << 0) /* physical counter */
 #define ARCH_TIMER_USR_VCT_ACCESS_EN	(1 << 1) /* virtual counter */
@@ -39,10 +41,12 @@ enum arch_timer_reg {
 #define ARCH_TIMER_USR_VT_ACCESS_EN	(1 << 8) /* virtual timer registers */
 #define ARCH_TIMER_USR_PT_ACCESS_EN	(1 << 9) /* physical timer registers */
 
+#define ARCH_TIMER_EVT_STREAM_FREQ	10000	/* 100us */
+
 #ifdef CONFIG_ARM_ARCH_TIMER
 
 extern u32 arch_timer_get_rate(void);
-extern u64 arch_timer_read_counter(void);
+extern u64 (*arch_timer_read_counter)(void);
 extern struct timecounter *arch_timer_get_timecounter(void);
 
 #else
diff --git a/include/dt-bindings/thermal/thermal.h b/include/dt-bindings/thermal/thermal.h
new file mode 100644
index 0000000..59822a9
--- /dev/null
+++ b/include/dt-bindings/thermal/thermal.h
@@ -0,0 +1,17 @@
+/*
+ * This header provides constants for most thermal bindings.
+ *
+ * Copyright (C) 2013 Texas Instruments
+ *	Eduardo Valentin <eduardo.valentin@ti.com>
+ *
+ * GPLv2 only
+ */
+
+#ifndef _DT_BINDINGS_THERMAL_THERMAL_H
+#define _DT_BINDINGS_THERMAL_THERMAL_H
+
+/* On cooling devices upper and lower limits */
+#define THERMAL_NO_LIMIT		(-1UL)
+
+#endif
+
diff --git a/include/linux/amba/bus.h b/include/linux/amba/bus.h
index b327a1b..b5ed1eb 100644
--- a/include/linux/amba/bus.h
+++ b/include/linux/amba/bus.h
@@ -21,7 +21,7 @@
 #include <linux/resource.h>
 #include <linux/regulator/consumer.h>
 
-#define AMBA_NR_IRQS	9
+#define AMBA_NR_IRQS	32
 #define AMBA_CID	0xb105f00d
 
 struct clk;
diff --git a/include/linux/arm-cci.h b/include/linux/arm-cci.h
new file mode 100644
index 0000000..79d6edf
--- /dev/null
+++ b/include/linux/arm-cci.h
@@ -0,0 +1,61 @@
+/*
+ * CCI cache coherent interconnect support
+ *
+ * Copyright (C) 2013 ARM Ltd.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program; if not, write to the Free Software
+ * Foundation, Inc., 59 Temple Place, Suite 330, Boston, MA  02111-1307  USA
+ */
+
+#ifndef __LINUX_ARM_CCI_H
+#define __LINUX_ARM_CCI_H
+
+#include <linux/errno.h>
+#include <linux/types.h>
+
+struct device_node;
+
+#ifdef CONFIG_ARM_CCI
+extern bool cci_probed(void);
+extern int cci_ace_get_port(struct device_node *dn);
+extern int cci_disable_port_by_cpu(u64 mpidr);
+extern int __cci_control_port_by_device(struct device_node *dn, bool enable);
+extern int __cci_control_port_by_index(u32 port, bool enable);
+#else
+static inline bool cci_probed(void) { return false; }
+static inline int cci_ace_get_port(struct device_node *dn)
+{
+	return -ENODEV;
+}
+static inline int cci_disable_port_by_cpu(u64 mpidr) { return -ENODEV; }
+static inline int __cci_control_port_by_device(struct device_node *dn,
+					       bool enable)
+{
+	return -ENODEV;
+}
+static inline int __cci_control_port_by_index(u32 port, bool enable)
+{
+	return -ENODEV;
+}
+#endif
+#define cci_disable_port_by_device(dev) \
+	__cci_control_port_by_device(dev, false)
+#define cci_enable_port_by_device(dev) \
+	__cci_control_port_by_device(dev, true)
+#define cci_disable_port_by_index(dev) \
+	__cci_control_port_by_index(dev, false)
+#define cci_enable_port_by_index(dev) \
+	__cci_control_port_by_index(dev, true)
+
+#endif
diff --git a/include/linux/cpu_cooling.h b/include/linux/cpu_cooling.h
index a5d52ee..c303d38 100644
--- a/include/linux/cpu_cooling.h
+++ b/include/linux/cpu_cooling.h
@@ -24,6 +24,7 @@
 #ifndef __CPU_COOLING_H__
 #define __CPU_COOLING_H__
 
+#include <linux/of.h>
 #include <linux/thermal.h>
 #include <linux/cpumask.h>
 
@@ -36,6 +37,24 @@ struct thermal_cooling_device *
 cpufreq_cooling_register(const struct cpumask *clip_cpus);
 
 /**
+ * of_cpufreq_cooling_register - create cpufreq cooling device based on DT.
+ * @np: a valid struct device_node to the cooling device device tree node.
+ * @clip_cpus: cpumask of cpus where the frequency constraints will happen
+ */
+#ifdef CONFIG_THERMAL_OF
+struct thermal_cooling_device *
+of_cpufreq_cooling_register(struct device_node *np,
+			    const struct cpumask *clip_cpus);
+#else
+static inline struct thermal_cooling_device *
+of_cpufreq_cooling_register(struct device_node *np,
+			    const struct cpumask *clip_cpus)
+{
+	return NULL;
+}
+#endif
+
+/**
  * cpufreq_cooling_unregister - function to remove cpufreq cooling device.
  * @cdev: thermal cooling device pointer.
  */
@@ -48,6 +67,12 @@ cpufreq_cooling_register(const struct cpumask *clip_cpus)
 {
 	return NULL;
 }
+static inline struct thermal_cooling_device *
+of_cpufreq_cooling_register(struct device_node *np,
+			    const struct cpumask *clip_cpus)
+{
+	return NULL;
+}
 static inline
 void cpufreq_cooling_unregister(struct thermal_cooling_device *cdev)
 {
diff --git a/include/linux/cpufreq.h b/include/linux/cpufreq.h
index b1d37c5..6f90155 100644
--- a/include/linux/cpufreq.h
+++ b/include/linux/cpufreq.h
@@ -324,6 +324,18 @@ cpufreq_verify_within_cpu_limits(struct cpufreq_policy *policy)
 static struct freq_attr _name =			\
 __ATTR(_name, 0444, show_##_name, NULL)
 
+#ifdef CONFIG_CPU_FREQ
+void cpufreq_suspend(void);
+void cpufreq_resume(void);
+#else
+static inline void cpufreq_suspend(void) {}
+static inline void cpufreq_resume(void) {}
+#endif
+
+/*********************************************************************
+ *                     CPUFREQ NOTIFIER INTERFACE                    *
+ *********************************************************************/
+
 #define cpufreq_freq_attr_ro_perm(_name, _perm)	\
 static struct freq_attr _name =			\
 __ATTR(_name, _perm, show_##_name, NULL)
diff --git a/include/linux/device.h b/include/linux/device.h
index eb2e096..96bce7c 100644
--- a/include/linux/device.h
+++ b/include/linux/device.h
@@ -26,6 +26,7 @@
 #include <linux/atomic.h>
 #include <linux/ratelimit.h>
 #include <linux/uidgid.h>
+#include <linux/gfp.h>
 #include <asm/device.h>
 
 struct device;
@@ -597,8 +598,24 @@ extern void devres_close_group(struct device *dev, void *id);
 extern void devres_remove_group(struct device *dev, void *id);
 extern int devres_release_group(struct device *dev, void *id);
 
-/* managed kzalloc/kfree for device drivers, no kmalloc, always use kzalloc */
-extern void *devm_kzalloc(struct device *dev, size_t size, gfp_t gfp);
+/* managed devm_k.alloc/kfree for device drivers */
+extern void *devm_kmalloc(struct device *dev, size_t size, gfp_t gfp);
+static inline void *devm_kzalloc(struct device *dev, size_t size, gfp_t gfp)
+{
+	return devm_kmalloc(dev, size, gfp | __GFP_ZERO);
+}
+static inline void *devm_kmalloc_array(struct device *dev,
+				       size_t n, size_t size, gfp_t flags)
+{
+	if (size != 0 && n > SIZE_MAX / size)
+		return NULL;
+	return devm_kmalloc(dev, n * size, flags);
+}
+static inline void *devm_kcalloc(struct device *dev,
+				 size_t n, size_t size, gfp_t flags)
+{
+	return devm_kmalloc_array(dev, n, size, flags | __GFP_ZERO);
+}
 extern void devm_kfree(struct device *dev, void *p);
 
 void __iomem *devm_ioremap_resource(struct device *dev, struct resource *res);
@@ -622,6 +639,8 @@ struct acpi_device;
 
 struct acpi_dev_node {
 #ifdef CONFIG_ACPI
+	/*FIXME_RCPL23*/
+	void 	*handle;
 	struct acpi_device *companion;
 #endif
 };
@@ -763,6 +782,14 @@ static inline struct device *kobj_to_dev(struct kobject *kobj)
 	return container_of(kobj, struct device, kobj);
 }
 
+#ifdef CONFIG_ACPI
+#define ACPI_HANDLE(dev)	((dev)->acpi_node.handle)
+#define ACPI_HANDLE_SET(dev, _handle_)	(dev)->acpi_node.handle = (_handle_)
+#else
+#define ACPI_HANDLE(dev)	(NULL)
+#define ACPI_HANDLE_SET(dev, _handle_)	do { } while (0)
+#endif
+
 /* Get the wakeup routines, which depend on struct device */
 #include <linux/pm_wakeup.h>
 
diff --git a/include/linux/dma-buf.h b/include/linux/dma-buf.h
index dfac5ed..b0cf8c0 100644
--- a/include/linux/dma-buf.h
+++ b/include/linux/dma-buf.h
@@ -30,6 +30,7 @@
 #include <linux/list.h>
 #include <linux/dma-mapping.h>
 #include <linux/fs.h>
+#include <linux/kds.h>
 
 struct device;
 struct dma_buf;
@@ -127,6 +128,7 @@ struct dma_buf {
 	void *vmap_ptr;
 	const char *exp_name;
 	struct list_head list_node;
+	struct kds_resource kds;
 	void *priv;
 };
 
@@ -162,6 +164,21 @@ static inline void get_dma_buf(struct dma_buf *dmabuf)
 	get_file(dmabuf->file);
 }
 
+/**
+ * get_dma_buf_kds_resource - get a KDS resource for this dma-buf
+ * @dmabuf:	[in]	pointer to dma_buf
+ *
+ * Returns a KDS resource that represents the dma-buf. This should be used by
+ * drivers to synchronize access to the buffer. Note that the caller should
+ * ensure that a reference to the dma-buf exists from the call to
+ * kds_async_wait until kds_resource_set_release is called.
+ */
+static inline struct kds_resource *
+	get_dma_buf_kds_resource(struct dma_buf *dmabuf)
+{
+	return &dmabuf->kds;
+}
+
 struct dma_buf_attachment *dma_buf_attach(struct dma_buf *dmabuf,
 							struct device *dev);
 void dma_buf_detach(struct dma_buf *dmabuf,
diff --git a/include/linux/dma-mapping.h b/include/linux/dma-mapping.h
index 8f7a2e8..48ef6f5 100644
--- a/include/linux/dma-mapping.h
+++ b/include/linux/dma-mapping.h
@@ -111,6 +111,16 @@ static inline int dma_set_mask_and_coherent(struct device *dev, u64 mask)
 	return rc;
 }
 
+/*
+ * Similar to the above, except it deals with the case where the device
+ * does not have dev->dma_mask appropriately setup.
+ */
+static inline int dma_coerce_mask_and_coherent(struct device *dev, u64 mask)
+{
+	dev->dma_mask = &dev->coherent_dma_mask;
+	return dma_set_mask_and_coherent(dev, mask);
+}
+
 extern u64 dma_get_required_mask(struct device *dev);
 
 static inline unsigned int dma_get_max_seg_size(struct device *dev)
diff --git a/include/linux/f_ipcu.h b/include/linux/f_ipcu.h
new file mode 100644
index 0000000..cdbc58c
--- /dev/null
+++ b/include/linux/f_ipcu.h
@@ -0,0 +1,21 @@
+#ifndef __F_IPCU_H
+#define __F_IPCU_H
+
+struct ipcu_mssg {
+	/*
+	 * TX : bit mask of destinations
+	 * RX : bit mask of source
+	 */
+	u32 mask;
+	/* Packet */
+	u32 data[9];
+};
+
+struct ipcu_client {
+	/* CPU i/f this client runs on */
+	int iface;
+	/* If client need to only receive messages */
+	bool ro;
+};
+
+#endif /* __F_IPCU_H */
diff --git a/include/linux/firmware.h b/include/linux/firmware.h
index e4279fe..e154c10 100644
--- a/include/linux/firmware.h
+++ b/include/linux/firmware.h
@@ -47,8 +47,6 @@ int request_firmware_nowait(
 	void (*cont)(const struct firmware *fw, void *context));
 
 void release_firmware(const struct firmware *fw);
-int cache_firmware(const char *name);
-int uncache_firmware(const char *name);
 #else
 static inline int request_firmware(const struct firmware **fw,
 				   const char *name,
@@ -68,15 +66,6 @@ static inline void release_firmware(const struct firmware *fw)
 {
 }
 
-static inline int cache_firmware(const char *name)
-{
-	return -ENOENT;
-}
-
-static inline int uncache_firmware(const char *name)
-{
-	return -EINVAL;
-}
 #endif
 
 #endif
diff --git a/include/linux/gpio-revision.h b/include/linux/gpio-revision.h
new file mode 100644
index 0000000..ecefc22
--- /dev/null
+++ b/include/linux/gpio-revision.h
@@ -0,0 +1,31 @@
+#ifndef __GPIO_REVISION_H__
+#define __GPIO_REVISION_H__
+
+#if defined(CONFIG_GPIO_REVISION)
+
+/* returns either -EPROBE_DEFER or the board revision index */
+int gpio_revision_get(void);
+
+/*
+ * if of_name property does not exist at of_node, return false
+ * if it does exist, return the nth bit of the u32 value of the property
+ * where n is the gpio_revision number that was detected
+ */
+bool gpio_revision_of_bitmap(struct device_node *of_node, const char *of_name);
+
+#else
+
+static inline int gpio_revision_get(void)
+{
+	return 0;
+}
+
+static inline bool gpio_revision_of_bitmap(
+		struct device_node *of_node, const char *of_name)
+{
+	return false;
+}
+
+#endif
+#endif
+
diff --git a/include/linux/hid.h b/include/linux/hid.h
index 4f8aa47..4c9a487 100644
--- a/include/linux/hid.h
+++ b/include/linux/hid.h
@@ -649,7 +649,7 @@ struct hid_driver {
 	int (*input_mapped)(struct hid_device *hdev,
 			struct hid_input *hidinput, struct hid_field *field,
 			struct hid_usage *usage, unsigned long **bit, int *max);
-	void (*input_configured)(struct hid_device *hdev,
+	int (*input_configured)(struct hid_device *hdev,
 				 struct hid_input *hidinput);
 	void (*feature_mapping)(struct hid_device *hdev,
 			struct hid_field *field,
diff --git a/include/linux/huge_mm.h b/include/linux/huge_mm.h
index a193bb3..3d36eb0 100644
--- a/include/linux/huge_mm.h
+++ b/include/linux/huge_mm.h
@@ -123,7 +123,7 @@ extern void __split_huge_page_pmd(struct vm_area_struct *vma,
 	} while (0)
 extern void split_huge_page_pmd_mm(struct mm_struct *mm, unsigned long address,
 		pmd_t *pmd);
-#if HPAGE_PMD_ORDER > MAX_ORDER
+#if HPAGE_PMD_ORDER >= MAX_ORDER
 #error "hugepages can't be allocated by the buddy allocator"
 #endif
 extern int hugepage_madvise(struct vm_area_struct *vma,
diff --git a/include/linux/iommu.h b/include/linux/iommu.h
index 450f1ad..00df857 100644
--- a/include/linux/iommu.h
+++ b/include/linux/iommu.h
@@ -23,9 +23,10 @@
 #include <linux/err.h>
 #include <linux/types.h>
 
-#define IOMMU_READ	(1)
-#define IOMMU_WRITE	(2)
-#define IOMMU_CACHE	(4) /* DMA cache coherency */
+#define IOMMU_READ	(1 << 0)
+#define IOMMU_WRITE	(1 << 1)
+#define IOMMU_CACHE	(1 << 2) /* DMA cache coherency */
+#define IOMMU_EXEC	(1 << 3)
 
 struct iommu_ops;
 struct iommu_group;
diff --git a/include/linux/irqchip/arm-gic.h b/include/linux/irqchip/arm-gic.h
index 442f016..cac496b 100644
--- a/include/linux/irqchip/arm-gic.h
+++ b/include/linux/irqchip/arm-gic.h
@@ -77,6 +77,7 @@ static inline void gic_init(unsigned int nr, int start,
 }
 
 void gic_send_sgi(unsigned int cpu_id, unsigned int irq);
+int gic_get_cpu_id(unsigned int cpu);
 void gic_migrate_target(unsigned int new_cpu_id);
 unsigned long gic_get_sgir_physaddr(void);
 
diff --git a/include/linux/irqchip/irq-mb8ac0300.h b/include/linux/irqchip/irq-mb8ac0300.h
new file mode 100644
index 0000000..8d51f22
--- /dev/null
+++ b/include/linux/irqchip/irq-mb8ac0300.h
@@ -0,0 +1,55 @@
+/*
+ * linux/arch/arm/mach-mb8ac0300/include/mach/exiu.h
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __MACH_EXIU_H
+#define __MACH_EXIU_H
+
+extern int exiu_irq_set_type(unsigned long irq_num, unsigned long type);
+
+#define EXIU_REG_EIMASK			0x00
+#define EXIU_REG_EISRCSEL		0x04
+#define EXIU_REG_EIREQSTA		0x08
+#define EXIU_REG_EIRAWREQSTA		0x0c
+#define EXIU_REG_EIREQCLR		0x10
+#define EXIU_REG_EILVL			0x14
+#define EXIU_REG_EIEDG			0x18
+#define EXIU_REG_EISIR			0x1c
+/* always last */
+#define EXIU_REG_EXTENT			0x20
+
+#define EXIU_EIMASK_ALL			0xffffffff
+
+#define EXIU_EISRCSEL_EXINT		0x0
+#define EXIU_EISRCSEL_SOFT		0x1
+#define EXIU_EISRCSEL_ALL_EXINT		0x00000000
+
+#define EXIU_EIREQCLR_ALL		0xffffffff
+
+#define EXIU_EILVL_LOW			0x0
+#define EXIU_EILVL_HIGH			0x1
+#define EXIU_EILVL_MASK			0x1
+#define EXIU_EILVL_ALL_HIGH		0xffffffff
+
+#define EXIU_EIEDG_LEVEL		0x0
+#define EXIU_EIEDG_EDGE			0x1
+#define EXIU_EIEDG_MASK			0x1
+#define EXIU_EIEDG_ALL_EDGE		0xffffffff
+
+#define EXTINT16_OFFSET			16
+
+#endif /* __MACH_EXIU_H */
diff --git a/include/linux/kds.h b/include/linux/kds.h
new file mode 100644
index 0000000..7514ac7
--- /dev/null
+++ b/include/linux/kds.h
@@ -0,0 +1,168 @@
+/*
+ *
+ * (C) COPYRIGHT 2012 ARM Limited. All rights reserved.
+ *
+ * This program is free software and is provided to you under the terms of the GNU General Public License version 2
+ * as published by the Free Software Foundation, and any use by you of this program is subject to the terms of such GNU licence.
+ *
+ * A copy of the licence is included with the program, and can also be obtained from Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
+ *
+ */
+
+
+
+#ifndef _KDS_H_
+#define _KDS_H_
+
+#include <linux/list.h>
+#include <linux/workqueue.h>
+
+#define KDS_WAIT_BLOCKING (ULONG_MAX)
+
+struct kds_resource_set;
+
+typedef void (*kds_callback_fn) (void *callback_parameter, void *callback_extra_parameter);
+
+struct kds_callback
+{
+	kds_callback_fn  user_cb; /* real cb */
+	int direct;               /* do direct or queued call? */
+	struct workqueue_struct *wq;
+};
+
+struct kds_link
+{
+	struct kds_resource_set *parent;
+	struct list_head         link;
+	unsigned long            state;
+};
+
+struct kds_resource
+{
+	struct kds_link waiters;
+};
+
+/* callback API */
+
+/* Initialize a callback object.
+ *
+ * Typically created per context or per hw resource.
+ *
+ * Callbacks can be performed directly if no nested locking can
+ * happen in the client.
+ *
+ * Nested locking can occur when a lock is held during the kds_async_waitall or
+ * kds_resource_set_release call. If the callback needs to take the same lock
+ * nested locking will happen.
+ *
+ * If nested locking could happen non-direct callbacks can be requested.
+ * Callbacks will then be called asynchronous to the triggering call.
+ */
+int kds_callback_init(struct kds_callback *cb, int direct, kds_callback_fn user_cb);
+
+/* Terminate the use of a callback object.
+ *
+ * If the callback object was set up as non-direct
+ * any pending callbacks will be flushed first.
+ * Note that to avoid a deadlock the lock callbacks needs
+ * can't be held when a callback object is terminated.
+ */
+void kds_callback_term(struct kds_callback *cb);
+
+
+/* resource object API */
+
+/* initialize a resource handle for a shared resource */
+void kds_resource_init(struct kds_resource * const resource);
+
+/*
+ * Will return 0 on success.
+ * If the resource is being used or waited -EBUSY is returned.
+ * The caller should NOT try to terminate a resource that could still have clients.
+ * After the function returns the resource is no longer known by kds.
+ */
+int kds_resource_term(struct kds_resource *resource);
+
+/* Asynchronous wait for a set of resources.
+ * Callback will be called when all resources are available.
+ * If all the resources was available the callback will be called before kds_async_waitall returns.
+ * So one must not hold any locks the callback code-flow can take when calling kds_async_waitall.
+ * Caller considered to own/use the resources until \a kds_rset_release is called.
+ * exclusive_access_bitmap is a bitmap where a high bit means exclusive access while a low bit means shared access.
+ * Use the Linux __set_bit API, where the index of the buffer to control is used as the bit index.
+ *
+ * Standard Linux error return value.
+ */
+int kds_async_waitall(
+		struct kds_resource_set ** const pprset,
+		struct kds_callback      *cb,
+		void                     *callback_parameter,
+		void                     *callback_extra_parameter,
+		int                       number_resources,
+		unsigned long            *exclusive_access_bitmap,
+		struct kds_resource     **resource_list);
+
+/* Synchronous wait for a set of resources.
+ * Function will return when one of these have happened:
+ * - all resources have been obtained
+ * - timeout lapsed while waiting
+ * - a signal was received while waiting
+ *
+ * To wait without a timeout, specify KDS_WAIT_BLOCKING for \a jifies_timeout, otherwise
+ * the timeout in jiffies. A zero timeout attempts to obtain all resources and returns
+ * immediately with a timeout if all resources could not be obtained.
+ *
+ * Caller considered to own/use the resources when the function returns.
+ * Caller must release the resources using \a kds_rset_release.
+ *
+ * Calling this function while holding already locked resources or other locking primitives is dangerous.
+ * One must if this is needed decide on a lock order of the resources and/or the other locking primitives
+ * and always take the resources/locking primitives in the specific order.
+ *
+ * Use the ERR_PTR framework to decode the return value.
+ * NULL = time out
+ * If IS_ERR then PTR_ERR gives:
+ *  ERESTARTSYS = signal received, retry call after signal
+ *  all other values = internal error, lock failed
+ * Other values  = successful wait, now the owner, must call kds_resource_set_release
+ */
+struct kds_resource_set *kds_waitall(
+		int                   number_resources,
+		unsigned long        *exclusive_access_bitmap,
+		struct kds_resource **resource_list,
+		unsigned long         jifies_timeout);
+
+/* Release resources after use.
+ * Caller must handle that other async callbacks will trigger,
+ * so must avoid holding any locks a callback will take.
+ *
+ * The function takes a pointer to your poiner to handle a race
+ * between a cancelation and a completion.
+ *
+ * If the caller can't guarantee that a race can't occur then
+ * the passed in pointer must be the same in both call paths
+ * to allow kds to manage the potential race.
+ */
+void kds_resource_set_release(struct kds_resource_set **pprset);
+
+/* Release resources after use and wait callbacks to complete.
+ * Caller must handle that other async callbacks will trigger,
+ * so must avoid holding any locks a callback will take.
+ *
+ * The function takes a pointer to your poiner to handle a race
+ * between a cancelation and a completion.
+ *
+ * If the caller can't guarantee that a race can't occur then
+ * the passed in pointer must be the same in both call paths
+ * to allow kds to manage the potential race.
+ *
+ * This should be used to cancel waits which are pending on a kds
+ * resource.
+ *
+ * It is a bug to call this from atomic contexts and from within
+ * a kds callback that now owns the kds_rseource.
+ */
+
+void kds_resource_set_release_sync(struct kds_resource_set **pprset);
+#endif /* _KDS_H_ */
diff --git a/include/linux/mailbox.h b/include/linux/mailbox.h
index 5161f63..69147bd 100644
--- a/include/linux/mailbox.h
+++ b/include/linux/mailbox.h
@@ -1,17 +1,24 @@
 /*
- * This program is free software; you can redistribute it and/or modify it
- * under the terms and conditions of the GNU General Public License,
- * version 2, as published by the Free Software Foundation.
- *
- * This program is distributed in the hope it will be useful, but WITHOUT
- * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
- * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
- * more details.
- *
- * You should have received a copy of the GNU General Public License along with
- * this program.  If not, see <http://www.gnu.org/licenses/>.
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
  */
 
-int pl320_ipc_transmit(u32 *data);
-int pl320_ipc_register_notifier(struct notifier_block *nb);
-int pl320_ipc_unregister_notifier(struct notifier_block *nb);
+#ifndef __MAILBOX_H
+#define __MAILBOX_H
+
+enum xfer_result {
+	XFER_OK = 0,
+	XFER_ERR,
+};
+
+typedef unsigned request_token_t;
+
+#ifdef CONFIG_DEBUG_MBOX
+#define mbox_dbg		printk
+#else
+#define mbox_dbg(fmt, args...)	do {} while (0)
+#endif
+
+#endif /* __MAILBOX_H */
+
diff --git a/include/linux/mailbox_client.h b/include/linux/mailbox_client.h
new file mode 100644
index 0000000..232fdc7
--- /dev/null
+++ b/include/linux/mailbox_client.h
@@ -0,0 +1,85 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __MAILBOX_CLIENT_H
+#define __MAILBOX_CLIENT_H
+
+#include <linux/mailbox.h>
+
+/**
+ * struct ipc_client - User of a mailbox
+ * @chan_name: the "controller:channel" this client wants
+ * @rxcb: atomic callback to provide client the data received
+ * @txcb: atomic callback to tell client of data transmission
+ * @tx_block: if the ipc_send_message should block until data is transmitted
+ * @tx_tout: Max block period in ms before TX is assumed failure
+ * @knows_txdone: if the client could run the TX state machine. Usually if
+ *    the client receives some ACK packet for transmission. Unused if the
+ *    controller already has TX_Done/RTR IRQ.
+ * @cntlr_data: Optional controller specific parameters during channel request
+ */
+struct ipc_client {
+	char *chan_name;
+	void (*rxcb)(void *data);
+	void (*txcb)(request_token_t t, enum xfer_result r);
+	bool tx_block;
+	unsigned long tx_tout;
+	bool knows_txdone;
+	void *cntlr_data;
+};
+
+/**
+ * The Client specifies it requirements and capabilities while asking for
+ * a channel/mailbox by name. It can't be called from atomic context.
+ * The channel is exclusively allocated and can't be used by another
+ * client before the owner calls ipc_free_channel.
+ */
+void *ipc_request_channel(struct ipc_client *cl);
+
+/**
+ * For client to submit data to the controller destined for a remote
+ * processor. If the client had set 'tx_block', the call will return
+ * either when the remote receives the data or when 'tx_tout' millisecs
+ * run out.
+ *  In non-blocking mode, the requests are buffered by the API and a
+ * non-zero token is returned for each queued request. If the queue
+ * was full the returned token will be 0. Upon failure or successful
+ * TX, the API calls 'txcb' from atomic context, from which the client
+ * could submit yet another request.
+ *  In blocking mode, 'txcb' is not called, effectively making the
+ * queue length 1. The returned value is 0 if TX timed out, some
+ * non-zero value upon success.
+ */
+request_token_t ipc_send_message(void *channel, void *data);
+
+/**
+ * The way for a client to run the TX state machine. This works
+ * only if the client sets 'knows_txdone' and the IPC controller
+ * don't get an IRQ for TX_Done.
+ */
+void ipc_client_txdone(void *channel, enum xfer_result r);
+
+/**
+ * The client relinquishes control of a mailbox by this call,
+ * make it available to other clients.
+ * The ipc_request/free_channel are light weight calls, so the
+ * client should avoid holding it when it doesn't need to
+ * transfer data.
+ */
+void ipc_free_channel(void *ch);
+
+/**
+ * The client make ask the API to be notified when a particular channel
+ * becomes available to be acquired again.
+ */
+int ipc_notify_chan_register(const char *name, struct notifier_block *nb);
+
+/**
+ * The client is no more interested in acquiring the channel.
+ */
+void ipc_notify_chan_unregister(const char *name, struct notifier_block *nb);
+
+#endif /* __MAILBOX_CLIENT_H */
diff --git a/include/linux/mailbox_controller.h b/include/linux/mailbox_controller.h
new file mode 100644
index 0000000..23b80e3
--- /dev/null
+++ b/include/linux/mailbox_controller.h
@@ -0,0 +1,102 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __MAILBOX_CONTROLLER_H
+#define __MAILBOX_CONTROLLER_H
+
+#include <linux/mailbox.h>
+
+/**
+ * struct ipc_link - s/w representation of a communication link
+ * @link_name: Literal name assigned to the link. Physically
+ *    identical channels may have the same name.
+ * @api_priv: hook for the API to map its private data on the link
+ *    Controller driver must not touch it.
+ */
+struct ipc_link {
+	char link_name[16];
+	void *api_priv;
+};
+
+/**
+ * struct ipc_link - s/w representation of a communication link
+ * @send_data: The API asks the IPC controller driver, in atomic
+ *    context try to transmit a message on the bus. Returns 0 if
+ *    data is accepted for transmission, -EBUSY while rejecting
+ *    if the remote hasn't yet read the last data sent. Actual
+ *    transmission of data is reported by the controller via
+ *    ipc_link_txdone (if it has some TX ACK irq). It must not
+ *    block.
+ * @startup: Called when a client requests the link. The controller
+ *    could ask clients for additional parameters of communication
+ *    to be provided via client's cntlr_data. This call may block.
+ *    After this call the Controller must forward any data received
+ *    on the link by calling ipc_link_received_data (which won't block)
+ * @shutdown: Called when a client relinquishes control of a link.
+ *    This call may block too. The controller must not forwared
+ *    any received data anymore.
+ * @last_tx_done: If the controller sets 'txdone_poll', the API calls
+ *    this to poll status of last TX. The controller must give priority
+ *    to IRQ method over polling and never set both txdone_poll and
+ *    txdone_irq. Only in polling mode 'send_data' is expected to
+ *    return -EBUSY. Used only if txdone_poll:=true && txdone_irq:=false
+ */
+struct ipc_link_ops {
+	int (*send_data)(struct ipc_link *link, void *data);
+	int (*startup)(struct ipc_link *link, void *params);
+	void (*shutdown)(struct ipc_link *link);
+	bool (*last_tx_done)(struct ipc_link *link);
+};
+
+/**
+ * struct ipc_controller - Controller of a class of communication links
+ * @controller_name: Literal name of the controller.
+ * @ops: Operators that work on each communication link
+ * @links: Null terminated array of links.
+ * @txdone_irq: Indicates if the controller can report to API when the
+ *    last transmitted data was read by the remote. Eg, if it has some
+ *    TX ACK irq.
+ * @txdone_poll: If the controller can read but not report the TX done.
+ *    Eg, is some register shows the TX status but no interrupt rises.
+ *    Ignored if 'txdone_irq' is set.
+ * @txpoll_period: If 'txdone_poll' is in effect, the API polls for
+ *    last TX's status after these many millisecs
+ */
+struct ipc_controller {
+	char controller_name[16];
+	struct ipc_link_ops *ops;
+	struct ipc_link **links;
+	bool txdone_irq;
+	bool txdone_poll;
+	unsigned txpoll_period;
+};
+
+/**
+ * The controller driver registers its communication links to the
+ * global pool managed by the API.
+ */
+int ipc_links_register(struct ipc_controller *ipc_con);
+
+/**
+ * After startup and before shutdown any data received on the link
+ * is pused to the API via atomic ipc_link_received_data() API.
+ * The controller should ACK the RX only after this call returns.
+ */
+void ipc_link_received_data(struct ipc_link *link, void *data);
+
+/**
+ * The controller the has IRQ for TX ACK calls this atomic API
+ * to tick the TX state machine. It works only if txdone_irq
+ * is set by the controller.
+ */
+void ipc_link_txdone(struct ipc_link *link, enum xfer_result r);
+
+/**
+ * Purge the links from the global pool maintained by the API.
+ */
+void ipc_links_unregister(struct ipc_controller *ipc_con);
+
+#endif /* __MAILBOX_CONTROLLER_H */
diff --git a/include/linux/mmc/host.h b/include/linux/mmc/host.h
index 3b0c33a..1b003c5 100644
--- a/include/linux/mmc/host.h
+++ b/include/linux/mmc/host.h
@@ -329,12 +329,17 @@ struct mmc_host {
 	int			claim_cnt;	/* "claim" nesting count */
 
 	struct delayed_work	detect;
+	//struct wake_lock	detect_wake_lock;
 	int			detect_change;	/* card detect flag */
 	struct mmc_slot		slot;
 
 	const struct mmc_bus_ops *bus_ops;	/* current bus driver */
 	unsigned int		bus_refs;	/* reference counter */
 
+	unsigned int		bus_resume_flags;
+#define MMC_BUSRESUME_MANUAL_RESUME	(1 << 0)
+#define MMC_BUSRESUME_NEEDS_RESUME	(1 << 1)
+
 	unsigned int		sdio_irqs;
 	struct task_struct	*sdio_irq_thread;
 	bool			sdio_irq_pending;
@@ -381,6 +386,18 @@ static inline void *mmc_priv(struct mmc_host *host)
 #define mmc_dev(x)	((x)->parent)
 #define mmc_classdev(x)	(&(x)->class_dev)
 #define mmc_hostname(x)	(dev_name(&(x)->class_dev))
+#define mmc_bus_needs_resume(host) ((host)->bus_resume_flags & MMC_BUSRESUME_NEEDS_RESUME)
+#define mmc_bus_manual_resume(host) ((host)->bus_resume_flags & MMC_BUSRESUME_MANUAL_RESUME)
+
+static inline void mmc_set_bus_resume_policy(struct mmc_host *host, int manual)
+{
+	if (manual)
+		host->bus_resume_flags |= MMC_BUSRESUME_MANUAL_RESUME;
+	else
+		host->bus_resume_flags &= ~MMC_BUSRESUME_MANUAL_RESUME;
+}
+
+extern int mmc_resume_bus(struct mmc_host *host);
 
 int mmc_suspend_host(struct mmc_host *);
 int mmc_resume_host(struct mmc_host *);
diff --git a/include/linux/mmc/sdhci.h b/include/linux/mmc/sdhci.h
index 40de6bf..d50daf2 100644
--- a/include/linux/mmc/sdhci.h
+++ b/include/linux/mmc/sdhci.h
@@ -96,8 +96,22 @@ struct sdhci_host {
 #define SDHCI_QUIRK2_NO_1_8_V				(1<<2)
 #define SDHCI_QUIRK2_PRESET_VALUE_BROKEN		(1<<3)
 #define SDHCI_QUIRK2_CARD_ON_NEEDS_BUS_ON		(1<<4)
+/* Controller has a non-standard host control register */
+#define SDHCI_QUIRK2_BROKEN_HOST_CONTROL		(1<<5)
 /* Controller does not support HS200 */
 #define SDHCI_QUIRK2_BROKEN_HS200			(1<<6)
+/* disable the block count for single block transactions */
+#define SDHCI_QUIRK2_SUPPORT_SINGLE			(1<<6)
+/* don't accept 3.0 or 3.1 voltage negotiation */
+#define SDHCI_QUIRK2_UNSUPPORT_3_0_V			(1<<7)
+/* Do a callback when switching voltages so do controller-specific actions */
+#define SDHCI_QUIRK2_VOLTAGE_SWITCH			(1<<8)
+/* forced tuned clock */
+#define SDHCI_QUIRK2_TUNING_WORK_AROUND			(1<<9)
+/* some controllers have errrata causing harmless extra command irqs */
+#define SDHCI_QUIRK2_IGNORE_UNEXPECTED_IRQ		(1<<10)
+/* retry to detect mmc device when resume */
+#define SDHCI_QUIRK2_RESUME_DETECT_RETRY		(1<<11)
 
 	int irq;		/* Device IRQ */
 	void __iomem *ioaddr;	/* Mapped address */
@@ -181,6 +195,10 @@ struct sdhci_host {
 #define SDHCI_TUNING_MODE_1	0
 	struct timer_list	tuning_timer;	/* Timer for tuning */
 
+	struct workqueue_struct *resume_detect_wq;	/* Workqueue for resume detection */
+	struct delayed_work resume_detect_work;		/* Delayed work for resume detection */
+	unsigned int		resume_detect_count;	/* Count for resume detection */
+
 	unsigned long private[0] ____cacheline_aligned;
 };
 #endif /* LINUX_MMC_SDHCI_H */
diff --git a/include/linux/mtd/spi-nor.h b/include/linux/mtd/spi-nor.h
new file mode 100644
index 0000000..63aeccf
--- /dev/null
+++ b/include/linux/mtd/spi-nor.h
@@ -0,0 +1,204 @@
+/*
+ * Copyright (C) 2014 Freescale Semiconductor, Inc.
+ *
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation; either version 2 of the License, or
+ * (at your option) any later version.
+ */
+
+#ifndef __LINUX_MTD_SPI_NOR_H
+#define __LINUX_MTD_SPI_NOR_H
+
+/*
+ * Note on opcode nomenclature: some opcodes have a format like
+ * SPINOR_OP_FUNCTION{4,}_x_y_z. The numbers x, y, and z stand for the number
+ * of I/O lines used for the opcode, address, and data (respectively). The
+ * FUNCTION has an optional suffix of '4', to represent an opcode which
+ * requires a 4-byte (32-bit) address.
+ */
+
+/* Flash opcodes. */
+#define SPINOR_OP_WREN		0x06	/* Write enable */
+#define SPINOR_OP_RDSR		0x05	/* Read status register */
+#define SPINOR_OP_WRSR		0x01	/* Write status register 1 byte */
+#define SPINOR_OP_READ		0x03	/* Read data bytes (low frequency) */
+#define SPINOR_OP_READ_FAST	0x0b	/* Read data bytes (high frequency) */
+#define SPINOR_OP_READ_1_1_2	0x3b	/* Read data bytes (Dual SPI) */
+#define SPINOR_OP_READ_1_1_4	0x6b	/* Read data bytes (Quad SPI) */
+#define SPINOR_OP_PP		0x02	/* Page program (up to 256 bytes) */
+#define SPINOR_OP_BE_4K		0x20	/* Erase 4KiB block */
+#define SPINOR_OP_BE_4K_PMC	0xd7	/* Erase 4KiB block on PMC chips */
+#define SPINOR_OP_BE_32K	0x52	/* Erase 32KiB block */
+#define SPINOR_OP_CHIP_ERASE	0xc7	/* Erase whole flash chip */
+#define SPINOR_OP_SE		0xd8	/* Sector erase (usually 64KiB) */
+#define SPINOR_OP_RDID		0x9f	/* Read JEDEC ID */
+#define SPINOR_OP_RDCR		0x35	/* Read configuration register */
+#define SPINOR_OP_RDFSR		0x70	/* Read flag status register */
+
+/* 4-byte address opcodes - used on Spansion and some Macronix flashes. */
+#define SPINOR_OP_READ4		0x13	/* Read data bytes (low frequency) */
+#define SPINOR_OP_READ4_FAST	0x0c	/* Read data bytes (high frequency) */
+#define SPINOR_OP_READ4_1_1_2	0x3c	/* Read data bytes (Dual SPI) */
+#define SPINOR_OP_READ4_1_1_4	0x6c	/* Read data bytes (Quad SPI) */
+#define SPINOR_OP_PP_4B		0x12	/* Page program (up to 256 bytes) */
+#define SPINOR_OP_SE_4B		0xdc	/* Sector erase (usually 64KiB) */
+
+/* Used for SST flashes only. */
+#define SPINOR_OP_BP		0x02	/* Byte program */
+#define SPINOR_OP_WRDI		0x04	/* Write disable */
+#define SPINOR_OP_AAI_WP	0xad	/* Auto address increment word program */
+
+/* Used for Macronix and Winbond flashes. */
+#define SPINOR_OP_EN4B		0xb7	/* Enter 4-byte mode */
+#define SPINOR_OP_EX4B		0xe9	/* Exit 4-byte mode */
+
+/* Used for Spansion flashes only. */
+#define SPINOR_OP_BRWR		0x17	/* Bank register write */
+
+/* Status Register bits. */
+#define SR_WIP			1	/* Write in progress */
+#define SR_WEL			2	/* Write enable latch */
+/* meaning of other SR_* bits may differ between vendors */
+#define SR_BP0			4	/* Block protect 0 */
+#define SR_BP1			8	/* Block protect 1 */
+#define SR_BP2			0x10	/* Block protect 2 */
+#define SR_SRWD			0x80	/* SR write protect */
+
+#define SR_QUAD_EN_MX		0x40	/* Macronix Quad I/O */
+
+/* Flag Status Register bits */
+#define FSR_READY		0x80
+
+/* Configuration Register bits. */
+#define CR_QUAD_EN_SPAN		0x2	/* Spansion Quad I/O */
+
+enum read_mode {
+	SPI_NOR_NORMAL = 0,
+	SPI_NOR_FAST,
+	SPI_NOR_DUAL,
+	SPI_NOR_QUAD,
+};
+
+/**
+ * struct spi_nor_xfer_cfg - Structure for defining a Serial Flash transfer
+ * @wren:		command for "Write Enable", or 0x00 for not required
+ * @cmd:		command for operation
+ * @cmd_pins:		number of pins to send @cmd (1, 2, 4)
+ * @addr:		address for operation
+ * @addr_pins:		number of pins to send @addr (1, 2, 4)
+ * @addr_width:		number of address bytes
+ *			(3,4, or 0 for address not required)
+ * @mode:		mode data
+ * @mode_pins:		number of pins to send @mode (1, 2, 4)
+ * @mode_cycles:	number of mode cycles (0 for mode not required)
+ * @dummy_cycles:	number of dummy cycles (0 for dummy not required)
+ */
+struct spi_nor_xfer_cfg {
+	u8		wren;
+	u8		cmd;
+	u8		cmd_pins;
+	u32		addr;
+	u8		addr_pins;
+	u8		addr_width;
+	u8		mode;
+	u8		mode_pins;
+	u8		mode_cycles;
+	u8		dummy_cycles;
+};
+
+#define SPI_NOR_MAX_CMD_SIZE	8
+enum spi_nor_ops {
+	SPI_NOR_OPS_READ = 0,
+	SPI_NOR_OPS_WRITE,
+	SPI_NOR_OPS_ERASE,
+	SPI_NOR_OPS_LOCK,
+	SPI_NOR_OPS_UNLOCK,
+};
+
+enum spi_nor_option_flags {
+	SNOR_F_USE_FSR		= BIT(0),
+};
+
+/**
+ * struct spi_nor - Structure for defining a the SPI NOR layer
+ * @mtd:		point to a mtd_info structure
+ * @lock:		the lock for the read/write/erase/lock/unlock operations
+ * @dev:		point to a spi device, or a spi nor controller device.
+ * @page_size:		the page size of the SPI NOR
+ * @addr_width:		number of address bytes
+ * @erase_opcode:	the opcode for erasing a sector
+ * @read_opcode:	the read opcode
+ * @read_dummy:		the dummy needed by the read operation
+ * @program_opcode:	the program opcode
+ * @flash_read:		the mode of the read
+ * @sst_write_second:	used by the SST write operation
+ * @flags:		flag options for the current SPI-NOR (SNOR_F_*)
+ * @cfg:		used by the read_xfer/write_xfer
+ * @cmd_buf:		used by the write_reg
+ * @prepare:		[OPTIONAL] do some preparations for the
+ *			read/write/erase/lock/unlock operations
+ * @unprepare:		[OPTIONAL] do some post work after the
+ *			read/write/erase/lock/unlock operations
+ * @read_xfer:		[OPTIONAL] the read fundamental primitive
+ * @write_xfer:		[OPTIONAL] the writefundamental primitive
+ * @read_reg:		[DRIVER-SPECIFIC] read out the register
+ * @write_reg:		[DRIVER-SPECIFIC] write data to the register
+ * @read:		[DRIVER-SPECIFIC] read data from the SPI NOR
+ * @write:		[DRIVER-SPECIFIC] write data to the SPI NOR
+ * @erase:		[DRIVER-SPECIFIC] erase a sector of the SPI NOR
+ *			at the offset @offs
+ * @priv:		the private data
+ */
+struct spi_nor {
+	struct mtd_info		*mtd;
+	struct mutex		lock;
+	struct device		*dev;
+	u32			page_size;
+	u8			addr_width;
+	u8			erase_opcode;
+	u8			read_opcode;
+	u8			read_dummy;
+	u8			program_opcode;
+	enum read_mode		flash_read;
+	bool			sst_write_second;
+	u32			flags;
+	struct spi_nor_xfer_cfg	cfg;
+	u8			cmd_buf[SPI_NOR_MAX_CMD_SIZE];
+
+	int (*prepare)(struct spi_nor *nor, enum spi_nor_ops ops);
+	void (*unprepare)(struct spi_nor *nor, enum spi_nor_ops ops);
+	int (*read_xfer)(struct spi_nor *nor, struct spi_nor_xfer_cfg *cfg,
+			 u8 *buf, size_t len);
+	int (*write_xfer)(struct spi_nor *nor, struct spi_nor_xfer_cfg *cfg,
+			  u8 *buf, size_t len);
+	int (*read_reg)(struct spi_nor *nor, u8 opcode, u8 *buf, int len);
+	int (*write_reg)(struct spi_nor *nor, u8 opcode, u8 *buf, int len,
+			int write_enable);
+
+	int (*read)(struct spi_nor *nor, loff_t from,
+			size_t len, size_t *retlen, u_char *read_buf);
+	void (*write)(struct spi_nor *nor, loff_t to,
+			size_t len, size_t *retlen, const u_char *write_buf);
+	int (*erase)(struct spi_nor *nor, loff_t offs);
+
+	void *priv;
+};
+
+/**
+ * spi_nor_scan() - scan the SPI NOR
+ * @nor:	the spi_nor structure
+ * @name:	the chip type name
+ * @mode:	the read mode supported by the driver
+ *
+ * The drivers can use this fuction to scan the SPI NOR.
+ * In the scanning, it will try to get all the necessary information to
+ * fill the mtd_info{} and the spi_nor{}.
+ *
+ * The chip type name can be provided through the @name parameter.
+ *
+ * Return: 0 for success, others for failure.
+ */
+int spi_nor_scan(struct spi_nor *nor, const char *name, enum read_mode mode);
+
+#endif
diff --git a/include/linux/of.h b/include/linux/of.h
index ae6e390..8b3ed80 100644
--- a/include/linux/of.h
+++ b/include/linux/of.h
@@ -347,6 +347,8 @@ const char *of_prop_next_string(struct property *prop, const char *cur);
 
 int of_device_is_stdout_path(struct device_node *dn);
 
+int of_device_is_stdout_path(struct device_node *dn);
+
 #else /* CONFIG_OF */
 
 static inline const char* of_node_full_name(struct device_node *np)
@@ -463,6 +465,12 @@ static inline struct device_node *of_get_cpu_node(int cpu,
 	return NULL;
 }
 
+static inline struct device_node *of_get_cpu_node(int cpu,
+					unsigned int *thread)
+{
+	return NULL;
+}
+
 static inline int of_property_read_u64(const struct device_node *np,
 				       const char *propname, u64 *out_value)
 {
@@ -521,6 +529,11 @@ static inline int of_device_is_stdout_path(struct device_node *dn)
 	return 0;
 }
 
+static inline int of_device_is_stdout_path(struct device_node *dn)
+{
+	return 0;
+}
+
 #define of_match_ptr(_ptr)	NULL
 #define of_match_node(_matches, _node)	NULL
 #define of_property_for_each_u32(np, propname, prop, p, u) \
@@ -649,4 +662,4 @@ extern void proc_device_tree_update_prop(struct proc_dir_entry *pde,
 					 struct property *oldprop);
 #endif
 
-#endif /* _LINUX_OF_H */
+#endif /* _LINUX_OF_H */
\ No newline at end of file
diff --git a/include/linux/of_address.h b/include/linux/of_address.h
index 0506eb5..4c2e6f2 100644
--- a/include/linux/of_address.h
+++ b/include/linux/of_address.h
@@ -4,6 +4,36 @@
 #include <linux/errno.h>
 #include <linux/of.h>
 
+struct of_pci_range_parser {
+	struct device_node *node;
+	const __be32 *range;
+	const __be32 *end;
+	int np;
+	int pna;
+};
+
+struct of_pci_range {
+	u32 pci_space;
+	u64 pci_addr;
+	u64 cpu_addr;
+	u64 size;
+	u32 flags;
+};
+
+#define for_each_of_pci_range(parser, range) \
+	for (; of_pci_range_parser_one(parser, range);)
+
+static inline void of_pci_range_to_resource(struct of_pci_range *range,
+					    struct device_node *np,
+					    struct resource *res)
+{
+	res->flags = range->flags;
+	res->start = range->cpu_addr;
+	res->end = range->cpu_addr + range->size - 1;
+	res->parent = res->child = res->sibling = NULL;
+	res->name = np->full_name;
+}
+
 #ifdef CONFIG_OF_ADDRESS
 extern u64 of_translate_address(struct device_node *np, const __be32 *addr);
 extern bool of_can_translate_address(struct device_node *dev);
@@ -27,6 +57,11 @@ static inline unsigned long pci_address_to_pio(phys_addr_t addr) { return -1; }
 #define pci_address_to_pio pci_address_to_pio
 #endif
 
+extern int of_pci_range_parser_init(struct of_pci_range_parser *parser,
+			struct device_node *node);
+extern struct of_pci_range *of_pci_range_parser_one(
+					struct of_pci_range_parser *parser,
+					struct of_pci_range *range);
 #else /* CONFIG_OF_ADDRESS */
 #ifndef of_address_to_resource
 static inline int of_address_to_resource(struct device_node *dev, int index,
@@ -53,6 +88,19 @@ static inline const __be32 *of_get_address(struct device_node *dev, int index,
 {
 	return NULL;
 }
+
+static inline int of_pci_range_parser_init(struct of_pci_range_parser *parser,
+			struct device_node *node)
+{
+	return -1;
+}
+
+static inline struct of_pci_range *of_pci_range_parser_one(
+					struct of_pci_range_parser *parser,
+					struct of_pci_range *range)
+{
+	return NULL;
+}
 #endif /* CONFIG_OF_ADDRESS */
 
 
diff --git a/include/linux/of_fdt.h b/include/linux/of_fdt.h
index ed136ad..c9722fd 100644
--- a/include/linux/of_fdt.h
+++ b/include/linux/of_fdt.h
@@ -66,7 +66,7 @@ extern char *of_fdt_get_string(struct boot_param_header *blob, u32 offset);
 extern void *of_fdt_get_property(struct boot_param_header *blob,
 				 unsigned long node,
 				 const char *name,
-				 unsigned long *size);
+				 int *size);
 extern int of_fdt_is_compatible(struct boot_param_header *blob,
 				unsigned long node,
 				const char *compat);
@@ -81,12 +81,11 @@ extern int __initdata dt_root_size_cells;
 extern struct boot_param_header *initial_boot_params;
 
 /* For scanning the flat device-tree at boot time */
-extern char *find_flat_dt_string(u32 offset);
 extern int of_scan_flat_dt(int (*it)(unsigned long node, const char *uname,
 				     int depth, void *data),
 			   void *data);
-extern void *of_get_flat_dt_prop(unsigned long node, const char *name,
-				 unsigned long *size);
+extern const void *of_get_flat_dt_prop(unsigned long node, const char *name,
+				       int *size);
 extern int of_flat_dt_is_compatible(unsigned long node, const char *name);
 extern int of_flat_dt_match(unsigned long node, const char *const *matches);
 extern unsigned long of_get_flat_dt_root(void);
@@ -96,9 +95,12 @@ extern int early_init_dt_scan_chosen(unsigned long node, const char *uname,
 extern void early_init_dt_check_for_initrd(unsigned long node);
 extern int early_init_dt_scan_memory(unsigned long node, const char *uname,
 				     int depth, void *data);
+extern void early_init_fdt_scan_reserved_mem(void);
 extern void early_init_dt_add_memory_arch(u64 base, u64 size);
+extern int early_init_dt_reserve_memory_arch(phys_addr_t base, phys_addr_t size,
+					     bool no_map);
 extern void * early_init_dt_alloc_memory_arch(u64 size, u64 align);
-extern u64 dt_mem_next_cell(int s, __be32 **cellp);
+extern u64 dt_mem_next_cell(int s, const __be32 **cellp);
 
 /*
  * If BLK_DEV_INITRD, the fdt early init code will call this function,
@@ -106,8 +108,7 @@ extern u64 dt_mem_next_cell(int s, __be32 **cellp);
  * physical addresses.
  */
 #ifdef CONFIG_BLK_DEV_INITRD
-extern void early_init_dt_setup_initrd_arch(unsigned long start,
-					    unsigned long end);
+extern void early_init_dt_setup_initrd_arch(u64 start, u64 end);
 #endif
 
 /* Early flat tree scan hooks */
@@ -118,6 +119,8 @@ extern int early_init_dt_scan_root(unsigned long node, const char *uname,
 extern void unflatten_device_tree(void);
 extern void early_init_devtree(void *);
 #else /* CONFIG_OF_FLATTREE */
+static inline void early_init_fdt_scan_reserved_mem(void) {}
+static inline const char *of_flat_dt_get_machine_name(void) { return NULL; }
 static inline void unflatten_device_tree(void) {}
 #endif /* CONFIG_OF_FLATTREE */
 
diff --git a/include/linux/of_reserved_mem.h b/include/linux/of_reserved_mem.h
new file mode 100644
index 0000000..9b1fbb7
--- /dev/null
+++ b/include/linux/of_reserved_mem.h
@@ -0,0 +1,53 @@
+#ifndef __OF_RESERVED_MEM_H
+#define __OF_RESERVED_MEM_H
+
+struct device;
+struct of_phandle_args;
+struct reserved_mem_ops;
+
+struct reserved_mem {
+	const char			*name;
+	unsigned long			fdt_node;
+	const struct reserved_mem_ops	*ops;
+	phys_addr_t			base;
+	phys_addr_t			size;
+	void				*priv;
+};
+
+struct reserved_mem_ops {
+	void	(*device_init)(struct reserved_mem *rmem,
+			       struct device *dev);
+	void	(*device_release)(struct reserved_mem *rmem,
+				  struct device *dev);
+};
+
+typedef int (*reservedmem_of_init_fn)(struct reserved_mem *rmem,
+				      unsigned long node, const char *uname);
+
+#ifdef CONFIG_OF_RESERVED_MEM
+void fdt_init_reserved_mem(void);
+void fdt_reserved_mem_save_node(unsigned long node, const char *uname,
+			       phys_addr_t base, phys_addr_t size);
+
+#define RESERVEDMEM_OF_DECLARE(name, compat, init)			\
+	static const struct of_device_id __reservedmem_of_table_##name	\
+		__used __section(__reservedmem_of_table)		\
+		 = { .compatible = compat,				\
+		     .data = (init == (reservedmem_of_init_fn)NULL) ?	\
+				init : init }
+
+#else
+static inline void fdt_init_reserved_mem(void) { }
+static inline void fdt_reserved_mem_save_node(unsigned long node,
+		const char *uname, phys_addr_t base, phys_addr_t size) { }
+
+#define RESERVEDMEM_OF_DECLARE(name, compat, init)			\
+	static const struct of_device_id __reservedmem_of_table_##name	\
+		__attribute__((unused))					\
+		 = { .compatible = compat,				\
+		     .data = (init == (reservedmem_of_init_fn)NULL) ?	\
+				init : init }
+
+#endif
+
+#endif /* __OF_RESERVED_MEM_H */
diff --git a/include/linux/pci.h b/include/linux/pci.h
index 829cd3d..b233ba7 100644
--- a/include/linux/pci.h
+++ b/include/linux/pci.h
@@ -1015,6 +1015,11 @@ void pdev_enable_device(struct pci_dev *);
 int pci_enable_resources(struct pci_dev *, int mask);
 void pci_fixup_irqs(u8 (*)(struct pci_dev *, u8 *),
 		    int (*)(const struct pci_dev *, u8, u8));
+
+void pci_fixup_irqs_dev(struct pci_dev *dev,
+	       u8 (*swizzle)(struct pci_dev *, u8 *),
+	       int (*map_irq)(const struct pci_dev *, u8, u8));
+
 #define HAVE_PCI_REQ_REGIONS	2
 int __must_check pci_request_regions(struct pci_dev *, const char *);
 int __must_check pci_request_regions_exclusive(struct pci_dev *, const char *);
diff --git a/include/linux/phy.h b/include/linux/phy.h
index 9e11039..bf5fc40 100644
--- a/include/linux/phy.h
+++ b/include/linux/phy.h
@@ -571,6 +571,9 @@ int phy_ethtool_get_eee(struct phy_device *phydev, struct ethtool_eee *data);
 int phy_ethtool_set_wol(struct phy_device *phydev, struct ethtool_wolinfo *wol);
 void phy_ethtool_get_wol(struct phy_device *phydev, struct ethtool_wolinfo *wol);
 
+int phy_init_eee_private(struct mii_bus *bus, unsigned int phy_addr, int speed,
+		      int duplex, bool clk_stop_enable);
+
 int __init mdio_bus_init(void);
 void mdio_bus_exit(void);
 
diff --git a/include/linux/pl320-ipc.h b/include/linux/pl320-ipc.h
new file mode 100644
index 0000000..69147bd
--- /dev/null
+++ b/include/linux/pl320-ipc.h
@@ -0,0 +1,24 @@
+/*
+ * This program is free software; you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License version 2 as
+ * published by the Free Software Foundation.
+ */
+
+#ifndef __MAILBOX_H
+#define __MAILBOX_H
+
+enum xfer_result {
+	XFER_OK = 0,
+	XFER_ERR,
+};
+
+typedef unsigned request_token_t;
+
+#ifdef CONFIG_DEBUG_MBOX
+#define mbox_dbg		printk
+#else
+#define mbox_dbg(fmt, args...)	do {} while (0)
+#endif
+
+#endif /* __MAILBOX_H */
+
diff --git a/include/linux/platform_data/dma-mb8ac0300-hdmac.h b/include/linux/platform_data/dma-mb8ac0300-hdmac.h
new file mode 100644
index 0000000..3259033
--- /dev/null
+++ b/include/linux/platform_data/dma-mb8ac0300-hdmac.h
@@ -0,0 +1,278 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/include/mach/hdmac.h
+ *
+ * Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+
+#ifndef __PLAT_HDMAC_H
+#define __PLAT_HDMAC_H
+
+#include <linux/device.h>
+
+#define CHIP_INDEX(chan)	((chan) / HDMAC_MAX_CHIP_CHANNELS)
+#define CHAN_INDEX(chan)	((chan) % HDMAC_MAX_CHIP_CHANNELS)
+#define OUTER_CHAN(chip, chan)	(((chip) * HDMAC_MAX_CHIP_CHANNELS) + (chan))
+
+#define HDMAC_CM3_CHIP		0
+#define HDMAC_MSIO_CHIP		1
+
+#define HDMAC_MAX_CHIPS		4
+#define HDMAC_MAX_CHIP_CHANNELS	8
+#define HDMAC_MAX_CHANNELS	(HDMAC_MAX_CHIPS * HDMAC_MAX_CHIP_CHANNELS)
+
+#define HDMAC_AUTOSTART_ENABLE	1
+#define HDMAC_AUTOSTART_DISABLE	0
+
+/* HDMAC registers */
+#define HDMAC_REG_DMACR		0x00	/* DMACR register offset */
+#define HDMAC_REG_DMACA		0x00	/* DMACA registers offset */
+#define HDMAC_REG_DMACB		0x04	/* DMACB registers offset */
+#define HDMAC_REG_DMACSA	0x08	/* DMACSA registers offset */
+#define HDMAC_REG_DMACDA	0x0C	/* DMACDA registers offset */
+
+#define DMACR		HDMAC_REG_DMACR
+#define DMACA(ch)	(HDMAC_REG_DMACA + (((ch) + 1) * 0x10))
+#define DMACB(ch)	(HDMAC_REG_DMACB + (((ch) + 1) * 0x10))
+#define DMACSA(ch)	(HDMAC_REG_DMACSA + (((ch) + 1) * 0x10))
+#define DMACDA(ch)	(HDMAC_REG_DMACDA + (((ch) + 1) * 0x10))
+
+/* HDMAC state */
+#define HDMAC_PREPARE			0
+#define HDMAC_IDLE			1
+#define HDMAC_RUNNING			2
+#define HDMAC_STOP_REQUEST		3
+#define HDMAC_STOP_REQUEST_NOWAIT	4
+
+/* HDMAC overall configuration register */
+#define HDMACR_DE	(0x01 << 31)	/* HDMA all channels are enable */
+#define HDMACR_DS	(0x01 << 30)	/*
+					 * The DMA transfers of all chnnels
+					 * are halted by disable/halt setting
+					 */
+#define HDMACR_PR	(0x01 << 28)	/* Priority is rotated */
+#define HDMACR_HALT	(0x01 << 24)	/* All channels are halted */
+
+/* DMAC Configuration A register */
+#define HDMACA_EB	(0x01 << 31)	/* This channel is enabled */
+#define HDMACA_PB	(0x01 << 30)	/* This channel is halted */
+#define HDMACA_ST_MASK	(0x01 << 29)
+#define HDMACA_ST	(0x01 << 29)	/* Software request */
+
+/* Input Select */
+#define HDMACA_IS_MASK	(0x1F << 24)
+#define HDMACA_IS_SW	(0x00 << 24)	/* Software */
+
+#define HDMACA_IS_DERQH	(0x0E << 24)	/* DREQ High level or Positive edge */
+#define HDMACA_IS_DERQL	(0x0F << 24)	/* DREQ Low level or Negative edge */
+					/* IDREQ High level or Positive edge */
+#define HDMACA_IS_IDREQ0H	(0x10 << 24)
+#define HDMACA_IS_IDREQ1H	(0x11 << 24)
+#define HDMACA_IS_IDREQ2H	(0x12 << 24)
+#define HDMACA_IS_IDREQ3H	(0x13 << 24)
+#define HDMACA_IS_IDREQ4H	(0x14 << 24)
+#define HDMACA_IS_IDREQ5H	(0x15 << 24)
+#define HDMACA_IS_IDREQ6H	(0x16 << 24)
+#define HDMACA_IS_IDREQ7H	(0x17 << 24)
+#define HDMACA_IS_IDREQ8H	(0x18 << 24)
+#define HDMACA_IS_IDREQ9H	(0x19 << 24)
+#define HDMACA_IS_IDREQ10H	(0x1A << 24)
+#define HDMACA_IS_IDREQ11H	(0x1B << 24)
+#define HDMACA_IS_IDREQ12H	(0x1C << 24)
+#define HDMACA_IS_IDREQ13H	(0x1D << 24)
+#define HDMACA_IS_IDREQ14H	(0x1E << 24)
+#define HDMACA_IS_IDREQ15H	(0x1F << 24)
+/* Beat Type*/
+#define HDMACA_BT_MASK		(0x0F << 20)
+#define HDMACA_BT_NORMAL	(0x00 << 20)
+#define HDMACA_BT_SINGLE	(0x08 << 20)	/* same as NORMAL*/
+#define HDMACA_BT_INCR		(0x09 << 20)
+#define HDMACA_BT_WRAP4		(0x0A << 20)
+#define HDMACA_BT_INCR4		(0x0B << 20)
+#define HDMACA_BT_WRAP8		(0x0C << 20)
+#define HDMACA_BT_INCR8		(0x0D << 20)
+#define HDMACA_BT_WRAP16	(0x0E << 20)
+#define HDMACA_BT_INCR16	(0x0F << 20)
+
+/* Block Count*/
+#define HDMACA_BC_MASK		(0x0F << 16)
+
+/* Transfer Count*/
+#define HDMACA_TC_MASK		(0xFF << 0)
+
+/* DMAC Configuration B register */
+#define HDMACB_TT_2CYCLE	(0x00 << 30)	/* 2cycle transfer */
+
+/* Mode Select */
+#define HDMACB_MS_MASK		(0x03 << 28)
+#define HDMACB_MS_BLOCK		(0x00 << 28)	/* Block transfer mode */
+#define HDMACB_MS_BURST		(0x01 << 28)	/* Burst transfer mode */
+#define HDMACB_MS_DEMAND	(0x02 << 28)	/* Demand transfer mode */
+
+/* Transfer Width */
+#define HDMACB_TW_BYTE		(0x00 << 26)	/* Byte */
+#define HDMACB_TW_HALFWORD	(0x01 << 26)	/* Half-word */
+#define HDMACB_TW_WORD		(0x02 << 26)	/* Word */
+
+/* Fixed Source */
+#define HDMACB_FS	(0x01 << 25)	/* Source address is fixed */
+/* Fixed Destination */
+#define HDMACB_FD	(0x01 << 24)	/* Destination address is fixed */
+/* Reload Count */
+#define HDMACB_RC	(0x01 << 23)	/* Transfer count is enabled */
+/* Reload Source */
+#define HDMACB_RS	(0x01 << 22)	/* Source address is enabled */
+/* Reload Destination */
+#define HDMACB_RD	(0x01 << 21)	/* Destination address is enabled */
+/* Error Interrupt */
+#define HDMACB_EI	(0x01 << 20)	/* Error irq issuance is enabled */
+/* Completion Interrupt */
+#define HDMACB_CI	(0x01 << 19)	/* Completion irq is enabled */
+
+/* Stop Status */
+#define HDMACB_SS_MASK				(0x07 << 16)
+#define HDMACB_SS_NONE				(0x00 << 16)
+#define HDMACB_SS_ADD_OVERFLOW			(0x01 << 16)
+#define HDMACB_SS_TRANSFER_STOP_REQUEST		(0x02 << 16)
+#define HDMACB_SS_SOURCE_ACCESS_ERROR		(0x03 << 16)
+#define HDMACB_SS_DESTINATION_ACCESS_ERROR	(0x04 << 16)
+#define HDMACB_SS_NORMAL_END			(0x05 << 16)
+#define HDMACB_SS_DMA_PAUSE			(0x07 << 16)
+#define HDMAC_REQ_DATA_FLUSHED			0xFF000000
+
+#ifndef __PLAT_XDMAC_H
+struct dma_req_data {
+	u32 size;		/* request size in bytes */
+	dma_addr_t src;		/* source of DMA data */
+	dma_addr_t dst;		/* destination of DMA data */
+
+	/* driver handles */
+	void *irq_data;			/* data for the callback */
+	void (*callback_fn)(u32 channel, void *irq_data, int state);
+					/*
+					 * DMA request done callback function
+					 * channel : channel number
+					 * irq_data: callback data
+					 * state : DMA stopped state
+					 */
+};
+#endif
+
+struct hdmac_chip;
+
+struct hdmac_chan {
+	struct mb8ac0300_hdmac_chip *chip; /* owning chip */
+	/* channel state flags and information */
+	u32 number;		/* number of this hdmac channel */
+	u8 state;		/* channel state */
+
+	char name[10];
+
+	/* channel's hardware position and configuration */
+	u32 irq;	/* channel irq */
+	u32 dmaca;	/* DMACA register value */
+	u32 dmacb;	/* DMACB register value */
+
+	/* channel configuration */
+	u8 autostart_flg;	/* DMA auto start when DMA request loaded */
+
+	/* hdmac request list and information */
+	struct list_head list;
+
+	spinlock_t lock;	/* protect req info */
+	struct completion stop_completion;
+};
+
+struct mb8ac0300_hdmac_chip {
+	int chip_index;
+	int channels;
+	void *base;
+	struct hdmac_chan chans[HDMAC_MAX_CHIP_CHANNELS];
+#ifdef CONFIG_PM
+	u32 hdmac_dmacr;
+#endif
+	struct device *dev;
+};
+
+struct hdmac_req {
+	struct dma_req_data req_data;	/* dma request data */
+	struct list_head node;
+	u32 dmaca;	/* DMACA */
+	u32 dmacb;	/* DMACB */
+};
+
+/*
+ * hdmac_get_channel
+ *
+ * request a dma channel exclusivley
+ */
+extern int hdmac_get_channel(u32 channel, u8 autostart_flg);
+
+/*
+ * hdmac_enqueue
+ *
+ * place the given DMA request onto the queue of operations for the channel.
+ */
+extern int hdmac_enqueue(u32 channel, struct hdmac_req *hdmac_req);
+
+/*
+ * hdmac_start
+ *
+ * start a dma channel going
+ */
+extern int hdmac_start(u32 channel);
+
+/*
+ * hdmac_getposition
+ *
+ * returns the current transfer points for the dma source and destination
+ */
+extern int hdmac_getposition(u32 channel, u32 *src, u32 *dst);
+
+/*
+ * hdmac_stop(synchronous)
+ *
+ * stop a dma channel
+ */
+extern int hdmac_stop(u32 channel);
+
+/*
+ * hdmac_stop(asynchronous)
+ *
+ * stop a dma channel
+ */
+extern int hdmac_stop_nowait(u32 channel);
+
+/*
+ * hdmac_flush
+ *
+ * remove all current and pending transfers
+ */
+extern int hdmac_flush(u32 channel);
+
+/*
+ * hdmac_free
+ * free the dma channel (will also abort any outstanding operations)
+ */
+extern int hdmac_free(u32 channel);
+
+/*
+ * hdmac_init
+ *
+ * initialization hdmac
+ */
+extern void hdmac_init(u32 chip, void *base);
+
+#endif /* __PLAT_HDMAC_H */
diff --git a/include/linux/platform_data/dma-mb8ac0300-xdmac.h b/include/linux/platform_data/dma-mb8ac0300-xdmac.h
new file mode 100644
index 0000000..2b03a27
--- /dev/null
+++ b/include/linux/platform_data/dma-mb8ac0300-xdmac.h
@@ -0,0 +1,307 @@
+/*
+ *
+ * Copyright (C) 2010-13 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __PLAT_XDMAC_H
+#define __PLAT_XDMAC_H
+
+#include <linux/device.h>
+
+#define MAX_XDMAC_CHANNELS	8
+
+#define XDMAC_REG_XDACS	0x0	/* XDACS registers offset */
+#define XDMAC_REG_XDTBC	0x0	/* XDTBC registers offset */
+#define XDMAC_REG_XDSSA	0x4	/* XDSSA registers offset */
+#define XDMAC_REG_XDDSA	0x8	/* XDDSA registers offset */
+#define XDMAC_REG_XDSAC	0xc	/* XDSAC registers offset */
+#define XDMAC_REG_XDDAC	0x10	/* XDDAC registers offset */
+#define XDMAC_REG_XDDCC	0x14	/* XDDCC registers offset */
+#define XDMAC_REG_XDDES	0x18	/* XDDES registers offset */
+#define XDMAC_REG_XDDPC	0x1c	/* XDDPC registers offset */
+#define XDMAC_REG_XDDSD	0x20	/* XDDSD registers offset */
+
+#define XDACS		XDMAC_REG_XDACS
+#define XDTBC(ch)  (XDMAC_REG_XDTBC + ((ch * 0x30) + 0x10))
+#define XDSSA(ch)  (XDMAC_REG_XDSSA + ((ch * 0x30) + 0x10))
+#define XDDSA(ch)  (XDMAC_REG_XDDSA + ((ch * 0x30) + 0x10))
+#define XDSAC(ch)  (XDMAC_REG_XDSAC + ((ch * 0x30) + 0x10))
+#define XDDAC(ch)  (XDMAC_REG_XDDAC + ((ch * 0x30) + 0x10))
+#define XDDCC(ch)  (XDMAC_REG_XDDCC + ((ch * 0x30) + 0x10))
+#define XDDES(ch)  (XDMAC_REG_XDDES + ((ch * 0x30) + 0x10))
+#define XDDPC(ch)  (XDMAC_REG_XDDPC + ((ch * 0x30) + 0x10))
+#define XDDSD(ch)  (XDMAC_REG_XDDSD + ((ch * 0x30) + 0x10))
+
+/* XDMAC state */
+#define XDMAC_PREPARE	0
+#define XDMAC_IDLE	1
+#define XDMAC_RUNNING	2
+#define XDMAC_STOP_REQUEST 3
+#define XDMAC_STOP_REQUEST_NOWAIT 4
+#define XDMAC_IRQ_HANDLED 0xF0000000
+
+
+/* XDACS all channel setting */
+#define XDACS_XE	(0x1 << 28)
+#define XDACS_CP	(0x1 << 24)
+#define XDACS_LP	(0x1 << 20)
+#define XDACS_XS	(0x1 << 16)
+
+/* XDSAC source access configuration */
+#define XDSAC_SCT_BUFFERABLE	(0x1 << 20)
+#define XDSAC_SCT_CACHEABLE	(0x1 << 21)
+#define XDSAC_SCT_READALLOC	(0x1 << 22)
+#define XDSAC_SCT_WRITEALLOC	(0x1 << 23)
+
+#define XDSAC_SBS_BYTE		(0x0 << 16)
+#define XDSAC_SBS_HALFWORD	(0x1 << 16)
+#define XDSAC_SBS_WORD		(0x2 << 16)
+#define XDSAC_SBS_DOUBLEWORD	(0x3 << 16)
+
+#define XDSAC_SBL_1	(0x0 << 8)
+#define XDSAC_SBL_2	(0x1 << 8)
+#define XDSAC_SBL_3	(0x2 << 8)
+#define XDSAC_SBL_4	(0x3 << 8)
+#define XDSAC_SBL_5	(0x4 << 8)
+#define XDSAC_SBL_6	(0x5 << 8)
+#define XDSAC_SBL_7	(0x6 << 8)
+#define XDSAC_SBL_8	(0x7 << 8)
+#define XDSAC_SBL_9	(0x8 << 8)
+#define XDSAC_SBL_10	(0x9 << 8)
+#define XDSAC_SBL_11	(0xa << 8)
+#define XDSAC_SBL_12	(0xb << 8)
+#define XDSAC_SBL_13	(0xc << 8)
+#define XDSAC_SBL_14	(0xd << 8)
+#define XDSAC_SBL_15	(0xe << 8)
+#define XDSAC_SBL_16	(0xf << 8)
+
+
+#define XDSAC_SAF	(0x1 << 2)
+#define XDSAC_SRL	(0x1 << 0)
+
+
+/* XDDAC destination acces configuration */
+#define XDDAC_DCT_BUFFERABLE	(0x1 << 20)
+#define XDDAC_DCT_CACHEABLE	(0x1 << 21)
+#define XDDAC_DCT_READALLOC	(0x1 << 22)
+#define XDDAC_DCT_WRITEALLOC	(0x1 << 23)
+
+#define XDDAC_DBS_BYTE	(0x0 << 16)
+#define XDDAC_DBS_HALFWORD	(0x1 << 16)
+#define XDDAC_DBS_WORD	(0x2 << 16)
+#define XDDAC_DBS_DOUBLEWORD	(0x3 << 16)
+
+#define XDDAC_DBL_1	(0x0 << 8)
+#define XDDAC_DBL_2	(0x1 << 8)
+#define XDDAC_DBL_3	(0x2 << 8)
+#define XDDAC_DBL_4	(0x3 << 8)
+#define XDDAC_DBL_5	(0x4 << 8)
+#define XDDAC_DBL_6	(0x5 << 8)
+#define XDDAC_DBL_7	(0x6 << 8)
+#define XDDAC_DBL_8	(0x7 << 8)
+#define XDDAC_DBL_9	(0x8 << 8)
+#define XDDAC_DBL_10	(0x9 << 8)
+#define XDDAC_DBL_11	(0xa << 8)
+#define XDDAC_DBL_12	(0xb << 8)
+#define XDDAC_DBL_13	(0xc << 8)
+#define XDDAC_DBL_14	(0xd << 8)
+#define XDDAC_DBL_15	(0xe << 8)
+#define XDDAC_DBL_16	(0xf << 8)
+
+
+#define XDDAC_DAF	(0x1 << 2)
+#define XDDAC_DRL	(0x1 << 0)
+
+
+/* XDDCC descriptor chain configuration */
+#define XDDCC_DCN	(0x1 << 0)
+
+
+/* XDDES DMA enable setting */
+#define XDDES_CE	(0x1 << 28)
+#define XDDES_SE	(0x1 << 24)
+#define XDDES_SE_MASK	(0x1 << 24)
+
+#define XDDES_TF_SOFTWEARE	(0x1 << 20)
+#define XDDES_TF_DREQ0		(0x2 << 20)
+#define XDDES_TF_DREQ1		(0x3 << 20)
+#define XDDES_TF_DREQ2		(0x4 << 20)
+#define XDDES_TF_DREQ3		(0x5 << 20)
+#define XDDES_TF_DREQ4		(0x6 << 20)
+#define XDDES_TF_DREQ5		(0x7 << 20)
+#define XDDES_TF_DREQ6		(0x8 << 20)
+#define XDDES_TF_DREQ7		(0x9 << 20)
+#define XDDES_TF_DREQ8		(0xa << 20)
+#define XDDES_TF_DREQ9		(0xb << 20)
+#define XDDES_TF_DREQ10		(0xc << 20)
+#define XDDES_TF_DREQ11		(0xd << 20)
+#define XDDES_TF_DREQ12		(0xe << 20)
+#define XDDES_TF_DREQ13		(0xf << 20)
+#define XDDES_TF_MASK		(0xf << 20)
+
+
+#define XDDES_SA	(0x1 << 15)
+#define XDDES_BURST	(0x0 << 12)
+#define XDDES_BLOCK	(0x1 << 12)
+#define XDDES_BR	(0x1 << 8)
+#define XDDES_AT_SRC	(0x0 << 4)
+#define XDDES_AT_DES	(0x1 << 4)
+#define XDDES_EI	(0x1 << 1)
+#define XDDES_TI	(0x1 << 0)
+
+
+/* XDDPC DMA protection control */
+#define XDDPC_SP0_PRIVILEGED_ACCESS	(0x1 << 4)
+#define XDDPC_SP1_SECURE_ACCESS	(0x1 << 5)
+#define XDDPC_SP2_DATA_ACCESS	(0x0 << 6)
+
+#define XDDPC_DP0_PRIVILEGED_ACCESS	(0x1 << 0)
+#define XDDPC_DP1_SECURE_ACCESS	(0x1 << 1)
+#define XDDPC_DP2_DATA_ACCESS	(0x0 << 2)
+
+
+/* XDDSD  status display */
+#define XDDSD_TS_RUNNING	(0x1 << 16)
+
+#define XDDSD_IS_MASK					(0xf)
+#define XDDSD_IS_NONE					(0x0)
+#define XDDSD_IS_STOP_BY_DSTP				(0x1)
+#define XDDSD_IS_DISABLE_CE_XE				(0x2)
+#define XDDSD_IS_SOURCE_ACCESS_ERROR			(0x4)
+#define XDDSD_IS_DESTINATION_ACCESS_ERROR		(0x5)
+#define XDDSD_IS_DESCRIPTOR_CHAIN_MEM_ACCESS_ERROR	(0x6)
+#define XDDSD_IS_NORMAL_END				(0x8)
+
+#define XDMAC_REQ_DATA_FLUSHED				0xFF000000
+
+
+#ifndef __PLAT_HDMAC_H
+struct dma_req_data {
+	u32	size;			/* request size in bytes */
+	dma_addr_t	src;		/* source of DMA data */
+	dma_addr_t	dst;		/* distnation of DMA data */
+	void *irq_data;			/* data for the callback */
+	/* driver handles */
+	void (*callback_fn)(u32 channel, void *irq_data, int state);
+				/* request done callback function
+				 * channel : channel number
+				 * irq_data: callback data
+				 * state : DMA stopped state
+				 */
+};
+#endif
+
+/* XDMAC Descriptor chain */
+struct xdmac_desc {
+	unsigned long xdtbc;	/* XDTBC register value */
+	unsigned long xdssa;	/* XDSSA register value */
+	unsigned long xddsa;	/* XDDSA register value */
+	unsigned long xdsac;	/* XDSAC register value */
+	unsigned long xddac;	/* XDDAC register value */
+	unsigned long xddcc;	/* XDDCC register value */
+} __aligned(4);
+
+
+struct xdmac_req {
+	struct dma_req_data req_data; /* DMA request */
+	u32 xdsac;	/* XDSAC register value */
+	u32 xddac;	/* XDDAC register value */
+	u32 xddcc;	/* XDDCC register value */
+	u32 xddes;	/* XDDES register value */
+	u32 xddpc;	/* XDDPC register value */
+	struct list_head node;
+};
+
+struct xdmac_chan {
+	/* channel state flags and information */
+	u32 number;	/* number of this xdma channel */
+	u32 irq;	/* channel irq */
+
+	/* channel state */
+	u8 state;	/* DMA state */
+
+	/* channel's hardware position and configuration */
+	u32 xdsac;	/* XDSAC register value */
+	u32 xddac;	/* XDDAC register value */
+	u32 xddcc;	/* XDDCC register value */
+	u32 xddes;	/* XDDES register value */
+	u32 xddpc;	/* XDDPC register value */
+
+	/* channel configuration */
+	u8 autostart_flg;	/* DMA auto start when request loaded */
+
+	/* xdmac request list and information */
+	struct list_head list;
+
+	spinlock_t lock;	/* protect req info */
+	struct completion stop_completion;
+};
+
+
+/* functions --------------------------------------------------------------- */
+
+/* xdmac_get_channel
+ *
+ * request a dma channel exclusivley
+ */
+extern int xdmac_get_channel(u32 channel, u8 autostart_flg);
+
+/* xdmac_enqueue
+ *
+ * place the given request onto the queue of operations for the channel.
+ */
+extern int xdmac_enqueue(u32 channel, struct xdmac_req *xdmac_req);
+
+/* xdmac_start
+ * start a dma channel going
+ */
+extern int xdmac_start(u32 channel);
+
+/* xdmac_getposition
+ *
+ * returns the current transfer points for the dma source and destination
+ */
+extern int xdmac_getposition(u32 channel, u32 *src, u32 *dst);
+
+/* xdmac_stop(synchronous)
+ *
+ * stop a dma channel
+ */
+extern int xdmac_stop(u32 channel);
+
+/* xdmac_stop_nowait(asynchronous)
+ *
+ * stop a dma channel
+ */
+extern int xdmac_stop_nowait(u32 channel);
+
+/* xdmac_flush
+ *
+ * remove all current and pending transfers
+ */
+extern int xdmac_flush(u32 channel);
+
+/* xdmac_free
+ * free the dma channel (will also abort any outstanding operations)
+ */
+extern int xdmac_free(u32 channel);
+/* xdmac_init
+ * initialization xdmac
+ */
+extern void xdmac_init(void *base);
+
+#endif /* __PLAT_XDMAC_H */
+
+
diff --git a/include/linux/platform_data/f_i2c.h b/include/linux/platform_data/f_i2c.h
new file mode 100644
index 0000000..00110a1
--- /dev/null
+++ b/include/linux/platform_data/f_i2c.h
@@ -0,0 +1,33 @@
+/*
+ * linux/include/linux/plaform_data/f_i2c.h
+ *
+ * Copyright (C) 2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __INCLUDE_LINUX_PLATFORM_DATA_F_I2C_H__
+#define __INCLUDE_LINUX_PLATFORM_DATA_F_I2C_H__
+
+/*
+ * i2c platform devices data
+ */
+struct f_i2c_platform_data {
+	unsigned int speed_khz;
+};
+
+#define F_I2C_SPEED_FM	400	/* Fast Mode */
+#define F_I2C_SPEED_SM	100	/* Standard Mode */
+
+#endif /* __INCLUDE_LINUX_PLATFORM_DATA_F_I2C_H__ */
+
diff --git a/include/linux/platform_data/f_usb20hdc.h b/include/linux/platform_data/f_usb20hdc.h
new file mode 100644
index 0000000..5f23d5c
--- /dev/null
+++ b/include/linux/platform_data/f_usb20hdc.h
@@ -0,0 +1,1455 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/include/mach/f_usb20hdc.h
+ *
+ * Copyright (C) 2012 FUJITSU SEMICONDUCTOR LIMITED.
+ *
+ *  This program is free software: you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation, version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __MACH_F_USB20HDC_H
+#define __MACH_F_USB20HDC_H
+
+/* defect of faked full-speed device on DP/DM */
+/* #define PHY_DEFECT_WORKAROUND */
+
+/*
+ * it's bad for USB suspend interrupt might be
+ * earlier than VBUS drop-to-low interrupt.
+ * This is still not identified as soft or hard defect.
+ */
+#define GADGET_CONNECTION_BUG
+
+/* faked power-off IP simulation*/
+#define POWER_OFF_SIMULATION
+
+/* make controller can resume from power off or hibernation */
+#define COLD_RESUME_SUPPORT
+
+/*
+ * F_USB20HDC IP has maximun two clock source so far,
+ * it has two clock source from clock modules in mb8ac0300 platfoem,
+ * and has one source in mb8aa0350.
+*/
+#define CLK_MAX_NUM	3
+
+static inline void ctrl_reg_cache_bits(void __iomem *base_addr, u32 register_id,
+	u32 *register_cache);
+
+/* F_USB20HDC USB controller DMA channel count */
+#define F_USB20HDC_MAX_DMA_CHANNELS		2
+
+/* platform data information structure of f_usb20hdc driver */
+struct f_usb20hdc_pdata {
+	u32	dma_dreq[F_USB20HDC_MAX_DMA_CHANNELS];		/* dreq number
+								 * for usb dma
+								 * channel array
+								 */
+	u32	hdmac_channel[F_USB20HDC_MAX_DMA_CHANNELS];	/* HDMAC channel
+								 * for usb dma
+								 * channel array
+								 */
+};
+
+/* F_USB20HDC C_HSEL area address offset */
+#define F_USB20HDC_C_HSEL_ADDR_OFFSET	0x00000
+/* F_USB20HDC D_HSEL area address offset */
+#define F_USB20HDC_D_HSEL_ADDR_OFFSET	0x10000
+
+/* endpoint counter count per one endpoint */
+#define F_USB20HDC_EP_BUFFER_COUNT		2
+
+/* endpoint configuration structure array table */
+struct endpont_cb {
+	/*
+	 * endpoint name string
+	 * [notice]:The following constant definition is used.
+	 *	endpoint 0 is "ep0" fixation
+	 *	unused	= ""
+	 *	used	= "ep(number)(in or out)-(bulk or iso or int)"
+	 *	[Example]
+	 *	"ep1", "ep2", ... address is fixed, not direction or type
+	 *	"ep1in, "ep2out", ... address and direction are fixed, not type
+	 *	"ep1-bulk", "ep2-int", ... address and type are fixed, not dir
+	 *	"ep1in-bulk", "ep2out-iso", ... all three are fixed
+	 */
+	char *name;
+	/*
+	 * endpoint transfer maximum packet size for high-speed
+	 * [notice]:unusing it is referred to as '0'
+	 */
+	u16	hs_maxpacket;
+	/*
+	 * endpoint transfer maximum packet size for full-speed
+	 * [notice]:unusing it is referred to as '0'
+	 */
+	u16	fs_maxpacket;
+	/*
+	 * endpoint buffer size(x1[bytes])
+	 * [notice]:unusing it is referred to as '0'
+	 */
+	u16	buffer_size;
+	/*
+	 * endpoint buffer count
+	 * [notice]:unusing it is referred to as '0', and endpoint 0 is 1 fixed
+	 */
+	u8	buffers;
+	/*
+	 * PIO transfer auto change flag
+	 * [notice]:unusing it is referred to as '0', and endpoint 0 is 0 fixed
+	 */
+	u8	pio_auto_change;
+	/*
+	 * IN transfer end notify timing to USB host flag
+	 * [notice]:unusing it is referred to as '0', and endpoint 0 is 0 fixed
+	 */
+	u8	trans_end_timing;
+	/*
+	 * USB controller DMA channel for endpoint
+	 * [notice]:unusing it is referred to as '-1'
+	 */
+	s8	usb_dma_channel;
+	/*
+	 * DMA controller DMA channel for endpoint
+	 * [notice]:unusing it is referred to as '-1'
+	 */
+	s8	dma_channel;
+};
+
+/* F_USB20HDC device driver DMA data structure */
+struct f_usb20hdc_dma_data {
+	u32 hdmac_channel;			/* HDMAC transfer channel */
+	u32 dreq;				/* HDMAC transfer dreq */
+	s8 endpoint_channel;			/* endpoint channel using DMA */
+	struct hdmac_req hdmac_req;		/* HDMAC request structure */
+	void *buffer;				/*
+						 * DMA transfer noncachable
+						 * buffer's virtual address
+						 */
+	dma_addr_t dma_buffer;			/*
+						 * DMA transfer noncachable
+						 * buffer's physical address
+						 */
+	dma_addr_t epbuf_dma_addr;		/*
+						 * DMA transfer buffer
+						 * pyhsical address
+						 */
+};
+
+/* F_USB20HDC HCD device driver structure */
+struct f_usb20hdc_otg {
+	struct platform_device *pdev;
+	struct device *dev;
+	struct clk *clk_table[CLK_MAX_NUM];
+	void __iomem *reg_base;	/* F_USB20HDC reg base addr */
+	int irq;				/* F_USB20HDC IRQ number */
+	struct resource *mem_res;
+	resource_size_t mem_start;
+	resource_size_t mem_size;
+	u8 host_working;
+	 int otg_id;
+	struct delayed_work switch_to_host;
+	struct delayed_work switch_to_gadget;
+	struct mutex role_switch_lock;
+	struct usb_hcd *hcd;
+	void *f_usb20hdc_udc;
+#if defined(CONFIG_USB_F_USB20HDC_OTG_USED_DMA_TRANSFER)
+	struct f_usb20hdc_pdata pdata;
+	struct f_usb20hdc_dma_data dma_data[F_USB20HDC_MAX_DMA_CHANNELS];
+#endif
+	s8 mode;
+	struct dentry *root;
+	struct dentry *file;
+	char debug_str[20];
+};
+
+/* F_USB20HDC controller register ID Enumeration constant */
+#define F_USB20HDC_REG_CONF		0	/* System Configuration */
+#define F_USB20HDC_REG_MODE		1	/* Oepration Mode */
+#define F_USB20HDC_REG_INTEN		2	/* Global Interrupt Enable */
+#define F_USB20HDC_REG_INTS		3	/* Global Interrupt Status */
+#define F_USB20HDC_REG_EPCMD0		4	/* EP0 Command */
+#define F_USB20HDC_REG_EPCMD1		5	/* EP1 Command */
+#define F_USB20HDC_REG_EPCMD2		6	/* EP2 Command */
+#define F_USB20HDC_REG_EPCMD3		7	/* EP3 Command */
+#define F_USB20HDC_REG_EPCMD4		8	/* EP4 Command */
+#define F_USB20HDC_REG_EPCMD5		9	/* EP5 Command */
+#define F_USB20HDC_REG_EPCMD6		10	/* EP6 Command */
+#define F_USB20HDC_REG_EPCMD7		11	/* EP7 Command */
+#define F_USB20HDC_REG_EPCMD8		12	/* EP8 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD9		13	/* EP9 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD10		14	/* EP10 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD11		15	/* EP11 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD12		16	/* EP12 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD13		17	/* EP13 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD14		18	/* EP14 Command *//*dev mode*/
+#define F_USB20HDC_REG_EPCMD15		19	/* EP15 Command *//*dev mode*/
+#define F_USB20HDC_REG_DEVC		20 /* Device Control */ /*dev mode*/
+#define F_USB20HDC_REG_DEVS		21	/* Device Status *//*dev mode*/
+#define F_USB20HDC_REG_FADDR		22 /* Function Address *//*dev mode*/
+#define F_USB20HDC_REG_TSTAMP		23 /* Device Time Stamp *//*dev mode*/
+#define F_USB20HDC_REG_PORTSC		24 /* Port Status  Control */
+#define F_USB20HDC_REG_PORTSTSC		25 /* Port Status Change */
+#define F_USB20HDC_REG_HOSTEVENTSRC	26 /* Host Event Factor */
+#define F_USB20HDC_REG_HOSTINTEN	27 /* Host Interrupt Enable */
+#define F_USB20HDC_REG_HCFRMIDX		28 /* HC Frame Index */
+#define F_USB20HDC_REG_HCFRMINIT	29 /* HC Frame Init */
+#define F_USB20HDC_REG_HCCTRL		30 /* HC Control */
+#define F_USB20HDC_REG_HCSTLINK		31 /* HC Start Link */
+#define F_USB20HDC_REG_OTGC		32	/* OTG Control */
+#define F_USB20HDC_REG_OTGSTS		33	/* OTG Status */
+#define F_USB20HDC_REG_OTGSTSC		34	/* OTG Status Change */
+#define F_USB20HDC_REG_OTGSTSFALL	35	/* OTG Status Fall Detect */
+#define F_USB20HDC_REG_OTGSTSRISE	36	/* OTG Status Rise Detect */
+#define F_USB20HDC_REG_OTGTC		37 /* OTG Timer Control */
+#define F_USB20HDC_REG_OTGT		38 /* OTG Timer */
+#define F_USB20HDC_REG_DMAC1		39 /* DMA1 Control */
+#define F_USB20HDC_REG_DMAS1		40 /* DMA1 Status */
+#define F_USB20HDC_REG_DMATCI1		41 /* DMA1 Total Trans Bytes */
+#define F_USB20HDC_REG_DMATC1		42 /* DMA1 Total Trans Bytes Counter */
+#define F_USB20HDC_REG_DMAC2		43 /* DMA2 Control */
+#define F_USB20HDC_REG_DMAS2		44 /* DMA2 Status */
+#define F_USB20HDC_REG_DMATCI2		45 /* DMA2 Total Trans Bytes */
+#define F_USB20HDC_REG_DMATC2		46 /* DMA2 Total Trans Bytes Counter */
+#define F_USB20HDC_REG_TESTC		47	/* Test Control */
+#define F_USB20HDC_REG_HCEPCTRL1_0	48 /* HCEP1_0 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_1	49 /* HCEP1_1 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_2	50 /* HCEP1_2 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_3	51 /* HCEP1_3 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_4	52 /* HCEP1_4 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_5	53 /* HCEP1_5 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_6	54 /* HCEP1_6 Control */
+#define F_USB20HDC_REG_HCEPCTRL1_7	55 /* HCEP1_7 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_0	56 /* HCEP2_0 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_1	57 /* HCEP2_1 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_2	58 /* HCEP2_2 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_3	59 /* HCEP2_3 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_4	60 /* HCEP2_4 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_5	61 /* HCEP2_5 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_6	62 /* HCEP2_6 Control */
+#define F_USB20HDC_REG_HCEPCTRL2_7	63 /* HCEP2_7 Control */
+#define F_USB20HDC_REG_EPCTRL0		64	/* EP0 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL1		65	/* EP1 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL2		66	/* EP2 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL3		67	/* EP3 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL4		68	/* EP4 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL5		69	/* EP5 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL6		70	/* EP6 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL7		71	/* EP7 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL8		72	/* EP8 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL9		73	/* EP9 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL10		74	/* EP10 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL11		75	/* EP11 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL12		76	/* EP12 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL13		77	/* EP13 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL14		78	/* EP14 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCTRL15		79	/* EP15 Control *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF0		80 /* HCEP0 Config */
+#define F_USB20HDC_REG_EPCONF1		81 /* HCEP1 Config */
+#define F_USB20HDC_REG_EPCONF2		82 /* HCEP2 Config */
+#define F_USB20HDC_REG_EPCONF3		83 /* HCEP3 Config */
+#define F_USB20HDC_REG_EPCONF4		84 /* HCEP4 Config */
+#define F_USB20HDC_REG_EPCONF5		85 /* HCEP5 Config */
+#define F_USB20HDC_REG_EPCONF6		86 /* HCEP6 Config */
+#define F_USB20HDC_REG_EPCONF7		87 /* HCEP7 Config */
+#define F_USB20HDC_REG_EPCONF8		88	/* EP8 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF9		89	/* EP9 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF10		90	/* EP10 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF11		91	/* EP11 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF12		92	/* EP12 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF13		93	/* EP13 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF14		94	/* EP14 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCONF15		95	/* EP15 Config *//*dev mode*/
+#define F_USB20HDC_REG_EPCOUNT0		96 /* HCEP0 Counter */
+#define F_USB20HDC_REG_EPCOUNT1		97 /* HCEP1 Counter */
+#define F_USB20HDC_REG_EPCOUNT2		98 /* HCEP2 Counter */
+#define F_USB20HDC_REG_EPCOUNT3		99 /* HCEP3 Counter */
+#define F_USB20HDC_REG_EPCOUNT4		100 /* HCEP4 Counter */
+#define F_USB20HDC_REG_EPCOUNT5		101 /* HCEP5 Counter */
+#define F_USB20HDC_REG_EPCOUNT6		102 /* HCEP6 Counter */
+#define F_USB20HDC_REG_EPCOUNT7		103 /* HCEP7 Counter */
+#define F_USB20HDC_REG_EPCOUNT8		104 /* HCEP8 Counter */
+#define F_USB20HDC_REG_EPCOUNT9		105 /* HCEP9 Counter */
+#define F_USB20HDC_REG_EPCOUNT10		106 /* HCEP10 Counter */
+#define F_USB20HDC_REG_EPCOUNT11		107 /* HCEP11 Counter */
+#define F_USB20HDC_REG_EPCOUNT12		108 /* HCEP12 Counter */
+#define F_USB20HDC_REG_EPCOUNT13		109 /* HCEP13 Counter */
+#define F_USB20HDC_REG_EPCOUNT14		110 /* HCEP14 Counter */
+#define F_USB20HDC_REG_EPCOUNT15		111 /* HCEP15 Counter */
+#define F_USB20HDC_REG_EPCOUNT16		112 /* HCEP16 Counter */
+#define F_USB20HDC_REG_EPCOUNT17		113 /* HCEP17 Counter */
+#define F_USB20HDC_REG_EPCOUNT18		114 /* HCEP18 Counter */
+#define F_USB20HDC_REG_EPCOUNT19		115 /* HCEP19 Counter */
+#define F_USB20HDC_REG_EPCOUNT20		116 /* HCEP20 Counter */
+#define F_USB20HDC_REG_EPCOUNT21		117 /* HCEP21 Counter */
+#define F_USB20HDC_REG_EPCOUNT22		118 /* HCEP22 Counter */
+#define F_USB20HDC_REG_EPCOUNT23		119 /* HCEP23 Counter */
+#define F_USB20HDC_REG_EPCOUNT24		120 /* HCEP24 Counter */
+#define F_USB20HDC_REG_EPCOUNT25		121 /* HCEP25 Counter */
+#define F_USB20HDC_REG_EPCOUNT26		122 /* HCEP26 Counter */
+#define F_USB20HDC_REG_EPCOUNT27		123 /* HCEP27 Counter */
+#define F_USB20HDC_REG_EPCOUNT28		124 /* HCEP28 Counter */
+#define F_USB20HDC_REG_EPCOUNT29	125 /* HCEP29 Counter */
+#define F_USB20HDC_REG_EPCOUNT30	126 /* HCEP30 Counter */
+#define F_USB20HDC_REG_EPCOUNT31	127 /* HCEP31 Counter */
+#define F_USB20HDC_REG_EPBUF		128 /* HCEP Buffer  */
+#define F_USB20HDC_REG_DMA1		129 /* DMA 1 Addr Convert Area */
+#define F_USB20HDC_REG_DMA2		130 /* DMA 1 Addr Convert Area */
+#define F_USB20HDC_REG_MAX		131 /* Max Value */
+
+/* DMAx Control register ID table array */
+static const u32 dmac_register[] = {
+	F_USB20HDC_REG_DMAC1,	/* DMA1 Control */
+	F_USB20HDC_REG_DMAC2,	/* DMA1 Control */
+};
+
+/* DMAx Status register ID table array */
+static const u32 dmas_register[] = {
+	F_USB20HDC_REG_DMAS1,	/* DMA1 Status */
+	F_USB20HDC_REG_DMAS2,	/* DMA1 Status */
+};
+
+/* DMAx Total Transfer Bytes Setting register ID table array */
+static const u32 dmatci_register[] = {
+	F_USB20HDC_REG_DMATCI1,	/* DMA1 Total Transfer Bytes Setting */
+	F_USB20HDC_REG_DMATCI2,	/* DMA2 Total Transfer Bytes Setting */
+};
+
+/* DMAx Total Transfer Bytes Counter register ID table array */
+static const u32 dmatc_register[] = {
+	F_USB20HDC_REG_DMATC1,	/* DMA1 Total Transfer Bytes Counter */
+	F_USB20HDC_REG_DMATC2,	/* DMA2 Total Transfer Bytes Counter */
+};
+
+#define U2H_R (1 << 0)
+#define U2H_W (1 << 1)
+
+/* F_USB20HDC controller register structures array */
+static const struct {
+	u32 address_offset;	/* register address offset */
+	u8 access;	/* register access flags  */
+} f_usb20hdc_register[F_USB20HDC_REG_MAX] = {
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0000, U2H_R | U2H_W },
+						/* F_USB20HDC_REG_CONF */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0004, U2H_R | U2H_W },
+						/* F_USB20HDC_REG_MODE */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0008, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_INTEN */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x000c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_INTS */
+					/* 0x00100x003FReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0040, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0044, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0048, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x004c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0050, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0054, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0058, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x005c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0060, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD8 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0064, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD9 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0068, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD10 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x006c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD11 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0070, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD12 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0074, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD13 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0078, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD14 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x007c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCMD15 */
+					/* 0x00800x01ffReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0200, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DEVC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0204, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DEVS */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0208, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_FADDR */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x020c, U2H_R},
+					/* F_USB20HDC_REG_TSTAMP */
+					/* 0x021`0x02ffFReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0100, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_PORTSC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0104, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_PORTSTSC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0108, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HOSTEVENTSRC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x010c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HOSTINTEN */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0110, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCFRMIDX */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0114, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCFRMINIT */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0118, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCCTRL */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x011c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCSTLINK */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0300, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGC */
+					/* 0x0304`0x030fFReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0310, U2H_R },
+					/* F_USB20HDC_REG_OTGSTS */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0314, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGSTSC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0318, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGSTSFALL */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x031c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGSTSRISE */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0320, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGTC */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0324, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_OTGT */
+					/* 0x032`0x03ffFReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0400, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DMAC1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0404, U2H_R },
+					/* F_USB20HDC_REG_DMAS1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0408, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DMATCI1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x040c, U2H_R },
+					/* F_USB20HDC_REG_DMATC1 */
+					/* 0x04100x041fReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0420, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DMAC2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0424, U2H_R },
+					/* F_USB20HDC_REG_DMAS2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0428, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DMATCI2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x042c, U2H_R },
+					/* F_USB20HDC_REG_DMATC2 */
+					/* 0x043`0x04ffReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x0500, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_TESTC */
+					/* 0x0504`0x7fffFReserved */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8000, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8008, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8010, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8018, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8020, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8028, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8030, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8038, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL1_7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8004, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x800c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8014, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x801c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8024, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x802c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8034, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x803c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_HCEPCTRL2_7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8000, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8004, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8008, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x800c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8010, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8014, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8018, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x801c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8020, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL8 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8024, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL9 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8028, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL10 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x802c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL11 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8030, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL12 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8034, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL13 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8038, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL14 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x803c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCTRL15 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8040, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8044, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8048, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x804c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8050, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8054, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8058, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x805c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8060, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF8 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8064, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF9 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8068, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF10 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x806c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF11 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8070, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF12 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8074, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF13 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8078, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF14 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x807c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCONF15 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8080, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT0 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8084, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT1 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8088, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT2 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x808c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT3 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8090, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT4 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8094, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT5 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8098, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT6 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x809c, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT7 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80a0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT8 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80a4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT9 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80a8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT10 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80ac, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT11 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80b0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT12 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80b4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT13 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80b8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT14 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80bc, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT15 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80c0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT16 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80c4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT17 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80c8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT18 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80cc, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT19 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80d0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT20 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80d4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT21 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80d8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT22 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80dc, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT23 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80e0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT24 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80e4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT25 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80e8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT26 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80ec, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT27 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80f0, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT28 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80f4, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT29 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80f8, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT30 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x80fc, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPCOUNT31 */
+	{ F_USB20HDC_C_HSEL_ADDR_OFFSET + 0x8100, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_EPBUF */
+	{ F_USB20HDC_D_HSEL_ADDR_OFFSET + 0x0000, U2H_R | U2H_W },
+					/* F_USB20HDC_REG_DMA1 */
+	{ F_USB20HDC_D_HSEL_ADDR_OFFSET + 0x8000, U2H_R | U2H_W }
+					/* F_USB20HDC_REG_DMA2 */
+};
+
+static inline void ctrl_default_reg_cache_bits(
+	void __iomem *base_addr, u32 register_id, u32 *register_cache)
+{
+	return;
+}
+
+static inline void ctrl_ints_reg_cache_bits(void __iomem *base_addr,
+	u32 register_id, u32 *register_cache)
+{
+/* Global Interrupt Status register bit feild position */
+#define F_USB20HDC_REG_INTS_BIT_PHY_ERR_INT	3 /* phy_err_int */
+#define F_USB20HDC_REG_INTS_BIT_CMD_INT		4 /* cmd_int */
+#define F_USB20HDC_REG_INTS_BIT_DMA1_INT	8 /* dma1_int */
+#define F_USB20HDC_REG_INTS_BIT_DMA2_INT	9 /* dma2_int */
+
+	*register_cache |=
+		(((u32)1 << F_USB20HDC_REG_INTS_BIT_PHY_ERR_INT) |
+		((u32)1 << F_USB20HDC_REG_INTS_BIT_CMD_INT) |
+		((u32)1 << F_USB20HDC_REG_INTS_BIT_DMA1_INT) |
+		((u32)1 << F_USB20HDC_REG_INTS_BIT_DMA2_INT));
+	return;
+}
+
+static inline void ctrl_otgstsc_reg_cache_bits(
+	void __iomem *base_address, u32 register_id,
+	u32 *register_cache)
+{
+/* OTG Status Change register bit feild position */
+#define F_USB20HDC_REG_OTGSTS_BIT_OTG_TMROUT_C		0 /* otg_tmrout_c */
+#define F_USB20HDC_REG_OTGSTS_BIT_ID_C			6 /* id_c */
+#define F_USB20HDC_REG_OTGSTS_BIT_VBUS_VLD_C		10 /* vbus_vld_c */
+
+	*register_cache |=
+		(((u32)1 << F_USB20HDC_REG_OTGSTS_BIT_OTG_TMROUT_C) |
+		((u32)1 << F_USB20HDC_REG_OTGSTS_BIT_ID_C) |
+		((u32)1 << F_USB20HDC_REG_OTGSTS_BIT_VBUS_VLD_C));
+	return;
+}
+
+static inline void ctrl_devs_reg_cache_bits(void __iomem *base_addr,
+	u32 register_id, u32 *register_cache)
+{
+/* Device Status register bit feild position */
+#define F_USB20HDC_REG_DEVS_BIT_SUSPENDE_INT	24	/* suspende_int */
+#define F_USB20HDC_REG_DEVS_BIT_SUSPENDB_INT	25	/* suspendb_int */
+#define F_USB20HDC_REG_DEVS_BIT_SOF_INT		26	/* sof_int */
+#define F_USB20HDC_REG_DEVS_BIT_SETUP_INT	27	/* setup_int */
+#define F_USB20HDC_REG_DEVS_BIT_USBRSTE_INT	28	/* usbrste_int */
+#define F_USB20HDC_REG_DEVS_BIT_USBRSTB_INT	29	/* usbrstb_int */
+#define F_USB20HDC_REG_DEVS_BIT_STATUS_OK_INT	30	/* status_ok_int */
+#define F_USB20HDC_REG_DEVS_BIT_STATUS_NG_INT	31	/* status_ng_int */
+
+	*register_cache |=
+		(((u32)1 << F_USB20HDC_REG_DEVS_BIT_SUSPENDE_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_SUSPENDB_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_SOF_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_SETUP_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_USBRSTE_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_USBRSTB_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_STATUS_OK_INT) |
+		((u32)1 << F_USB20HDC_REG_DEVS_BIT_STATUS_NG_INT));
+	return;
+}
+
+
+static inline void ctrl_portsc_reg_cache_bits(
+	void __iomem *base_address, u32 register_id,
+	u32 *register_cache)
+{
+/* Port Status / Control register bit feild position */
+#define F_USB20HDC_REG_PORTSC_BIT_RESET_REQ	25 /* port_reset_req */
+#define F_USB20HDC_REG_PORTSC_BIT_SUSPEND_REQ	30 /* port_suspend_req */
+#define F_USB20HDC_REG_PORTSC_BIT_RESUME_REQ	31 /* port_resume_req */
+
+	*register_cache &=
+		~(((u32)1 << F_USB20HDC_REG_PORTSC_BIT_RESET_REQ) |
+		((u32)1 << F_USB20HDC_REG_PORTSC_BIT_SUSPEND_REQ) |
+		((u32)1 << F_USB20HDC_REG_PORTSC_BIT_RESUME_REQ));
+	return;
+}
+
+static inline void ctrl_portstsc_reg_cache_bits(
+	void __iomem *base_address, u32 register_id,
+	u32 *register_cache)
+{
+/* Port Status Change register bit feild position */
+#define F_USB20HDC_REG_PORTSTSC_BIT_CONNECTION_C	0 /* port_conection */
+#define F_USB20HDC_REG_PORTSTSC_BIT_ENABLE_C		1 /* port_enable_c */
+#define F_USB20HDC_REG_PORTSTSC_BIT_SUSPEND_C		2 /* port_suspend_c */
+#define F_USB20HDC_REG_PORTSTSC_BIT_OV_CURR_C		3 /* port_ov_curr_c */
+#define F_USB20HDC_REG_PORTSTSC_BIT_RESET_C		4 /* port_reset_c */
+
+	*register_cache |= (((u32)1 <<
+				F_USB20HDC_REG_PORTSTSC_BIT_CONNECTION_C) |
+		((u32)1 << F_USB20HDC_REG_PORTSTSC_BIT_ENABLE_C) |
+		((u32)1 << F_USB20HDC_REG_PORTSTSC_BIT_SUSPEND_C) |
+		((u32)1 << F_USB20HDC_REG_PORTSTSC_BIT_OV_CURR_C) |
+		((u32)1 << F_USB20HDC_REG_PORTSTSC_BIT_RESET_C));
+	return;
+}
+
+static inline void ctrl_hosteventsrc_reg_cache_bits(
+	void __iomem *base_address, u32 register_id,
+	u32 *register_cache)
+{
+/* Host Event Source register bit feild position */
+#define F_USB20HDC_REG_HOSTEVENTSRC_BIT_SOFSTART	0 /* sofstart */
+#define F_USB20HDC_REG_HOSTEVENTSRC_BIT_FRAMEOV		1 /* frameov */
+#define F_USB20HDC_REG_HOSTEVENTSRC_BIT_TRANS_DONE	8 /* trans_done[0:7] */
+
+	*register_cache |= (((u32)1 <<
+				F_USB20HDC_REG_HOSTEVENTSRC_BIT_SOFSTART) |
+		((u32)1 << F_USB20HDC_REG_HOSTEVENTSRC_BIT_FRAMEOV) |
+		((u32)0xff <<
+				F_USB20HDC_REG_HOSTEVENTSRC_BIT_TRANS_DONE));
+	return;
+}
+
+static inline u32 get_epctrl_register_bits(
+	void __iomem *base_addr,
+	u32 register_id, u8 start_bit,
+	u8 bit_length)
+{
+	u32 counter;
+	u32 register_cache[3] = {0, 0, 0};
+	u32 mask = (u32)-1 >> (32 - bit_length);
+
+	if (!(f_usb20hdc_register[register_id].access & U2H_R))
+		return 0;
+
+	for (counter = 0xffff; counter; counter--) {
+		register_cache[0] = __raw_readl(base_addr +
+					f_usb20hdc_register[
+					register_id].address_offset) >>
+					start_bit & mask;
+		register_cache[1] = __raw_readl(base_addr +
+					f_usb20hdc_register[
+					register_id].address_offset) >>
+					start_bit & mask;
+		register_cache[2] = __raw_readl(base_addr +
+					f_usb20hdc_register[
+					register_id].address_offset) >>
+					start_bit & mask;
+		if ((register_cache[0] == register_cache[1]) &&
+			(register_cache[1] == register_cache[2]))
+			break;
+	}
+
+	return register_cache[2];
+}
+
+static inline void set_register_bits(void __iomem *base_addr,
+	u32 register_id, u8 start_bit,
+	u8 bit_length, u32 value)
+{
+	u32 register_cache = 0;
+	u32 mask = (u32)-1 >> (32 - bit_length);
+
+	value &= mask;
+
+	if (f_usb20hdc_register[register_id].access & U2H_R)
+		register_cache = __raw_readl(base_addr +
+			f_usb20hdc_register[register_id].address_offset);
+
+	ctrl_reg_cache_bits(base_addr, register_id, &register_cache);
+
+	register_cache &= ~(mask << start_bit);
+	register_cache |= (value << start_bit);
+
+	if (f_usb20hdc_register[register_id].access & U2H_W)
+		__raw_writel(register_cache, base_addr +
+			f_usb20hdc_register[register_id].address_offset);
+
+	return;
+}
+
+static inline u32 get_register_bits(void __iomem *base_addr,
+	u32 register_id, u8 start_bit,
+	u8 bit_length)
+{
+	u32 register_cache = 0;
+	u32 mask = (u32)-1 >> (32 - bit_length);
+
+	if (f_usb20hdc_register[register_id].access & U2H_R)
+		register_cache = __raw_readl(base_addr +
+			f_usb20hdc_register[register_id].address_offset);
+
+	return register_cache >> start_bit & mask;
+}
+
+/* host mode reg function */
+static inline void set_port_power_ctl_req(void __iomem *base_addr,
+					u8 power_control_on)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_PORTSC, 21, 1,
+				power_control_on);
+}
+
+static inline void set_port_power_req(void __iomem *base_addr,
+						u8 power_on)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_PORTSC, 22, 1, power_on);
+}
+
+static inline u8 get_port_power_rhs(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,
+						F_USB20HDC_REG_PORTSC, 2, 1);
+}
+
+/*common registers in both host mode and device mode*/
+static inline void set_byte_order(void __iomem *base_addr, u8 big_endian)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_CONF, 0, 1, big_endian);
+}
+
+static inline void set_burst_wait(void __iomem *base_addr, u8 waits)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_CONF, 1, 1, waits);
+}
+
+static inline void set_soft_reset(void __iomem *base_addr)
+{
+	u32 counter;
+	set_register_bits(base_addr, F_USB20HDC_REG_CONF, 2, 1, 1);
+	for (counter = 0xffff; ((counter) && (get_register_bits(base_addr,
+				F_USB20HDC_REG_CONF, 2, 1))); counter--)
+				;
+}
+
+static inline void set_host_en(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_MODE, 0, 1, enable);
+}
+
+static inline u8 get_host_en(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_MODE, 0, 1);
+}
+
+static inline void set_dev_en(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_MODE, 1, 1, enable);
+}
+
+static inline u8 get_dev_en(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_MODE, 1, 1);
+}
+
+static inline void set_host_inten(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 0, 1, enable);
+}
+
+static inline u8 get_host_inten(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN, 0, 1);
+}
+
+static inline void set_dev_int_mode(void __iomem *base_addr, u8 mode)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_MODE, 2, 1, mode);
+}
+
+static inline void set_dev_addr_load_mode(void __iomem *base_addr, u8 mode)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_MODE, 3, 1, mode);
+}
+
+static inline void set_dev_inten(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 1, 1, enable);
+}
+
+static inline u8 get_dev_inten(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,
+					F_USB20HDC_REG_INTEN, 1, 1);
+}
+
+static inline void set_otg_inten(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 2, 1, enable);
+}
+
+static inline u8 get_otg_inten(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN, 2, 1);
+}
+
+static inline void set_phy_err_inten(void __iomem *base_addr, u8 en)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 3, 1, en);
+}
+
+static inline u8 get_phy_err_inten(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN, 3, 1);
+}
+
+static inline void set_cmd_inten(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 4, 1, enable);
+}
+
+static inline u8 get_cmd_inten(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN, 4, 1);
+}
+
+static inline void set_dma_inten(void __iomem *base_addr, u8 dma_chan, u8 en)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 8 + dma_chan, 1, en);
+}
+
+/* USB DMA channel symbolic constant */
+#define F_USB20HDC_DMA_CH1	0	/* USB DMA channel 1 */
+#define F_USB20HDC_DMA_CH2	1	/* USB DMA channel 2 */
+
+static inline u8 get_dma_inten(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN,
+		8 + dma_ch, 1);
+}
+
+static inline void set_dev_ep_inten(void __iomem *base_addr, u8 ep_ch, u8 en)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTEN, 16 + ep_ch, 1, en);
+}
+
+/* endpoint channel symbolic constant */
+#define F_USB20HDC_EP0	0	/* endpoint 0 */
+#define F_USB20HDC_EP1	1	/* endpoint 1 */
+#define F_USB20HDC_EP2	2	/* endpoint 2 */
+#define F_USB20HDC_EP3	3	/* endpoint 3 */
+#define F_USB20HDC_EP4	4	/* endpoint 4 */
+#define F_USB20HDC_EP5	5	/* endpoint 5 */
+#define F_USB20HDC_EP6	6	/* endpoint 6 */
+#define F_USB20HDC_EP7	7	/* endpoint 7 */
+#define F_USB20HDC_EP8	8	/* endpoint 8 */
+#define F_USB20HDC_EP9	9	/* endpoint 9 */
+#define F_USB20HDC_EP10	10	/* endpoint 10 */
+#define F_USB20HDC_EP11	11	/* endpoint 11 */
+#define F_USB20HDC_EP12	12	/* endpoint 12 */
+#define F_USB20HDC_EP13	13	/* endpoint 13 */
+#define F_USB20HDC_EP14	14	/* endpoint 14 */
+#define F_USB20HDC_EP15	15	/* endpoint 15 */
+
+static inline u8 get_dev_ep_inten(void __iomem *base_addr, u8 ep_ch)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTEN,
+		16 + ep_ch, 1);
+}
+
+static inline u8 get_host_int(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS,
+						0, 1);
+}
+
+static inline u8 get_dev_int(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS,
+						1, 1);
+}
+
+static inline u8 get_otg_int(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS, 2, 1);
+}
+
+static inline u8 get_phy_err_int(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS, 3, 1);
+}
+
+static inline void clear_phy_err_int(void __iomem *base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTS, 3, 1, 0);
+}
+
+static inline u8 get_cmd_int(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS,
+						4, 1);
+}
+
+static inline void clear_cmd_int(void __iomem *base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTS, 4, 1, 0);
+}
+
+static inline u8 get_dma_int(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS,
+						8 + dma_ch, 1);
+}
+
+static inline void clear_dma_int(void __iomem *base_addr, u8 dma_ch)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_INTS, 8 + dma_ch, 1, 0);
+}
+
+static inline u8 get_dev_ep_int(void __iomem *base_addr, u8 ep_ch)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_INTS,
+						16 + ep_ch, 1);
+}
+
+
+/*endpoint command register*/
+static void f_cmd0_spin(void __iomem *base_addr, u8 ep_chan)
+{
+	u32 counter;
+	for (counter = 0xffff; ((counter) && (get_register_bits(base_addr,
+			F_USB20HDC_REG_EPCMD0 + ep_chan, 31, 1))); counter--)
+		;
+}
+
+static inline void set_start(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 0, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+
+static inline void set_init(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 2, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+static inline void set_bufwr(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 3, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+static inline void set_bufrd(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 4, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+static inline void set_toggle_clear(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 8, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+static inline void set_toggle_set(void __iomem *base_addr, u8 ep_ch)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch, 7, 1, 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+static inline void set_et(void __iomem *base_addr, u8 ep_ch, u8 type)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch,
+								23, 2, type);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+
+static inline void set_bnum(void __iomem *base_addr, u8 ep_ch, u8 buffers)
+{
+	f_cmd0_spin(base_addr, ep_ch);
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCMD0 + ep_ch,
+		26, 2, buffers - 1);
+	f_cmd0_spin(base_addr, ep_ch);
+}
+
+
+/*OTG register in F_USB20HDC Macro*/
+static inline void set_dm_pull_down(void __iomem *base_addr, u8 pull_down)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGC, 7, 1, pull_down);
+}
+
+static inline void set_dp_pull_down(void __iomem *base_addr, u8 pull_down)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGC, 8, 1, pull_down);
+}
+
+static inline void set_id_pull_up(void __iomem *base_addr, u8 pull_up)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGC, 9, 1, pull_up);
+}
+
+static inline u8 get_otg_tmrout(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,	F_USB20HDC_REG_OTGSTS, 0, 1);
+}
+
+static inline u8 get_id(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,	F_USB20HDC_REG_OTGSTS, 6, 1);
+}
+
+static inline u8 get_tmrout_c(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 0, 1);
+}
+
+static inline u8 get_id_c(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 6, 1);
+}
+
+static inline u8 get_vbus_vld(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_OTGSTS, 10, 1);
+}
+
+static inline void clear_tmrout_c(void __iomem *base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 0, 1, 0);
+}
+
+static inline void clear_id_c(void __iomem *base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 6, 1, 0);
+}
+
+static inline u8 get_vbus_vld_c(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 10, 1);
+}
+
+static inline void clear_vbus_vld_c(void __iomem *base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSC, 10, 1, 0);
+}
+
+static inline void set_vbus_vld_fen(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSFALL, 10, 1, enable);
+}
+
+static inline void set_vbus_vld_ren(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSRISE, 10, 1, enable);
+}
+
+static inline u8 get_vbus_vld_ren(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,
+					F_USB20HDC_REG_OTGSTSRISE, 10, 1);
+}
+
+static inline u8 get_vbus_vld_fen(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,
+					F_USB20HDC_REG_OTGSTSFALL, 10, 1);
+}
+
+static inline void set_otg_tmrout_ren(void __iomem *base_addr, u8 en)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSRISE, 0, 1, en);
+}
+
+static inline u8 get_otg_tmrout_ren(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,	F_USB20HDC_REG_OTGSTSRISE,
+		0, 1);
+}
+
+static inline void set_id_fen(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSFALL, 6, 1, enable);
+}
+
+static inline void set_id_ren(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGSTSRISE, 6, 1, enable);
+}
+
+static inline u8 get_id_fen(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr,
+					F_USB20HDC_REG_OTGSTSFALL, 6, 1);
+}
+
+static inline u8 get_id_ren(void __iomem *base_addr)
+{
+	return (u8)get_register_bits(base_addr, F_USB20HDC_REG_OTGSTSRISE,
+		6, 1);
+}
+
+static inline void set_start_tmr(void __iomem *base_addr, u8 start)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGTC, 0, 1, start);
+}
+
+static inline void set_tmr_init_val(void __iomem *base_addr, u16 val)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_OTGT, 0, 16, val);
+}
+
+
+/*DMA register in F_USB20HDC Macro*/
+static inline void set_dma_st(void __iomem *base_addr, u8 dma_ch, u8 start)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 0, 1, start);
+}
+
+static inline void set_dma_mode(void __iomem *base_addr, u8 dma_ch, u8 mode)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 2, 1, mode);
+}
+
+/* DMA mode symbolic constant */
+#define DMA_MODE_DEMAND	0	/* demand transter */
+#define DMA_MODE_BLOCK	1	/* block transter */
+
+static inline void set_dma_sendnull(void __iomem *base_addr, u8 dma_ch, u8 send)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 3, 1, send);
+}
+
+static inline u8 get_dma_busy(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, dmac_register[dma_ch], 1, 1);
+}
+
+static inline u8 get_dma_ep(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, dmac_register[dma_ch], 8, 4);
+}
+
+static inline u8 get_dma_np(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, dmas_register[dma_ch], 0, 1);
+}
+
+static inline void set_dma_int_empty(void __iomem *base_addr, u8 dma_ch,
+	u8 empty)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 4, 1, empty);
+}
+
+static inline void set_dma_spr(void __iomem *base_addr, u8 dma_ch, u8 spr)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 5, 1, spr);
+}
+
+static inline void set_dma_ep(void __iomem *base_addr, u8 dma_ch, u8 ep_ch)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 8, 4, ep_ch);
+}
+
+static inline void set_dma_blksize(void __iomem *base_addr, u8 dma_ch, u16 size)
+{
+	set_register_bits(base_addr, dmac_register[dma_ch], 16, 11, size);
+}
+
+static inline u8 get_dma_sp(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u8)get_register_bits(base_addr, dmas_register[dma_ch], 1, 1);
+}
+
+static inline void set_dma_tci(void __iomem *base_addr, u8 dma_ch, u32 bytes)
+{
+	set_register_bits(base_addr, dmatci_register[dma_ch], 0, 32, bytes);
+}
+
+static inline u32 get_dma_tci(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u32)get_register_bits(base_addr,
+					dmatci_register[dma_ch], 0, 32);
+}
+
+static inline u32 get_dma_tc(void __iomem *base_addr, u8 dma_ch)
+{
+	return (u32)get_register_bits(base_addr, dmatc_register[dma_ch], 0, 32);
+}
+
+/*TEST register in F_USB20HDC Macro*/
+static inline void set_testp(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_TESTC, 0, 1, enable);
+}
+
+static inline void set_testj(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_TESTC, 1, 1, enable);
+}
+
+static inline void set_testk(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_TESTC, 2, 1, enable);
+}
+
+static inline void set_testse0nack(void __iomem *base_addr, u8 enable)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_TESTC, 3, 1, enable);
+}
+
+/*endpoint configuration and count register in F_USB20HDC Macro*/
+static inline void clear_epconf(void __iomem *base_addr, u8 ep_ch)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCONF0 + ep_ch,
+				0, 32, 0x00000000);
+}
+
+static inline void set_base(void __iomem *base_addr, u8 ep_ch,
+	u16 buffer_base_addr)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCONF0 + ep_ch, 0, 13,
+				(buffer_base_addr & 0x7fff) >> 2);
+}
+
+static inline void set_size(void __iomem *base_addr, u8 ep_ch,
+	u16 size)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCONF0 + ep_ch,
+		13, 11, size);
+}
+
+static inline void set_countidx(void __iomem *base_addr, u8 ep_ch, u8 index)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCONF0 + ep_ch,
+		24, 5, index);
+}
+
+static inline void clear_epcount(void __iomem *base_addr, u8 index)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCOUNT0 + index,
+				0, 32, 0x00000000);
+}
+
+static inline void set_appcnt(void __iomem *base_addr, u8 index,
+	u16 bytes)
+{
+	set_register_bits(base_addr, F_USB20HDC_REG_EPCOUNT0 +
+		index, 0, 11, bytes);
+}
+
+static inline u16 get_appcnt(void __iomem *base_addr, u8 index)
+{
+	return (u16) get_register_bits(base_addr,
+				F_USB20HDC_REG_EPCOUNT0 + index, 0, 11);
+}
+
+static inline u16 get_phycnt(void __iomem *base_addr, u8 index)
+{
+	return (u16)get_register_bits(base_addr,
+				F_USB20HDC_REG_EPCOUNT0 + index, 16, 11);
+}
+
+static inline u8 get_rdatapid(void __iomem *base_addr,
+	u8 index)
+{
+	return (u16)get_register_bits(base_addr,
+				F_USB20HDC_REG_EPCOUNT0 + index, 30, 2);
+}
+/* receive data PID symbolic constant */
+#define RDATAPID_DATA0	0	/* DATA0 */
+#define RDATAPID_DATA1	1	/* DATA1 */
+#define RDATAPID_DATA2	2	/* DATA2 */
+#define RDATAPID_MDATA	3	/* MDATA */
+
+static inline u32 get_epbuf_address_offset(void)
+{
+	return f_usb20hdc_register[F_USB20HDC_REG_EPBUF].address_offset -
+					F_USB20HDC_C_HSEL_ADDR_OFFSET;
+}
+
+static inline unsigned long get_epbuf_dma_address(phys_addr_t base_addr,
+	u8 dma_ch)
+{
+	return (unsigned long)(base_addr + f_usb20hdc_register[
+			F_USB20HDC_REG_DMA1 + dma_ch].address_offset);
+}
+
+static inline void write_epbuf(void __iomem *base_addr, u32 offset,
+	u32 *data, u32 bytes)
+{
+	memcpy_toio(base_addr + F_USB20HDC_C_HSEL_ADDR_OFFSET +
+		offset, data, bytes);
+}
+
+static inline void read_epbuf(void __iomem *base_addr, u32 offset, u32 *data,
+	u32 bytes)
+{
+	memcpy_fromio(data, base_addr + F_USB20HDC_C_HSEL_ADDR_OFFSET +
+		offset, bytes);
+}
+
+static inline void __iomem *remap_iomem_region(unsigned long physical_base_addr,
+	unsigned long size)
+{
+	/* convert physical address to virtula address */
+	return (void __iomem *)ioremap(physical_base_addr, size);
+}
+
+static inline void unmap_iomem_region(void __iomem *virtual_base_addr)
+{
+	iounmap(virtual_base_addr);
+}
+
+static inline u32 get_irq_flag(void)
+{
+	return IRQF_SHARED;
+}
+
+static inline u32 get_dma_irq_flag(u8 dma_chan)
+{
+	return IRQF_SHARED;
+}
+
+
+static inline void enable_bus_power_supply(void __iomem *base_addr,
+	u8 enable)
+{
+	u32 counter;
+
+	set_port_power_req(base_addr, enable ? 1 : 0);
+	if (enable)
+		for (counter = 0xffff; ((counter) &&
+				(!get_port_power_rhs(base_addr))); counter--)
+			;
+	else
+		for (counter = 0xffff; ((counter) &&
+				(get_port_power_rhs(base_addr))); counter--)
+			;
+}
+
+int f_usb20hdc_dma_attach(struct f_usb20hdc_dma_data *dma_data,
+	int ch_num, int size);
+int f_usb20hdc_dma_detach(struct f_usb20hdc_dma_data *dma_data,
+	int ch_num, int size);
+
+
+#endif /* __MACH_F_USB20HDC_H */
diff --git a/include/linux/platform_data/mb86s70-iomap.h b/include/linux/platform_data/mb86s70-iomap.h
new file mode 100644
index 0000000..6de31d6
--- /dev/null
+++ b/include/linux/platform_data/mb86s70-iomap.h
@@ -0,0 +1,34 @@
+
+#include <linux/sizes.h>
+
+#define SEC_RSTADDR_OFF		0x3fc
+
+#define MB86S70_MHU_PHYS		0x2b1f0000
+#define MB86S70_MHU_SIZE		SZ_4K
+
+/* Internal SRAM */
+#define MB86S70_ISRAM_PHYS		0x2e000000
+#define MB86S70_ISRAM_SIZE		SZ_16K
+#define MB86S70_TRAMPOLINE_PHYS		(MB86S70_ISRAM_PHYS + 0x3c00)
+
+#define MB86S70_SSRAM_PHYS		0x04000000
+#define MB86S70_SSRAM_SIZE		SZ_32K
+#define SEC_BACKDOOR_OFF		0x208
+
+#define MB86S70_UART0_PHYS		0x31040000
+#define MB86S70_UART0_SIZE		SZ_4K
+
+#define MB86S70_UART1_PHYS		0x31050000
+#define MB86S70_UART1_SIZE		SZ_4K
+
+#define MB86S70_UART0_VIRT		IOMEM(0xfe000000)
+#define MB86S70_UART1_VIRT	(MB86S70_UART0_VIRT + MB86S70_UART0_SIZE)
+#define MB86S70_ISRAM_VIRT	(MB86S70_UART1_VIRT + MB86S70_UART1_SIZE)
+#define MB86S70_MHU_VIRT	(MB86S70_ISRAM_VIRT + MB86S70_ISRAM_SIZE)
+
+#define MB86S70_SSRAM_VIRT	IOMEM(0xfe100000)
+
+
+/* Non-Secure High-Priority Channel is used */
+#define MB86S70_SHM_FROM_SCB_VIRT	(MB86S70_ISRAM_VIRT + 0x3000)
+#define MB86S70_TRAMPOLINE_VIRT		(MB86S70_ISRAM_VIRT + 0x3c00)
diff --git a/include/linux/platform_data/mb8ac0300-gpio.h b/include/linux/platform_data/mb8ac0300-gpio.h
new file mode 100644
index 0000000..033bfcb
--- /dev/null
+++ b/include/linux/platform_data/mb8ac0300-gpio.h
@@ -0,0 +1,67 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/include/mach/gpio.h
+ *
+ *  Copyright (C) 2011 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ *  This program is free software: you can redistribute it and/or modify
+ *  it under the terms of the GNU General Public License as published by
+ *  the Free Software Foundation, version 2 of the License.
+ *
+ *  This program is distributed in the hope that it will be useful,
+ *  but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *  GNU General Public License for more details.
+ *
+ *  You should have received a copy of the GNU General Public License
+ *  along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __MACH_GPIO_H
+#define __MACH_GPIO_H
+
+#include <linux/spinlock.h>
+#include <linux/clk.h>
+
+#define MB8AC0300_GPIO_NR		32	/* gpio pin number for every
+							gpio controller*/
+#define MB8AC0300_GPIO_NR_PER_REG	8	/* gpio pin number for every
+							register hold */
+
+#define MB8AC0300_GPIO_REG_PDR0		0x00UL	/* offset of PDR register */
+#define MB8AC0300_GPIO_REG_DDR0		0x10UL	/* offset of DDR register */
+#define MB8AC0300_GPIO_REG_PFR0		0x20UL	/* offset of PFR register */
+
+/* peripheral function is set */
+#define MB8AC0300_GPIO_IS_PERIPHERAL		1
+/* output direction is set */
+#define MB8AC0300_GPIO_IS_OUTPUT		1
+
+/* shift gpio pin number offset to register address offset */
+#define MB8AC0300_GPIO_OFFSET_TO_REG(offset) \
+				(((offset) / MB8AC0300_GPIO_NR_PER_REG) * 4)
+
+/* platform data information structure of gpio driver */
+struct mb8ac0300_gpio_platform_data {
+	unsigned int	gpio_base;	/* first pin number of gpio chip      */
+	unsigned int	irq_base;	/* first extint number of gpio chip   */
+	unsigned int	irq_gpio_min;	/* first gpio pin number for extint   */
+	unsigned int	irq_gpio_max;	/* last gpio pin number for extint    */
+};
+
+/* gpio chip structure for gpio driver use */
+struct mb8ac0300_gpio_chip {
+	spinlock_t	lock;		/* lock for protect register accesses */
+	void		*base;		/* base virtual address of registers  */
+	struct mb8ac0300_gpio_platform_data	*pdata;
+					/* pointer of platform data           */
+	struct gpio_chip gc;		/* generic gpio chip interface        */
+	struct clk	*clk;		/* pointer of clock device            */
+	unsigned char	directions[MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG];
+					/* gpio direction register init value */
+	unsigned char	values[MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG];
+					/* gpio data register init value      */
+	unsigned char	functions[MB8AC0300_GPIO_NR/MB8AC0300_GPIO_NR_PER_REG];
+					/* gpio function register init value  */
+};
+
+#endif /* __MACH_GPIO_H */
diff --git a/include/linux/platform_data/mb8ac0300-hs_spi.h b/include/linux/platform_data/mb8ac0300-hs_spi.h
new file mode 100644
index 0000000..8f9342e
--- /dev/null
+++ b/include/linux/platform_data/mb8ac0300-hs_spi.h
@@ -0,0 +1,166 @@
+/*
+ *  linux/arch/arm/mach-mb8ac0300/include/mach/hs_spi.h
+ *
+ * Copyright (C) 2010-2012 FUJITSU SEMICONDUCTOR LIMITED
+ *
+ * This program is free software: you can redistribute it and/or modify
+ * it under the terms of the GNU General Public License as published by
+ * the Free Software Foundation, version 2 of the License.
+ *
+ * This program is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ * GNU General Public License for more details.
+ *
+ * You should have received a copy of the GNU General Public License
+ * along with this program.  If not, see <http://www.gnu.org/licenses/>.
+ */
+
+#ifndef __MACH_HS_SPI_H
+#define __MACH_HS_SPI_H
+
+#include <linux/spi/spi.h>
+#include <linux/spi/spi_bitbang.h>
+
+#define	HS_SPI_CS_DELAY		3	/* Slave-Select to Clock Delay */
+#define	HS_SPI_FIFO_WIDTH	0	/* FIFO Width (1byte) */
+#define	HS_SPI_TX_FIFO_LEVEL	1	/* TX-FIFO Threshold Level */
+#define	HS_SPI_RX_FIFO_LEVEL	0	/* RX-FIFO Threshold Level */
+#define	HS_SPI_FIFO_LEN		16	/* FIFO Length */
+
+#define	HS_SPI_MAX_TRANSFER_LEN	0xFFFF	/* Max numbers of transfer bytes */
+/*
+ * HS_SPI Direct Mode Transfer Protocol
+ */
+#define	HS_SPI_LEGACY_TX_RX	0x00	/* TX and RX, in legacy mode */
+#define	HS_SPI_LEGACY_RX_ONLY	0x04	/* RX only, in legacy mode */
+#define	HS_SPI_DUAL_RX_ONLY	0x05	/* RX only, in dual mode */
+#define	HS_SPI_QUAD_RX_ONLY	0x06	/* RX only, in quad mode */
+#define	HS_SPI_LEGACY_TX_ONLY	0x08	/* TX only, in legacy mode */
+#define	HS_SPI_DUAL_TX_ONLY	0x09	/* TX only, in dual mode */
+#define	HS_SPI_QUAD_TX_ONLY	0x0A	/* TX only, in quad mode */
+
+/*
+ * HS_SPI Command Sequencer Mode
+ */
+#define	HS_SPI_IDLE_TIME	0xFFFF	/* Command Sequencer Idle Time */
+#define	HS_SPI_CS_CONT_BIT	0x08	/* Command Sequencer Continuous bit */
+#define	HS_SPI_CS_1BIT		(3<<1)	/* Command Sequencer Protocol 1bit*/
+#define	HS_SPI_CS_2BIT		(2<<1)	/* Command Sequencer Protocol 2bit*/
+#define	HS_SPI_CS_4BIT		(1<<1)	/* Command Sequencer Protocol 4bit*/
+
+#define	HS_SPI_DECODE_ADDR3	0x0301	/* Transmit address bits [31:24] */
+#define	HS_SPI_DECODE_ADDR2	0x0201	/* Transmit address bits [23:16] */
+#define	HS_SPI_DECODE_ADDR1	0x0101	/* Transmit address bits [15:08] */
+#define	HS_SPI_DECODE_ADDR0	0x0001	/* Transmit address bits [07:00] */
+#define	HS_SPI_HIGH_Z_BYTE	0x0401	/* High-Z byte for 1 byte time */
+#define	HS_SPI_LIST_END		0x0701	/* Command Sequencer List End */
+/*
+ * HS_SPI Command Sequencer Multi Bit Mode
+ */
+#define	HS_SPI_LEGACY_BIT	0x00	/* use the legacy SPI protocol */
+#define	HS_SPI_DUAL_BIT		0x01	/* use the dual-bit SPI protocol */
+#define	HS_SPI_QUAD_BIT		0x02	/* use the quad-bit SPI protocol */
+#define	HS_SPI_CR_QUAD		0x02	/* Quad Mode Flag */
+#define	HS_SPI_SR_WIP		0x01	/* Write In Progress Flag */
+#define	HS_SPI_CONTINUOUS_READ_MODE 0xA0 /* Continuous read mode bits value */
+
+/*
+ * Flash operation codes
+ */
+#define	HS_SPI_CMD_RDID		0x9f	/* Read JEDEC ID */
+#define	HS_SPI_CMD_NORM_READ	0x03	/* Read data bytes (low frequency) */
+#define	HS_SPI_CMD_FAST_READ	0x0b	/* Read data bytes (high frequency) */
+#define	HS_SPI_CMD_DOR		0x3B	/* Dual Output Read */
+#define	HS_SPI_CMD_QOR		0x6B	/* Quad Output Read */
+#define	HS_SPI_CMD_DIOR		0xBB	/* Dual I/O High Performance Read */
+#define	HS_SPI_CMD_QIOR		0xEB	/* Quad I/O High Performance Read */
+#define	HS_SPI_CMD_WREN		0x06	/* Write enable */
+#define	HS_SPI_CMD_WRDN		0x04	/* Write Disable */
+#define	HS_SPI_CMD_CHIP_ERASE	0xc7	/* Erase whole flash chip */
+#define	HS_SPI_CMD_SE		0xd8	/* Sector erase (usually 64KiB) */
+#define	HS_SPI_CMD_BE_4K	0x20	/* Erase 4KiB block */
+#define	HS_SPI_CMD_PP		0x02	/* Page program (up to 256 bytes) */
+#define	HS_SPI_CMD_QPP		0x32	/* Quad Page Programming */
+#define	HS_SPI_CMD_RDSR		0x05	/* Read status register */
+#define	HS_SPI_CMD_WRSR		0x01	/* Write status register 1 byte */
+#define	HS_SPI_CMD_RCR		0x35	/* Read Configuration Register (CFG) */
+
+
+#define	HS_SPI_DIRECT_MODE		1
+#define	HS_SPI_COMMAND_SEQUENCER	2
+
+#define	HS_SPI_HCLK			0
+#define	HS_SPI_PCLK			1
+
+/**
+ * struct hs_spi_pdata - HS SPI device platform data
+ * @mode: Direct mode or command sequencer mode.
+ * @read_operation: Read operation for direct mode and command sequencer mode.
+ * @write_operation: Write operation only for direct mode.
+ * @num_chipselect: Max numbers of slaves can be selected.
+ * @addr_width: Address width (number of bytes) for slaves.
+ * @bits_per_word: Data transfers involve one or more words; word sizes
+ * @bank_size: Bank size of each external memory devices.
+ *
+ * A @hs_spi_pdata is used by hs spi driver.
+ */
+struct hs_spi_pdata {
+	int	mode;
+	int	num_chipselect;		/* total chipselects */
+	int	bank_size;		/* bank size */
+	int	clock;
+	int	syncon;
+};
+
+/* HS_SPI Controller driver's data. */
+
+/**
+ * struct hs_spi - HS_SPI Controller driver's data.
+ * @csa: Command sequencer access area.
+ *
+ * Other members of the struct are private.
+ *
+ * A @csa is used by flash driver hs_spi_flash.
+ */
+struct hs_spi {
+	/* private  */
+
+	/* bitbang has to be first */
+	struct spi_bitbang	bitbang;
+	struct completion	done;
+
+	int			tx_irq;
+	int			rx_irq;
+	int			fault_irq;
+	unsigned int		len;
+	unsigned int		tx_cnt;
+	unsigned int		rx_cnt;
+	int			fault_flag;
+	int			bank_size;
+
+	int			stop;
+
+	/* data buffers */
+	const unsigned char	*tx;
+	unsigned char		*rx;
+
+	struct spi_master	*master;
+
+	struct clk		*clk;
+	struct device		*dev;
+	struct hs_spi_pdata	*pdata;
+
+	void __iomem		*reg;
+
+	/* external */
+	void __iomem		*csa;
+	unsigned int		csa_size;
+
+	struct spi_device	*spi[4];
+	unsigned int		pcc[4];
+	/* spi transfer protocol */
+	unsigned int		bitwidth;
+};
+
+#endif /* __MACH_HS_SPI_H */
diff --git a/include/linux/platform_data/mb8ac0300-iomap.h b/include/linux/platform_data/mb8ac0300-iomap.h
new file mode 100644
index 0000000..a744644
--- /dev/null
+++ b/include/linux/platform_data/mb8ac0300-iomap.h
@@ -0,0 +1,39 @@
+#include <linux/sizes.h>
+
+#define MB8AC0300_UART0_PHYS		0xfff6b000
+#define MB8AC0300_UART0_SIZE		SZ_4K
+
+#define MB8AC0300_CORE_PHYS		0xf8100000
+#define MB8AC0300_CORE_SIZE		SZ_8K
+
+#define MB8AC0300_MRBC_PHYS		0xfff68000
+#define MB8AC0300_MRBC_SIZE		SZ_4K
+
+#define MB8AC0300_CRG11_MAIN_PHYS	0xfff60000
+#define MB8AC0300_CRG11_MAIN_SIZE	SZ_4K
+#define MB8AC0300_CRG11_GMAC_PHYS	0xfff4d000
+#define MB8AC0300_CRG11_GMAC_SIZE	SZ_4K
+#define MB8AC0300_CRG11_LCD_PHYS	0xfff4c000
+#define MB8AC0300_CRG11_LCD_SIZE	SZ_4K
+#define MB8AC0300_CRG11_DMC_PHYS	0xfff4b000
+#define MB8AC0300_CRG11_DMC_SIZE	SZ_4K
+
+#define MB8AC0300_XSRAM_PHYS		0x01000000
+#define MB8AC0300_XSRAM_SIZE		SZ_128K
+
+#define MB8AC0300_DDRC_PHYS		0xfff4a000
+#define MB8AC0300_DDRC_SIZE		SZ_4K
+
+
+#define MB8AC0300_UART0_VIRT		IOMEM(0xfe06b000)
+#define MB8AC0300_CORE_VIRT		IOMEM(0xfd100000)
+#define MB8AC0300_MRBC_VIRT		IOMEM(0xfe068000)
+#define MB8AC0300_CRG11_MAIN_VIRT	IOMEM(0xfe060000)
+#define MB8AC0300_CRG11_GMAC_VIRT	IOMEM(0xfe04d000)
+#define MB8AC0300_CRG11_LCD_VIRT	IOMEM(0xfe04c000)
+#define MB8AC0300_CRG11_DMC_VIRT	IOMEM(0xfe04b000)
+#define MB8AC0300_DDRC_VIRT		IOMEM(0xfe04a000)
+#define MB8AC0300_XSRAM_VIRT		IOMEM(0xfe020000)
+
+#define MB8AC0300_GIC_CPU_VIRT (MB8AC0300_CORE_VIRT + 0x1000)
+
diff --git a/include/linux/platform_data/mb8ac0300-irqs.h b/include/linux/platform_data/mb8ac0300-irqs.h
new file mode 100644
index 0000000..9e465ce
--- /dev/null
+++ b/include/linux/platform_data/mb8ac0300-irqs.h
@@ -0,0 +1,73 @@
+#define IRQ_ARM_GLOBAL_TIMER		27
+#define IRQ_ARM_FIQ			28
+#define IRQ_ARM_PRIVATE_TIMER		29
+#define IRQ_ARM_WDT			30
+#define IRQ_ARM_IRQ			31
+
+#define IRQ_OFFSET			32
+
+#define IRQ_COMMTX0			(0 + IRQ_OFFSET)
+#define IRQ_COMMRX0			(1 + IRQ_OFFSET)
+#define IRQ_COMMTX1			(2 + IRQ_OFFSET)
+#define IRQ_COMMRX1			(3 + IRQ_OFFSET)
+
+#define IRQ_L2CCINTR			(8 + IRQ_OFFSET)
+#define IRQ_TIMINT0			(9 + IRQ_OFFSET)
+#define IRQ_TIMINT1			(10 + IRQ_OFFSET)
+#define IRQ_WDOGINT			(11 + IRQ_OFFSET)
+#define IRQ_UART0			(12 + IRQ_OFFSET)
+#define IRQ_UART1			(13 + IRQ_OFFSET)
+#define IRQ_MEMCTRLINT			(14 + IRQ_OFFSET)
+#define IRQ_RELC_INT			(15 + IRQ_OFFSET)
+
+#define IRQ_XDMAC0			(16 + IRQ_OFFSET)
+#define IRQ_XDMAC1			(17 + IRQ_OFFSET)
+#define IRQ_XDMAC2			(18 + IRQ_OFFSET)
+#define IRQ_XDMAC3			(19 + IRQ_OFFSET)
+#define IRQ_XDMAC4			(20 + IRQ_OFFSET)
+#define IRQ_XDMAC5			(21 + IRQ_OFFSET)
+#define IRQ_XDMAC6			(22 + IRQ_OFFSET)
+#define IRQ_XDMAC7			(23 + IRQ_OFFSET)
+
+#define IRQ_EXTINT0			(24 + IRQ_OFFSET)
+#define IRQ_EXTINT1			(25 + IRQ_OFFSET)
+#define IRQ_EXTINT2			(26 + IRQ_OFFSET)
+#define IRQ_EXTINT3			(27 + IRQ_OFFSET)
+#define IRQ_EXTINT4			(28 + IRQ_OFFSET)
+#define IRQ_EXTINT5			(29 + IRQ_OFFSET)
+#define IRQ_EXTINT6			(30 + IRQ_OFFSET)
+#define IRQ_EXTINT7			(31 + IRQ_OFFSET)
+#define IRQ_EXTINT8			(32 + IRQ_OFFSET)
+#define IRQ_EXTINT9			(33 + IRQ_OFFSET)
+#define IRQ_EXTINT10			(34 + IRQ_OFFSET)
+#define IRQ_EXTINT11			(35 + IRQ_OFFSET)
+#define IRQ_EXTINT12			(36 + IRQ_OFFSET)
+#define IRQ_EXTINT13			(37 + IRQ_OFFSET)
+#define IRQ_EXTINT14			(38 + IRQ_OFFSET)
+#define IRQ_EXTINT15			(39 + IRQ_OFFSET)
+
+#define IRQ_IPCUINT0			(40 + IRQ_OFFSET)
+#define IRQ_IPCUINT1			(41 + IRQ_OFFSET)
+
+#define IRQ_EXTINT16			(112 + IRQ_OFFSET)
+#define IRQ_EXTINT17			(113 + IRQ_OFFSET)
+#define IRQ_EXTINT18			(114 + IRQ_OFFSET)
+#define IRQ_EXTINT19			(115 + IRQ_OFFSET)
+#define IRQ_EXTINT20			(116 + IRQ_OFFSET)
+#define IRQ_EXTINT21			(117 + IRQ_OFFSET)
+#define IRQ_EXTINT22			(118 + IRQ_OFFSET)
+#define IRQ_EXTINT23			(119 + IRQ_OFFSET)
+#define IRQ_EXTINT24			(120 + IRQ_OFFSET)
+#define IRQ_EXTINT25			(121 + IRQ_OFFSET)
+#define IRQ_EXTINT26			(122 + IRQ_OFFSET)
+#define IRQ_EXTINT27			(123 + IRQ_OFFSET)
+#define IRQ_EXTINT28			(124 + IRQ_OFFSET)
+#define IRQ_EXTINT29			(125 + IRQ_OFFSET)
+#define IRQ_EXTINT30			(126 + IRQ_OFFSET)
+#define IRQ_EXTINT31			(127 + IRQ_OFFSET)
+
+#define MB8AC0300_NR_IRQS		256     /* (32 + 128) */
+#define MB8AC0300_IRQ_OFFSET		32
+#ifndef CONFIG_SPARSE_IRQ
+#define NR_IRQS				MB8AC0300_NR_IRQS
+#endif
diff --git a/include/linux/pm_domain.h b/include/linux/pm_domain.h
index 7c1d252..04473d4 100644
--- a/include/linux/pm_domain.h
+++ b/include/linux/pm_domain.h
@@ -310,4 +310,50 @@ static inline void pm_genpd_syscore_poweron(struct device *dev)
 	pm_genpd_syscore_switch(dev, false);
 }
 
+/* OF power domain providers */
+struct of_device_id;
+
+struct genpd_onecell_data {
+	struct generic_pm_domain **domains;
+	unsigned int domain_num;
+};
+
+typedef struct generic_pm_domain *(*genpd_xlate_t)(struct of_phandle_args *args,
+						   void *data);
+
+#ifdef CONFIG_PM_GENERIC_DOMAINS_OF
+int of_genpd_add_provider(struct device_node *np, genpd_xlate_t xlate,
+			  void *data);
+void of_genpd_del_provider(struct device_node *np);
+
+struct generic_pm_domain *of_genpd_xlate_simple(
+					struct of_phandle_args *genpdspec,
+					void *data);
+struct generic_pm_domain *of_genpd_xlate_onecell(
+					struct of_phandle_args *genpdspec,
+					void *data);
+
+int genpd_bind_domain(struct device *dev);
+int genpd_unbind_domain(struct device *dev);
+#else /* !CONFIG_PM_GENERIC_DOMAINS_OF */
+static inline int of_genpd_add_provider(struct device_node *np,
+					genpd_xlate_t xlate, void *data)
+{
+	return 0;
+}
+static inline void of_genpd_del_provider(struct device_node *np) {}
+
+#define of_genpd_xlate_simple		NULL
+#define of_genpd_xlate_onecell		NULL
+
+static inline int genpd_bind_domain(struct device *dev)
+{
+	return 0;
+}
+static inline int genpd_unbind_domain(struct device *dev)
+{
+	return 0;
+}
+#endif /* CONFIG_PM_GENERIC_DOMAINS_OF */
+
 #endif /* _LINUX_PM_DOMAIN_H */
diff --git a/include/linux/scb_mhu_api.h b/include/linux/scb_mhu_api.h
new file mode 100644
index 0000000..e8daa6b
--- /dev/null
+++ b/include/linux/scb_mhu_api.h
@@ -0,0 +1,452 @@
+#ifndef __SCB_MHU_API_H
+#define __SCB_MHU_API_H
+
+#define CMD_MASK    0x7f    /* 128 possible commands */
+#define RESP_BIT    (1 << 7) /* If it's a response */
+
+#define ENC_CMD(c)  ((c) & CMD_MASK)
+#define DEC_CMD(v)  (((v) & ~CMD_MASK) ? CMD_INVALID : ((v) & CMD_MASK))
+
+#define ENC_REP(r)  (((r) & CMD_MASK) | RESP_BIT)
+
+/* If v is the reply to command c */
+#define IS_A_REP(v, c)  (((v) & RESP_BIT) && (((v) & CMD_MASK) == (c)))
+
+enum {
+	CMD_INVALID = 0,
+	/* I2C PassThrough
+	 con_id, addr, speed, flags  must be same in REQ and REP
+	 If SUCCESS on SCB, REP_len == REQ_len
+	 If ERROR on SCB REP_len == actual number of bytes read/written (< REP_len)
+	*/
+	CMD_PERI_POWER_SET_REQ = 2,
+	/*
+	peri_id  must be same in REQ and REP;
+	If SUCCESS on SCB,  REP_en == current status{0,1} of gate
+	If ERROR on SCB(only means clk doesn't exist),  REP_en == 0, that is, clk is disabled/gated
+	 */
+	CMD_PERI_CLOCK_GATE_SET_REQ = 3,
+	/*
+	 cntrlr, domain, port  must be same in REQ and REP;
+	 If SUCCESS on SCB,  REP_en == current status{0,1} of gate
+	 If ERROR on SCB(only means clk doesn't exist),  REP_en == 0, that is, clk is disabled/gated
+	 freqency  ignored in REQ and REP;
+	*/
+	CMD_PERI_CLOCK_GATE_GET_REQ = 4,
+	/*
+	 cntrlr, domain, port must be same in REQ and REP;
+	 If SUCCESS on SCB, REP_frequency == REQ_frequency
+	 If ERROR on SCB,  REP_frequency == original/old rate of clock
+	 'en' is ignored in REQ and REP;
+	*/
+	CMD_PERI_CLOCK_RATE_SET_REQ = 5,
+	/*
+	 cntrlr, domain, port must be same in REQ and REP;
+	 If SUCCESS on SCB, REP_frequency == current rate of clock
+	 If ERROR on SCB(only means clk doesn't exist),  REP_frequency == 0
+	 'en' is ignored in REQ and REP;
+	*/
+	CMD_PERI_CLOCK_RATE_GET_REQ = 6,
+	CMD_CPU_CLOCK_GATE_SET_REQ = 7,
+	CMD_CPU_CLOCK_GATE_GET_REQ = 8,
+	CMD_CPU_CLOCK_RATE_SET_REQ = 9,
+	CMD_CPU_CLOCK_RATE_GET_REQ = 0xa,
+	CMD_CLUSTER_OPP_GET_REQ = 0xb,
+	CMD_CLOCK_DSI_PIXEL_REQ = 0xc,
+	CMD_SCB_CAPABILITY_GET_REQ = 0xd,
+	CMD_SYS_RESET_CAUSE_GET_REQ = 0xe,
+	CMD_SYS_SPECIFIC_INFO_GET_REQ = 0xf,
+	CMD_REBOOT_AP_AFTER_REQ = 0x10,
+
+	CMD_TAIKI_REQ = 0x11,
+	CMD_TAIKI_ASYNC_MSG_REQ = 0x12,
+	CMD_GET_WORD_REQ = 0x13,
+	CMD_HARD_RESET_REQ = 0x14,
+	
+	CMD_MAINTENANCE_MODE_REQ = 0x15,
+
+	CMD_MEMORY_LAYOUT_GET_REQ = 0x19,
+
+	CMD_POWERDOMAIN_GET_REQ = 0x1a,
+	CMD_POWERDOMAIN_SET_REQ = 0x1b,
+
+	CMD_THERMAL_INFO_REQ = 0x1d,
+	CMD_RESUME_ENTRY_POINT_SET_REQ = 0x1e,
+	CMD_RESUME_ENTRY_POINT_GET_REQ = 0x1f,
+
+	/* Do NOT add new commands below this line */
+	MHU_NUM_CMDS,
+};
+
+#define CMD_PERI_POWER_SET_REP	ENC_REP(CMD_PERI_POWER_SET_REQ)
+#define CMD_PERI_CLOCK_GATE_SET_REP	ENC_REP(CMD_PERI_CLOCK_GATE_SET_REQ)
+#define CMD_PERI_CLOCK_GATE_GET_REP	ENC_REP(CMD_PERI_CLOCK_GATE_GET_REQ)
+#define CMD_PERI_CLOCK_RATE_SET_REP	ENC_REP(CMD_PERI_CLOCK_RATE_SET_REQ)
+#define CMD_PERI_CLOCK_RATE_GET_REP	ENC_REP(CMD_PERI_CLOCK_RATE_GET_REQ)
+#define CMD_CPU_CLOCK_GATE_SET_REP	ENC_REP(CMD_CPU_CLOCK_GATE_SET_REQ)
+#define CMD_CPU_CLOCK_GATE_GET_REP	ENC_REP(CMD_CPU_CLOCK_GATE_GET_REQ)
+#define CMD_CPU_CLOCK_RATE_SET_REP	ENC_REP(CMD_CPU_CLOCK_RATE_SET_REQ)
+#define CMD_CPU_CLOCK_RATE_GET_REP	ENC_REP(CMD_CPU_CLOCK_RATE_GET_REQ)
+#define CMD_CLUSTER_OPP_GET_REP	ENC_REP(CMD_CLUSTER_OPP_GET_REQ)
+#define CMD_CLOCK_DSI_PIXEL_REP	ENC_REP(CMD_CLOCK_DSI_PIXEL_REQ)
+#define CMD_SCB_CAPABILITY_GET_REP	ENC_REP(CMD_SCB_CAPABILITY_GET_REQ)
+#define CMD_SYS_RESET_CAUSE_GET_REP	ENC_REP(CMD_SYS_RESET_CAUSE_GET_REQ)
+#define CMD_SYS_SPECIFIC_INFO_GET_REP	ENC_REP(CMD_SYS_SPECIFIC_INFO_GET_REQ)
+#define CMD_GET_WORD_REP	ENC_REP(CMD_GET_WORD_REQ)
+#define CMD_REBOOT_AP_AFTER_REP	ENC_REP(CMD_REBOOT_AP_AFTER_REQ)
+#define CMD_TAIKI_REP			ENC_REP(CMD_TAIKI_REQ)
+#define CMD_TAIKI_ASYNC_MSG_REP		ENC_REP(CMD_TAIKI_ASYNC_MSG_REQ)
+#define CMD_HARD_RESET_REP		ENC_REP(CMD_HARD_RESET_REQ)
+#define CMD_MAINTENANCE_MODE_REP	ENC_RSP(CMD_MAINTENANCE_MODE_REQ)
+#define CMD_MEMORY_LAYOUT_GET_REP	ENC_REP(CMD_MEMORY_LAYOUT_GET_REQ)
+#define CMD_POWERDOMAIN_GET_REP		ENC_REP(CMD_POWERDOMAIN_GET_REQ)
+#define CMD_POWERDOMAIN_SET_REP		ENC_REP(CMD_POWERDOMAIN_SET_REQ)
+#define CMD_THERMAL_INFO_REP		ENC_REP(CMD_THERMAL_INFO_REQ)
+#define CMD_RESUME_ENTRY_POINT_SET_REP		ENC_REP(CMD_RESUME_ENTRY_POINT_SET_REQ)
+#define CMD_RESUME_ENTRY_POINT_GET_REP		ENC_REP(CMD_RESUME_ENTRY_POINT_GET_REQ)
+
+struct cmd_periclk_control {
+	u32 payload_size;
+	u32 cntrlr;
+	u32 domain;
+	u32 port;
+	u32 en;
+	u64 freqency;
+} __packed;
+
+struct cmd_peripower_control {
+	u32 payload_size;
+	u32 peri_id;
+	u32 en;
+} __packed;
+
+#define AT_WFI_DO_NOTHING	0x0
+#define AT_WFI_DO_SUSPEND	0x1
+#define AT_WFI_DO_POWEROFF	0x2
+#define AT_WFI_COLOR_MASK	0x3
+
+/*
+ * Offset of 4-bytes wfi 'color' register from ISRAM Start
+ * The 4-bytes store 'color' for each cpu core as
+ *   A15_0, A15_1, A7_0, A7_1
+ */
+#define WFI_COLOR_REG_OFFSET	0x3f00
+
+/**
+ * struct CMD_CPU_CLOCK_GATE_SET_REQ
+ *        CMD_CPU_CLOCK_GATE_SET_REP
+ *        CMD_CPU_CLOCK_GATE_GET_REQ
+ *        CMD_CPU_CLOCK_GATE_GET_REP
+ *
+ * @cluster_class : 0 fixed
+ * @cluster_id    : 0x0:A15, 0x1:A7
+ * @cpu_id        : 0x0:CPU0, 0x1:CPU1, 0x2:CPU2
+ * @cpu_state     : 0x0:OFF, 0x1:ON, 0x2:SLEEP0,
+ * 		    0x3:SLEEP1, 0xffffffff:CPU does not exist
+ *
+ * This packet format is for the commands
+ * CMD_CPU_CLOCK_GATE_SET_REQ and CMD_CPU_CLOCK_GATE_SET_REP
+ * CMD_CPU_CLOCK_GATE_GET_REQ and CMD_CPU_CLOCK_GATE_GET_REP
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'cpu_control'
+ */
+struct cmd_cpu_control_gate {
+	u32 payload_size;
+	u32 cluster_class;
+	u32 cluster_id;
+	u32 cpu_id;
+#define SCB_CPU_STATE_OFF	0x0
+#define SCB_CPU_STATE_ON	0x1
+#define SCB_CPU_STATE_SUSP	0x2
+	u32 cpu_state;
+} __packed;
+
+
+/**
+ * struct CMD_CPU_CLOCK_RATE_SET_REQ
+ *        CMD_CPU_CLOCK_RATE_SET_REP
+ *        CMD_CPU_CLOCK_RATE_GET_REQ
+ *        CMD_CPU_CLOCK_RATE_GET_REP
+ *
+ * @cluster_class : 0 fixed
+ * @cluster_id    : 0x0:A15, 0x1:A7
+ * @freqency      : (mHz)
+ *
+ * This packet format is for the commands
+ * CMD_CPU_CLOCK_RATE_SET_REQ and CMD_CPU_CLOCK_RATE_SET_REP
+ * CMD_CPU_CLOCK_RATE_GET_REQ and CMD_CPU_CLOCK_RATE_GET_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'cpu_control'
+ */
+struct cmd_cpu_control_rate {
+	u32 payload_size;
+	u32 cluster_class;
+	u32 cluster_id;
+	u32 cpu_id;
+	u64 freqency;
+} __packed;
+
+
+/**
+ * @cluster_opp_voltage  : (mV)
+ * @cluster_opp_freqency : (mHz)
+ */
+struct cluster_opp {
+	u32 voltage;
+	u64 freqency;
+} __packed;
+
+/**
+ * struct CMD_CLUSTER_OPP_GET_REQ
+ *        CMD_CLUSTER_OPP_GET_REP
+ *
+ * @cmd_status_id       : Parameters for notifying Error Information of the response command
+ * @cluster_class       : always fixed the value to be 0
+ * @cluster_id          : 0x0:A15, 0x1:A7
+ * @opp_num             : elements in the operating points array 'opp_param'
+ * @opp_param       : array of OPPs
+ *
+ * This packet format is for the commands CMD_CLUSTER_OPP_GET_REQ and CMD_CLUSTER_OPP_GET_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ */
+struct cmd_cluster_opp_get {
+	u32 payload_size;
+	u32 cluster_class;
+	u32 cluster_id;
+	u32 opp_num;
+	struct cluster_opp opp[0];
+} __packed;
+
+/* Payload for CMD_THERMAL_INFO_REQ from SCB to AP */
+struct mb86s7x_thermal_info {
+	u32 payload_size;
+	s32 temp[16];
+};
+
+/**
+ * struct CMD_CLOCK_DSI_PIXEL_REQ
+ *        CMD_CLOCK_DSI_PIXEL_REP
+ *
+ * @mode          : T.B.D
+ *
+ * This packet format is for the commands CMD_CLOCK_DSI_PIXEL_REQ and CMD_CLOCK_DSI_PIXEL_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'mode'
+ */
+struct cmd_clk_dsi_pixel {
+	u32 payload_size;
+	u32 mode;
+} __packed;
+
+
+/**
+ * struct CMD_SCB_CAPABILITY_GET_REQ
+ *        CMD_SCB_CAPABILITY_GET_REP
+ *
+ * @version    : Protocol version
+ *
+ * This packet format is for the commands CMD_SCB_CAPABILITY_GET_REQ and CMD_SCB_CAPABILITY_GET_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'specific_info'
+ */
+struct cmd_scb_version {
+	u32 payload_size;
+	u32 version;
+	u32 config_version;
+#define S73_SCB_CAPABILITY0_STR_ENABLE    (1 << 0)
+#define S73_SCB_CAPABILITY0_HSSPI_ACCESS  (1 << 1)
+#define S7X_SCB_CAPABILITY0_SECURE_AP     (1 << 2)
+	u32 capabilities[2];
+
+} __packed;
+
+
+/**
+ * struct CMD_SYS_RESET_CAUSE_GET_REQ
+ *        CMD_SYS_RESET_CAUSE_GET_REP
+ *
+ * @factor : Parameters to notify the reset source
+ *
+ * This packet format is for the commands CMD_SYS_RESET_CAUSE_GET_REQ and CMD_SYS_RESET_CAUSE_GET_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'reason'
+ */
+struct cmd_sys_reset_cause {
+	u32 payload_size;
+#define RESET_BY_COLDBOOT	0x0
+#define RESET_BY_SW		0x1
+#define RESET_BY_WDOG		0x2
+#define RESET_BY_xxyz		0x3
+	u32 cause;
+} __packed;
+
+
+/**
+ * struct CMD_SYS_SPECIFIC_INFO_GET_REQ
+ *        CMD_SYS_SPECIFIC_INFO_GET_REP
+ *
+ * @version    : Protocol version
+ *
+ * This packet format is for the commands CMD_SYS_SPECIFIC_INFO_GET_REQ and CMD_SYS_SPECIFIC_INFO_GET_REP.
+ * AP populates it as a command. SCB populates it as a response.
+ *
+ * SCB's response should always show the final status via 'specific_info'
+ */
+struct cmd_sys_specific_info {
+	u32 payload_size;
+	u64 specific_info;
+} __packed;
+
+/* Command structure for CMD_REBOOT_AP_AFTER_REQ */
+struct cmd_reboot_ap {
+	u32 payload_size;
+	u32 delay; /* Reboot the AP after 'delay' milliseconds */
+} __packed;
+
+/* Used with CMD_GET_WORD_REQ for debugging purposes */
+struct cmd_get_word {
+	u32 payload_size;
+#define REG_TOP_STATUS_CFG_CTL		0x44010000
+#define REG_TOP_STATUS_CRG11_ALW	0xF1000000
+#define REG_TOP_STATUS_CRG11_DDR3	0xF1010000
+#define REG_TOP_STATUS_CRG11_MAIN	0xF1020000
+#define REG_TOP_STATUS_CRG11_HDMI	0xF1040000
+#define REG_TOP_STATUS_CRG11_DPHY	0xF1050000
+#define REG_TOP_STATUS_CRGSYSOC		0xF1080000
+#define REG_TOP_STATUS_PRMUX		0xCA4D0000
+	u32 address;
+	u32 data;
+} __packed;
+
+/**
+ * struct CMD_TAIKI_REQ
+ *        CMD_TAIKI_REP
+ *
+ * @payload_data : Data address
+ *
+ * This packet format is for the command CMD_TAIKI_REQ and CMD_TAIKI_REP.
+ *
+ */
+struct cmd_taiki {
+	u32	payload_data[64];
+} __packed;
+
+/**
+ * struct CMD_TAIKI_ASYNC_MSG_REQ
+ *        CMD_TAIKI_ASYNC_MSG_REP
+ *
+ * @payload_data : Data address
+ *
+ * This packet format is for the command CMD_TAIKI_ASYNC_MSG_REQ and CMD_TAIKI_ASYNC_MSG_REP.
+ *
+ */
+struct cmd_taiki_async_msg {
+	u32	payload_data[64];
+} __packed;
+
+/**
+ * struct CMD_HARD_RESET_REQ
+ *        CMD_HARD_RESET_REP
+ */
+struct cmd_hard_reset {
+    u32 payload_size;
+    u32 delay;         /* hard-reset after 'delay' ms */
+} __attribute__ ((packed));
+
+
+
+/**
+ * struct CMD_MEMORY_LAYOUT_GET_REQ
+ *        CMD_MEMORY_LAYOUT_GET_REP
+ */
+
+struct memory_layout_region {
+	u64 start;
+	u64 length; /* currently only 32-bit region length used by DT */
+}  __attribute__ ((packed));
+
+struct cmd_memory_layout {
+	u32 payload_size;
+	u32 count_regions; /* filled in by SCB */
+	struct memory_layout_region regions[0]; /* filled in by SCB */
+} __attribute__ ((packed));
+
+/**
+ * CMD_POWERDOMAIN_GET_REQ
+ * CMD_POWERDOMAIN_GET_REP
+ * CMD_POWERDOMAIN_SET_REQ
+ * CMD_POWERDOMAIN_SET_REP
+ */
+
+struct cmd_powerdomain {
+	u32 payload_size;
+	u32 powerdomain_index;
+	u32 state; /* 0 = OFF, 1 = ON; AP fills for SET, SCB fills for GET */
+} __attribute__ ((packed));
+
+/**
+ * struct CMD_RESUME_ENTRY_POINT_SET_REQ
+ *        CMD_RESUME_ENTRY_POINT_SET_REP
+ *        CMD_RESUME_ENTRY_POINT_GET_REQ
+ *        CMD_RESUME_ENTRY_POINT_GET_REP
+ *
+ * @resume_flag : 1 - AP is resumed from retention state.
+ *                0 - AP is coldboot'ed.
+ *     Valid only in CMD_RESUME_ENTRY_POINT_GET_REP.
+ *
+ * @resume_entry_point : Primary core's resume entry physical address from retention state.
+ *     Valid only in CMD_RESUME_ENTRY_POINT_SET_REQ or in CMD_RESUME_ENTRY_POINT_GET_REP.
+ *
+ * Tell/Ask SCB of AP's resume entry point
+ *
+ */
+struct cmd_resume_entry_point_msg {
+	uint32_t payload_size;
+	uint32_t resume_flag;
+	uint32_t resume_entry_point;
+} __attribute__ ((packed));
+
+
+
+#define MB86S70_VERSION_SUPPORT_BLOCKDEV 0x509
+
+
+#if 0
+# At any time, there may be many clients wanting to send a command
+   but at most one that could send a reply.
+# A reply from remote is always routed to the client whose command
+   was the last sent.
+# A command from remote is matched against list of register handlers
+   and routed to the one for the command if it exists.
+#endif
+
+/*
+ cmd : Command code
+ buf : pointer to command packet
+ len : size of buffer = max(sizeof(cmd_packet), sizeof(reply_packet))
+ c   : Client never init's the completion.
+       If mhu_send_packet() returns 1, client waits on it.
+       If mhu_send_packet() returns 0, client assumes 'buf'
+        already contains reply and doesn't have to wait on 'c'.
+       If mhu_send_packet() returns < 0, submission failed.
+       'c == NULL' implies client is in atomic context.
+ */
+int mhu_send_packet(int cmd, void *buf, int len, struct completion *c);
+
+/* This function _must_ not sleep */
+typedef void (*mhu_handler_t)(u32 cmd, u8 rcbuf[]);
+int mhu_hndlr_set(u32 cmd, mhu_handler_t);
+void mhu_hndlr_clr(u32 cmd, mhu_handler_t);
+
+extern int skip_mhu;
+extern const struct cmd_scb_version *mb86s7x_get_scb_version(void);
+extern void mb86s70_pm_power_up_setup(unsigned int affinity_level);
+extern void mb86s70_reboot(u32 delay);
+extern u32 phys_mcpm_entry_point, phys_cpu_reset, boot_cpu_color_offset;
+
+#endif /* __SCB_MHU_API_H */
diff --git a/include/linux/serial_core.h b/include/linux/serial_core.h
index 2ea3a99..9430998 100644
--- a/include/linux/serial_core.h
+++ b/include/linux/serial_core.h
@@ -67,6 +67,7 @@ struct uart_ops {
 	void		(*pm)(struct uart_port *, unsigned int state,
 			      unsigned int oldstate);
 	int		(*set_wake)(struct uart_port *, unsigned int state);
+	void		(*wake_peer)(struct uart_port *);
 
 	/*
 	 * Return a string describing the type of the port
@@ -289,6 +290,22 @@ static inline int uart_poll_timeout(struct uart_port *port)
 /*
  * Console helpers.
  */
+struct earlycon_device {
+	struct console *con;
+	struct uart_port port;
+	char options[16];		/* e.g., 115200n8 */
+	unsigned int baud;
+};
+int setup_earlycon(char *buf, const char *match,
+		   int (*setup)(struct earlycon_device *, const char *));
+
+#define EARLYCON_DECLARE(name, func) \
+static int __init name ## _setup_earlycon(char *buf) \
+{ \
+	return setup_earlycon(buf, __stringify(name), func); \
+} \
+early_param("earlycon", name ## _setup_earlycon);
+
 struct uart_port *uart_get_console(struct uart_port *ports, int nr,
 				   struct console *c);
 void uart_parse_options(char *options, int *baud, int *parity, int *bits,
diff --git a/include/linux/thermal.h b/include/linux/thermal.h
index a386a1c..f7e11c7 100644
--- a/include/linux/thermal.h
+++ b/include/linux/thermal.h
@@ -25,6 +25,7 @@
 #ifndef __THERMAL_H__
 #define __THERMAL_H__
 
+#include <linux/of.h>
 #include <linux/idr.h>
 #include <linux/device.h>
 #include <linux/workqueue.h>
@@ -143,6 +144,7 @@ struct thermal_cooling_device {
 	int id;
 	char type[THERMAL_NAME_LENGTH];
 	struct device device;
+	struct device_node *np;
 	void *devdata;
 	const struct thermal_cooling_device_ops *ops;
 	bool updated; /* true if the cooling device does not need update */
@@ -172,7 +174,7 @@ struct thermal_zone_device {
 	int emul_temperature;
 	int passive;
 	unsigned int forced_passive;
-	const struct thermal_zone_device_ops *ops;
+	struct thermal_zone_device_ops *ops;
 	const struct thermal_zone_params *tzp;
 	struct thermal_governor *governor;
 	struct list_head thermal_instances;
@@ -207,6 +209,16 @@ struct thermal_bind_params {
 	 * See Documentation/thermal/sysfs-api.txt for more information.
 	 */
 	int trip_mask;
+
+	/*
+	 * This is an array of cooling state limits. Must have exactly
+	 * 2 * thermal_zone.number_of_trip_points. It is an array consisting
+	 * of tuples <lower-state upper-state> of state limits. Each trip
+	 * will be associated with one state limit tuple when binding.
+	 * A NULL pointer means <THERMAL_NO_LIMITS THERMAL_NO_LIMITS>
+	 * on all trips.
+	 */
+	unsigned long *binding_limits;
 	int (*match) (struct thermal_zone_device *tz,
 			struct thermal_cooling_device *cdev);
 };
@@ -214,6 +226,14 @@ struct thermal_bind_params {
 /* Structure to define Thermal Zone parameters */
 struct thermal_zone_params {
 	char governor_name[THERMAL_NAME_LENGTH];
+
+	/*
+	 * a boolean to indicate if the thermal to hwmon sysfs interface
+	 * is required. when no_hwmon == false, a hwmon sysfs interface
+	 * will be created. when no_hwmon == true, nothing will be done
+	 */
+	bool no_hwmon;
+
 	int num_tbps;	/* Number of tbp entries */
 	struct thermal_bind_params *tbp;
 };
@@ -224,8 +244,31 @@ struct thermal_genl_event {
 };
 
 /* Function declarations */
+#ifdef CONFIG_THERMAL_OF
+struct thermal_zone_device *
+thermal_zone_of_sensor_register(struct device *dev, int id,
+				void *data, int (*get_temp)(void *, long *),
+				int (*get_trend)(void *, long *));
+void thermal_zone_of_sensor_unregister(struct device *dev,
+				       struct thermal_zone_device *tz);
+#else
+static inline struct thermal_zone_device *
+thermal_zone_of_sensor_register(struct device *dev, int id,
+				void *data, int (*get_temp)(void *, long *),
+				int (*get_trend)(void *, long *))
+{
+	return NULL;
+}
+
+static inline
+void thermal_zone_of_sensor_unregister(struct device *dev,
+				       struct thermal_zone_device *tz)
+{
+}
+
+#endif
 struct thermal_zone_device *thermal_zone_device_register(const char *, int, int,
-		void *, const struct thermal_zone_device_ops *,
+		void *, struct thermal_zone_device_ops *,
 		const struct thermal_zone_params *, int, int);
 void thermal_zone_device_unregister(struct thermal_zone_device *);
 
@@ -238,6 +281,9 @@ void thermal_zone_device_update(struct thermal_zone_device *);
 
 struct thermal_cooling_device *thermal_cooling_device_register(char *, void *,
 		const struct thermal_cooling_device_ops *);
+struct thermal_cooling_device *
+thermal_of_cooling_device_register(struct device_node *np, char *, void *,
+				   const struct thermal_cooling_device_ops *);
 void thermal_cooling_device_unregister(struct thermal_cooling_device *);
 struct thermal_zone_device *thermal_zone_get_zone_by_name(const char *name);
 int thermal_zone_get_temp(struct thermal_zone_device *tz, unsigned long *temp);
diff --git a/include/linux/usb/composite.h b/include/linux/usb/composite.h
index 5e61589..c464f42 100644
--- a/include/linux/usb/composite.h
+++ b/include/linux/usb/composite.h
@@ -242,6 +242,10 @@ struct usb_configuration {
 
 	struct usb_composite_dev	*cdev;
 
+	/* USB 3.0 additions */
+	int			(*get_status)(struct usb_function *);
+	int			(*func_suspend)(struct usb_function *,
+						u8 suspend_opt);
 	/* private: */
 	/* internals */
 	struct list_head	list;
diff --git a/include/linux/usb/gadget.h b/include/linux/usb/gadget.h
index 942ef5e..73c33f9 100644
--- a/include/linux/usb/gadget.h
+++ b/include/linux/usb/gadget.h
@@ -884,6 +884,9 @@ extern void usb_del_gadget_udc(struct usb_gadget *gadget);
 extern int udc_attach_driver(const char *name,
 		struct usb_gadget_driver *driver);
 
+extern int usb_add_gadget_udc(struct device *parent, struct usb_gadget *gadget);
+extern void usb_del_gadget_udc(struct usb_gadget *gadget);
+
 /*-------------------------------------------------------------------------*/
 
 /* utility to simplify dealing with string descriptors */
diff --git a/include/linux/usb/phy.h b/include/linux/usb/phy.h
index 4403680..6c0b1c5 100644
--- a/include/linux/usb/phy.h
+++ b/include/linux/usb/phy.h
@@ -142,7 +142,7 @@ extern void usb_remove_phy(struct usb_phy *);
 /* helpers for direct access thru low-level io interface */
 static inline int usb_phy_io_read(struct usb_phy *x, u32 reg)
 {
-	if (x->io_ops && x->io_ops->read)
+	if (x && x->io_ops && x->io_ops->read)
 		return x->io_ops->read(x, reg);
 
 	return -EINVAL;
@@ -150,7 +150,7 @@ static inline int usb_phy_io_read(struct usb_phy *x, u32 reg)
 
 static inline int usb_phy_io_write(struct usb_phy *x, u32 val, u32 reg)
 {
-	if (x->io_ops && x->io_ops->write)
+	if (x && x->io_ops && x->io_ops->write)
 		return x->io_ops->write(x, val, reg);
 
 	return -EINVAL;
@@ -159,7 +159,7 @@ static inline int usb_phy_io_write(struct usb_phy *x, u32 val, u32 reg)
 static inline int
 usb_phy_init(struct usb_phy *x)
 {
-	if (x->init)
+	if (x && x->init)
 		return x->init(x);
 
 	return 0;
@@ -168,14 +168,14 @@ usb_phy_init(struct usb_phy *x)
 static inline void
 usb_phy_shutdown(struct usb_phy *x)
 {
-	if (x->shutdown)
+	if (x && x->shutdown)
 		x->shutdown(x);
 }
 
 static inline int
 usb_phy_vbus_on(struct usb_phy *x)
 {
-	if (!x->set_vbus)
+	if (!x || !x->set_vbus)
 		return 0;
 
 	return x->set_vbus(x, true);
@@ -184,7 +184,7 @@ usb_phy_vbus_on(struct usb_phy *x)
 static inline int
 usb_phy_vbus_off(struct usb_phy *x)
 {
-	if (!x->set_vbus)
+	if (!x || !x->set_vbus)
 		return 0;
 
 	return x->set_vbus(x, false);
@@ -258,7 +258,7 @@ usb_phy_set_power(struct usb_phy *x, unsigned mA)
 static inline int
 usb_phy_set_suspend(struct usb_phy *x, int suspend)
 {
-	if (x->set_suspend != NULL)
+	if (x && x->set_suspend != NULL)
 		return x->set_suspend(x, suspend);
 	else
 		return 0;
@@ -267,7 +267,7 @@ usb_phy_set_suspend(struct usb_phy *x, int suspend)
 static inline int
 usb_phy_notify_connect(struct usb_phy *x, enum usb_device_speed speed)
 {
-	if (x->notify_connect)
+	if (x && x->notify_connect)
 		return x->notify_connect(x, speed);
 	else
 		return 0;
@@ -276,7 +276,7 @@ usb_phy_notify_connect(struct usb_phy *x, enum usb_device_speed speed)
 static inline int
 usb_phy_notify_disconnect(struct usb_phy *x, enum usb_device_speed speed)
 {
-	if (x->notify_disconnect)
+	if (x && x->notify_disconnect)
 		return x->notify_disconnect(x, speed);
 	else
 		return 0;
diff --git a/include/uapi/linux/f_media-shell.h b/include/uapi/linux/f_media-shell.h
new file mode 100644
index 0000000..4f140b7
--- /dev/null
+++ b/include/uapi/linux/f_media-shell.h
@@ -0,0 +1,28 @@
+
+
+#ifndef __VPU_DRV_H__
+#define __VPU_DRV_H__
+
+#define JDI_IOCTL_MAGIC  'V'
+
+#define JDI_IOCTL_ALLOCATE_PHYSICAL_MEMORY	_IO(JDI_IOCTL_MAGIC, 0)
+#define JDI_IOCTL_FREE_PHYSICALMEMORY		_IO(JDI_IOCTL_MAGIC, 1)
+#define JDI_IOCTL_WAIT_INTERRUPT		_IO(JDI_IOCTL_MAGIC, 2)
+#define JDI_IOCTL_SET_CLOCK_GATE		_IO(JDI_IOCTL_MAGIC, 3)
+#define JDI_IOCTL_RESET				_IO(JDI_IOCTL_MAGIC, 4)
+#define JDI_IOCTL_GET_INSTANCE_POOL		_IO(JDI_IOCTL_MAGIC, 5)
+#define JDI_IOCTL_GET_RESERVED_VIDEO_MEMORY_INFO _IO(JDI_IOCTL_MAGIC, 8)
+
+#endif
+
+
+/* for userland IOCTL data strcut and
+ * it's data length have to same as struct ms_buffer
+ */
+typedef struct ms_buffer {
+	uint32_t size;
+	uint64_t phys_addr;
+	uint32_t *base;		/* kernel logical address in use kernel */
+	uint32_t *virt_addr;	/* virtual user space address */
+	uint32_t drm_gem_handle;
+} ms_buffer;
diff --git a/kernel/power/Kconfig b/kernel/power/Kconfig
index 4645596..f618092 100644
--- a/kernel/power/Kconfig
+++ b/kernel/power/Kconfig
@@ -291,6 +291,17 @@ config PM_GENERIC_DOMAINS_RUNTIME
 	def_bool y
 	depends on PM_RUNTIME && PM_GENERIC_DOMAINS
 
+config PM_GENERIC_DOMAINS_OF
+	def_bool y
+	depends on PM_GENERIC_DOMAINS && OF
+
 config CPU_PM
 	bool
 	depends on SUSPEND || CPU_IDLE
+
+config SUSPEND_TIME
+	bool "Log time spent in suspend"
+	---help---
+	  Prints the time spent in suspend in the kernel log, and
+	  keeps statistics on the time spent in suspend in
+	  /sys/kernel/debug/suspend_time
diff --git a/mm/hugetlb.c b/mm/hugetlb.c
index 7de4f67..38f852f 100644
--- a/mm/hugetlb.c
+++ b/mm/hugetlb.c
@@ -435,25 +435,6 @@ static int is_vma_resv_set(struct vm_area_struct *vma, unsigned long flag)
 	return (get_vma_private_data(vma) & flag) != 0;
 }
 
-/* Decrement the reserved pages in the hugepage pool by one */
-static void decrement_hugepage_resv_vma(struct hstate *h,
-			struct vm_area_struct *vma)
-{
-	if (vma->vm_flags & VM_NORESERVE)
-		return;
-
-	if (vma->vm_flags & VM_MAYSHARE) {
-		/* Shared mappings always use reserves */
-		h->resv_huge_pages--;
-	} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
-		/*
-		 * Only the process that called mmap() has reserves for
-		 * private mappings.
-		 */
-		h->resv_huge_pages--;
-	}
-}
-
 /* Reset counters to 0 and clear all HPAGE_RESV_* flags */
 void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 {
@@ -463,12 +444,35 @@ void reset_vma_resv_huge_pages(struct vm_area_struct *vma)
 }
 
 /* Returns true if the VMA has associated reserve pages */
-static int vma_has_reserves(struct vm_area_struct *vma)
+static int vma_has_reserves(struct vm_area_struct *vma, long chg)
 {
+	if (vma->vm_flags & VM_NORESERVE) {
+		/*
+		 * This address is already reserved by other process(chg == 0),
+		 * so, we should decrement reserved count. Without decrementing,
+		 * reserve count remains after releasing inode, because this
+		 * allocated page will go into page cache and is regarded as
+		 * coming from reserved pool in releasing step.  Currently, we
+		 * don't have any other solution to deal with this situation
+		 * properly, so add work-around here.
+		 */
+		if (vma->vm_flags & VM_MAYSHARE && chg == 0)
+			return 1;
+		else
+			return 0;
+	}
+
+	/* Shared mappings always use reserves */
 	if (vma->vm_flags & VM_MAYSHARE)
 		return 1;
+
+	/*
+	 * Only the process that called mmap() has reserves for
+	 * private mappings.
+	 */
 	if (is_vma_resv_set(vma, HPAGE_RESV_OWNER))
 		return 1;
+
 	return 0;
 }
 
@@ -536,7 +540,8 @@ static struct page *dequeue_huge_page_node(struct hstate *h, int nid)
 
 static struct page *dequeue_huge_page_vma(struct hstate *h,
 				struct vm_area_struct *vma,
-				unsigned long address, int avoid_reserve)
+				unsigned long address, int avoid_reserve,
+				long chg)
 {
 	struct page *page = NULL;
 	struct mempolicy *mpol;
@@ -555,7 +560,7 @@ retry_cpuset:
 	 * have no page reserves. This check ensures that reservations are
 	 * not "stolen". The child may still get SIGKILLed
 	 */
-	if (!vma_has_reserves(vma) &&
+	if (!vma_has_reserves(vma, chg) &&
 			h->free_huge_pages - h->resv_huge_pages == 0)
 		goto err;
 
@@ -568,8 +573,13 @@ retry_cpuset:
 		if (cpuset_zone_allowed_softwall(zone, htlb_alloc_mask)) {
 			page = dequeue_huge_page_node(h, zone_to_nid(zone));
 			if (page) {
-				if (!avoid_reserve)
-					decrement_hugepage_resv_vma(h, vma);
+				if (avoid_reserve)
+					break;
+				if (!vma_has_reserves(vma, chg))
+					break;
+
+				SetPagePrivate(page);
+				h->resv_huge_pages--;
 				break;
 			}
 		}
@@ -627,15 +637,20 @@ static void free_huge_page(struct page *page)
 	int nid = page_to_nid(page);
 	struct hugepage_subpool *spool =
 		(struct hugepage_subpool *)page_private(page);
+	bool restore_reserve;
 
 	set_page_private(page, 0);
 	page->mapping = NULL;
 	BUG_ON(page_count(page));
 	BUG_ON(page_mapcount(page));
+	restore_reserve = PagePrivate(page);
 
 	spin_lock(&hugetlb_lock);
 	hugetlb_cgroup_uncharge_page(hstate_index(h),
 				     pages_per_huge_page(h), page);
+	if (restore_reserve)
+		h->resv_huge_pages++;
+
 	if (h->surplus_huge_pages_node[nid] && huge_page_order(h) < MAX_ORDER) {
 		/* remove the page from active list */
 		list_del(&page->lru);
@@ -796,33 +811,6 @@ static int hstate_next_node_to_alloc(struct hstate *h,
 	return nid;
 }
 
-static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)
-{
-	struct page *page;
-	int start_nid;
-	int next_nid;
-	int ret = 0;
-
-	start_nid = hstate_next_node_to_alloc(h, nodes_allowed);
-	next_nid = start_nid;
-
-	do {
-		page = alloc_fresh_huge_page_node(h, next_nid);
-		if (page) {
-			ret = 1;
-			break;
-		}
-		next_nid = hstate_next_node_to_alloc(h, nodes_allowed);
-	} while (next_nid != start_nid);
-
-	if (ret)
-		count_vm_event(HTLB_BUDDY_PGALLOC);
-	else
-		count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);
-
-	return ret;
-}
-
 /*
  * helper for free_pool_huge_page() - return the previously saved
  * node ["this node"] from which to free a huge page.  Advance the
@@ -841,6 +829,40 @@ static int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)
 	return nid;
 }
 
+#define for_each_node_mask_to_alloc(hs, nr_nodes, node, mask)		\
+	for (nr_nodes = nodes_weight(*mask);				\
+		nr_nodes > 0 &&						\
+		((node = hstate_next_node_to_alloc(hs, mask)) || 1);	\
+		nr_nodes--)
+
+#define for_each_node_mask_to_free(hs, nr_nodes, node, mask)		\
+	for (nr_nodes = nodes_weight(*mask);				\
+		nr_nodes > 0 &&						\
+		((node = hstate_next_node_to_free(hs, mask)) || 1);	\
+		nr_nodes--)
+
+static int alloc_fresh_huge_page(struct hstate *h, nodemask_t *nodes_allowed)
+{
+	struct page *page;
+	int nr_nodes, node;
+	int ret = 0;
+
+	for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
+		page = alloc_fresh_huge_page_node(h, node);
+		if (page) {
+			ret = 1;
+			break;
+		}
+	}
+
+	if (ret)
+		count_vm_event(HTLB_BUDDY_PGALLOC);
+	else
+		count_vm_event(HTLB_BUDDY_PGALLOC_FAIL);
+
+	return ret;
+}
+
 /*
  * Free huge page from pool from next node to free.
  * Attempt to keep persistent huge pages more or less
@@ -850,36 +872,31 @@ static int hstate_next_node_to_free(struct hstate *h, nodemask_t *nodes_allowed)
 static int free_pool_huge_page(struct hstate *h, nodemask_t *nodes_allowed,
 							 bool acct_surplus)
 {
-	int start_nid;
-	int next_nid;
+	int nr_nodes, node;
 	int ret = 0;
 
-	start_nid = hstate_next_node_to_free(h, nodes_allowed);
-	next_nid = start_nid;
-
-	do {
+	for_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {
 		/*
 		 * If we're returning unused surplus pages, only examine
 		 * nodes with surplus pages.
 		 */
-		if ((!acct_surplus || h->surplus_huge_pages_node[next_nid]) &&
-		    !list_empty(&h->hugepage_freelists[next_nid])) {
+		if ((!acct_surplus || h->surplus_huge_pages_node[node]) &&
+		    !list_empty(&h->hugepage_freelists[node])) {
 			struct page *page =
-				list_entry(h->hugepage_freelists[next_nid].next,
+				list_entry(h->hugepage_freelists[node].next,
 					  struct page, lru);
 			list_del(&page->lru);
 			h->free_huge_pages--;
-			h->free_huge_pages_node[next_nid]--;
+			h->free_huge_pages_node[node]--;
 			if (acct_surplus) {
 				h->surplus_huge_pages--;
-				h->surplus_huge_pages_node[next_nid]--;
+				h->surplus_huge_pages_node[node]--;
 			}
 			update_and_free_page(h, page);
 			ret = 1;
 			break;
 		}
-		next_nid = hstate_next_node_to_free(h, nodes_allowed);
-	} while (next_nid != start_nid);
+	}
 
 	return ret;
 }
@@ -968,9 +985,10 @@ static struct page *alloc_buddy_huge_page(struct hstate *h, int nid)
  */
 struct page *alloc_huge_page_node(struct hstate *h, int nid)
 {
-	struct page *page;
+	struct page *page = NULL;
 
 	spin_lock(&hugetlb_lock);
+	if (h->free_huge_pages - h->resv_huge_pages > 0)
 	page = dequeue_huge_page_node(h, nid);
 	spin_unlock(&hugetlb_lock);
 
@@ -1059,11 +1077,8 @@ free:
 	spin_unlock(&hugetlb_lock);
 
 	/* Free unnecessary surplus pages to the buddy allocator */
-	if (!list_empty(&surplus_list)) {
-		list_for_each_entry_safe(page, tmp, &surplus_list, lru) {
+	list_for_each_entry_safe(page, tmp, &surplus_list, lru)
 			put_page(page);
-		}
-	}
 	spin_lock(&hugetlb_lock);
 
 	return ret;
@@ -1131,9 +1146,9 @@ static long vma_needs_reservation(struct hstate *h,
 	} else  {
 		long err;
 		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
-		struct resv_map *reservations = vma_resv_map(vma);
+		struct resv_map *resv = vma_resv_map(vma);
 
-		err = region_chg(&reservations->regions, idx, idx + 1);
+		err = region_chg(&resv->regions, idx, idx + 1);
 		if (err < 0)
 			return err;
 		return 0;
@@ -1151,10 +1166,10 @@ static void vma_commit_reservation(struct hstate *h,
 
 	} else if (is_vma_resv_set(vma, HPAGE_RESV_OWNER)) {
 		pgoff_t idx = vma_hugecache_offset(h, vma, addr);
-		struct resv_map *reservations = vma_resv_map(vma);
+		struct resv_map *resv = vma_resv_map(vma);
 
 		/* Mark this page used in the map. */
-		region_add(&reservations->regions, idx, idx + 1);
+		region_add(&resv->regions, idx, idx + 1);
 	}
 }
 
@@ -1180,38 +1195,35 @@ static struct page *alloc_huge_page(struct vm_area_struct *vma,
 	chg = vma_needs_reservation(h, vma, addr);
 	if (chg < 0)
 		return ERR_PTR(-ENOMEM);
-	if (chg)
-		if (hugepage_subpool_get_pages(spool, chg))
+	if (chg || avoid_reserve)
+		if (hugepage_subpool_get_pages(spool, 1))
 			return ERR_PTR(-ENOSPC);
 
 	ret = hugetlb_cgroup_charge_cgroup(idx, pages_per_huge_page(h), &h_cg);
 	if (ret) {
-		hugepage_subpool_put_pages(spool, chg);
+		if (chg || avoid_reserve)
+			hugepage_subpool_put_pages(spool, 1);
 		return ERR_PTR(-ENOSPC);
 	}
 	spin_lock(&hugetlb_lock);
-	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve);
-	if (page) {
-		/* update page cgroup details */
-		hugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h),
-					     h_cg, page);
-		spin_unlock(&hugetlb_lock);
-	} else {
+	page = dequeue_huge_page_vma(h, vma, addr, avoid_reserve, chg);
+	if (!page) {
 		spin_unlock(&hugetlb_lock);
 		page = alloc_buddy_huge_page(h, NUMA_NO_NODE);
 		if (!page) {
 			hugetlb_cgroup_uncharge_cgroup(idx,
 						       pages_per_huge_page(h),
 						       h_cg);
-			hugepage_subpool_put_pages(spool, chg);
+			if (chg || avoid_reserve)
+				hugepage_subpool_put_pages(spool, 1);
 			return ERR_PTR(-ENOSPC);
 		}
 		spin_lock(&hugetlb_lock);
-		hugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h),
-					     h_cg, page);
 		list_move(&page->lru, &h->hugepage_activelist);
-		spin_unlock(&hugetlb_lock);
+		/* Fall through */
 	}
+	hugetlb_cgroup_commit_charge(idx, pages_per_huge_page(h), h_cg, page);
+	spin_unlock(&hugetlb_lock);
 
 	set_page_private(page, (unsigned long)spool);
 
@@ -1222,14 +1234,12 @@ static struct page *alloc_huge_page(struct vm_area_struct *vma,
 int __weak alloc_bootmem_huge_page(struct hstate *h)
 {
 	struct huge_bootmem_page *m;
-	int nr_nodes = nodes_weight(node_states[N_MEMORY]);
+	int nr_nodes, node;
 
-	while (nr_nodes) {
+	for_each_node_mask_to_alloc(h, nr_nodes, node, &node_states[N_MEMORY]) {
 		void *addr;
 
-		addr = __alloc_bootmem_node_nopanic(
-				NODE_DATA(hstate_next_node_to_alloc(h,
-						&node_states[N_MEMORY])),
+		addr = __alloc_bootmem_node_nopanic(NODE_DATA(node),
 				huge_page_size(h), huge_page_size(h), 0);
 
 		if (addr) {
@@ -1241,7 +1251,6 @@ int __weak alloc_bootmem_huge_page(struct hstate *h)
 			m = addr;
 			goto found;
 		}
-		nr_nodes--;
 	}
 	return 0;
 
@@ -1380,48 +1389,28 @@ static inline void try_to_free_low(struct hstate *h, unsigned long count,
 static int adjust_pool_surplus(struct hstate *h, nodemask_t *nodes_allowed,
 				int delta)
 {
-	int start_nid, next_nid;
-	int ret = 0;
+	int nr_nodes, node;
 
 	VM_BUG_ON(delta != -1 && delta != 1);
 
-	if (delta < 0)
-		start_nid = hstate_next_node_to_alloc(h, nodes_allowed);
-	else
-		start_nid = hstate_next_node_to_free(h, nodes_allowed);
-	next_nid = start_nid;
-
-	do {
-		int nid = next_nid;
 		if (delta < 0)  {
-			/*
-			 * To shrink on this node, there must be a surplus page
-			 */
-			if (!h->surplus_huge_pages_node[nid]) {
-				next_nid = hstate_next_node_to_alloc(h,
-								nodes_allowed);
-				continue;
-			}
+		for_each_node_mask_to_alloc(h, nr_nodes, node, nodes_allowed) {
+			if (h->surplus_huge_pages_node[node])
+				goto found;
 		}
-		if (delta > 0) {
-			/*
-			 * Surplus cannot exceed the total number of pages
-			 */
-			if (h->surplus_huge_pages_node[nid] >=
-						h->nr_huge_pages_node[nid]) {
-				next_nid = hstate_next_node_to_free(h,
-								nodes_allowed);
-				continue;
+	} else {
+		for_each_node_mask_to_free(h, nr_nodes, node, nodes_allowed) {
+			if (h->surplus_huge_pages_node[node] <
+					h->nr_huge_pages_node[node])
+				goto found;
 			}
 		}
+	return 0;
 
+found:
 		h->surplus_huge_pages += delta;
-		h->surplus_huge_pages_node[nid] += delta;
-		ret = 1;
-		break;
-	} while (next_nid != start_nid);
-
-	return ret;
+	h->surplus_huge_pages_node[node] += delta;
+	return 1;
 }
 
 #define persistent_huge_pages(h) (h->nr_huge_pages - h->surplus_huge_pages)
@@ -2233,7 +2222,7 @@ out:
 
 static void hugetlb_vm_op_open(struct vm_area_struct *vma)
 {
-	struct resv_map *reservations = vma_resv_map(vma);
+	struct resv_map *resv = vma_resv_map(vma);
 
 	/*
 	 * This new VMA should share its siblings reservation map if present.
@@ -2243,34 +2232,34 @@ static void hugetlb_vm_op_open(struct vm_area_struct *vma)
 	 * after this open call completes.  It is therefore safe to take a
 	 * new reference here without additional locking.
 	 */
-	if (reservations)
-		kref_get(&reservations->refs);
+	if (resv)
+		kref_get(&resv->refs);
 }
 
 static void resv_map_put(struct vm_area_struct *vma)
 {
-	struct resv_map *reservations = vma_resv_map(vma);
+	struct resv_map *resv = vma_resv_map(vma);
 
-	if (!reservations)
+	if (!resv)
 		return;
-	kref_put(&reservations->refs, resv_map_release);
+	kref_put(&resv->refs, resv_map_release);
 }
 
 static void hugetlb_vm_op_close(struct vm_area_struct *vma)
 {
 	struct hstate *h = hstate_vma(vma);
-	struct resv_map *reservations = vma_resv_map(vma);
+	struct resv_map *resv = vma_resv_map(vma);
 	struct hugepage_subpool *spool = subpool_vma(vma);
 	unsigned long reserve;
 	unsigned long start;
 	unsigned long end;
 
-	if (reservations) {
+	if (resv) {
 		start = vma_hugecache_offset(h, vma, vma->vm_start);
 		end = vma_hugecache_offset(h, vma, vma->vm_end);
 
 		reserve = (end - start) -
-			region_count(&reservations->regions, start, end);
+			region_count(&resv->regions, start, end);
 
 		resv_map_put(vma);
 
@@ -2363,16 +2352,26 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 	int cow;
 	struct hstate *h = hstate_vma(vma);
 	unsigned long sz = huge_page_size(h);
+	unsigned long mmun_start;	/* For mmu_notifiers */
+	unsigned long mmun_end;		/* For mmu_notifiers */
+	int ret = 0;
 
 	cow = (vma->vm_flags & (VM_SHARED | VM_MAYWRITE)) == VM_MAYWRITE;
 
+	mmun_start = vma->vm_start;
+	mmun_end = vma->vm_end;
+	if (cow)
+		mmu_notifier_invalidate_range_start(src, mmun_start, mmun_end);
+
 	for (addr = vma->vm_start; addr < vma->vm_end; addr += sz) {
 		src_pte = huge_pte_offset(src, addr);
 		if (!src_pte)
 			continue;
 		dst_pte = huge_pte_alloc(dst, addr, sz);
-		if (!dst_pte)
-			goto nomem;
+		if (!dst_pte) {
+			ret = -ENOMEM;
+			break;
+		}
 
 		/* If the pagetables are shared don't copy or take references */
 		if (dst_pte == src_pte)
@@ -2409,10 +2408,11 @@ int copy_hugetlb_page_range(struct mm_struct *dst, struct mm_struct *src,
 		spin_unlock(&src->page_table_lock);
 		spin_unlock(&dst->page_table_lock);
 	}
-	return 0;
 
-nomem:
-	return -ENOMEM;
+	if (cow)
+		mmu_notifier_invalidate_range_end(src, mmun_start, mmun_end);
+
+	return ret;
 }
 
 void __unmap_hugepage_range(struct mmu_gather *tlb, struct vm_area_struct *vma,
@@ -2599,7 +2599,6 @@ static int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 {
 	struct hstate *h = hstate_vma(vma);
 	struct page *old_page, *new_page;
-	int avoidcopy;
 	int outside_reserve = 0;
 	unsigned long mmun_start;	/* For mmu_notifiers */
 	unsigned long mmun_end;		/* For mmu_notifiers */
@@ -2609,9 +2608,7 @@ static int hugetlb_cow(struct mm_struct *mm, struct vm_area_struct *vma,
 retry_avoidcopy:
 	/* If no-one else is actually using this page, avoid the copy
 	 * and just make the page writable */
-	avoidcopy = (page_mapcount(old_page) == 1);
-	if (avoidcopy) {
-		if (PageAnon(old_page))
+	if (page_mapcount(old_page) == 1 && PageAnon(old_page)) {
 			page_move_anon_rmap(old_page, vma, address);
 		set_huge_ptep_writable(vma, address, ptep);
 		return 0;
@@ -2626,8 +2623,7 @@ retry_avoidcopy:
 	 * at the time of fork() could consume its reserves on COW instead
 	 * of the full address range.
 	 */
-	if (!(vma->vm_flags & VM_MAYSHARE) &&
-			is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&
+	if (is_vma_resv_set(vma, HPAGE_RESV_OWNER) &&
 			old_page != pagecache_page)
 		outside_reserve = 1;
 
@@ -2699,6 +2695,8 @@ retry_avoidcopy:
 	spin_lock(&mm->page_table_lock);
 	ptep = huge_pte_offset(mm, address & huge_page_mask(h));
 	if (likely(pte_same(huge_ptep_get(ptep), pte))) {
+		ClearPagePrivate(new_page);
+
 		/* Break COW */
 		huge_ptep_clear_flush(vma, address, ptep);
 		set_huge_pte_at(mm, address, ptep,
@@ -2710,10 +2708,11 @@ retry_avoidcopy:
 	}
 	spin_unlock(&mm->page_table_lock);
 	mmu_notifier_invalidate_range_end(mm, mmun_start, mmun_end);
-	/* Caller expects lock to be held */
-	spin_lock(&mm->page_table_lock);
 	page_cache_release(new_page);
 	page_cache_release(old_page);
+
+	/* Caller expects lock to be held */
+	spin_lock(&mm->page_table_lock);
 	return 0;
 }
 
@@ -2809,6 +2808,7 @@ retry:
 					goto retry;
 				goto out;
 			}
+			ClearPagePrivate(page);
 
 			spin_lock(&inode->i_lock);
 			inode->i_blocks += blocks_per_huge_page(h);
@@ -2855,8 +2855,10 @@ retry:
 	if (!huge_pte_none(huge_ptep_get(ptep)))
 		goto backout;
 
-	if (anon_rmap)
+	if (anon_rmap) {
+		ClearPagePrivate(page);
 		hugepage_add_new_anon_rmap(page, vma, address);
+	}
 	else
 		page_dup_rmap(page);
 	new_pte = make_huge_pte(vma, page, ((vma->vm_flags & VM_WRITE)
@@ -2990,15 +2992,6 @@ out_mutex:
 	return ret;
 }
 
-/* Can be overriden by architectures */
-__attribute__((weak)) struct page *
-follow_huge_pud(struct mm_struct *mm, unsigned long address,
-	       pud_t *pud, int write)
-{
-	BUG();
-	return NULL;
-}
-
 long follow_hugetlb_page(struct mm_struct *mm, struct vm_area_struct *vma,
 			 struct page **pages, struct vm_area_struct **vmas,
 			 unsigned long *position, unsigned long *nr_pages,
diff --git a/mm/page_io.c b/mm/page_io.c
index 7617859..63f1966 100644
--- a/mm/page_io.c
+++ b/mm/page_io.c
@@ -21,6 +21,7 @@
 #include <linux/writeback.h>
 #include <linux/frontswap.h>
 #include <linux/aio.h>
+#include <linux/blkdev.h>
 #include <asm/pgtable.h>
 
 static struct bio *get_swap_bio(gfp_t gfp_flags,
@@ -80,9 +81,54 @@ void end_swap_bio_read(struct bio *bio, int err)
 				imajor(bio->bi_bdev->bd_inode),
 				iminor(bio->bi_bdev->bd_inode),
 				(unsigned long long)bio->bi_iter.bi_sector);
-	} else {
+		goto out;
+	}
+
 		SetPageUptodate(page);
+
+	/*
+	 * There is no guarantee that the page is in swap cache - the software
+	 * suspend code (at least) uses end_swap_bio_read() against a non-
+	 * swapcache page.  So we must check PG_swapcache before proceeding with
+	 * this optimization.
+	 */
+	if (likely(PageSwapCache(page))) {
+		struct swap_info_struct *sis;
+
+		sis = page_swap_info(page);
+		if (sis->flags & SWP_BLKDEV) {
+			/*
+			 * The swap subsystem performs lazy swap slot freeing,
+			 * expecting that the page will be swapped out again.
+			 * So we can avoid an unnecessary write if the page
+			 * isn't redirtied.
+			 * This is good for real swap storage because we can
+			 * reduce unnecessary I/O and enhance wear-leveling
+			 * if an SSD is used as the as swap device.
+			 * But if in-memory swap device (eg zram) is used,
+			 * this causes a duplicated copy between uncompressed
+			 * data in VM-owned memory and compressed data in
+			 * zram-owned memory.  So let's free zram-owned memory
+			 * and make the VM-owned decompressed page *dirty*,
+			 * so the page should be swapped out somewhere again if
+			 * we again wish to reclaim it.
+			 */
+			struct gendisk *disk = sis->bdev->bd_disk;
+			if (disk->fops->swap_slot_free_notify) {
+				swp_entry_t entry;
+				unsigned long offset;
+
+				entry.val = page_private(page);
+				offset = swp_offset(entry);
+
+				SetPageDirty(page);
+				disk->fops->swap_slot_free_notify(sis->bdev,
+						offset);
+			}
+	}
 	}
+
+out:
 	unlock_page(page);
 	bio_put(bio);
 }
diff --git a/mm/shmem.c b/mm/shmem.c
index a35e5a4..3107f89 100644
--- a/mm/shmem.c
+++ b/mm/shmem.c
@@ -1312,6 +1312,42 @@ static int shmem_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	 * Trinity finds that probing a hole which tmpfs is punching can
 	 * prevent the hole-punch from ever completing: which in turn
 	 * locks writers out with its hold on i_mutex.  So refrain from
+	 * faulting pages into the hole while it's being punched, and
+	 * wait on i_mutex to be released if vmf->flags permits, 
+	 */
+	if (unlikely(inode->i_private)) {
+		struct shmem_falloc *shmem_falloc;
+		spin_lock(&inode->i_lock);
+		shmem_falloc = inode->i_private;
+		if (!shmem_falloc ||
+		    shmem_falloc->mode != FALLOC_FL_PUNCH_HOLE ||
+		    vmf->pgoff < shmem_falloc->start ||
+		    vmf->pgoff >= shmem_falloc->next)
+			shmem_falloc = NULL;
+		spin_unlock(&inode->i_lock);
+		/*
+		 * i_lock has protected us from taking shmem_falloc seriously
+		 * once return from shmem_fallocate() went back up that stack.
+		 * i_lock does not serialize with i_mutex at all, but it does
+		 * not matter if sometimes we wait unnecessarily, or sometimes
+		 * miss out on waiting: we just need to make those cases rare.
+		 */
+		if (shmem_falloc) {
+			if ((vmf->flags & FAULT_FLAG_ALLOW_RETRY) &&
+			   !(vmf->flags & FAULT_FLAG_RETRY_NOWAIT)) {
+				up_read(&vma->vm_mm->mmap_sem);
+				mutex_lock(&inode->i_mutex);
+				mutex_unlock(&inode->i_mutex);
+				return VM_FAULT_RETRY;
+			}
+			/* cond_resched? Leave that to GUP or return to user */
+			return VM_FAULT_NOPAGE;
+		}
+	}
+	/*
+	 * Trinity finds that probing a hole which tmpfs is punching can
+	 * prevent the hole-punch from ever completing: which in turn
+	 * locks writers out with its hold on i_mutex.  So refrain from
 	 * faulting pages into the hole while it's being punched.  Although
 	 * shmem_undo_range() does remove the additions, it may be unable to
 	 * keep up, as each new page needs its own unmap_mapping_range() call,
@@ -1883,6 +1919,8 @@ static long shmem_fallocate(struct file *file, int mode, loff_t offset,
 
 	mutex_lock(&inode->i_mutex);
 
+	shmem_falloc.mode = mode & ~FALLOC_FL_KEEP_SIZE;
+
 	if (mode & FALLOC_FL_PUNCH_HOLE) {
 		struct address_space *mapping = file->f_mapping;
 		loff_t unmap_start = round_up(offset, PAGE_SIZE);
diff --git a/scripts/Makefile.lib b/scripts/Makefile.lib
index f97869f..c4b37f6 100644
--- a/scripts/Makefile.lib
+++ b/scripts/Makefile.lib
@@ -152,6 +152,7 @@ ld_flags       = $(LDFLAGS) $(ldflags-y)
 dtc_cpp_flags  = -Wp,-MD,$(depfile).pre.tmp -nostdinc                    \
 		 -I$(srctree)/arch/$(SRCARCH)/boot/dts                   \
 		 -I$(srctree)/arch/$(SRCARCH)/boot/dts/include           \
+		 -I$(srctree)/include                                    \
 		 -undef -D__DTS__
 
 # Finds the multi-part object the current object will be linked into
-- 
1.7.9.5

